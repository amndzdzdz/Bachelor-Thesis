

--- Page 1 ---

BOA Z BA R A K
INT RODUCT ION TO
THE ORE T ICA L
COMP UT E R S CIE NCE
TE X TBOOK IN P R E PA R ATION.
AVA IL A B L E ON HTTPS://INTROTCS.ORG


--- Page 2 ---

Text available on ÔÇõhttps://github.com/boazbk/tcs - please post any issues there - thank you!
This version was compiled on Wednesday 26th August, 2020 18:10
Copyright ¬© 2020 Boaz Barak
This work is licensed under a Creative
Commons ‚ÄúAttribution-NonCommercial-
NoDerivatives 4.0 International‚Äù license.


--- Page 3 ---

To Ravit, Alma and Goren.


--- Page 4 ---



--- Page 5 ---

Contents
Preface
9
Preliminaries
17
0
Introduction
19
1
Mathematical Background
37
2
Computation and Representation
73
I
Finite computation
111
3
Defining computation
113
4
Syntactic sugar, and computing every function
149
5
Code as data, data as code
175
II
Uniform computation
205
6
Functions with Infinite domains, Automata, and Regular
expressions
207
7
Loops and infinity
241
8
Equivalent models of computation
271
9
Universality and uncomputability
315
10 Restricted computational models
347
11 Is every theorem provable?
365
Compiled on 8.26.2020 18:10


--- Page 6 ---

6
III
Efficient algorithms
385
12 Efficient computation: An informal introduction
387
13 Modeling running time
407
14 Polynomial-time reductions
439
15 NP, NP completeness, and the Cook-Levin Theorem
465
16 What if P equals NP?
483
17 Space bounded computation
503
IV
Randomized computation
505
18 Probability Theory 101
507
19 Probabilistic computation
527
20 Modeling randomized computation
539
V
Advanced topics
561
21 Cryptography
563
22 Proofs and algorithms
591
23 Quantum computing
593
VI
Appendices
625


--- Page 7 ---

Contents (detailed)
Preface
9
0.1
To the student . . . . . . . . . . . . . . . . . . . . . . . .
10
0.1.1
Is the effort worth it?
. . . . . . . . . . . . . . . .
11
0.2
To potential instructors . . . . . . . . . . . . . . . . . . .
12
0.3
Acknowledgements . . . . . . . . . . . . . . . . . . . . .
14
Preliminaries
17
0
Introduction
19
0.1
Integer multiplication: an example of an algorithm . . .
20
0.2
Extended Example: A faster way to multiply (optional)
22
0.3
Algorithms beyond arithmetic . . . . . . . . . . . . . . .
27
0.4
On the importance of negative results
. . . . . . . . . .
28
0.5
Roadmap to the rest of this book
. . . . . . . . . . . . .
29
0.5.1
Dependencies between chapters . . . . . . . . . .
30
0.6
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . .
32
0.7
Bibliographical notes . . . . . . . . . . . . . . . . . . . .
33
1
Mathematical Background
37
1.1
This chapter: a reader‚Äôs manual . . . . . . . . . . . . . .
37
1.2
A quick overview of mathematical prerequisites . . . .
38
1.3
Reading mathematical texts . . . . . . . . . . . . . . . .
39
1.3.1
Definitions
. . . . . . . . . . . . . . . . . . . . . .
40
1.3.2
Assertions: Theorems, lemmas, claims . . . . . .
40
1.3.3
Proofs . . . . . . . . . . . . . . . . . . . . . . . . .
40
1.4
Basic discrete math objects . . . . . . . . . . . . . . . . .
41
1.4.1
Sets
. . . . . . . . . . . . . . . . . . . . . . . . . .
41
1.4.2
Special sets . . . . . . . . . . . . . . . . . . . . . .
42
1.4.3
Functions . . . . . . . . . . . . . . . . . . . . . . .
44
1.4.4
Graphs
. . . . . . . . . . . . . . . . . . . . . . . .
46
1.4.5
Logic operators and quantifiers
. . . . . . . . . .
49
1.4.6
Quantifiers for summations and products
. . . .
50
1.4.7
Parsing formulas: bound and free variables
. . .
50
1.4.8
Asymptotics and Big-ùëÇnotation . . . . . . . . . .
52


--- Page 8 ---

8
1.4.9
Some ‚Äúrules of thumb‚Äù for Big-ùëÇnotation . . . .
53
1.5
Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
1.5.1
Proofs and programs
. . . . . . . . . . . . . . . .
55
1.5.2
Proof writing style . . . . . . . . . . . . . . . . . .
55
1.5.3
Patterns in proofs
. . . . . . . . . . . . . . . . . .
56
1.6
Extended example: Topological Sorting
. . . . . . . . .
58
1.6.1
Mathematical induction . . . . . . . . . . . . . . .
60
1.6.2
Proving the result by induction
. . . . . . . . . .
61
1.6.3
Minimality and uniqueness . . . . . . . . . . . . .
63
1.7
This book: notation and conventions . . . . . . . . . . .
65
1.7.1
Variable name conventions . . . . . . . . . . . . .
66
1.7.2
Some idioms . . . . . . . . . . . . . . . . . . . . .
67
1.8
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
1.9
Bibliographical notes . . . . . . . . . . . . . . . . . . . .
71
2
Computation and Representation
73
2.1
Defining representations . . . . . . . . . . . . . . . . . .
75
2.1.1
Representing natural numbers . . . . . . . . . . .
76
2.1.2
Meaning of representations (discussion) . . . . .
78
2.2
Representations beyond natural numbers . . . . . . . .
78
2.2.1
Representing (potentially negative) integers . . .
79
2.2.2
Two‚Äôs complement representation (optional)
. .
79
2.2.3
Rational numbers, and representing pairs of
strings . . . . . . . . . . . . . . . . . . . . . . . . .
80
2.3
Representing real numbers . . . . . . . . . . . . . . . . .
82
2.4
Cantor‚Äôs Theorem, countable sets, and string represen-
tations of the real numbers . . . . . . . . . . . . . . . . .
83
2.4.1
Corollary: Boolean functions are uncountable . .
89
2.4.2
Equivalent conditions for countability
. . . . . .
89
2.5
Representing objects beyond numbers . . . . . . . . . .
90
2.5.1
Finite representations . . . . . . . . . . . . . . . .
91
2.5.2
Prefix-free encoding . . . . . . . . . . . . . . . . .
91
2.5.3
Making representations prefix-free
. . . . . . . .
94
2.5.4
‚ÄúProof by Python‚Äù (optional)
. . . . . . . . . . .
95
2.5.5
Representing letters and text . . . . . . . . . . . .
97
2.5.6
Representing vectors, matrices, images . . . . . .
99
2.5.7
Representing graphs . . . . . . . . . . . . . . . . .
99
2.5.8
Representing lists and nested lists . . . . . . . . .
99
2.5.9
Notation . . . . . . . . . . . . . . . . . . . . . . . . 100
2.6
Defining computational tasks as mathematical functions 100
2.6.1
Distinguish functions from programs!
. . . . . . 102
2.7
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . 104
2.8
Bibliographical notes . . . . . . . . . . . . . . . . . . . . 108


--- Page 9 ---

9
I
Finite computation
111
3
Defining computation
113
3.1
Defining computation . . . . . . . . . . . . . . . . . . . . 115
3.2
Computing using AND, OR, and NOT. . . . . . . . . . . 116
3.2.1
Some properties of AND and OR
. . . . . . . . . 118
3.2.2
Extended example: Computing XOR from
AND, OR, and NOT . . . . . . . . . . . . . . . . . 119
3.2.3
Informally defining ‚Äúbasic operations‚Äù and
‚Äúalgorithms‚Äù . . . . . . . . . . . . . . . . . . . . . 121
3.3
Boolean Circuits . . . . . . . . . . . . . . . . . . . . . . . 123
3.3.1
Boolean circuits: a formal definition . . . . . . . . 124
3.3.2
Equivalence of circuits and straight-line programs 127
3.4
Physical implementations of computing devices (di-
gression) . . . . . . . . . . . . . . . . . . . . . . . . . . . 130
3.4.1
Transistors
. . . . . . . . . . . . . . . . . . . . . . 131
3.4.2
Logical gates from transistors
. . . . . . . . . . . 132
3.4.3
Biological computing . . . . . . . . . . . . . . . . 132
3.4.4
Cellular automata and the game of life . . . . . . 132
3.4.5
Neural networks . . . . . . . . . . . . . . . . . . . 132
3.4.6
A computer made from marbles and pipes . . . . 133
3.5
The NAND function
. . . . . . . . . . . . . . . . . . . . 134
3.5.1
NAND Circuits . . . . . . . . . . . . . . . . . . . . 135
3.5.2
More examples of NAND circuits (optional) . . . 136
3.5.3
The NAND-CIRC Programming language . . . . 138
3.6
Equivalence of all these models . . . . . . . . . . . . . . 140
3.6.1
Circuits with other gate sets
. . . . . . . . . . . . 141
3.6.2
Specification vs. implementation (again) . . . . . 142
3.7
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
3.8
Biographical notes . . . . . . . . . . . . . . . . . . . . . . 146
4
Syntactic sugar, and computing every function
149
4.1
Some examples of syntactic sugar . . . . . . . . . . . . . 151
4.1.1
User-defined procedures . . . . . . . . . . . . . . 151
4.1.2
Proof by Python (optional) . . . . . . . . . . . . . 153
4.1.3
Conditional statements . . . . . . . . . . . . . . . 154
4.2
Extended example: Addition and Multiplication (op-
tional) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
4.3
The LOOKUP function . . . . . . . . . . . . . . . . . . . 158
4.3.1
Constructing a NAND-CIRC program for
LOOKUP . . . . . . . . . . . . . . . . . . . . . . . 159
4.4
Computing every function
. . . . . . . . . . . . . . . . . 161
4.4.1
Proof of NAND‚Äôs Universality . . . . . . . . . . . 162
4.4.2
Improving by a factor of ùëõ(optional) . . . . . . . 163


--- Page 10 ---

10
4.5
Computing every function: An alternative proof . . . . 165
4.6
The class SIZE(ùëá) . . . . . . . . . . . . . . . . . . . . . . 167
4.7
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . 170
4.8
Bibliographical notes . . . . . . . . . . . . . . . . . . . . 174
5
Code as data, data as code
175
5.1
Representing programs as strings . . . . . . . . . . . . . 177
5.2
Counting programs, and lower bounds on the size of
NAND-CIRC programs . . . . . . . . . . . . . . . . . . . 178
5.2.1
Size hierarchy theorem (optional) . . . . . . . . . 180
5.3
The tuples representation
. . . . . . . . . . . . . . . . . 182
5.3.1
From tuples to strings . . . . . . . . . . . . . . . . 183
5.4
A NAND-CIRC interpreter in NAND-CIRC . . . . . . . 184
5.4.1
Efficient universal programs . . . . . . . . . . . . 185
5.4.2
A NAND-CIRC interpeter in ‚Äúpseudocode‚Äù . . . 186
5.4.3
A NAND interpreter in Python
. . . . . . . . . . 187
5.4.4
Constructing the NAND-CIRC interpreter in
NAND-CIRC . . . . . . . . . . . . . . . . . . . . . 188
5.5
A Python interpreter in NAND-CIRC (discussion) . . . 191
5.6
The physical extended Church-Turing thesis (discus-
sion)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193
5.6.1
Attempts at refuting the PECTT . . . . . . . . . . 195
5.7
Recap of Part I: Finite Computation . . . . . . . . . . . . 199
5.8
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
5.9
Bibliographical notes . . . . . . . . . . . . . . . . . . . . 203
II
Uniform computation
205
6
Functions with Infinite domains, Automata, and Regular
expressions
207
6.1
Functions with inputs of unbounded length . . . . . . . 208
6.1.1
Varying inputs and outputs . . . . . . . . . . . . . 209
6.1.2
Formal Languages . . . . . . . . . . . . . . . . . . 211
6.1.3
Restrictions of functions . . . . . . . . . . . . . . . 211
6.2
Deterministic finite automata (optional) . . . . . . . . . 212
6.2.1
Anatomy of an automaton (finite vs. unbounded) 215
6.2.2
DFA-computable functions . . . . . . . . . . . . . 216
6.3
Regular expressions . . . . . . . . . . . . . . . . . . . . . 217
6.3.1
Algorithms for matching regular expressions
. . 221
6.4
Efficient matching of regular expressions (optional) . . 223
6.4.1
Matching regular expressions using DFAs . . . . 227
6.4.2
Equivalence of regular expressions and automata 228
6.4.3
Closure properties of regular expressions
. . . . 230


--- Page 11 ---

11
6.5
Limitations of regular expressions and the pumping
lemma
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 231
6.6
Answering semantic questions about regular expres-
sions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236
6.7
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . 239
6.8
Bibliographical notes . . . . . . . . . . . . . . . . . . . . 240
7
Loops and infinity
241
7.1
Turing Machines . . . . . . . . . . . . . . . . . . . . . . . 242
7.1.1
Extended example: A Turing machine for palin-
dromes
. . . . . . . . . . . . . . . . . . . . . . . . 244
7.1.2
Turing machines: a formal definition . . . . . . . 245
7.1.3
Computable functions . . . . . . . . . . . . . . . . 247
7.1.4
Infinite loops and partial functions
. . . . . . . . 248
7.2
Turing machines as programming languages
. . . . . . 249
7.2.1
The NAND-TM Programming language
. . . . . 251
7.2.2
Sneak peak: NAND-TM vs Turing machines . . . 254
7.2.3
Examples . . . . . . . . . . . . . . . . . . . . . . . 254
7.3
Equivalence of Turing machines and NAND-TM pro-
grams . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
7.3.1
Specification vs implementation (again) . . . . . 260
7.4
NAND-TM syntactic sugar . . . . . . . . . . . . . . . . . 261
7.4.1
‚ÄúGOTO‚Äù and inner loops . . . . . . . . . . . . . . 261
7.5
Uniformity, and NAND vs NAND-TM (discussion)
. . 264
7.6
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
7.7
Bibliographical notes . . . . . . . . . . . . . . . . . . . . 268
8
Equivalent models of computation
271
8.1
RAM machines and NAND-RAM . . . . . . . . . . . . . 273
8.2
The gory details (optional) . . . . . . . . . . . . . . . . . 277
8.2.1
Indexed access in NAND-TM . . . . . . . . . . . . 277
8.2.2
Two dimensional arrays in NAND-TM
. . . . . . 279
8.2.3
All the rest
. . . . . . . . . . . . . . . . . . . . . . 279
8.3
Turing equivalence (discussion) . . . . . . . . . . . . . . 280
8.3.1
The ‚ÄúBest of both worlds‚Äù paradigm
. . . . . . . 281
8.3.2
Let‚Äôs talk about abstractions
. . . . . . . . . . . . 281
8.3.3
Turing completeness and equivalence, a formal
definition (optional) . . . . . . . . . . . . . . . . . 283
8.4
Cellular automata . . . . . . . . . . . . . . . . . . . . . . 284
8.4.1
One dimensional cellular automata are Turing
complete
. . . . . . . . . . . . . . . . . . . . . . . 286
8.4.2
Configurations of Turing machines and the
next-step function . . . . . . . . . . . . . . . . . . 287


--- Page 12 ---

12
8.5
Lambda calculus and functional programming lan-
guages
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 290
8.5.1
Applying functions to functions . . . . . . . . . . 290
8.5.2
Obtaining multi-argument functions via Currying 291
8.5.3
Formal description of the Œª calculus . . . . . . . . 292
8.5.4
Infinite loops in the Œª calculus . . . . . . . . . . . 295
8.6
The ‚ÄúEnhanced‚Äù Œª calculus
. . . . . . . . . . . . . . . . 295
8.6.1
Computing a function in the enhanced Œª calculus 297
8.6.2
Enhanced Œª calculus is Turing-complete
. . . . . 298
8.7
From enhanced to pure Œª calculus
. . . . . . . . . . . . 301
8.7.1
List processing . . . . . . . . . . . . . . . . . . . . 302
8.7.2
The Y combinator, or recursion without recursion 303
8.8
The Church-Turing Thesis (discussion)
. . . . . . . . . 306
8.8.1
Different models of computation . . . . . . . . . . 307
8.9
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . 308
8.10
Bibliographical notes . . . . . . . . . . . . . . . . . . . . 312
9
Universality and uncomputability
315
9.1
Universality or a meta-circular evaluator . . . . . . . . . 316
9.1.1
Proving the existence of a universal Turing
Machine . . . . . . . . . . . . . . . . . . . . . . . . 318
9.1.2
Implications of universality (discussion) . . . . . 320
9.2
Is every function computable? . . . . . . . . . . . . . . . 321
9.3
The Halting problem . . . . . . . . . . . . . . . . . . . . 323
9.3.1
Is the Halting problem really hard? (discussion)
326
9.3.2
A direct proof of the uncomputability of HALT
(optional) . . . . . . . . . . . . . . . . . . . . . . . 327
9.4
Reductions . . . . . . . . . . . . . . . . . . . . . . . . . . 329
9.4.1
Example: Halting on the zero problem . . . . . . 330
9.5
Rice‚Äôs Theorem and the impossibility of general soft-
ware verification . . . . . . . . . . . . . . . . . . . . . . . 333
9.5.1
Rice‚Äôs Theorem . . . . . . . . . . . . . . . . . . . . 335
9.5.2
Halting and Rice‚Äôs Theorem for other Turing-
complete models . . . . . . . . . . . . . . . . . . . 339
9.5.3
Is software verification doomed? (discussion) . . 340
9.6
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . 342
9.7
Bibliographical notes . . . . . . . . . . . . . . . . . . . . 345
10 Restricted computational models
347
10.1
Turing completeness as a bug . . . . . . . . . . . . . . . 347
10.2
Context free grammars . . . . . . . . . . . . . . . . . . . 349
10.2.1
Context-free grammars as a computational model 351
10.2.2
The power of context free grammars
. . . . . . . 353
10.2.3
Limitations of context-free grammars (optional)
355


--- Page 13 ---

13
10.3
Semantic properties of context free languages . . . . . . 357
10.3.1
Uncomputability of context-free grammar
equivalence (optional)
. . . . . . . . . . . . . . . 357
10.4
Summary of semantic properties for regular expres-
sions and context-free grammars . . . . . . . . . . . . . 360
10.5
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . 361
10.6
Bibliographical notes . . . . . . . . . . . . . . . . . . . . 362
11 Is every theorem provable?
365
11.1
Hilbert‚Äôs Program and G√∂del‚Äôs Incompleteness Theorem 365
11.1.1
Defining ‚ÄúProof Systems‚Äù . . . . . . . . . . . . . . 367
11.2
G√∂del‚Äôs Incompleteness Theorem: Computational
variant
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 368
11.3
Quantified integer statements . . . . . . . . . . . . . . . 370
11.4
Diophantine equations and the MRDP Theorem
. . . . 372
11.5
Hardness of quantified integer statements . . . . . . . . 374
11.5.1
Step 1: Quantified mixed statements and com-
putation histories
. . . . . . . . . . . . . . . . . . 375
11.5.2
Step 2: Reducing mixed statements to integer
statements
. . . . . . . . . . . . . . . . . . . . . . 378
11.6
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . 380
11.7
Bibliographical notes . . . . . . . . . . . . . . . . . . . . 382
III
Efficient algorithms
385
12 Efficient computation: An informal introduction
387
12.1
Problems on graphs . . . . . . . . . . . . . . . . . . . . . 389
12.1.1
Finding the shortest path in a graph . . . . . . . . 390
12.1.2
Finding the longest path in a graph . . . . . . . . 392
12.1.3
Finding the minimum cut in a graph
. . . . . . . 392
12.1.4
Min-Cut Max-Flow and Linear programming . . 393
12.1.5
Finding the maximum cut in a graph . . . . . . . 395
12.1.6
A note on convexity . . . . . . . . . . . . . . . . . 395
12.2
Beyond graphs . . . . . . . . . . . . . . . . . . . . . . . . 397
12.2.1
SAT
. . . . . . . . . . . . . . . . . . . . . . . . . . 397
12.2.2
Solving linear equations . . . . . . . . . . . . . . . 398
12.2.3
Solving quadratic equations
. . . . . . . . . . . . 399
12.3
More advanced examples
. . . . . . . . . . . . . . . . . 399
12.3.1
Determinant of a matrix . . . . . . . . . . . . . . . 399
12.3.2
Permanent of a matrix . . . . . . . . . . . . . . . . 401
12.3.3
Finding a zero-sum equilibrium . . . . . . . . . . 401
12.3.4
Finding a Nash equilibrium
. . . . . . . . . . . . 402
12.3.5
Primality testing . . . . . . . . . . . . . . . . . . . 402
12.3.6
Integer factoring . . . . . . . . . . . . . . . . . . . 403


--- Page 14 ---

14
12.4
Our current knowledge . . . . . . . . . . . . . . . . . . . 403
12.5
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . 404
12.6
Bibliographical notes . . . . . . . . . . . . . . . . . . . . 404
12.7
Further explorations
. . . . . . . . . . . . . . . . . . . . 405
13 Modeling running time
407
13.1
Formally defining running time . . . . . . . . . . . . . . 409
13.1.1
Polynomial and Exponential Time . . . . . . . . . 410
13.2
Modeling running time using RAM Machines /
NAND-RAM . . . . . . . . . . . . . . . . . . . . . . . . . 412
13.3
Extended Church-Turing Thesis (discussion) . . . . . . 417
13.4
Efficient universal machine: a NAND-RAM inter-
preter in NAND-RAM
. . . . . . . . . . . . . . . . . . . 418
13.4.1
Timed Universal Turing Machine
. . . . . . . . . 420
13.5
The time hierarchy theorem . . . . . . . . . . . . . . . . 421
13.6
Non uniform computation . . . . . . . . . . . . . . . . . 425
13.6.1
Oblivious NAND-TM programs . . . . . . . . . . 427
13.6.2
‚ÄúUnrolling the loop‚Äù: algorithmic transforma-
tion of Turing Machines to circuits . . . . . . . . . 430
13.6.3
Can uniform algorithms simulate non uniform
ones?
. . . . . . . . . . . . . . . . . . . . . . . . . 432
13.6.4
Uniform vs. Nonuniform computation: A recap . 434
13.7
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . 435
13.8
Bibliographical notes . . . . . . . . . . . . . . . . . . . . 438
14 Polynomial-time reductions
439
14.1
Formal definitions of problems
. . . . . . . . . . . . . . 440
14.2
Polynomial-time reductions . . . . . . . . . . . . . . . . 441
14.2.1
Whistling pigs and flying horses . . . . . . . . . . 442
14.3
Reducing 3SAT to zero one and quadratic equations . . 444
14.3.1
Quadratic equations . . . . . . . . . . . . . . . . . 447
14.4
The independent set problem . . . . . . . . . . . . . . . 449
14.5
Some exercises and anatomy of a reduction. . . . . . . . 452
14.5.1
Dominating set . . . . . . . . . . . . . . . . . . . . 453
14.5.2
Anatomy of a reduction . . . . . . . . . . . . . . . 456
14.6
Reducing Independent Set to Maximum Cut
. . . . . . 458
14.7
Reducing 3SAT to Longest Path . . . . . . . . . . . . . . 459
14.7.1
Summary of relations . . . . . . . . . . . . . . . . 462
14.8
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . 462
14.9
Bibliographical notes . . . . . . . . . . . . . . . . . . . . 463
15 NP, NP completeness, and the Cook-Levin Theorem
465
15.1
The class NP . . . . . . . . . . . . . . . . . . . . . . . . . 465
15.1.1
Examples of functions in NP . . . . . . . . . . . . 468
15.1.2
Basic facts about NP . . . . . . . . . . . . . . . . . 470


--- Page 15 ---

15
15.2
From NP to 3SAT: The Cook-Levin Theorem . . . . . . . 471
15.2.1
What does this mean? . . . . . . . . . . . . . . . . 473
15.2.2
The Cook-Levin Theorem: Proof outline . . . . . 473
15.3
The NANDSAT Problem, and why it is NP hard
. . . . 474
15.4
The 3NAND problem . . . . . . . . . . . . . . . . . . . . 476
15.5
From 3NAND to 3SAT
. . . . . . . . . . . . . . . . . . . 479
15.6
Wrapping up . . . . . . . . . . . . . . . . . . . . . . . . . 480
15.7
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . 481
15.8
Bibliographical notes . . . . . . . . . . . . . . . . . . . . 482
16 What if P equals NP?
483
16.1
Search-to-decision reduction . . . . . . . . . . . . . . . . 484
16.2
Optimization . . . . . . . . . . . . . . . . . . . . . . . . . 487
16.2.1
Example: Supervised learning . . . . . . . . . . . 490
16.2.2
Example: Breaking cryptosystems . . . . . . . . . 491
16.3
Finding mathematical proofs
. . . . . . . . . . . . . . . 491
16.4
Quantifier elimination (advanced) . . . . . . . . . . . . 492
16.4.1
Application: self improving algorithm for 3SAT . 495
16.5
Approximating counting problems and posterior
sampling (advanced, optional)
. . . . . . . . . . . . . . 495
16.6
What does all of this imply? . . . . . . . . . . . . . . . . 496
16.7
Can P ‚â†NP be neither true nor false?
. . . . . . . . . . 498
16.8
Is P = NP ‚Äúin practice‚Äù?
. . . . . . . . . . . . . . . . . . 499
16.9
What if P ‚â†NP? . . . . . . . . . . . . . . . . . . . . . . . 500
16.10 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . 502
16.11 Bibliographical notes . . . . . . . . . . . . . . . . . . . . 502
17 Space bounded computation
503
17.1
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . 503
17.2
Bibliographical notes . . . . . . . . . . . . . . . . . . . . 503
IV
Randomized computation
505
18 Probability Theory 101
507
18.1
Random coins . . . . . . . . . . . . . . . . . . . . . . . . 507
18.1.1
Random variables . . . . . . . . . . . . . . . . . . 510
18.1.2
Distributions over strings . . . . . . . . . . . . . . 512
18.1.3
More general sample spaces
. . . . . . . . . . . . 512
18.2
Correlations and independence . . . . . . . . . . . . . . 513
18.2.1
Independent random variables . . . . . . . . . . . 514
18.2.2
Collections of independent random variables
. . 516
18.3
Concentration and tail bounds . . . . . . . . . . . . . . . 516
18.3.1
Chebyshev‚Äôs Inequality . . . . . . . . . . . . . . . 518
18.3.2
The Chernoff bound . . . . . . . . . . . . . . . . . 519


--- Page 16 ---

16
18.3.3
Application: Supervised learning and empirical
risk minimization
. . . . . . . . . . . . . . . . . . 520
18.4
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . 522
18.5
Bibliographical notes . . . . . . . . . . . . . . . . . . . . 525
19 Probabilistic computation
527
19.1
Finding approximately good maximum cuts . . . . . . . 528
19.1.1
Amplifying the success of randomized algorithms 529
19.1.2
Success amplification . . . . . . . . . . . . . . . . 530
19.1.3
Two-sided amplification . . . . . . . . . . . . . . . 531
19.1.4
What does this mean? . . . . . . . . . . . . . . . . 531
19.1.5
Solving SAT through randomization
. . . . . . . 532
19.1.6
Bipartite matching . . . . . . . . . . . . . . . . . . 534
19.2
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . 537
19.3
Bibliographical notes . . . . . . . . . . . . . . . . . . . . 537
19.4
Acknowledgements . . . . . . . . . . . . . . . . . . . . . 537
20 Modeling randomized computation
539
20.1
Modeling randomized computation
. . . . . . . . . . . 540
20.1.1
An alternative view: random coins as an ‚Äúextra
input‚Äù . . . . . . . . . . . . . . . . . . . . . . . . . 542
20.1.2
Success amplification of two-sided error algo-
rithms . . . . . . . . . . . . . . . . . . . . . . . . . 544
20.2
BPP and NP completeness . . . . . . . . . . . . . . . . . 545
20.3
The power of randomization . . . . . . . . . . . . . . . . 547
20.3.1
Solving BPP in exponential time . . . . . . . . . . 547
20.3.2
Simulating randomized algorithms by circuits . . 547
20.4
Derandomization . . . . . . . . . . . . . . . . . . . . . . 549
20.4.1
Pseudorandom generators . . . . . . . . . . . . . 550
20.4.2
From existence to constructivity . . . . . . . . . . 552
20.4.3
Usefulness of pseudorandom generators . . . . . 553
20.5
P = NP and BPP vs P . . . . . . . . . . . . . . . . . . . . 554
20.6
Non-constructive existence of pseudorandom genera-
tors (advanced, optional)
. . . . . . . . . . . . . . . . . 557
20.7
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . 560
20.8
Bibliographical notes . . . . . . . . . . . . . . . . . . . . 560
V
Advanced topics
561
21 Cryptography
563
21.1
Classical cryptosystems . . . . . . . . . . . . . . . . . . . 564
21.2
Defining encryption . . . . . . . . . . . . . . . . . . . . . 566
21.3
Defining security of encryption . . . . . . . . . . . . . . 567
21.4
Perfect secrecy . . . . . . . . . . . . . . . . . . . . . . . . 569


--- Page 17 ---

17
21.4.1
Example: Perfect secrecy in the battlefield
. . . . 570
21.4.2
Constructing perfectly secret encryption . . . . . 571
21.5
Necessity of long keys
. . . . . . . . . . . . . . . . . . . 572
21.6
Computational secrecy . . . . . . . . . . . . . . . . . . . 574
21.6.1
Stream ciphers or the ‚Äúderandomized one-time
pad‚Äù . . . . . . . . . . . . . . . . . . . . . . . . . . 576
21.7
Computational secrecy and NP . . . . . . . . . . . . . . 579
21.8
Public key cryptography . . . . . . . . . . . . . . . . . . 581
21.8.1
Defining public key encryption
. . . . . . . . . . 583
21.8.2
Diffie-Hellman key exchange . . . . . . . . . . . . 584
21.9
Other security notions
. . . . . . . . . . . . . . . . . . . 586
21.10 Magic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 586
21.10.1 Zero knowledge proofs . . . . . . . . . . . . . . . 586
21.10.2 Fully homomorphic encryption
. . . . . . . . . . 587
21.10.3 Multiparty secure computation
. . . . . . . . . . 587
21.11 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . 589
21.12 Bibliographical notes . . . . . . . . . . . . . . . . . . . . 589
22 Proofs and algorithms
591
22.1
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . 591
22.2
Bibliographical notes . . . . . . . . . . . . . . . . . . . . 591
23 Quantum computing
593
23.1
The double slit experiment . . . . . . . . . . . . . . . . . 594
23.2
Quantum amplitudes . . . . . . . . . . . . . . . . . . . . 594
23.2.1
Linear algebra quick review
. . . . . . . . . . . . 597
23.3
Bell‚Äôs Inequality . . . . . . . . . . . . . . . . . . . . . . . 598
23.4
Quantum weirdness . . . . . . . . . . . . . . . . . . . . . 600
23.5
Quantum computing and computation - an executive
summary . . . . . . . . . . . . . . . . . . . . . . . . . . . 600
23.6
Quantum systems . . . . . . . . . . . . . . . . . . . . . . 602
23.6.1
Quantum amplitudes . . . . . . . . . . . . . . . . 604
23.6.2
Quantum systems: an executive summary . . . . 605
23.7
Analysis of Bell‚Äôs Inequality (optional) . . . . . . . . . . 605
23.8
Quantum computation . . . . . . . . . . . . . . . . . . . 608
23.8.1
Quantum circuits
. . . . . . . . . . . . . . . . . . 608
23.8.2
QNAND-CIRC programs (optional)
. . . . . . . 611
23.8.3
Uniform computation . . . . . . . . . . . . . . . . 612
23.9
Physically realizing quantum computation
. . . . . . . 613
23.10 Shor‚Äôs Algorithm: Hearing the shape of prime factors . 614
23.10.1 Period finding
. . . . . . . . . . . . . . . . . . . . 614
23.10.2 Shor‚Äôs Algorithm: A bird‚Äôs eye view . . . . . . . . 615
23.11 Quantum Fourier Transform (advanced, optional) . . . 617


--- Page 18 ---

18
23.11.1 Quantum Fourier Transform over the Boolean
Cube: Simon‚Äôs Algorithm . . . . . . . . . . . . . . 619
23.11.2 From Fourier to Period finding: Simon‚Äôs Algo-
rithm (advanced, optional) . . . . . . . . . . . . . 620
23.11.3 From Simon to Shor (advanced, optional) . . . . 621
23.12 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . 623
23.13 Bibliographical notes . . . . . . . . . . . . . . . . . . . . 623
VI
Appendices
625


--- Page 19 ---

Preface
‚ÄúWe make ourselves no promises, but we cherish the hope that the unobstructed
pursuit of useless knowledge will prove to have consequences in the future
as in the past‚Äù ‚Ä¶ ‚ÄúAn institution which sets free successive generations of
human souls is amply justified whether or not this graduate or that makes a
so-called useful contribution to human knowledge. A poem, a symphony, a
painting, a mathematical truth, a new scientific fact, all bear in themselves all
the justification that universities, colleges, and institutes of research need or
require‚Äù, Abraham Flexner, The Usefulness of Useless Knowledge, 1939.
‚ÄúI suggest that you take the hardest courses that you can, because you learn
the most when you challenge yourself‚Ä¶ CS 121 I found pretty hard.‚Äù, Mark
Zuckerberg, 2005.
This is a textbook for an undergraduate introductory course on
Theoretical Computer Science. The educational goals of this book are
to convey the following:
‚Ä¢ That computation arises in a variety of natural and human-made
systems, and not only in modern silicon-based computers.
‚Ä¢ Similarly, beyond being an extremely important tool, computation
also serves as a useful lens to describe natural, physical, mathemati-
cal and even social concepts.
‚Ä¢ The notion of universality of many different computational models,
and the related notion of the duality between code and data.
‚Ä¢ The idea that one can precisely define a mathematical model of
computation, and then use that to prove (or sometimes only conjec-
ture) lower bounds and impossibility results.
‚Ä¢ Some of the surprising results and discoveries in modern theoreti-
cal computer science, including the prevalence of NP-completeness,
the power of interaction, the power of randomness on one hand
and the possibility of derandomization on the other, the ability
to use hardness ‚Äúfor good‚Äù in cryptography, and the fascinating
possibility of quantum computing.
Compiled on 8.26.2020 18:10


--- Page 20 ---

20
I hope that following this course, students would be able to rec-
ognize computation, with both its power and pitfalls, as it arises in
various settings, including seemingly ‚Äústatic‚Äù content or ‚Äúrestricted‚Äù
formalisms such as macros and scripts. They should be able to follow
through the logic of proofs about computation, including the cen-
tral concept of a reduction, as well as understanding ‚Äúself-referential‚Äù
proofs (such as diagonalization-based proofs that involve programs
given their own code as input). Students should understand that
some problems are inherently intractable, and be able to recognize the
potential for intractability when they are faced with a new problem.
While this book only touches on cryptography, students should un-
derstand the basic idea of how we can use computational hardness for
cryptographic purposes. However, more than any specific skill, this
book aims to introduce students to a new way of thinking of computa-
tion as an object in its own right and to illustrate how this new way of
thinking leads to far-reaching insights and applications.
My aim in writing this text is to try to convey these concepts in the
simplest possible way and try to make sure that the formal notation
and model help elucidate, rather than obscure, the main ideas. I also
tried to take advantage of modern students‚Äô familiarity (or at least
interest!) in programming, and hence use (highly simplified) pro-
gramming languages to describe our models of computation. That
said, this book does not assume fluency with any particular program-
ming language, but rather only some familiarity with the general
notion of programming. We will use programming metaphors and
idioms, occasionally mentioning specific programming languages
such as Python, C, or Lisp, but students should be able to follow these
descriptions even if they are not familiar with these languages.
Proofs in this book, including the existence of a universal Turing
Machine, the fact that every finite function can be computed by some
circuit, the Cook-Levin theorem, and many others, are often con-
structive and algorithmic, in the sense that they ultimately involve
transforming one program to another. While it is possible to follow
these proofs without seeing the code, I do think that having access
to the code, and the ability to play around with it and see how it acts
on various programs, can make these theorems more concrete for the
students. To that end, an accompanying website (which is still work
in progress) allows executing programs in the various computational
models we define, as well as see constructive proofs of some of the
theorems.
0.1 TO THE STUDENT
This book can be challenging, mainly because it brings together a
variety of ideas and techniques in the study of computation. There


--- Page 21 ---

21
are quite a few technical hurdles to master, whether it is following
the diagonalization argument for proving the Halting Problem is
undecidable, combinatorial gadgets in NP-completeness reductions,
analyzing probabilistic algorithms, or arguing about the adversary to
prove the security of cryptographic primitives.
The best way to engage with this material is to read these notes ac-
tively, so make sure you have a pen ready. While reading, I encourage
you to stop and think about the following:
‚Ä¢ When I state a theorem, stop and take a shot at proving it on your
own before reading the proof. You will be amazed by how much
better you can understand a proof even after only 5 minutes of
attempting it on your own.
‚Ä¢ When reading a definition, make sure that you understand what
the definition means, and what the natural examples are of objects
that satisfy it and objects that do not. Try to think of the motivation
behind the definition, and whether there are other natural ways to
formalize the same concept.
‚Ä¢ Actively notice which questions arise in your mind as you read the
text, and whether or not they are answered in the text.
As a general rule, it is more important that you understand the
definitions than the theorems, and it is more important that you
understand a theorem statement than its proof. After all, before you
can prove a theorem, you need to understand what it states, and to
understand what a theorem is about, you need to know the definitions
of the objects involved. Whenever a proof of a theorem is at least
somewhat complicated, I provide a ‚Äúproof idea.‚Äù Feel free to skip the
actual proof in a first reading, focusing only on the proof idea.
This book contains some code snippets, but this is by no means
a programming text. You don‚Äôt need to know how to program to
follow this material. The reason we use code is that it is a precise way
to describe computation. Particular implementation details are not
as important to us, and so we will emphasize code readability at the
expense of considerations such as error handling, encapsulation, etc.
that can be extremely important for real-world programming.
0.1.1 Is the effort worth it?
This is not an easy book, and you might reasonably wonder why
should you spend the effort in learning this material. A traditional
justification for a ‚ÄúTheory of Computation‚Äù course is that you might
encounter these concepts later on in your career. Perhaps you will
come across a hard problem and realize it is NP complete, or find a
need to use what you learned about regular expressions. This might


--- Page 22 ---

22
1 An earlier book that starts with circuits as the initial
model is John Savage‚Äôs [Sav98].
very well be true, but the main benefit of this book is not in teaching
you any practical tool or technique, but instead in giving you a differ-
ent way of thinking: an ability to recognize computational phenomena
even when they occur in non-obvious settings, a way to model compu-
tational tasks and questions, and to reason about them.
Regardless of any use you will derive from this book, I believe
learning this material is important because it contains concepts that
are both beautiful and fundamental. The role that energy and matter
played in the 20th century is played in the 21st by computation and
information, not just as tools for our technology and economy, but also
as the basic building blocks we use to understand the world. This
book will give you a taste of some of the theory behind those, and
hopefully spark your curiosity to study more.
0.2 TO POTENTIAL INSTRUCTORS
I wrote this book for my Harvard course, but I hope that other lectur-
ers will find it useful as well. To some extent, it is similar in content
to ‚ÄúTheory of Computation‚Äù or ‚ÄúGreat Ideas‚Äù courses such as those
taught at CMU or MIT.
The most significant difference between our approach and more
traditional ones (such as Hopcroft and Ullman‚Äôs [HU69; HU79] and
Sipser‚Äôs [Sip97]) is that we do not start with finite automata as our ini-
tial computational model. Instead, our initial computational model
is Boolean Circuits.1 We believe that Boolean Circuits are more fun-
damental to the theory of computing (and even its practice!) than
automata. In particular, Boolean Circuits are a prerequisite for many
concepts that one would want to teach in a modern course on Theoret-
ical Computer Science, including cryptography, quantum computing,
derandomization, attempts at proving P ‚â†NP, and more. Even in
cases where Boolean Circuits are not strictly required, they can of-
ten offer significant simplifications (as in the case of the proof of the
Cook-Levin Theorem).
Furthermore, I believe there are pedagogical reasons to start with
Boolean circuits as opposed to finite automata. Boolean circuits are a
more natural model of computation, and one that corresponds more
closely to computing in silicon, making the connection to practice
more immediate to the students. Finite functions are arguably easier
to grasp than infinite ones, as we can fully write down their truth ta-
ble. The theorem that every finite function can be computed by some
Boolean circuit is both simple enough and important enough to serve
as an excellent starting point for this course. Moreover, many of the
main conceptual points of the theory of computation, including the
notions of the duality between code and data, and the idea of universal-
ity, can already be seen in this context.


--- Page 23 ---

23
After Boolean circuits, we move on to Turing machines and prove
results such as the existence of a universal Turing machine, the un-
computability of the halting problem, and Rice‚Äôs Theorem. Automata
are discussed after we see Turing machines and undecidability, as an
example for a restricted computational model where problems such as
determining halting can be effectively solved.
While this is not our motivation, the order we present circuits, Tur-
ing machines, and automata roughly corresponds to the chronological
order of their discovery. Boolean algebra goes back to Boole‚Äôs and
DeMorgan‚Äôs works in the 1840s [Boo47; De 47] (though the defini-
tion of Boolean circuits and the connection to physical computation
was given 90 years later by Shannon [Sha38]). Alan Turing defined
what we now call ‚ÄúTuring Machines‚Äù in the 1930s [Tur37], while finite
automata were introduced in the 1943 work of McCulloch and Pitts
[MP43] but only really understood in the seminal 1959 work of Rabin
and Scott [RS59].
More importantly, while models such as finite-state machines, reg-
ular expressions, and context-free grammars are incredibly important
for practice, the main applications for these models (whether it is for
parsing, for analyzing properties such as liveness and safety, or even for
software-defined routing tables) rely crucially on the fact that these
are tractable models for which we can effectively answer semantic ques-
tions. This practical motivation can be better appreciated after students
see the undecidability of semantic properties of general computing
models.
The fact that we start with circuits makes proving the Cook-Levin
Theorem much easier. In fact, our proof of this theorem can be (and
is) done using a handful of lines of Python. Combining this proof
with the standard reductions (which are also implemented in Python)
allows students to appreciate visually how a question about computa-
tion can be mapped into a question about (for example) the existence
of an independent set in a graph.
Some other differences between this book and previous texts are
the following:
1. For measuring time complexity, we use the standard RAM machine
model used (implicitly) in algorithms courses, rather than Tur-
ing machines. While these two models are of course polynomially
equivalent, and hence make no difference for the definitions of the
classes P, NP, and EXP, our choice makes the distinction between
notions such as ùëÇ(ùëõ) or ùëÇ(ùëõ2) time more meaningful. This choice
also ensures that these finer-grained time complexity classes corre-
spond to the informal definitions of linear and quadratic time that


--- Page 24 ---

24
students encounter in their algorithms lectures (or their whiteboard
coding interviews‚Ä¶).
2. We use the terminology of functions rather than languages. That is,
rather than saying that a Turing Machine ùëÄdecides a language ùêø‚äÜ
{0, 1}‚àó, we say that it computes a function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}. The
terminology of ‚Äúlanguages‚Äù arises from Chomsky‚Äôs work [Cho56],
but it is often more confusing than illuminating. The language
terminology also makes it cumbersome to discuss concepts such
as algorithms that compute functions with more than one bit of
output (including basic tasks such as addition, multiplication,
etc‚Ä¶). The fact that we use functions rather than languages means
we have to be extra vigilant about students distinguishing between
the specification of a computational task (e.g., the function) and its
implementation (e.g., the program). On the other hand, this point is
so important that it is worth repeatedly emphasizing and drilling
into the students, regardless of the notation used. The book does
mention the language terminology and reminds of it occasionally,
to make it easier for students to consult outside resources.
Reducing the time dedicated to finite automata and context-free
languages allows instructors to spend more time on topics that a mod-
ern course in the theory of computing needs to touch upon. These
include randomness and computation, the interactions between proofs
and programs (including G√∂del‚Äôs incompleteness theorem, interactive
proof systems, and even a bit on the ùúÜ-calculus and the Curry-Howard
correspondence), cryptography, and quantum computing.
This book contains sufficient detail to enable its use for self-study.
Toward that end, every chapter starts with a list of learning objectives,
ends with a recap, and is peppered with ‚Äúpause boxes‚Äù which encour-
age students to stop and work out an argument or make sure they
understand a definition before continuing further.
Section 0.5 contains a ‚Äúroadmap‚Äù for this book, with descriptions
of the different chapters, as well as the dependency structure between
them. This can help in planning a course based on this book.
0.3 ACKNOWLEDGEMENTS
This text is continually evolving, and I am getting input from many
people, for which I am deeply grateful. Salil Vadhan co-taught with
me the first iteration of this course and gave me a tremendous amount
of useful feedback and insights during this process. Michele Amoretti
and Marika Swanberg carefully read several chapters of this text and
gave extremely helpful detailed comments. Dave Evans and Richard
Xu contributed many pull requests fixing errors and improving phras-


--- Page 25 ---

25
ing. Thanks to Anil Ada, Venkat Guruswami, and Ryan O‚ÄôDonnell for
helpful tips from their experience in teaching CMU 15-251.
Thanks to everyone that sent me comments, typo reports, or posted
issues or pull requests on the GitHub repository https://github.
com/boazbk/tcs. In particular I would like to acknowledge helpful
feedback from Scott Aaronson, Michele Amoretti, Aadi Bajpai, Mar-
guerite Basta, Anindya Basu, Sam Benkelman, Jaros≈Çaw B≈Çasiok, Emily
Chan, Christy Cheng, Michelle Chiang, Daniel Chiu, Chi-Ning Chou,
Michael Colavita, Rodrigo Daboin Sanchez, Robert Darley Waddilove,
Anlan Du, Juan Esteller, David Evans, Michael Fine, Simon Fischer,
Leor Fishman, Zaymon Foulds-Cook, William Fu, Kent Furuie, Piotr
Galuszka, Carolyn Ge, Mark Goldstein, Alexander Golovnev, Sayan
Goswami, Michael Haak, Rebecca Hao, Joosep Hook, Thomas HUET,
Emily Jia, Chan Kang, Nina Katz-Christy, Vidak Kazic, Eddie Kohler,
Estefania Lahera, Allison Lee, Benjamin Lee, Ond≈ôej Leng√°l, Raymond
Lin, Emma Ling, Alex Lombardi, Lisa Lu, Aditya Mahadevan, Chris-
tian May, Jacob Meyerson, Leon Mlodzian, George Moe, Glenn Moss,
Hamish Nicholson, Owen Niles, Sandip Nirmel, Sebastian Oberhoff,
Thomas Orton, Joshua Pan, Pablo Parrilo, Juan Perdomo, Banks Pick-
ett, Aaron Sachs, Abdelrhman Saleh, Brian Sapozhnikov, Anthony
Scemama, Peter Sch√§fer, Josh Seides, Alaisha Sharma, Haneul Shin,
Noah Singer, Matthew Smedberg, Miguel Solano, Hikari Sorensen,
David Steurer, Alec Sun, Amol Surati, Everett Sussman, Marika Swan-
berg, Garrett Tanzer, Eric Thomas, Sarah Turnill, Salil Vadhan, Patrick
Watts, Jonah Weissman, Ryan Williams, Licheng Xu, Richard Xu, Wan-
qian Yang, Elizabeth Yeoh-Wang, Josh Zelinsky, Fred Zhang, Grace
Zhang, and Jessica Zhu.
I am using many open source software packages in the production
of these notes for which I am grateful. In particular, I am thankful to
Donald Knuth and Leslie Lamport for LaTeX and to John MacFarlane
for Pandoc. David Steurer wrote the original scripts to produce this
text. The current version uses Sergio Correia‚Äôs panflute. The templates
for the LaTeX and HTML versions are derived from Tufte LaTeX,
Gitbook and Bookdown. Thanks to Amy Hendrickson for some LaTeX
consulting. Juan Esteller and Gabe Montague initially implemented
the NAND* programming languages in OCaml and Javascript. I used
the Jupyter project to write the supplemental code snippets.
Finally, I would like to thank my family: my wife Ravit, and my
children Alma and Goren. Working on this book (and the correspond-
ing course) took so much of my time that Alma wrote an essay for her
fifth-grade class saying that ‚Äúuniversities should not pressure profes-
sors to work too much.‚Äù I‚Äôm afraid all I have to show for this effort is
600 pages of ultra-boring mathematical text.


--- Page 26 ---



--- Page 27 ---

PRELIMINARIES


--- Page 28 ---



--- Page 29 ---

1 This quote is typically read as disparaging the
importance of actual physical computers in Computer
Science, but note that telescopes are absolutely
essential to astronomy as they provide us with the
means to connect theoretical predictions with actual
experimental observations.
2 To be fair, in the following sentence Graham says
‚Äúyou need to know how to calculate time and space
complexity and about Turing completeness‚Äù. This
book includes these topics, as well as others such as
NP-hardness, randomization, cryptography, quantum
computing, and more.
0
Introduction
‚ÄúComputer Science is no more about computers than astronomy is about
telescopes‚Äù, attributed to Edsger Dijkstra.1
‚ÄúHackers need to understand the theory of computation about as much as
painters need to understand paint chemistry.‚Äù, Paul Graham 2003.2
‚ÄúThe subject of my talk is perhaps most directly indicated by simply asking
two questions: first, is it harder to multiply than to add? and second, why?‚Ä¶I
(would like to) show that there is no algorithm for multiplication computation-
ally as simple as that for addition, and this proves something of a stumbling
block.‚Äù, Alan Cobham, 1964
The origin of much of science and medicine can be traced back to
the ancient Babylonians. However, the Babylonians‚Äô most significant
contribution to humanity was arguably the invention of the place-value
number system. The place-value system represents any number using
a collection of digits, whereby the position of the digit is used to de-
termine its value, as opposed to a system such as Roman numerals,
where every symbol has a fixed numerical value regardless of posi-
tion. For example, the average distance to the moon is approximately
238,900 of our miles or 259,956 Roman miles. The latter quantity, ex-
pressed in standard Roman numerals is
MMMMMMMMMMMMMMMMMMMMMMMMMMMMMM
MMMMMMMMMMMMMMMMMMMMMMMMMMMMMM
MMMMMMMMMMMMMMMMMMMMMMMMMMMMMM
MMMMMMMMMMMMMMMMMMMMMMMMMMMMMM
MMMMMMMMMMMMMMMMMMMMMMMMMMMMMM
MMMMMMMMMMMMMMMMMMMMMMMMMMMMMM
MMMMMMMMMMMMMMMMMMMMMMMMMMMMMM
MMMMMMMMMMMMMMMMMMMMMMMMMMMMMM
MMMMMMMMMMMMMMMMMMMDCCCCLVI
Writing the distance to the sun in Roman numerals would require
about 100,000 symbols: a 50-page book just containing this single
number!
Compiled on 8.26.2020 18:10
Learning Objectives:
‚Ä¢ Introduce and motivate the study of
computation for its own sake, irrespective of
particular implementations.
‚Ä¢ The notion of an algorithm and some of its
history.
‚Ä¢ Algorithms as not just tools, but also ways of
thinking and understanding.
‚Ä¢ Taste of Big-ùëÇanalysis and the surprising
creativity in the design of efficient
algorithms.


--- Page 30 ---

30
introduction to theoretical computer science
For someone who thinks of numbers in an additive system like
Roman numerals, quantities like the distance to the moon or sun are
not merely large‚Äîthey are unspeakable: cannot be expressed or even
grasped. It‚Äôs no wonder that Eratosthenes, who was the first person
to calculate the earth‚Äôs diameter (up to about ten percent error) and
Hipparchus who was the first to calculate the distance to the moon,
did not use a Roman-numeral type system but rather the Babylonian
sexagesimal (i.e., base 60) place-value system.
0.1 INTEGER MULTIPLICATION: AN EXAMPLE OF AN ALGORITHM
In the language of Computer Science, the place-value system for rep-
resenting numbers is known as a data structure: a set of instructions,
or ‚Äúrecipe‚Äù, for representing objects as symbols. An algorithm is a set
of instructions, or ‚Äúrecipe‚Äù, for performing operations on such rep-
resentations. Data structures and algorithms have enabled amazing
applications that have transformed human society, but their impor-
tance goes beyond their practical utility. Structures from computer
science, such as bits, strings, graphs, and even the notion of a program
itself, as well as concepts such as universality and replication, have not
just found (many) practical uses but contributed a new language and
a new way to view the world.
In addition to coming up with the place-value system, the Babylo-
nians also invented the ‚Äústandard algorithms‚Äù that we were all taught
in elementary school for adding and multiplying numbers. These al-
gorithms have been essential throughout the ages for people using
abaci, papyrus, or pencil and paper, but in our computer age, do they
still serve any purpose beyond torturing third graders? To see why
these algorithms are still very much relevant, let us compare the Baby-
lonian digit-by-digit multiplication algorithm (‚Äúgrade-school multi-
plication‚Äù) with the naive algorithm that multiplies numbers through
repeated addition. We start by formally describing both algorithms,
see Algorithm 0.1 and Algorithm 0.2.
Algorithm 0.1 ‚Äî Multiplication via repeated addition.
Input: Non-negative integers ùë•, ùë¶
Output: Product ùë•‚ãÖùë¶
1: Let ùëüùëíùë†ùë¢ùëôùë°‚Üê0.
2: for ùëñ= 1, ‚Ä¶ , ùë¶do
3:
ùëüùëíùë†ùë¢ùëôùë°‚Üêùëüùëíùë†ùë¢ùëôùë°+ ùë•
4: end for
5: return ùëüùëíùë†ùë¢ùëôùë°


--- Page 31 ---

introduction
31
Algorithm 0.2 ‚Äî Grade-school multiplication.
Input: Non-negative integers ùë•, ùë¶
Output: Product ùë•‚ãÖùë¶
1: Write ùë•= ùë•ùëõ‚àí1ùë•ùëõ‚àí2 ‚ãØùë•0 and ùë¶= ùë¶ùëö‚àí1ùë¶ùëö‚àí2 ‚ãØùë¶0 in dec-
imal place-value notation. # ùë•0 is the ones digit of ùë•, ùë•1 is
the tens digit, etc.
2: Let ùëüùëíùë†ùë¢ùëôùë°‚Üê0
3: for ùëñ= 0, ‚Ä¶ , ùëõ‚àí1 do
4:
for ùëó= 0, ‚Ä¶ , ùëö‚àí1 do
5:
ùëüùëíùë†ùë¢ùëôùë°‚Üêùëüùëíùë†ùë¢ùëôùë°+ 10ùëñ+ùëó‚ãÖùë•ùëñ‚ãÖùë¶ùëó
6:
end for
7: end for
8: return ùëüùëíùë†ùë¢ùëôùë°
Both Algorithm 0.1 and Algorithm 0.2 assume that we already
know how to add numbers, and Algorithm 0.2 also assumes that we
can multiply a number by a power of 10 (which is, after all, a sim-
ple shift). Suppose that ùë•and ùë¶are two integers of ùëõ= 20 decimal
digits each. (This roughly corresponds to 64 binary digits, which is
a common size in many programming languages.) Computing ùë•‚ãÖùë¶
using Algorithm 0.1 entails adding ùë•to itself ùë¶times which entails
(since ùë¶is a 20-digit number) at least 1019 additions. In contrast, the
grade-school algorithm (i.e., Algorithm 0.2) involves ùëõ2 shifts and
single-digit products, and so at most 2ùëõ2 = 800 single-digit opera-
tions. To understand the difference, consider that a grade-schooler can
perform a single-digit operation in about 2 seconds, and so would re-
quire about 1, 600 seconds (about half an hour) to compute ùë•‚ãÖùë¶using
Algorithm 0.2. In contrast, even though it is more than a billion times
faster than a human, if we used Algorithm 0.1 to compute ùë•‚ãÖùë¶using a
modern PC, it would take us 1020/109 = 1011 seconds (which is more
than three millennia!) to compute the same result.
Computers have not made algorithms obsolete. On the contrary,
the vast increase in our ability to measure, store, and communicate
data has led to much higher demand for developing better and more
sophisticated algorithms that empower us to make better decisions
based on these data. We also see that in no small extent the notion of
algorithm is independent of the actual computing device that executes
it. The digit-by-digit multiplication algorithm is vastly better than iter-
ated addition, regardless whether the technology we use to implement
it is a silicon-based chip, or a third grader with pen and paper.
Theoretical computer science is concerned with the inherent proper-
ties of algorithms and computation; namely, those properties that are
independent of current technology. We ask some questions that were


--- Page 32 ---

32
introduction to theoretical computer science
already pondered by the Babylonians, such as ‚Äúwhat is the best way to
multiply two numbers?‚Äù, but also questions that rely on cutting-edge
science such as ‚Äúcould we use the effects of quantum entanglement to
factor numbers faster?‚Äù.
R
Remark 0.3 ‚Äî Specification, implementation and analysis
of algorithms.. A full description of an algorithm has
three components:
‚Ä¢ Specification: What is the task that the algorithm
performs (e.g., multiplication in the case of Algo-
rithm 0.1 and Algorithm 0.2.)
‚Ä¢ Implementation: How is the task accomplished:
what is the sequence of instructions to be per-
formed. Even though Algorithm 0.1 and Algo-
rithm 0.2 perform the same computational task
(i.e., they have the same specification), they do it in
different ways (i.e., they have different implementa-
tions).
‚Ä¢ Analysis: Why does this sequence of instructions
achieve the desired task. A full description of Algo-
rithm 0.1 and Algorithm 0.2 will include a proof for
each one of these algorithms that on input ùë•, ùë¶, the
algorithm does indeed output ùë•‚ãÖùë¶.
Often as part of the analysis we show that the algo-
rithm is not only correct but also efficient. That is, we
want to show that not only will the algorithm compute
the desired task, but will do so in prescribed number
of operations. For example Algorithm 0.2 computes
the multiplication function on inputs of ùëõdigits using
ùëÇ(ùëõ2) operations, while Algorithm 0.4 (described
below) computes the same function using ùëÇ(ùëõ1.6)
operations. (We define the ùëÇnotations used here in
Section 1.4.8.)
0.2 EXTENDED EXAMPLE: A FASTER WAY TO MULTIPLY (OP-
TIONAL)
Once you think of the standard digit-by-digit multiplication algo-
rithm, it seems like the ‚Äúobviously best‚Äô ‚Äô way to multiply numbers.
In 1960, the famous mathematician Andrey Kolmogorov organized
a seminar at Moscow State University in which he conjectured that
every algorithm for multiplying two ùëõdigit numbers would require
a number of basic operations that is proportional to ùëõ2 (Œ©(ùëõ2) opera-
tions, using ùëÇ-notation as defined in Chapter 1). In other words, Kol-
mogorov conjectured that in any multiplication algorithm, doubling
the number of digits would quadruple the number of basic operations
required. A young student named Anatoly Karatsuba was in the au-


--- Page 33 ---

introduction
33
Figure 1: The grade-school multiplication algorithm
illustrated for multiplying ùë•= 10ùë•+ ùë•and ùë¶=
10ùë¶+ ùë¶. It uses the formula (10ùë•+ ùë•) √ó (10ùë¶+ ùë¶) =
100ùë•ùë¶+ 10(ùë•ùë¶+ ùë•ùë¶) + ùë•ùë¶.
3 If ùë•is a number then ‚åäùë•‚åãis the integer obtained by
rounding it down, see Section 1.7.
dience, and within a week he disproved Kolmogorov‚Äôs conjecture by
discovering an algorithm that requires only about ùê∂ùëõ1.6 operations
for some constant ùê∂. Such a number becomes much smaller than ùëõ2
as ùëõgrows and so for large ùëõKaratsuba‚Äôs algorithm is superior to the
grade-school one. (For example, Python‚Äôs implementation switches
from the grade-school algorithm to Karatsuba‚Äôs algorithm for num-
bers that are 1000 bits or larger.) While the difference between an
ùëÇ(ùëõ1.6) and an ùëÇ(ùëõ2) algorithm can be sometimes crucial in practice
(see Section 0.3 below), in this book we will mostly ignore such dis-
tinctions. However, we describe Karatsuba‚Äôs algorithm below since it
is a good example of how algorithms can often be surprising, as well
as a demonstration of the analysis of algorithms, which is central to this
book and to theoretical computer science at large.
Karatsuba‚Äôs algorithm is based on a faster way to multiply two-digit
numbers. Suppose that ùë•, ùë¶‚àà[100] = {0, ‚Ä¶ , 99} are a pair of two-
digit numbers. Let‚Äôs write ùë•for the ‚Äútens‚Äù digit of ùë•, and ùë•for the
‚Äúones‚Äù digit, so that ùë•= 10ùë•+ ùë•, and write similarly ùë¶= 10ùë¶+ ùë¶for
ùë•, ùë•, ùë¶, ùë¶‚àà[10]. The grade-school algorithm for multiplying ùë•and ùë¶is
illustrated in Fig. 1.
The grade-school algorithm can be thought of as transforming the
task of multiplying a pair of two-digit numbers into four single-digit
multiplications via the formula
(10ùë•+ ùë•) √ó (10ùë¶+ ùë¶) = 100ùë•ùë¶+ 10(ùë•ùë¶+ ùë•ùë¶) + ùë•ùë¶
(1)
Generally, in the grade-school algorithm doubling the number of
digits in the input results in quadrupling the number of operations,
leading to an ùëÇ(ùëõ2) times algorithm. In contrast, Karatsuba‚Äôs algo-
rithm is based on the observation that we can express Eq. (1) also
as
(10ùë•+ùë•)√ó(10ùë¶+ùë¶) = (100‚àí10)ùë•ùë¶+10 [(ùë•+ ùë•)(ùë¶+ ùë¶)]‚àí(10‚àí1)ùë•ùë¶(2)
which reduces multiplying the two-digit number ùë•and ùë¶to com-
puting the following three simpler products: ùë•ùë¶, ùë•ùë¶and (ùë•+ ùë•)(ùë¶+ ùë¶).
By repeating the same strategy recursively, we can reduce the task of
multiplying two ùëõ-digit numbers to the task of multiplying three pairs
of ‚åäùëõ/2‚åã+ 1 digit numbers.3 Since every time we double the number of
digits we triple the number of operations, we will be able to multiply
numbers of ùëõ= 2‚Ñìdigits using about 3‚Ñì= ùëõlog2 3 ‚àºùëõ1.585 operations.
The above is the intuitive idea behind Karatsuba‚Äôs algorithm, but is
not enough to fully specify it. A complete description of an algorithm
entails a precise specification of its operations together with its analysis:
proof that the algorithm does in fact do what it‚Äôs supposed to do. The


--- Page 34 ---

34
introduction to theoretical computer science
Figure 2: Karatsuba‚Äôs multiplication algorithm illus-
trated for multiplying ùë•= 10ùë•+ ùë•and ùë¶= 10ùë¶+ ùë¶.
We compute the three orange, green and purple prod-
ucts ùë•ùë¶, ùë•ùë¶and (ùë•+ ùë•)(ùë¶+ ùë¶) and then add and
subtract them to obtain the result.
Figure 3: Running time of Karatsuba‚Äôs algorithm
vs. the grade-school algorithm. (Python implementa-
tion available online.) Note the existence of a ‚Äúcutoff‚Äù
length, where for sufficiently large inputs Karat-
suba becomes more efficient than the grade-school
algorithm. The precise cutoff location varies by imple-
mentation and platform details, but will always occur
eventually.
operations of Karatsuba‚Äôs algorithm are detailed in Algorithm 0.4,
while the analysis is given in Lemma 0.5 and Lemma 0.6.
Algorithm 0.4 ‚Äî Karatsuba multiplication.
Input: nonnegative integers ùë•, ùë¶each of at most ùëõdigits
Output: ùë•‚ãÖùë¶
1: procedure Karatsuba(ùë•,ùë¶)
2:
if ùëõ‚â§4 then return ùë•‚ãÖùë¶;
3:
Let ùëö= ‚åäùëõ/2‚åã
4:
Write ùë•= 10ùëöùë•+ ùë•and ùë¶= 10ùëöùë¶+ ùë¶
5:
ùê¥‚ÜêKaratsuba(ùë•, ùë¶)
6:
ùêµ‚ÜêKaratsuba(ùë•+ ùë•, ùë¶+ ùë¶)
7:
ùê∂‚ÜêKaratsuba(ùë•, ùë¶)
8:
return (10ùëõ‚àí10ùëö) ‚ãÖùê¥+ 10ùëö‚ãÖùêµ+ (1 ‚àí10ùëö) ‚ãÖùê∂
9: end procedure
Algorithm 0.4 is only half of the full description of Karatsuba‚Äôs
algorithm. The other half is the analysis, which entails proving that (1)
Algorithm 0.4 indeed computes the multiplication operation and (2)
it does so using ùëÇ(ùëõlog2 3) operations. We now turn to showing both
facts:
Lemma 0.5 For every nonnegative integers ùë•, ùë¶, when given input ùë•, ùë¶
Algorithm 0.4 will output ùë•‚ãÖùë¶.
Proof. Let ùëõbe the maximum number of digits of ùë•and ùë¶. We prove
the lemma by induction on ùëõ. The base case is ùëõ‚â§4 where the algo-
rithm returns ùë•‚ãÖùë¶by definition. (It does not matter which algorithm
we use to multiply four-digit numbers - we can even use repeated
addition.) Otherwise, if ùëõ> 4, we define ùëö= ‚åäùëõ/2‚åã, and write
ùë•= 10ùëöùë•+ ùë•and ùë¶= 10ùëöùë¶+ ùë¶.
Plugging this into ùë•‚ãÖùë¶, we get
ùë•‚ãÖùë¶= 102ùëöùë•ùë¶+ 10ùëö(ùë•ùë¶+ ùë•ùë¶) + ùë•ùë¶.
(3)
Rearranging the terms we see that
ùë•‚ãÖùë¶= 102ùëöùë•ùë¶+ 10ùëö[(ùë•+ ùë•)(ùë¶+ ùë¶) ‚àíùë•ùë¶‚àíùë•ùë¶] + ùë•ùë¶.
(4)
since the numbers ùë•,ùë•, ùë¶,ùë¶,ùë•+ ùë•,ùë¶+ ùë¶all have at most ùëö+ 1 < ùëõdigits,
the induction hypothesis implies that the values ùê¥, ùêµ, ùê∂computed
by the recursive calls will satisfy ùê¥= ùë•ùë¶, ùêµ= (ùë•+ ùë•)(ùë¶+ ùë¶) and
ùê∂= ùë•ùë¶. Plugging this into (4) we see that ùë•‚ãÖùë¶equals the value
(102ùëö‚àí10ùëö) ‚ãÖùê¥+ 10ùëö‚ãÖùêµ+ (1 ‚àí10ùëö) ‚ãÖùê∂computed by Algorithm 0.4.
‚ñ†


--- Page 35 ---

introduction
35
Lemma 0.6 If ùë•, ùë¶are integers of at most ùëõdigits, Algorithm 0.4 will
take ùëÇ(ùëõlog2 3) operations on input ùë•, ùë¶.
Proof. Fig. 2 illustrates the idea behind the proof, which we only
sketch here, leaving filling out the details as Exercise 0.4. The proof
is again by induction. We define ùëá(ùëõ) to be the maximum number of
steps that Algorithm 0.4 takes on inputs of length at most ùëõ. Since in
the base case ùëõ‚â§2, Exercise 0.4 performs a constant number of com-
putation, we know that ùëá(2) ‚â§ùëêfor some constant ùëêand for ùëõ> 2, it
satisfies the recursive equation
ùëá(ùëõ) ‚â§3ùëá(‚åäùëõ/2‚åã+ 1) + ùëê‚Ä≤ùëõ
(5)
for some constant ùëê‚Ä≤ (using the fact that addition can be done in ùëÇ(ùëõ)
operations).
The recursive equation (5) solves to ùëÇ(ùëõlog2 3). The intuition be-
hind this is presented in Fig. 2, and this is also a consequence of the
so called ‚ÄúMaster Theorem‚Äù on recurrence relations. As mentioned
above, we leave completing the proof to the reader as Exercise 0.4.
‚ñ†
Figure 4: Karatsuba‚Äôs algorithm reduces an ùëõ-bit
multiplication to three ùëõ/2-bit multiplications,
which in turn are reduced to nine ùëõ/4-bit multi-
plications and so on. We can represent the compu-
tational cost of all these multiplications in a 3-ary
tree of depth log2 ùëõ, where at the root the extra cost
is ùëêùëõoperations, at the first level the extra cost is
ùëê(ùëõ/2) operations, and at each of the 3ùëñnodes of
level ùëñ, the extra cost is ùëê(ùëõ/2ùëñ). The total cost is
ùëêùëõ‚àë
log2 ùëõ
ùëñ=0
(3/2)ùëñ‚â§10ùëêùëõlog2 3 by the formula for
summing a geometric series.
Karatsuba‚Äôs algorithm is by no means the end of the line for multi-
plication algorithms. In the 1960‚Äôs, Toom and Cook extended Karat-
suba‚Äôs ideas to get an ùëÇ(ùëõlogùëò(2ùëò‚àí1)) time multiplication algorithm for
every constant ùëò. In 1971, Sch√∂nhage and Strassen got even better al-
gorithms using the Fast Fourier Transform; their idea was to somehow
treat integers as ‚Äúsignals‚Äù and do the multiplication more efficiently
by moving to the Fourier domain. (The Fourier transform is a central
tool in mathematics and engineering, used in a great many applica-
tions; if you have not seen it yet, you are likely encounter it at some
point in your studies.) In the years that followed researchers kept im-
proving the algorithm, and only very recently Harvey and Van Der


--- Page 36 ---

36
introduction to theoretical computer science
Hoeven managed to obtain an ùëÇ(ùëõlog ùëõ) time algorithm for multipli-
cation (though it only starts beating the Sch√∂nhage-Strassen algorithm
for truly astronomical numbers). Yet, despite all this progress, we
still don‚Äôt know whether or not there is an ùëÇ(ùëõ) time algorithm for
multiplying two ùëõdigit numbers!
R
Remark 0.7 ‚Äî Matrix Multiplication (advanced note).
(This book contains many ‚Äúadvanced‚Äù or ‚Äúoptional‚Äù
notes and sections. These may assume background
that not every student has, and can be safely skipped
over as none of the future parts depends on them.)
Ideas similar to Karatsuba‚Äôs can be used to speed up
matrix multiplications as well. Matrices are a powerful
way to represent linear equations and operations,
widely used in a great many applications of scientific
computing, graphics, machine learning, and many
many more.
One of the basic operations one can do with
two matrices is to multiply them. For example,
if ùë•
=
(ùë•0,0
ùë•0,1
ùë•1,0
ùë•1,1
) and ùë¶
=
(ùë¶0,0
ùë¶0,1
ùë¶1,0
ùë¶1,1
)
then the product of ùë•and ùë¶is the matrix
(ùë•0,0ùë¶0,0 + ùë•0,1ùë¶1,0
ùë•0,0ùë¶0,1 + ùë•0,1ùë¶1,1
ùë•1,0ùë¶0,0 + ùë•1,1ùë¶1,0
ùë•1,0ùë¶0,1 + ùë•1,1ùë¶1,1
). You can
see that we can compute this matrix by eight products
of numbers.
Now suppose that ùëõis even and ùë•and ùë¶are a pair of
ùëõ
√ó
ùëõmatrices which we can think of as each com-
posed of four (ùëõ/2) √ó (ùëõ/2) blocks ùë•0,0, ùë•0,1, ùë•1,0, ùë•1,1
and ùë¶0,0, ùë¶0,1, ùë¶1,0, ùë¶1,1. Then the formula for the matrix
product of ùë•and ùë¶can be expressed in the same way
as above, just replacing products ùë•ùëé,ùëèùë¶ùëê,ùëëwith matrix
products, and addition with matrix addition. This
means that we can use the formula above to give an
algorithm that doubles the dimension of the matrices
at the expense of increasing the number of operation
by a factor of 8, which for ùëõ
=
2‚Ñìresults in 8‚Ñì
=
ùëõ3
operations.
In 1969 Volker Strassen noted that we can compute
the product of a pair of two-by-two matrices using
only seven products of numbers by observing that
each entry of the matrix ùë•ùë¶can be computed by
adding and subtracting the following seven terms:
ùë°1 = (ùë•0,0 + ùë•1,1)(ùë¶0,0 + ùë¶1,1), ùë°2 = (ùë•0,0 + ùë•1,1)ùë¶0,0,
ùë°3
=
ùë•0,0(ùë¶0,1 ‚àíùë¶1,1), ùë°4
=
ùë•1,1(ùë¶0,1 ‚àíùë¶0,0),
ùë°5 = (ùë•0,0 + ùë•0,1)ùë¶1,1, ùë°6 = (ùë•1,0 ‚àíùë•0,0)(ùë¶0,0 + ùë¶0,1),
ùë°7 = (ùë•0,1 ‚àíùë•1,1)(ùë¶1,0 + ùë¶1,1). Indeed, one can verify
that ùë•ùë¶= (ùë°1 + ùë°4 ‚àíùë°5 + ùë°7
ùë°3 + ùë°5
ùë°2 + ùë°4
ùë°1 + ùë°3 ‚àíùë°2 + ùë°6
).


--- Page 37 ---

introduction
37
Using this observation, we can obtain an algorithm
such that doubling the dimension of the matrices
results in increasing the number of operations by a
factor of 7, which means that for ùëõ
=
2‚Ñìthe cost is
7‚Ñì
=
ùëõlog2 7
‚àº
ùëõ2.807. A long sequence of work has
since improved this algorithm, and the current record
has running time about ùëÇ(ùëõ2.373). However, unlike the
case of integer multiplication, at the moment we don‚Äôt
know of any algorithm for matrix multiplication that
runs in time linear or even close to linear in the size
of the input matrices (e.g., an ùëÇ(ùëõ2ùëùùëúùëôùë¶ùëôùëúùëî(ùëõ)) time
algorithm). People have tried to use group represen-
tations, which can be thought of as generalizations of
the Fourier transform, to obtain faster algorithms, but
this effort has not yet succeeded.
0.3 ALGORITHMS BEYOND ARITHMETIC
The quest for better algorithms is by no means restricted to arithmetic
tasks such as adding, multiplying or solving equations. Many graph
algorithms, including algorithms for finding paths, matchings, span-
ning trees, cuts, and flows, have been discovered in the last several
decades, and this is still an intensive area of research. (For example,
the last few years saw many advances in algorithms for the maximum
flow problem, borne out of unexpected connections with electrical cir-
cuits and linear equation solvers.) These algorithms are being used
not just for the ‚Äúnatural‚Äù applications of routing network traffic or
GPS-based navigation, but also for applications as varied as drug dis-
covery through searching for structures in gene-interaction graphs to
computing risks from correlations in financial investments.
Google was founded based on the PageRank algorithm, which is
an efficient algorithm to approximate the ‚Äúprincipal eigenvector‚Äù of
(a dampened version of) the adjacency matrix of the web graph. The
Akamai company was founded based on a new data structure, known
as consistent hashing, for a hash table where buckets are stored at dif-
ferent servers. The backpropagation algorithm, which computes partial
derivatives of a neural network in ùëÇ(ùëõ) instead of ùëÇ(ùëõ2) time, under-
lies many of the recent phenomenal successes of learning deep neural
networks. Algorithms for solving linear equations under sparsity
constraints, a concept known as compressed sensing, have been used
to drastically reduce the amount and quality of data needed to ana-
lyze MRI images. This made a critical difference for MRI imaging of
cancer tumors in children, where previously doctors needed to use
anesthesia to suspend breath during the MRI exam, sometimes with
dire consequences.


--- Page 38 ---

38
introduction to theoretical computer science
Even for classical questions, studied through the ages, new dis-
coveries are still being made. For example, for the question of de-
termining whether a given integer is prime or composite, which has
been studied since the days of Pythagoras, efficient probabilistic algo-
rithms were only discovered in the 1970s, while the first deterministic
polynomial-time algorithm was only found in 2002. For the related
problem of actually finding the factors of a composite number, new
algorithms were found in the 1980s, and (as we‚Äôll see later in this
course) discoveries in the 1990s raised the tantalizing prospect of
obtaining faster algorithms through the use of quantum mechanical
effects.
Despite all this progress, there are still many more questions than
answers in the world of algorithms. For almost all natural prob-
lems, we do not know whether the current algorithm is the ‚Äúbest‚Äù,
or whether a significantly better one is still waiting to be discovered.
As alluded in Cobham‚Äôs opening quote for this chapter, even for the
basic problem of multiplying numbers we have not yet answered the
question of whether there is a multiplication algorithm that is as ef-
ficient as our algorithms for addition. But at least we now know the
right way to ask it.
0.4 ON THE IMPORTANCE OF NEGATIVE RESULTS
Finding better algorithms for problems such as multiplication, solv-
ing equations, graph problems, or fitting neural networks to data, is
undoubtedly a worthwhile endeavor. But why is it important to prove
that such algorithms don‚Äôt exist? One motivation is pure intellectual
curiosity. Another reason to study impossibility results is that they
correspond to the fundamental limits of our world. In other words,
impossibility results are laws of nature.
Here are some examples of impossibility results outside computer
science (see Section 0.7 for more about these). In physics, the impos-
sibility of building a perpetual motion machine corresponds to the law
of conservation of energy. The impossibility of building a heat engine
beating Carnot‚Äôs bound corresponds to the second law of thermo-
dynamics, while the impossibility of faster-than-light information
transmission is a cornerstone of special relativity. In mathematics,
while we all learned the formula for solving quadratic equations in
high school, the impossibility of generalizing this formula to equations
of degree five or more gave birth to group theory. The impossibility
of proving Euclid‚Äôs fifth axiom from the first four gave rise to non-
Euclidean geometries, which ended up crucial for the theory of general
relativity.
In an analogous way, impossibility results for computation corre-
spond to ‚Äúcomputational laws of nature‚Äù that tell us about the fun-


--- Page 39 ---

introduction
39
damental limits of any information processing apparatus, whether
based on silicon, neurons, or quantum particles. Moreover, computer
scientists found creative approaches to apply computational limitations
to achieve certain useful tasks. For example, much of modern Internet
traffic is encrypted using the RSA encryption scheme, which relies on
its security on the (conjectured) impossibility of efficiently factoring
large integers. More recently, the Bitcoin system uses a digital ana-
log of the ‚Äúgold standard‚Äù where, instead of using a precious metal,
new currency is obtained by ‚Äúmining‚Äù solutions for computationally
difficult problems.
‚úì
Chapter Recap
‚Ä¢ The history of algorithms goes back thousands
of years; they have been essential much of hu-
man progress and these days form the basis of
multi-billion dollar industries, as well as life-saving
technologies.
‚Ä¢ There is often more than one algorithm to achieve
the same computational task. Finding a faster al-
gorithm can often make a much bigger difference
than improving computing hardware.
‚Ä¢ Better algorithms and data structures don‚Äôt just
speed up calculations, but can yield new qualitative
insights.
‚Ä¢ One question we will study is to find out what is
the most efficient algorithm for a given problem.
‚Ä¢ To show that an algorithm is the most efficient one
for a given problem, we need to be able to prove
that it is impossible to solve the problem using a
smaller amount of computational resources.
0.5 ROADMAP TO THE REST OF THIS BOOK
Often, when we try to solve a computational problem, whether it is
solving a system of linear equations, finding the top eigenvector of a
matrix, or trying to rank Internet search results, it is enough to use the
‚ÄúI know it when I see it‚Äù standard for describing algorithms. As long
as we find some way to solve the problem, we are happy and might
not care much on the exact mathematical model for our algorithm.
But when we want to answer a question such as ‚Äúdoes there exist an
algorithm to solve the problem ùëÉ?‚Äù we need to be much more precise.
In particular, we will need to (1) define exactly what it means to
solve ùëÉ, and (2) define exactly what an algorithm is. Even (1) can
sometimes be non-trivial but (2) is particularly challenging; it is not
at all clear how (and even whether) we can encompass all potential
ways to design algorithms. We will consider several simple models of


--- Page 40 ---

40
introduction to theoretical computer science
computation, and argue that, despite their simplicity, they do capture
all ‚Äúreasonable‚Äù approaches to achieve computing, including all those
that are currently used in modern computing devices.
Once we have these formal models of computation, we can try
to obtain impossibility results for computational tasks, showing that
some problems can not be solved (or perhaps can not be solved within
the resources of our universe). Archimedes once said that given a
fulcrum and a long enough lever, he could move the world. We will
see how reductions allow us to leverage one hardness result into a
slew of a great many others, illuminating the boundaries between
the computable and uncomputable (or tractable and intractable)
problems.
Later in this book we will go back to examining our models of
computation, and see how resources such as randomness or quantum
entanglement could potentially change the power of our model. In
the context of probabilistic algorithms, we will see a glimpse of how
randomness has become an indispensable tool for understanding
computation, information, and communication. We will also see how
computational difficulty can be an asset rather than a hindrance, and
be used for the ‚Äúderandomization‚Äù of probabilistic algorithms. The
same ideas also show up in cryptography, which has undergone not
just a technological but also an intellectual revolution in the last few
decades, much of it building on the foundations that we explore in
this course.
Theoretical Computer Science is a vast topic, branching out and
touching upon many scientific and engineering disciplines. This book
provides a very partial (and biased) sample of this area. More than
anything, I hope I will manage to ‚Äúinfect‚Äù you with at least some of
my love for this field, which is inspired and enriched by the connec-
tion to practice, but is also deep and beautiful regardless of applica-
tions.
0.5.1 Dependencies between chapters
This book is divided into the following parts, see Fig. 5.
‚Ä¢ Preliminaries: Introduction, mathematical background, and repre-
senting objects as strings.
‚Ä¢ Part I: Finite computation (Boolean circuits): Equivalence of cir-
cuits and straight-line programs. Universal gate sets. Existence of a
circuit for every function, representing circuits as strings, universal
circuit, lower bound on circuit size using the counting argument.
‚Ä¢ Part II: Uniform computation (Turing machines): Equivalence of
Turing machines and programs with loops. Equivalence of models


--- Page 41 ---

introduction
41
(including RAM machines, ùúÜcalculus, and cellular automata), con-
figurations of Turing machines, existence of a universal Turing ma-
chine, uncomputable functions (including the Halting problem and
Rice‚Äôs Theorem), G√∂del‚Äôs incompleteness theorem, restricted com-
putational models models (regular and context free languages).
‚Ä¢ Part III: Efficient computation: Definition of running time, time
hierarchy theorem, P and NP, P/poly, NP completeness and the
Cook-Levin Theorem, space bounded computation.
‚Ä¢ Part IV: Randomized computation: Probability, randomized algo-
rithms, BPP, amplification, BPP ‚äÜP/ùëùùëúùëôùë¶, pseudorandom genera-
tors and derandomization.
‚Ä¢ Part V: Advanced topics: Cryptography, proofs and algorithms
(interactive and zero knowledge proofs, Curry-Howard correspon-
dence), quantum computing.
Figure 5: The dependency structure of the different
parts. Part I introduces the model of Boolean cir-
cuits to study finite functions with an emphasis on
quantitative questions (how many gates to compute
a function). Part II introduces the model of Turing
machines to study functions that have unbounded input
lengths with an emphasis on qualitative questions (is
this function computable or not). Much of Part II does
not depend on Part I, as Turing machines can be used
as the first computational model. Part III depends
on both parts as it introduces a quantitative study of
functions with unbounded input length. The more
advanced parts IV (randomized computation) and
V (advanced topics) rely on the material of Parts I, II
and III.
The book largely proceeds in linear order, with each chapter build-
ing on the previous ones, with the following exceptions:
‚Ä¢ The topics of ùúÜcalculus (Section 8.5 and Section 8.5), G√∂del‚Äôs in-
completeness theorem (Chapter 11), Automata/regular expres-
sions and context-free grammars (Chapter 10), and space-bounded
computation (Chapter 17), are not used in the following chapters.
Hence you can choose whether to cover or skip any subset of them.
‚Ä¢ Part II (Uniform Computation / Turing Machines) does not have
a strong dependency on Part I (Finite computation / Boolean cir-
cuits) and it should be possible to teach them in the reverse order


--- Page 42 ---

42
introduction to theoretical computer science
with minor modification. Boolean circuits are used Part III (efficient
computation) for results such as P ‚äÜP/poly and the Cook-Levin
Theorem, as well as in Part IV (for BPP ‚äÜP/poly and derandom-
ization) and Part V (specifically in cryptography and quantum
computing).
‚Ä¢ All chapters in Part V (Advanced topics) are independent of one
another and can be covered in any order.
A course based on this book can use all of Parts I, II, and III (possi-
bly skipping over some or all of the ùúÜcalculus, Chapter 11, Chapter 10
or Chapter 17), and then either cover all or some of Part IV (random-
ized computation), and add a ‚Äúsprinkling‚Äù of advanced topics from
Part V based on student or instructor interest.
0.6 EXERCISES
Exercise 0.1 Rank the significance of the following inventions in speed-
ing up multiplication of large (that is 100-digit or more) numbers.
That is, use ‚Äúback of the envelope‚Äù estimates to order them in terms of
the speedup factor they offered over the previous state of affairs.
a. Discovery of the grade-school digit by digit algorithm (improving
upon repeated addition)
b. Discovery of Karatsuba‚Äôs algorithm (improving upon the digit by
digit algorithm)
c. Invention of modern electronic computers (improving upon calcu-
lations with pen and paper).
‚ñ†
Exercise 0.2 The 1977 Apple II personal computer had a processor
speed of 1.023 Mhz or about 106 operations per seconds. At the
time of this writing the world‚Äôs fastest supercomputer performs 93
‚Äúpetaflops‚Äù (1015 floating point operations per second) or about 1018
basic steps per second. For each one of the following running times
(as a function of the input length ùëõ), compute for both computers how
large an input they could handle in a week of computation, if they run
an algorithm that has this running time:
a. ùëõoperations.
b. ùëõ2 operations.
c. ùëõlog ùëõoperations.
d. 2ùëõoperations.


--- Page 43 ---

introduction
43
4 As we will see in Chapter Chapter 21, almost any
company relying on cryptography needs to assume
the non existence of certain algorithms. In particular,
RSA Security was founded based on the security
of the RSA cryptosystem, which presumes the non
existence of an efficient algorithm to compute the
prime factorization of large integers.
5 Hint: Use a proof by induction - suppose that this is
true for all ùëõ‚Äôs from 1 to ùëöand prove that this is true
also for ùëö+ 1.
e. ùëõ! operations.
‚ñ†
Exercise 0.3 ‚Äî Usefulness of algorithmic non-existence. In this chapter we
mentioned several companies that were founded based on the discov-
ery of new algorithms. Can you give an example for a company that
was founded based on the non existence of an algorithm? See footnote
for hint.4
‚ñ†
Exercise 0.4 ‚Äî Analysis of Karatsuba‚Äôs Algorithm. a. Suppose that
ùëá1, ùëá2, ùëá3, ‚Ä¶ is a sequence of numbers such that ùëá2 ‚â§10 and
for every ùëõ, ùëáùëõ‚â§3ùëá‚åäùëõ/2‚åã+1 + ùê∂ùëõfor some ùê∂‚â•1. Prove that
ùëáùëõ‚â§20ùê∂ùëõlog2 3 for every ùëõ> 2.5
b. Prove that the number of single-digit operations that Karatsuba‚Äôs
algorithm takes to multiply two ùëõdigit numbers is at most
1000ùëõlog2 3.
‚ñ†
Exercise 0.5 Implement in the programming language of your
choice functions Gradeschool_multiply(x,y) and Karat-
suba_multiply(x,y) that take two arrays of digits x and y and return
an array representing the product of x and y (where x is identified
with the number x[0]+10*x[1]+100*x[2]+... etc..) using the
grade-school algorithm and the Karatsuba algorithm respectively.
At what number of digits does the Karatsuba algorithm beat the
grade-school one?
‚ñ†
Exercise 0.6 ‚Äî Matrix Multiplication (optional, advanced). In this exercise, we
show that if for some ùúî> 2, we can write the product of two ùëò√ó ùëò
real-valued matrices ùê¥, ùêµusing at most ùëòùúîmultiplications, then we
can multiply two ùëõ√ó ùëõmatrices in roughly ùëõùúîtime for every large
enough ùëõ.
To make this precise, we need to make some notation that is unfor-
tunately somewhat cumbersome. Assume that there is some ùëò‚àà‚Ñï
and ùëö‚â§ùëòùúîsuch that for every ùëò√ó ùëòmatrices ùê¥, ùêµ, ùê∂such that
ùê∂= AB, we can write for every ùëñ, ùëó‚àà[ùëò]:
ùê∂ùëñ,ùëó=
ùëö‚àí1
‚àë
‚Ñì=0
ùõº‚Ñì
ùëñ,ùëóùëì‚Ñì(ùê¥)ùëî‚Ñì(ùêµ)
(6)
for some linear functions ùëì0, ‚Ä¶ , ùëìùëö‚àí1, ùëî0, ‚Ä¶ , ùëîùëö‚àí1 ‚à∂‚Ñùùëõ2 ‚Üí‚Ñùand
coefficients {ùõº‚Ñì
ùëñ,ùëó}ùëñ,ùëó‚àà[ùëò],‚Ñì‚àà[ùëö]. Prove that under this assumption for
every ùúñ> 0, if ùëõis sufficiently large, then there is an algorithm that
computes the product of two ùëõ√ó ùëõmatrices using at most ùëÇ(ùëõùúî+ùúñ)
arithmetic operations. See footnote for hint.6


--- Page 44 ---

44
introduction to theoretical computer science
‚ñ†
0.7 BIBLIOGRAPHICAL NOTES
For a brief overview of what we‚Äôll see in this book, you could do far
worse than read Bernard Chazelle‚Äôs wonderful essay on the Algo-
rithm as an Idiom of modern science. The book of Moore and Mertens
[MM11] gives a wonderful and comprehensive overview of the theory
of computation, including much of the content discussed in this chap-
ter and the rest of this book. Aaronson‚Äôs book [Aar13] is another great
read that touches upon many of the same themes.
For more on the algorithms the Babylonians used, see Knuth‚Äôs
paper and Neugebauer‚Äôs classic book.
Many of the algorithms we mention in this chapter are covered
in algorithms textbooks such as those by Cormen, Leiserson, Rivert,
and Stein [Cor+09], Kleinberg and Tardos [KT06], and Dasgupta, Pa-
padimitriou and Vazirani [DPV08], as well as Jeff Erickson‚Äôs textbook.
Erickson‚Äôs book is freely available online and contains a great exposi-
tion of recursive algorithms in general and Karatsuba‚Äôs algorithm in
particular.
The story of Karatsuba‚Äôs discovery of his multiplication algorithm
is recounted by him in [Kar95]. As mentioned above, further improve-
ments were made by Toom and Cook [Too63; Coo66], Sch√∂nhage and
Strassen [SS71], F√ºrer [F√ºr07], and recently by Harvey and Van Der
Hoeven [HV19], see this article for a nice overview. The last papers
crucially rely on the Fast Fourier transform algorithm. The fascinating
story of the (re)discovery of this algorithm by John Tukey in the con-
text of the cold war is recounted in [Coo87]. (We say re-discovery
because it later turned out that the algorithm dates back to Gauss
[HJB85].) The Fast Fourier Transform is covered in some of the books
mentioned below, and there are also online available lectures such as
Jeff Erickson‚Äôs. See also this popular article by David Austin. Fast ma-
trix multiplication was discovered by Strassen [Str69], and since then
this has been an active area of research. [Bl√§13] is a recommended
self-contained survey of this area.
The Backpropagation algorithm for fast differentiation of neural net-
works was invented by Werbos [Wer74]. The Pagerank algorithm was
invented by Larry Page and Sergey Brin [Pag+99]. It is closely related
to the HITS algorithm of Kleinberg [Kle99]. The Akamai company was
founded based on the consistent hashing data structure described in
[Kar+97]. Compressed sensing has a long history but two foundational
papers are [CRT06; Don06]. [Lus+08] gives a survey of applications
of compressed sensing to MRI; see also this popular article by Ellen-
berg [Ell10]. The deterministic polynomial-time algorithm for testing
primality was given by Agrawal, Kayal, and Saxena [AKS04].


--- Page 45 ---

introduction
45
We alluded briefly to classical impossibility results in mathematics,
including the impossibility of proving Euclid‚Äôs fifth postulate from
the other four, impossibility of trisecting an angle with a straightedge
and compass and the impossibility of solving a quintic equation via
radicals. A geometric proof of the impossibility of angle trisection
(one of the three geometric problems of antiquity, going back to the
ancient Greeks) is given in this blog post of Tao. The book of Mario
Livio [Liv05] covers some of the background and ideas behind these
impossibility results. Some exciting recent research is focused on
trying to use computational complexity to shed light on fundamental
questions in physics such understanding black holes and reconciling
general relativity with quantum mechanics


--- Page 46 ---



--- Page 47 ---

1
Mathematical Background
‚ÄúI found that every number, which may be expressed from one to ten, surpasses
the preceding by one unit: afterwards the ten is doubled or tripled ‚Ä¶ until
a hundred; then the hundred is doubled and tripled in the same manner as
the units and the tens ‚Ä¶ and so forth to the utmost limit of numeration.‚Äù,
Muhammad ibn M≈´sƒÅ al-KhwƒÅrizmƒ´, 820, translation by Fredric Rosen,
1831.
In this chapter we review some of the mathematical concepts that
we use in this book. These concepts are typically covered in courses
or textbooks on ‚Äúmathematics for computer science‚Äù or ‚Äúdiscrete
mathematics‚Äù; see the ‚ÄúBibliographical Notes‚Äù section (Section 1.9)
for several excellent resources on these topics that are freely-available
online.
A mathematician‚Äôs apology. Some students might wonder why this
book contains so much math. The reason is that mathematics is sim-
ply a language for modeling concepts in a precise and unambiguous
way. In this book we use math to model the concept of computation.
For example, we will consider questions such as ‚Äúis there an efficient
algorithm to find the prime factors of a given integer?‚Äù. (We will see that
this question is particularly interesting, touching on areas as far apart
as Internet security and quantum mechanics!) To even phrase such a
question, we need to give a precise definition of the notion of an algo-
rithm, and of what it means for an algorithm to be efficient. Also, since
there is no empirical experiment to prove the nonexistence of an algo-
rithm, the only way to establish such a result is using a mathematical
proof.
1.1 THIS CHAPTER: A READER‚ÄôS MANUAL
Depending on your background, you can approach this chapter in two
different ways:
‚Ä¢ If you have already taken a ‚Äúdiscrete mathematics‚Äù, ‚Äúmathematics
for computer science‚Äù or similar courses, you do not need to read
Compiled on 8.26.2020 18:10
Learning Objectives:
‚Ä¢ Recall basic mathematical notions such as
sets, functions, numbers, logical operators
and quantifiers, strings, and graphs.
‚Ä¢ Rigorously define Big-ùëÇnotation.
‚Ä¢ Proofs by induction.
‚Ä¢ Practice with reading mathematical
definitions, statements, and proofs.
‚Ä¢ Transform an intuitive argument into a
rigorous proof.


--- Page 48 ---

48
introduction to theoretical computer science
the whole chapter. You can just take quick look at Section 1.2 to see
the main tools we will use, Section 1.7 for our notation and conven-
tions, and then skip ahead to the rest of this book. Alternatively,
you can sit back, relax, and read this chapter just to get familiar
with our notation, as well as to enjoy (or not) my philosophical
musings and attempts at humor.
‚Ä¢ If your background is less extensive, see Section 1.9 for some re-
sources on these topics. This chapter briefly covers the concepts
that we need, but you may find it helpful to see a more in-depth
treatment. As usual with math, the best way to get comfort with
this material is to work out exercises on your own.
‚Ä¢ You might also want to start brushing up on discrete probability,
which we‚Äôll use later in this book (see Chapter 18).
1.2 A QUICK OVERVIEW OF MATHEMATICAL PREREQUISITES
The main mathematical concepts we will use are the following. We
just list these notions below, deferring their definitions to the rest of
this chapter. If you are familiar with all of these, then you might want
to just skip to Section 1.7 to see the full list of notation we use.
‚Ä¢ Proofs: First and foremost, this book involves a heavy dose of for-
mal mathematical reasoning, which includes mathematical defini-
tions, statements, and proofs.
‚Ä¢ Sets and set operations: We will use extensively mathematical sets.
We use the basic set relations of membership (‚àà) and containment
(‚äÜ), and set operations, principally union (‚à™), intersection (‚à©), and
set difference (‚ßµ).
‚Ä¢ Cartesian product and Kleene star operation: We also use the
Cartesian product of two sets ùê¥and ùêµ, denoted as ùê¥√ó ùêµ(that is,
ùê¥√ó ùêµthe set of pairs (ùëé, ùëè) where ùëé‚ààùê¥and ùëè‚ààùêµ). We denote by
ùê¥ùëõthe ùëõfold Cartesian product (e.g., ùê¥3 = ùê¥√ó ùê¥√ó ùê¥) and by ùê¥‚àó
(known as the Kleene star) the union of ùê¥ùëõfor all ùëõ‚àà{0, 1, 2, ‚Ä¶}.
‚Ä¢ Functions: The domain and codomain of a function, properties such
as being one-to-one (also known as injective) or onto (also known
as surjective) functions, as well as partial functions (that, unlike
standard or ‚Äútotal‚Äù functions, are not necessarily defined on all
elements of their domain).
‚Ä¢ Logical operations: The operations AND (‚àß), OR (‚à®), and NOT
(¬¨) and the quantifiers ‚Äúthere exists‚Äù (‚àÉ) and ‚Äúfor all‚Äù (‚àÄ).
‚Ä¢ Basic combinatorics: Notions such as (ùëõ
ùëò) (the number of ùëò-sized
subsets of a set of size ùëõ).


--- Page 49 ---

mathematical background
49
Figure 1.1: A snippet from the ‚Äúmethods‚Äù section of
the ‚ÄúAlphaGo Zero‚Äù paper by Silver et al, Nature, 2017.
Figure 1.2: A snippet from the ‚ÄúZerocash‚Äù paper of
Ben-Sasson et al, that forms the basis of the cryptocur-
rency startup Zcash.
‚Ä¢ Graphs: Undirected and directed graphs, connectivity, paths, and
cycles.
‚Ä¢ Big-ùëÇnotation: ùëÇ, ùëú, Œ©, ùúî, Œò notation for analyzing asymptotic
growth of functions.
‚Ä¢ Discrete probability: We will use probability theory, and specifi-
cally probability over finite samples spaces such as tossing ùëõcoins,
including notions such as random variables, expectation, and concen-
tration. We will only use probability theory in the second half of
this text, and will review it beforehand in Chapter 18. However,
probabilistic reasoning is a subtle (and extremely useful!) skill, and
it‚Äôs always good to start early in acquiring it.
In the rest of this chapter we briefly review the above notions. This
is partially to remind the reader and reinforce material that might
not be fresh in your mind, and partially to introduce our notation
and conventions which might occasionally differ from those you‚Äôve
encountered before.
1.3 READING MATHEMATICAL TEXTS
Mathematicians use jargon for the same reason that it is used in many
other professions such engineering, law, medicine, and others. We
want to make terms precise and introduce shorthand for concepts
that are frequently reused. Mathematical texts tend to ‚Äúpack a lot
of punch‚Äù per sentence, and so the key is to read them slowly and
carefully, parsing each symbol at a time.
With time and practice you will see that reading mathematical texts
becomes easier and jargon is no longer an issue. Moreover, reading
mathematical texts is one of the most transferable skills you could take
from this book. Our world is changing rapidly, not just in the realm
of technology, but also in many other human endeavors, whether it
is medicine, economics, law or even culture. Whatever your future
aspirations, it is likely that you will encounter texts that use new con-
cepts that you have not seen before (see Fig. 1.1 and Fig. 1.2 for two
recent examples from current ‚Äúhot areas‚Äù). Being able to internalize
and then apply new definitions can be hugely important. It is a skill
that‚Äôs much easier to acquire in the relatively safe and stable context of
a mathematical course, where one at least has the guarantee that the
concepts are fully specified, and you have access to your teaching staff
for questions.
The basic components of a mathematical text are definitions, asser-
tions and proofs.


--- Page 50 ---

50
introduction to theoretical computer science
Figure 1.3: An annotated form of Definition 1.1,
marking which part is being defined and how.
1.3.1 Definitions
Mathematicians often define new concepts in terms of old concepts.
For example, here is a mathematical definition which you may have
encountered in the past (and will see again shortly):
Definition 1.1 ‚Äî One to one function. Let ùëÜ, ùëábe sets. We say that a
function ùëì‚à∂ùëÜ‚Üíùëáis one to one (also known as injective) if for every
two elements ùë•, ùë•‚Ä≤ ‚ààùëÜ, if ùë•‚â†ùë•‚Ä≤ then ùëì(ùë•) ‚â†ùëì(ùë•‚Ä≤).
Definition 1.1 captures a simple concept, but even so it uses quite
a bit of notation. When reading such a definition, it is often useful to
annotate it with a pen as you‚Äôre going through it (see Fig. 1.3). For
example, when you see an identifier such as ùëì, ùëÜor ùë•, make sure that
you realize what sort of object is it: is it a set, a function, an element,
a number, a gremlin? You might also find it useful to explain the
definition in words to a friend (or to yourself).
1.3.2 Assertions: Theorems, lemmas, claims
Theorems, lemmas, claims and the like are true statements about the
concepts we defined. Deciding whether to call a particular statement a
‚ÄúTheorem‚Äù, a ‚ÄúLemma‚Äù or a ‚ÄúClaim‚Äù is a judgement call, and does not
make a mathematical difference. All three correspond to statements
which were proven to be true. The difference is that a Theorem refers
to a significant result, that we would want to remember and highlight.
A Lemma often refers to a technical result, that is not necessarily im-
portant in its own right, but can be often very useful in proving other
theorems. A Claim is a ‚Äúthrow away‚Äù statement, that we need to use
in order to prove some other bigger results, but do not care so much
about for its own sake.
1.3.3 Proofs
Mathematical proofs are the arguments we use to demonstrate that our
theorems, lemmas, and claims area indeed true. We discuss proofs in
Section 1.5 below, but the main point is that the mathematical stan-
dard of proof is very high. Unlike in some other realms, in mathe-
matics a proof is an ‚Äúairtight‚Äù argument that demonstrates that the
statement is true beyond a shadow of a doubt. Some examples in this
section for mathematical proofs are given in Solved Exercise 1.1 and
Section 1.6. As mentioned in the preface, as a general rule, it is more
important you understand the definitions than the theorems, and it is
more important you understand a theorem statement than its proof.


--- Page 51 ---

mathematical background
51
1.4 BASIC DISCRETE MATH OBJECTS
In this section we quickly review some of the mathematical objects
(the ‚Äúbasic data structures‚Äù of mathematics, if you will) we use in this
book.
1.4.1 Sets
A set is an unordered collection of objects. For example, when we
write ùëÜ= {2, 4, 7}, we mean that ùëÜdenotes the set that contains the
numbers 2, 4, and 7. (We use the notation ‚Äú2 ‚ààùëÜ‚Äù to denote that 2 is
an element of ùëÜ.) Note that the set {2, 4, 7} and {7, 4, 2} are identical,
since they contain the same elements. Also, a set either contains an
element or does not contain it ‚Äì there is no notion of containing it
‚Äútwice‚Äù ‚Äì and so we could even write the same set ùëÜas {2, 2, 4, 7}
(though that would be a little weird). The cardinality of a finite set ùëÜ,
denoted by |ùëÜ|, is the number of elements it contains. (Cardinality can
be defined for infinite sets as well; see the sources in Section 1.9.) So,
in the example above, |ùëÜ| = 3. A set ùëÜis a subset of a set ùëá, denoted
by ùëÜ‚äÜùëá, if every element of ùëÜis also an element of ùëá. (We can
also describe this by saying that ùëáis a superset of ùëÜ.) For example,
{2, 7} ‚äÜ{2, 4, 7}. The set that contains no elements is known as the
empty set and it is denoted by ‚àÖ. If ùê¥is a subset of ùêµthat is not equal
to ùêµwe say that ùê¥is a strict subset of ùêµ, and denote this by ùê¥‚ääùêµ.
We can define sets by either listing all their elements or by writing
down a rule that they satisfy such as
EVEN = {ùë•| ùë•= 2ùë¶for some non-negative integer ùë¶} .
(1.1)
Of course there is more than one way to write the same set, and of-
ten we will use intuitive notation listing a few examples that illustrate
the rule. For example, we can also define EVEN as
EVEN = {0, 2, 4, ‚Ä¶} .
(1.2)
Note that a set can be either finite (such as the set {2, 4, 7}) or in-
finite (such as the set EVEN). Also, the elements of a set don‚Äôt have
to be numbers. We can talk about the sets such as the set {ùëé, ùëí, ùëñ, ùëú, ùë¢}
of all the vowels in the English language, or the set {New York, Los
Angeles, Chicago, Houston, Philadelphia, Phoenix, San Antonio,
San Diego, Dallas} of all cities in the U.S. with population more than
one million per the 2010 census. A set can even have other sets as ele-
ments, such as the set {‚àÖ, {1, 2}, {2, 3}, {1, 3}} of all even-sized subsets
of {1, 2, 3}.
Operations on sets:
The union of two sets ùëÜ, ùëá, denoted by ùëÜ‚à™ùëá,
is the set that contains all elements that are either in ùëÜor in ùëá. The
intersection of ùëÜand ùëá, denoted by ùëÜ‚à©ùëá, is the set of elements that are


--- Page 52 ---

52
introduction to theoretical computer science
1 The letter Z stands for the German word ‚ÄúZahlen‚Äù,
which means numbers.
both in ùëÜand in ùëá. The set difference of ùëÜand ùëá, denoted by ùëÜ‚ßµùëá(and
in some texts also by ùëÜ‚àíùëá), is the set of elements that are in ùëÜbut not
in ùëá.
Tuples, lists, strings, sequences:
A tuple is an ordered collection of items.
For example (1, 5, 2, 1) is a tuple with four elements (also known as
a 4-tuple or quadruple). Since order matters, this is not the same
tuple as the 4-tuple (1, 1, 5, 2) or the 3-tuple (1, 5, 2). A 2-tuple is also
known as a pair. We use the terms tuples and lists interchangeably.
A tuple where every element comes from some finite set Œ£ (such as
{0, 1}) is also known as a string. Analogously to sets, we denote the
length of a tuple ùëáby |ùëá|. Just like sets, we can also think of infinite
analogues of tuples, such as the ordered collection (1, 4, 9, ‚Ä¶) of all
perfect squares. Infinite ordered collections are known as sequences;
we might sometimes use the term ‚Äúinfinite sequence‚Äù to emphasize
this, and use ‚Äúfinite sequence‚Äù as a synonym for a tuple. (We can
identify a sequence (ùëé0, ùëé1, ùëé2, ‚Ä¶) of elements in some set ùëÜwith a
function ùê¥‚à∂‚Ñï‚ÜíùëÜ(where ùëéùëõ= ùê¥(ùëõ) for every ùëõ‚àà‚Ñï). Similarly,
we can identify a ùëò-tuple (ùëé0, ‚Ä¶ , ùëéùëò‚àí1) of elements in ùëÜwith a function
ùê¥‚à∂[ùëò] ‚ÜíùëÜ.)
Cartesian product:
If ùëÜand ùëáare sets, then their Cartesian product,
denoted by ùëÜ√ó ùëá, is the set of all ordered pairs (ùë†, ùë°) where ùë†‚ààùëÜand
ùë°‚ààùëá. For example, if ùëÜ= {1, 2, 3} and ùëá= {10, 12}, then ùëÜ√ó ùëá
contains the 6 elements (1, 10), (2, 10), (3, 10), (1, 12), (2, 12), (3, 12).
Similarly if ùëÜ, ùëá, ùëàare sets then ùëÜ√ó ùëá√ó ùëàis the set of all ordered
triples (ùë†, ùë°, ùë¢) where ùë†‚ààùëÜ, ùë°‚ààùëá, and ùë¢‚ààùëà. More generally, for
every positive integer ùëõand sets ùëÜ0, ‚Ä¶ , ùëÜùëõ‚àí1, we denote by ùëÜ0 √ó ùëÜ1 √ó
‚ãØ√ó ùëÜùëõ‚àí1 the set of ordered ùëõ-tuples (ùë†0, ‚Ä¶ , ùë†ùëõ‚àí1) where ùë†ùëñ‚ààùëÜùëñfor
every ùëñ‚àà{0, ‚Ä¶ , ùëõ‚àí1}. For every set ùëÜ, we denote the set ùëÜ√ó ùëÜby ùëÜ2,
ùëÜ√ó ùëÜ√ó ùëÜby ùëÜ3, ùëÜ√ó ùëÜ√ó ùëÜ√ó ùëÜby ùëÜ4, and so on and so forth.
1.4.2 Special sets
There are several sets that we will use in this book time and again. The
set
‚Ñï= {0, 1, 2, ‚Ä¶}
(1.3)
contains all natural numbers, i.e., non-negative integers. For any natural
number ùëõ‚àà‚Ñï, we define the set [ùëõ] as {0, ‚Ä¶ , ùëõ‚àí1} = {ùëò‚àà‚Ñï‚à∂
ùëò< ùëõ}. (We start our indexing of both ‚Ñïand [ùëõ] from 0, while many
other texts index those sets from 1. Starting from zero or one is simply
a convention that doesn‚Äôt make much difference, as long as one is
consistent about it.)
We will also occasionally use the set ‚Ñ§= {‚Ä¶ , ‚àí2, ‚àí1, 0, +1, +2, ‚Ä¶}
of (negative and non-negative) integers,1 as well as the set ‚Ñùof real


--- Page 53 ---

mathematical background
53
numbers. (This is the set that includes not just the integers, but also
fractional and irrational numbers; e.g., ‚Ñùcontains numbers such as
+0.5, ‚àíùúã, etc.) We denote by ‚Ñù+ the set {ùë•‚àà‚Ñù‚à∂ùë•> 0} of positive real
numbers. This set is sometimes also denoted as (0, ‚àû).
Strings:
Another set we will use time and again is
{0, 1}ùëõ= {(ùë•0, ‚Ä¶ , ùë•ùëõ‚àí1) ‚à∂ùë•0, ‚Ä¶ , ùë•ùëõ‚àí1 ‚àà{0, 1}}
(1.4)
which is the set of all ùëõ-length binary strings for some natural number
ùëõ. That is {0, 1}ùëõis the set of all ùëõ-tuples of zeroes and ones. This is
consistent with our notation above: {0, 1}2 is the Cartesian product
{0, 1} √ó {0, 1}, {0, 1}3 is the product {0, 1} √ó {0, 1} √ó {0, 1} and so on.
We will write the string (ùë•0, ùë•1, ‚Ä¶ , ùë•ùëõ‚àí1) as simply ùë•0ùë•1 ‚ãØùë•ùëõ‚àí1. For
example,
{0, 1}3 = {000, 001, 010, 011, 100, 101, 110, 111} .
(1.5)
For every string ùë•‚àà{0, 1}ùëõand ùëñ‚àà[ùëõ], we write ùë•ùëñfor the ùëñùë°‚Ñé
element of ùë•.
We will also often talk about the set of binary strings of all lengths,
which is
{0, 1}‚àó= {(ùë•0, ‚Ä¶ , ùë•ùëõ‚àí1) ‚à∂ùëõ‚àà‚Ñï, , ùë•0, ‚Ä¶ , ùë•ùëõ‚àí1 ‚àà{0, 1}} .
(1.6)
Another way to write this set is as
{0, 1}‚àó= {0, 1}0 ‚à™{0, 1}1 ‚à™{0, 1}2 ‚à™‚ãØ
(1.7)
or more concisely as
{0, 1}‚àó= ‚à™ùëõ‚àà‚Ñï{0, 1}ùëõ.
(1.8)
The set {0, 1}‚àóincludes the ‚Äústring of length 0‚Äù or ‚Äúthe empty
string‚Äù, which we will denote by "". (In using this notation we fol-
low the convention of many programming languages. Other texts
sometimes use ùúñor ùúÜto denote the empty string.)
Generalizing the star operation:
For every set Œ£, we define
Œ£‚àó= ‚à™ùëõ‚àà‚ÑïŒ£ùëõ.
(1.9)
For example, if Œ£ = {ùëé, ùëè, ùëê, ùëë, ‚Ä¶ , ùëß} then Œ£‚àódenotes the set of all finite
length strings over the alphabet a-z.
Concatenation:
The concatenation of two strings ùë•‚ààŒ£ùëõand ùë¶‚ààŒ£ùëöis
the (ùëõ+ ùëö)-length string ùë•ùë¶obtained by writing ùë¶after ùë•. That is, if
ùë•‚àà{0, 1}ùëõand ùë¶‚àà{0, 1}ùëö, then ùë•ùë¶is equal to the string ùëß‚àà{0, 1}ùëõ+ùëö
such that for ùëñ‚àà[ùëõ], ùëßùëñ= ùë•ùëñand for ùëñ‚àà{ùëõ, ‚Ä¶ , ùëõ+ ùëö‚àí1}, ùëßùëñ= ùë¶ùëñ‚àíùëõ.


--- Page 54 ---

54
introduction to theoretical computer science
2 For two natural numbers ùë•and ùëé, ùë•mod ùëé(short-
hand for ‚Äúmodulo‚Äù) denotes the remainder of ùë•
when it is divided by ùëé. That is, it is the number ùëüin
{0, ‚Ä¶ , ùëé‚àí1} such that ùë•= ùëéùëò+ ùëüfor some integer ùëò.
We sometimes also use the notation ùë•= ùë¶( mod ùëé)
to denote the assertion that ùë•mod ùëéis the same as ùë¶
mod ùëé.
1.4.3 Functions
If ùëÜand ùëáare nonempty sets, a function ùêπmapping ùëÜto ùëá, denoted
by ùêπ‚à∂ùëÜ‚Üíùëá, associates with every element ùë•‚ààùëÜan element
ùêπ(ùë•) ‚ààùëá. The set ùëÜis known as the domain of ùêπand the set ùëá
is known as the codomain of ùêπ. The image of a function ùêπis the set
{ùêπ(ùë•) | ùë•‚ààùëÜ} which is the subset of ùêπ‚Äôs codomain consisting of all
output elements that are mapped from some input. (Some texts use
range to denote the image of a function, while other texts use range
to denote the codomain of a function. Hence we will avoid using the
term ‚Äúrange‚Äù altogether.) As in the case of sets, we can write a func-
tion either by listing the table of all the values it gives for elements
in ùëÜor by using a rule. For example if ùëÜ= {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}
and ùëá= {0, 1}, then the table below defines a function ùêπ‚à∂ùëÜ‚Üíùëá.
Note that this function is the same as the function defined by the rule
ùêπ(ùë•) = (ùë•mod 2).2
Table 1.1: An example of a function.
Input
Output
0
0
1
1
2
0
3
1
4
0
5
1
6
0
7
1
8
0
9
1
If ùëì‚à∂ùëÜ‚Üíùëásatisfies that ùëì(ùë•) ‚â†ùëì(ùë¶) for all ùë•‚â†ùë¶then we say
that ùëìis one-to-one (Definition 1.1, also known as an injective function
or simply an injection). If ùêπsatisfies that for every ùë¶‚ààùëáthere is some
ùë•‚ààùëÜsuch that ùêπ(ùë•) = ùë¶then we say that ùêπis onto (also known as a
surjective function or simply a surjection). A function that is both one-
to-one and onto is known as a bijective function or simply a bijection.
A bijection from a set ùëÜto itself is also known as a permutation of ùëÜ. If
ùêπ‚à∂ùëÜ‚Üíùëáis a bijection then for every ùë¶‚ààùëáthere is a unique ùë•‚ààùëÜ
such that ùêπ(ùë•) = ùë¶. We denote this value ùë•by ùêπ‚àí1(ùë¶). Note that ùêπ‚àí1
is itself a bijection from ùëáto ùëÜ(can you see why?).
Giving a bijection between two sets is often a good way to show
they have the same size. In fact, the standard mathematical definition
of the notion that ‚ÄúùëÜand ùëáhave the same cardinality‚Äù is that there


--- Page 55 ---

mathematical background
55
Figure 1.4: We can represent finite functions as a
directed graph where we put an edge from ùë•to
ùëì(ùë•). The onto condition corresponds to requiring
that every vertex in the codomain of the function
has in-degree at least one. The one-to-one condition
corresponds to requiring that every vertex in the
codomain of the function has in-degree at most one. In
the examples above ùêπis an onto function, ùê∫is one to
one, and ùêªis neither onto nor one to one.
exists a bijection ùëì‚à∂ùëÜ‚Üíùëá. Further, the cardinality of a set ùëÜis
defined to be ùëõif there is a bijection from ùëÜto the set {0, ‚Ä¶ , ùëõ‚àí1}.
As we will see later in this book, this is a definition that generalizes to
defining the cardinality of infinite sets.
Partial functions:
We will sometimes be interested in partial functions
from ùëÜto ùëá. A partial function is allowed to be undefined on some
subset of ùëÜ. That is, if ùêπis a partial function from ùëÜto ùëá, then for
every ùë†‚ààùëÜ, either there is (as in the case of standard functions) an
element ùêπ(ùë†) in ùëá, or ùêπ(ùë†) is undefined. For example, the partial func-
tion ùêπ(ùë•) = ‚àöùë•is only defined on non-negative real numbers. When
we want to distinguish between partial functions and standard (i.e.,
non-partial) functions, we will call the latter total functions. When we
say ‚Äúfunction‚Äù without any qualifier then we mean a total function.
The notion of partial functions is a strict generalization of func-
tions, and so every function is a partial function, but not every partial
function is a function. (That is, for every nonempty ùëÜand ùëá, the set
of partial functions from ùëÜto ùëáis a proper superset of the set of total
functions from ùëÜto ùëá.) When we want to emphasize that a function
ùëìfrom ùê¥to ùêµmight not be total, we will write ùëì‚à∂ùê¥‚Üíùëùùêµ. We can
think of a partial function ùêπfrom ùëÜto ùëáalso as a total function from
ùëÜto ùëá‚à™{‚ä•} where ‚ä•is a special ‚Äúfailure symbol‚Äù. So, instead of
saying that ùêπis undefined at ùë•, we can say that ùêπ(ùë•) = ‚ä•.
Basic facts about functions:
Verifying that you can prove the following
results is an excellent way to brush up on functions:
‚Ä¢ If ùêπ‚à∂ùëÜ‚Üíùëáand ùê∫‚à∂ùëá‚Üíùëàare one-to-one functions, then their
composition ùêª‚à∂ùëÜ‚Üíùëàdefined as ùêª(ùë†) = ùê∫(ùêπ(ùë†)) is also one to
one.
‚Ä¢ If ùêπ‚à∂ùëÜ‚Üíùëáis one to one, then there exists an onto function
ùê∫‚à∂ùëá‚ÜíùëÜsuch that ùê∫(ùêπ(ùë†)) = ùë†for every ùë†‚ààùëÜ.
‚Ä¢ If ùê∫‚à∂ùëá‚ÜíùëÜis onto then there exists a one-to-one function ùêπ‚à∂ùëÜ‚Üí
ùëásuch that ùê∫(ùêπ(ùë†)) = ùë†for every ùë†‚ààùëÜ.
‚Ä¢ If ùëÜand ùëáare finite sets then the following conditions are equiva-
lent to one another: (a) |ùëÜ| ‚â§|ùëá|, (b) there is a one-to-one function
ùêπ‚à∂ùëÜ‚Üíùëá, and (c) there is an onto function ùê∫‚à∂ùëá‚ÜíùëÜ. (This is
actually true even for infinite ùëÜand ùëá: in that case (b) (or equiva-
lently (c)) is the commonly accepted definition for |ùëÜ| ‚â§|ùëá|.)
P
You can find the proofs of these results in many dis-
crete math texts, including for example, Section 4.5
in the Lehman-Leighton-Meyer notes. However, I


--- Page 56 ---

56
introduction to theoretical computer science
3 It is possible, and sometimes useful, to think of an
undirected graph as the special case of a directed
graph that has the special property that for every pair
ùë¢, ùë£either both the edges (ùë¢, ùë£) and (ùë£, ùë¢) are present
or neither of them is. However, in many settings there
is a significant difference between undirected and
directed graphs, and so it‚Äôs typically best to think of
them as separate categories.
Figure 1.5: An example of an undirected and a di-
rected graph. The undirected graph has vertex set
{1, 2, 3, 4} and edge set {{1, 2}, {2, 3}, {2, 4}}. The
directed graph has vertex set {ùëé, ùëè, ùëê} and the edge
set {(ùëé, ùëè), (ùëè, ùëê), (ùëê, ùëé), (ùëé, ùëê)}.
strongly suggest you try to prove them on your own,
or at least convince yourself that they are true by
proving special cases of those for small sizes (e.g.,
|ùëÜ| = 3, |ùëá| = 4, |ùëà| = 5).
Let us prove one of these facts as an example:
Lemma 1.2 If ùëÜ, ùëáare non-empty sets and ùêπ‚à∂ùëÜ‚Üíùëáis one to one, then
there exists an onto function ùê∫‚à∂ùëá‚ÜíùëÜsuch that ùê∫(ùêπ(ùë†)) = ùë†for
every ùë†‚ààùëÜ.
Proof. Choose some ùë†0 ‚ààùëÜ. We will define the function ùê∫‚à∂ùëá‚ÜíùëÜas
follows: for every ùë°‚ààùëá, if there is some ùë†‚ààùëÜsuch that ùêπ(ùë†) = ùë°then
set ùê∫(ùë°) = ùë†(the choice of ùë†is well defined since by the one-to-one
property of ùêπ, there cannot be two distinct ùë†, ùë†‚Ä≤ that both map to ùë°).
Otherwise, set ùê∫(ùë°) = ùë†0. Now for every ùë†‚ààùëÜ, by the definition of ùê∫,
if ùë°= ùêπ(ùë†) then ùê∫(ùë°) = ùê∫(ùêπ(ùë†)) = ùë†. Moreover, this also shows that
ùê∫is onto, since it means that for every ùë†‚ààùëÜthere is some ùë°, namely
ùë°= ùêπ(ùë†), such that ùê∫(ùë°) = ùë†.
‚ñ†
1.4.4 Graphs
Graphs are ubiquitous in Computer Science, and many other fields as
well. They are used to model a variety of data types including social
networks, scheduling constraints, road networks, deep neural nets,
gene interactions, correlations between observations, and a great
many more. Formal definitions of several kinds of graphs are given
next, but if you have not seen graphs before in a course, I urge you to
read up on them in one of the sources mentioned in Section 1.9.
Graphs come in two basic flavors: undirected and directed.3
Definition 1.3 ‚Äî Undirected graphs. An undirected graph ùê∫= (ùëâ, ùê∏) con-
sists of a set ùëâof vertices and a set ùê∏of edges. Every edge is a size
two subset of ùëâ. We say that two vertices ùë¢, ùë£‚ààùëâare neighbors, if
the edge {ùë¢, ùë£} is in ùê∏.
Given this definition, we can define several other properties of
graphs and their vertices. We define the degree of ùë¢to be the number
of neighbors ùë¢has. A path in the graph is a tuple (ùë¢0, ‚Ä¶ , ùë¢ùëò) ‚ààùëâùëò+1,
for some ùëò> 0 such that ùë¢ùëñ+1 is a neighbor of ùë¢ùëñfor every ùëñ‚àà[ùëò]. A
simple path is a path (ùë¢0, ‚Ä¶ , ùë¢ùëò‚àí1) where all the ùë¢ùëñ‚Äôs are distinct. A cycle
is a path (ùë¢0, ‚Ä¶ , ùë¢ùëò) where ùë¢0 = ùë¢ùëò. We say that two vertices ùë¢, ùë£‚ààùëâ
are connected if either ùë¢= ùë£or there is a path from (ùë¢0, ‚Ä¶ , ùë¢ùëò) where
ùë¢0 = ùë¢and ùë¢ùëò= ùë£. We say that the graph ùê∫is connected if every pair of
vertices in it is connected.


--- Page 57 ---

mathematical background
57
Here are some basic facts about undirected graphs. We give some
informal arguments below, but leave the full proofs as exercises (the
proofs can be found in many of the resources listed in Section 1.9).
Lemma 1.4 In any undirected graph ùê∫= (ùëâ, ùê∏), the sum of the degrees
of all vertices is equal to twice the number of edges.
Lemma 1.4 can be shown by seeing that every edge {ùë¢, ùë£} con-
tributes twice to the sum of the degrees (once for ùë¢and the second
time for ùë£).
Lemma 1.5 The connectivity relation is transitive, in the sense that if ùë¢is
connected to ùë£, and ùë£is connected to ùë§, then ùë¢is connected to ùë§.
Lemma 1.5 can be shown by simply attaching a path of the form
(ùë¢, ùë¢1, ùë¢2, ‚Ä¶ , ùë¢ùëò‚àí1, ùë£) to a path of the form (ùë£, ùë¢‚Ä≤
1, ‚Ä¶ , ùë¢‚Ä≤
ùëò‚Ä≤‚àí1, ùë§) to obtain
the path (ùë¢, ùë¢1, ‚Ä¶ , ùë¢ùëò‚àí1, ùë£, ùë¢‚Ä≤
1, ‚Ä¶ , ùë¢‚Ä≤
ùëò‚Ä≤‚àí1, ùë§) that connects ùë¢to ùë§.
Lemma 1.6 For every undirected graph ùê∫= (ùëâ, ùê∏) and connected pair
ùë¢, ùë£, the shortest path from ùë¢to ùë£is simple. In particular, for every
connected pair there exists a simple path that connects them.
Lemma 1.6 can be shown by ‚Äúshortcutting‚Äù any non simple path
from ùë¢to ùë£where the same vertex ùë§appears twice to remove it (see
Fig. 1.6). It is a good exercise to transforming this intuitive reasoning
to a formal proof:
Figure 1.6: If there is a path from ùë¢to ùë£in a graph
that passes twice through a vertex ùë§then we can
‚Äúshortcut‚Äù it by removing the loop from ùë§to itself to
find a path from ùë¢to ùë£that only passes once through
ùë§.
Solved Exercise 1.1 ‚Äî Connected vertices have simple paths. Prove Lemma 1.6
‚ñ†
Solution:
The proof follows the idea illustrated in Fig. 1.6. One complica-
tion is that there can be more than one vertex that is visited twice
by a path, and so ‚Äúshortcutting‚Äù might not necessarily result in a


--- Page 58 ---

58
introduction to theoretical computer science
simple path; we deal with this by looking at a shortest path between
ùë¢and ùë£. Details follow.
Let ùê∫
=
(ùëâ, ùê∏) be a graph and ùë¢and ùë£in ùëâbe two connected
vertices in ùê∫. We will prove that there is a simple graph between
ùë¢and ùë£. Let ùëòbe the shortest length of a path between ùë¢and ùë£
and let ùëÉ
= (ùë¢0, ùë¢1, ùë¢2, ‚Ä¶ , ùë¢ùëò‚àí1, ùë¢ùëò) be a ùëò-length path from ùë¢to ùë£
(there can be more than one such path: if so we just choose one of
them). (That is ùë¢0 = ùë¢, ùë¢ùëò= ùë£, and (ùë¢‚Ñì, ùë¢‚Ñì+1) ‚ààùê∏for all ‚Ñì‚àà[ùëò].)
We claim that ùëÉis simple. Indeed, suppose otherwise that there is
some vertex ùë§that occurs twice in the path: ùë§= ùë¢ùëñand ùë§= ùë¢ùëófor
some ùëñ< ùëó. Then we can ‚Äúshortcut‚Äù the path ùëÉby considering the
path ùëÉ‚Ä≤
=
(ùë¢0, ùë¢1, ‚Ä¶ , ùë¢ùëñ‚àí1, ùë§, ùë¢ùëó+1, ‚Ä¶ , ùë¢ùëò) obtained by taking the
first ùëñvertices of ùëÉ(from ùë¢0
=
0 to the first occurrence of ùë§) and
the last ùëò
‚àí
ùëóones (from the vertex ùë¢ùëó+1 following the second oc-
currence of ùë§to ùë¢ùëò= ùë£). The path ùëÉ‚Ä≤ is a valid path between ùë¢and
ùë£since every consecutive pair of vertices in it is connected by an
edge (in particular, since ùë§= ùë¢ùëñ= ùë§ùëó, both (ùë¢ùëñ‚àí1, ùë§) and (ùë§, ùë¢ùëó+1)
are edges in ùê∏), but since the length of ùëÉ‚Ä≤ is ùëò‚àí(ùëó‚àíùëñ) < ùëò, this
contradicts the minimality of ùëÉ.
‚ñ†
R
Remark 1.7 ‚Äî Finding proofs. Solved Exercise 1.1 is a
good example of the process of finding a proof. You
start by ensuring you understand what the statement
means, and then come up with an informal argument
why it should be true. You then transform the infor-
mal argument into a rigorous proof. This proof need
not be very long or overly formal, but should clearly
establish why the conclusion of the statement follows
from its assumptions.
The concepts of degrees and connectivity extend naturally to di-
rected graphs, defined as follows.
Definition 1.8 ‚Äî Directed graphs. A directed graph ùê∫
=
(ùëâ, ùê∏) consists
of a set ùëâand a set ùê∏‚äÜùëâ√ó ùëâof ordered pairs of ùëâ. We sometimes
denote the edge (ùë¢, ùë£) also as ùë¢‚Üíùë£. If the edge ùë¢‚Üíùë£is present
in the graph then we say that ùë£is an out-neighbor of ùë¢and ùë¢is an
in-neighbor of ùë£.
A directed graph might contain both ùë¢‚Üíùë£and ùë£‚Üíùë¢in which
case ùë¢will be both an in-neighbor and an out-neighbor of ùë£and vice
versa. The in-degree of ùë¢is the number of in-neighbors it has, and the
out-degree of ùë£is the number of out-neighbors it has. A path in the


--- Page 59 ---

mathematical background
59
graph is a tuple (ùë¢0, ‚Ä¶ , ùë¢ùëò) ‚ààùëâùëò+1, for some ùëò> 0 such that ùë¢ùëñ+1 is an
out-neighbor of ùë¢ùëñfor every ùëñ‚àà[ùëò]. As in the undirected case, a simple
path is a path (ùë¢0, ‚Ä¶ , ùë¢ùëò‚àí1) where all the ùë¢ùëñ‚Äôs are distinct and a cycle
is a path (ùë¢0, ‚Ä¶ , ùë¢ùëò) where ùë¢0 = ùë¢ùëò. One type of directed graphs we
often care about is directed acyclic graphs or DAGs, which, as their name
implies, are directed graphs without any cycles:
Definition 1.9 ‚Äî Directed Acyclic Graphs. We say that ùê∫
=
(ùëâ, ùê∏) is a
directed acyclic graph (DAG) if it is a directed graph and there does
not exist a list of vertices ùë¢0, ùë¢1, ‚Ä¶ , ùë¢ùëò‚ààùëâsuch that ùë¢0 = ùë¢ùëòand
for every ùëñ‚àà[ùëò], the edge ùë¢ùëñ‚Üíùë¢ùëñ+1 is in ùê∏.
The lemmas we mentioned above have analogs for directed graphs.
We again leave the proofs (which are essentially identical to their
undirected analogs) as exercises.
Lemma 1.10 In any directed graph ùê∫= (ùëâ, ùê∏), the sum of the in-
degrees is equal to the sum of the out-degrees, which is equal to the
number of edges.
Lemma 1.11 In any directed graph ùê∫, if there is a path from ùë¢to ùë£and a
path from ùë£to ùë§, then there is a path from ùë¢to ùë§.
Lemma 1.12 For every directed graph ùê∫= (ùëâ, ùê∏) and a pair ùë¢, ùë£such
that there is a path from ùë¢to ùë£, the shortest path from ùë¢to ùë£is simple.
R
Remark 1.13 ‚Äî Labeled graphs. For some applications
we will consider labeled graphs, where the vertices or
edges have associated labels (which can be numbers,
strings, or members of some other set). We can think
of such a graph as having an associated (possibly
partial) labelling function ùêø‚à∂ùëâ‚à™ùê∏‚Üí‚Ñí, where ‚Ñíis
the set of potential labels. However we will typically
not refer explicitly to this labeling function and simply
say things such as ‚Äúvertex ùë£has the label ùõº‚Äù.
1.4.5 Logic operators and quantifiers
If ùëÉand ùëÑare some statements that can be true or false, then ùëÉAND
ùëÑ(denoted as ùëÉ‚àßùëÑ) is a statement that is true if and only if both ùëÉ
and ùëÑare true, and ùëÉOR ùëÑ(denoted as ùëÉ‚à®ùëÑ) is a statement that is
true if and only if either ùëÉor ùëÑis true. The negation of ùëÉ, denoted as
¬¨ùëÉor ùëÉ, is true if and only if ùëÉis false.
Suppose that ùëÉ(ùë•) is a statement that depends on some parameter ùë•
(also sometimes known as an unbound variable) in the sense that for
every instantiation of ùë•with a value from some set ùëÜ, ùëÉ(ùë•) is either
true or false. For example, ùë•> 7 is a statement that is not a priori


--- Page 60 ---

60
introduction to theoretical computer science
4 In this book, we place the variable bound by a quan-
tifier in a subscript and so write ‚àÄùë•‚ààùëÜùëÉ(ùë•). Many
other texts do not use this subscript notation and so
will write the same statement as ‚àÄùë•‚ààùëÜ, ùëÉ(ùë•).
true or false, but becomes true or false whenever we instantiate ùë•with
some real number. We denote by ‚àÄùë•‚ààùëÜùëÉ(ùë•) the statement that is true
if and only if ùëÉ(ùë•) is true for every ùë•‚ààùëÜ.4 We denote by ‚àÉùë•‚ààùëÜùëÉ(ùë•) the
statement that is true if and only if there exists some ùë•‚ààùëÜsuch that
ùëÉ(ùë•) is true.
For example, the following is a formalization of the true statement
that there exists a natural number ùëõlarger than 100 that is not divisi-
ble by 3:
‚àÉùëõ‚àà‚Ñï(ùëõ> 100) ‚àß(‚àÄùëò‚ààùëÅùëò+ ùëò+ ùëò‚â†ùëõ) .
(1.10)
‚ÄùFor sufficiently large n.‚Äù
One expression that we will see come up
time and again in this book is the claim that some statement ùëÉ(ùëõ) is
true ‚Äúfor sufficiently large ùëõ‚Äù. What this means is that there exists an
integer ùëÅ0 such that ùëÉ(ùëõ) is true for every ùëõ> ùëÅ0. We can formalize
this as ‚àÉùëÅ0‚àà‚Ñï‚àÄùëõ>ùëÅ0ùëÉ(ùëõ).
1.4.6 Quantifiers for summations and products
The following shorthands for summing up or taking products of sev-
eral numbers are often convenient. If ùëÜ= {ùë†0, ‚Ä¶ , ùë†ùëõ‚àí1} is a finite set
and ùëì‚à∂ùëÜ‚Üí‚Ñùis a function, then we write ‚àëùë•‚ààùëÜùëì(ùë•) as shorthand for
ùëì(ùë†0) + ùëì(ùë†1) + ùëì(ùë†2) + ‚Ä¶ + ùëì(ùë†ùëõ‚àí1) ,
(1.11)
and ‚àèùë•‚ààùëÜùëì(ùë•) as shorthand for
ùëì(ùë†0) ‚ãÖùëì(ùë†1) ‚ãÖùëì(ùë†2) ‚ãÖ‚Ä¶ ‚ãÖùëì(ùë†ùëõ‚àí1) .
(1.12)
For example, the sum of the squares of all numbers from 1 to 100
can be written as
‚àë
ùëñ‚àà{1,‚Ä¶,100}
ùëñ2 .
(1.13)
Since summing up over intervals of integers is so common, there
is a special notation for it. For every two integers, ùëé‚â§ùëè, ‚àë
ùëè
ùëñ=ùëéùëì(ùëñ)
denotes ‚àëùëñ‚ààùëÜùëì(ùëñ) where ùëÜ= {ùë•‚àà‚Ñ§‚à∂ùëé‚â§ùë•‚â§ùëè}. Hence, we can
write the sum (1.13) as
100
‚àë
ùëñ=1
ùëñ2 .
(1.14)
1.4.7 Parsing formulas: bound and free variables
In mathematics, as in coding, we often have symbolic ‚Äúvariables‚Äù or
‚Äúparameters‚Äù. It is important to be able to understand, given some
formula, whether a given variable is bound or free in this formula. For


--- Page 61 ---

mathematical background
61
example, in the following statement ùëõis free but ùëéand ùëèare bound by
the ‚àÉquantifier:
‚àÉùëé,ùëè‚àà‚Ñï(ùëé‚â†1) ‚àß(ùëé‚â†ùëõ) ‚àß(ùëõ= ùëé√ó ùëè)
(1.15)
Since ùëõis free, it can be set to any value, and the truth of the state-
ment (1.15) depends on the value of ùëõ. For example, if ùëõ= 8 then
(1.15) is true, but for ùëõ= 11 it is false. (Can you see why?)
The same issue appears when parsing code. For example, in the
following snippet from the C programming language
for (int i=0 ; i<n ; i=i+1) {
printf("*");
}
the variable i is bound within the for block but the variable n is
free.
The main property of bound variables is that we can rename them
(as long as the new name doesn‚Äôt conflict with another used variable)
without changing the meaning of the statement. Thus for example the
statement
‚àÉùë•,ùë¶‚àà‚Ñï(ùë•‚â†1) ‚àß(ùë•‚â†ùëõ) ‚àß(ùëõ= ùë•√ó ùë¶)
(1.16)
is equivalent to (1.15) in the sense that it is true for exactly the same
set of ùëõ‚Äôs.
Similarly, the code
for (int j=0 ; j<n ; j=j+1) {
printf("*");
}
produces the same result as the code above that used i instead of j.
R
Remark 1.14 ‚Äî Aside: mathematical vs programming no-
tation. Mathematical notation has a lot of similarities
with programming language, and for the same rea-
sons. Both are formalisms meant to convey complex
concepts in a precise way. However, there are some
cultural differences. In programming languages, we
often try to use meaningful variable names such as
NumberOfVertices while in math we often use short
identifiers such as ùëõ. Part of it might have to do with
the tradition of mathematical proofs as being hand-
written and verbally presented, as opposed to typed
up and compiled. Another reason is if the wrong
variable name is used in a proof, at worst is causes
confusion to readers; when the wrong variable name


--- Page 62 ---

62
introduction to theoretical computer science
is used in a program, planes might crash, patients
might die, and rockets could explode.
One consequence of that is that in mathematics we
often end up reusing identifiers, and also ‚Äúrun out‚Äù
of letters and hence use Greek letters too, as well as
distinguish between small and capital letters and
different font faces. Similarly, mathematical notation
tends to use quite a lot of ‚Äúoverloading‚Äù, using oper-
ators such as + for a great variety of objects (e.g., real
numbers, matrices, finite field elements, etc..), and
assuming that the meaning can be inferred from the
context.
Both fields have a notion of ‚Äútypes‚Äù, and in math
we often try to reserve certain letters for variables
of a particular type. For example, variables such as
ùëñ, ùëó, ùëò, ‚Ñì, ùëö, ùëõwill often denote integers, and ùúñwill
often denote a small positive real number (see Sec-
tion 1.7 for more on these conventions). When reading
or writing mathematical texts, we usually don‚Äôt have
the advantage of a ‚Äúcompiler‚Äù that will check type
safety for us. Hence it is important to keep track of the
type of each variable, and see that the operations that
are performed on it ‚Äúmake sense‚Äù.
Kun‚Äôs book [Kun18] contains an extensive discus-
sion on the similarities and differences between the
cultures of mathematics and programming.
1.4.8 Asymptotics and Big-ùëÇnotation
‚Äúlog log log ùëõhas been proved to go to infinity, but has never been observed to
do so.‚Äù, Anonymous, quoted by Carl Pomerance (2000)
It is often very cumbersome to describe precisely quantities such
as running time and is also not needed, since we are typically mostly
interested in the ‚Äúhigher order terms‚Äù. That is, we want to understand
the scaling behavior of the quantity as the input variable grows. For
example, as far as running time goes, the difference between an ùëõ5-
time algorithm and an ùëõ2-time one is much more significant than the
difference between an 100ùëõ2 + 10ùëõtime algorithm and an 10ùëõ2 time
algorithm. For this purpose, ùëÇ-notation is extremely useful as a way
to ‚Äúdeclutter‚Äù our text and focus our attention on what really matters.
For example, using ùëÇ-notation, we can say that both 100ùëõ2 + 10ùëõ
and 10ùëõ2 are simply Œò(ùëõ2) (which informally means ‚Äúthe same up to
constant factors‚Äù), while ùëõ2 = ùëú(ùëõ5) (which informally means that ùëõ2
is ‚Äúmuch smaller than‚Äù ùëõ5).
Generally (though still informally), if ùêπ, ùê∫are two functions map-
ping natural numbers to non-negative reals, then ‚Äúùêπ= ùëÇ(ùê∫)‚Äù means
that ùêπ(ùëõ) ‚â§ùê∫(ùëõ) if we don‚Äôt care about constant factors, while
‚Äúùêπ= ùëú(ùê∫)‚Äù means that ùêπis much smaller than ùê∫, in the sense that no
matter by what constant factor we multiply ùêπ, if we take ùëõto be large


--- Page 63 ---

mathematical background
63
Figure 1.7: If ùêπ(ùëõ) = ùëú(ùê∫(ùëõ)) then for sufficiently
large ùëõ, ùêπ(ùëõ) will be smaller than ùê∫(ùëõ). For example,
if Algorithm ùê¥runs in time 1000 ‚ãÖùëõ+ 106 and
Algorithm ùêµruns in time 0.01 ‚ãÖùëõ2 then even though
ùêµmight be more efficient for smaller inputs, when
the inputs get sufficiently large, ùê¥will run much faster
than ùêµ.
enough then ùê∫will be bigger (for this reason, sometimes ùêπ= ùëú(ùê∫)
is written as ùêπ‚â™ùê∫). We will write ùêπ= Œò(ùê∫) if ùêπ= ùëÇ(ùê∫) and
ùê∫= ùëÇ(ùêπ), which one can think of as saying that ùêπis the same as ùê∫if
we don‚Äôt care about constant factors. More formally, we define Big-ùëÇ
notation as follows:
Definition 1.15 ‚Äî Big-ùëÇnotation. Let ‚Ñù+ = {ùë•‚àà‚Ñù| ùë•> 0} be the set
of positive real numbers. For two functions ùêπ, ùê∫‚à∂‚Ñï‚Üí‚Ñù+, we say
that ùêπ= ùëÇ(ùê∫) if there exist numbers ùëé, ùëÅ0 ‚àà‚Ñïsuch that ùêπ(ùëõ) ‚â§
ùëé‚ãÖùê∫(ùëõ) for every ùëõ> ùëÅ0. We say that ùêπ= Œò(ùê∫) if ùêπ= ùëÇ(ùê∫) and
ùê∫= ùëÇ(ùêπ). We say that ùêπ= Œ©(ùê∫) if ùê∫= ùëÇ(ùêπ).
We say that ùêπ
= ùëú(ùê∫) if for every ùúñ> 0 there is some ùëÅ0 such
that ùêπ(ùëõ) < ùúñùê∫(ùëõ) for every ùëõ> ùëÅ0. We say that ùêπ
= ùúî(ùê∫) if
ùê∫= ùëú(ùêπ).
It‚Äôs often convenient to use ‚Äúanonymous functions‚Äù in the context of
ùëÇ-notation. For example, when we write a statement such as ùêπ(ùëõ) =
ùëÇ(ùëõ3), we mean that ùêπ= ùëÇ(ùê∫) where ùê∫is the function defined by
ùê∫(ùëõ) = ùëõ3. Chapter 7 in Jim Apsnes‚Äô notes on discrete math provides
a good summary of ùëÇnotation; see also this tutorial for a gentler and
more programmer-oriented introduction.
ùëÇis not equality. Using the equality sign for ùëÇ-notation is extremely
common, but is somewhat of a misnomer, since a statement such as
ùêπ= ùëÇ(ùê∫) really means that ùêπis in the set {ùê∫‚Ä≤ ‚à∂‚àÉùëÅ,ùëês.t. ‚àÄùëõ>ùëÅùê∫‚Ä≤(ùëõ) ‚â§
ùëêùê∫(ùëõ)}. If anything, it makes more sense to use inequalities and write
ùêπ‚â§ùëÇ(ùê∫) and ùêπ‚â•Œ©(ùê∫), reserving equality for ùêπ= Œò(ùê∫), and
so we will sometimes use this notation too, but since the equality
notation is quite firmly entrenched we often stick to it as well. (Some
texts write ùêπ‚ààùëÇ(ùê∫) instead of ùêπ= ùëÇ(ùê∫), but we will not use this
notation.) Despite the misleading equality sign, you should remember
that a statement such as ùêπ= ùëÇ(ùê∫) means that ùêπis ‚Äúat most‚Äù ùê∫in
some rough sense when we ignore constants, and a statement such as
ùêπ= Œ©(ùê∫) means that ùêπis ‚Äúat least‚Äù ùê∫in the same rough sense.
1.4.9 Some ‚Äúrules of thumb‚Äù for Big-ùëÇnotation
There are some simple heuristics that can help when trying to com-
pare two functions ùêπand ùê∫:
‚Ä¢ Multiplicative constants don‚Äôt matter in ùëÇ-notation, and so if
ùêπ(ùëõ) = ùëÇ(ùê∫(ùëõ)) then 100ùêπ(ùëõ) = ùëÇ(ùê∫(ùëõ)).
‚Ä¢ When adding two functions, we only care about the larger one. For
example, for the purpose of ùëÇ-notation, ùëõ3 + 100ùëõ2 is the same as
ùëõ3, and in general in any polynomial, we only care about the larger
exponent.


--- Page 64 ---

64
introduction to theoretical computer science
‚Ä¢ For every two constants ùëé, ùëè> 0, ùëõùëé= ùëÇ(ùëõùëè) if and only if ùëé‚â§ùëè,
and ùëõùëé= ùëú(ùëõùëè) if and only if ùëé< ùëè. For example, combining the two
observations above, 100ùëõ2 + 10ùëõ+ 100 = ùëú(ùëõ3).
‚Ä¢ Polynomial is always smaller than exponential: ùëõùëé= ùëú(2ùëõùúñ) for
every two constants ùëé> 0 and ùúñ> 0 even if ùúñis much smaller than
ùëé. For example, 100ùëõ100 = ùëú(2
‚àöùëõ).
‚Ä¢ Similarly, logarithmic is always smaller than polynomial: (log ùëõ)ùëé
(which we write as logùëéùëõ) is ùëú(ùëõùúñ) for every two constants ùëé, ùúñ> 0.
For example, combining the observations above, 100ùëõ2 log100 ùëõ=
ùëú(ùëõ3).
R
Remark 1.16 ‚Äî Big ùëÇfor other applications (optional).
While Big-ùëÇnotation is often used to analyze running
time of algorithms, this is by no means the only ap-
plication. We can use ùëÇnotation to bound asymptotic
relations between any functions mapping integers
to positive numbers. It can be used regardless of
whether these functions are a measure of running
time, memory usage, or any other quantity that may
have nothing to do with computation. Here is one
example which is unrelated to this book (and hence
one that you can feel free to skip): one way to state the
Riemann Hypothesis (one of the most famous open
questions in mathematics) is that it corresponds to
the conjecture that the number of primes between 0
and ùëõis equal to ‚à´
ùëõ
2
1
ln ùë•ùëëùë•up to an additive error of
magnitude at most ùëÇ(‚àöùëõlog ùëõ).
1.5 PROOFS
Many people think of mathematical proofs as a sequence of logical
deductions that starts from some axioms and ultimately arrives at a
conclusion. In fact, some dictionaries define proofs that way. This is
not entirely wrong, but at its essence mathematical proof of a state-
ment X is simply an argument that convinces the reader that X is true
beyond a shadow of a doubt.
To produce such a proof you need to:
1. Understand precisely what X means.
2. Convince yourself that X is true.
3. Write your reasoning down in plain, precise and concise English
(using formulas or notation only when they help clarity).


--- Page 65 ---

mathematical background
65
In many cases, the first part is the most important one. Understand-
ing what a statement means is oftentimes more than halfway towards
understanding why it is true. In third part, to convince the reader
beyond a shadow of a doubt, we will often want to break down the
reasoning to ‚Äúbasic steps‚Äù, where each basic step is simple enough
to be ‚Äúself evident‚Äù. The combination of all steps yields the desired
statement.
1.5.1 Proofs and programs
There is a great deal of similarity between the process of writing proofs
and that of writing programs, and both require a similar set of skills.
Writing a program involves:
1. Understanding what is the task we want the program to achieve.
2. Convincing yourself that the task can be achieved by a computer,
perhaps by planning on a whiteboard or notepad how you will
break it up to simpler tasks.
3. Converting this plan into code that a compiler or interpreter can
understand, by breaking up each task into a sequence of the basic
operations of some programming language.
In programs as in proofs, step 1 is often the most important one.
A key difference is that the reader for proofs is a human being and
the reader for programs is a computer. (This difference is eroding
with time as more proofs are being written in a machine verifiable form;
moreover, to ensure correctness and maintainability of programs, it
is important that they can be read and understood by humans.) Thus
our emphasis is on readability and having a clear logical flow for our
proof (which is not a bad idea for programs as well). When writing a
proof, you should think of your audience as an intelligent but highly
skeptical and somewhat petty reader, that will ‚Äúcall foul‚Äù at every step
that is not well justified.
1.5.2 Proof writing style
A mathematical proof is a piece of writing, but it is a specific genre
of writing with certain conventions and preferred styles. As in any
writing, practice makes perfect, and it is also important to revise your
drafts for clarity.
In a proof for the statement ùëã, all the text between the words
‚ÄúProof:‚Äù and ‚ÄúQED‚Äù should be focused on establishing that ùëãis true.
Digressions, examples, or ruminations should be kept outside these
two words, so they do not confuse the reader. The proof should have
a clear logical flow in the sense that every sentence or equation in it
should have some purpose and it should be crystal-clear to the reader


--- Page 66 ---

66
introduction to theoretical computer science
what this purpose is. When you write a proof, for every equation or
sentence you include, ask yourself:
1. Is this sentence or equation stating that some statement is true?
2. If so, does this statement follow from the previous steps, or are we
going to establish it in the next step?
3. What is the role of this sentence or equation? Is it one step towards
proving the original statement, or is it a step towards proving some
intermediate claim that you have stated before?
4. Finally, would the answers to questions 1-3 be clear to the reader?
If not, then you should reorder, rephrase or add explanations.
Some helpful resources on mathematical writing include this hand-
out by Lee, this handout by Hutching, as well as several of the excel-
lent handouts in Stanford‚Äôs CS 103 class.
1.5.3 Patterns in proofs
‚ÄúIf it was so, it might be; and if it were so, it would be; but as it isn‚Äôt, it ain‚Äôt.
That‚Äôs logic.‚Äù, Lewis Carroll, Through the looking-glass.
Just like in programming, there are several common patterns of
proofs that occur time and again. Here are some examples:
Proofs by contradiction:
One way to prove that ùëãis true is to show
that if ùëãwas false it would result in a contradiction. Such proofs
often start with a sentence such as ‚ÄúSuppose, towards a contradiction,
that ùëãis false‚Äù and end with deriving some contradiction (such as a
violation of one of the assumptions in the theorem statement). Here is
an example:
Lemma 1.17 There are no natural numbers ùëé, ùëèsuch that
‚àö
2 = ùëé
ùëè.
Proof. Suppose, towards a contradiction that this is false, and so let
ùëé‚àà‚Ñïbe the smallest number such that there exists some ùëè‚àà‚Ñï
satisfying
‚àö
2 = ùëé
ùëè. Squaring this equation we get that 2 = ùëé2/ùëè2 or
ùëé2 = 2ùëè2 (‚àó). But this means that ùëé2 is even, and since the product of
two odd numbers is odd, it means that ùëéis even as well, or in other
words, ùëé= 2ùëé‚Ä≤ for some ùëé‚Ä≤ ‚àà‚Ñï. Yet plugging this into (‚àó) shows that
4ùëé‚Ä≤2 = 2ùëè2 which means ùëè2 = 2ùëé‚Ä≤2 is an even number as well. By the
same considerations as above we get that ùëèis even and hence ùëé/2 and
ùëè/2 are two natural numbers satisfying ùëé/2
ùëè/2 =
‚àö
2, contradicting the
minimality of ùëé.
‚ñ†


--- Page 67 ---

mathematical background
67
Proofs of a universal statement:
Often we want to prove a statement ùëãof
the form ‚ÄúEvery object of type ùëÇhas property ùëÉ.‚Äù Such proofs often
start with a sentence such as ‚ÄúLet ùëúbe an object of type ùëÇ‚Äù and end by
showing that ùëúhas the property ùëÉ. Here is a simple example:
Lemma 1.18 For every natural number ùëõ‚ààùëÅ, either ùëõor ùëõ+ 1 is even.
Proof. Let ùëõ‚ààùëÅbe some number. If ùëõ/2 is a whole number then
we are done, since then ùëõ= 2(ùëõ/2) and hence it is even. Otherwise,
ùëõ/2 + 1/2 is a whole number, and hence 2(ùëõ/2 + 1/2) = ùëõ+ 1 is even.
‚ñ†
Proofs of an implication:
Another common case is that the statement ùëã
has the form ‚Äúùê¥implies ùêµ‚Äù. Such proofs often start with a sentence
such as ‚ÄúAssume that ùê¥is true‚Äù and end with a derivation of ùêµfrom
ùê¥. Here is a simple example:
Lemma 1.19 If ùëè2 ‚â•4ùëéùëêthen there is a solution to the quadratic equa-
tion ùëéùë•2 + ùëèùë•+ ùëê= 0.
Proof. Suppose that ùëè2 ‚â•4ùëéùëê. Then ùëë= ùëè2 ‚àí4ùëéùëêis a non-negative
number and hence it has a square root ùë†. Thus ùë•= (‚àíùëè+ ùë†)/(2ùëé)
satisfies
ùëéùë•2 + ùëèùë•+ ùëê= ùëé(‚àíùëè+ ùë†)2/(4ùëé2) + ùëè(‚àíùëè+ ùë†)/(2ùëé) + ùëê
= (ùëè2 ‚àí2ùëèùë†+ ùë†2)/(4ùëé) + (‚àíùëè2 + ùëèùë†)/(2ùëé) + ùëê.
(1.17)
‚ñ†
Rearranging the terms of (1.17) we get
ùë†2/(4ùëé) + ùëê‚àíùëè2/(4ùëé) = (ùëè2 ‚àí4ùëéùëê)/(4ùëé) + ùëê‚àíùëè2/(4ùëé) = 0
(1.18)
Proofs of equivalence:
If a statement has the form ‚Äúùê¥if and only if
ùêµ‚Äù (often shortened as ‚Äúùê¥iff ùêµ‚Äù) then we need to prove both that ùê¥
implies ùêµand that ùêµimplies ùê¥. We call the implication that ùê¥implies
ùêµthe ‚Äúonly if‚Äù direction, and the implication that ùêµimplies ùê¥the ‚Äúif‚Äù
direction.
Proofs by combining intermediate claims:
When a proof is more complex,
it is often helpful to break it apart into several steps. That is, to prove
the statement ùëã, we might first prove statements ùëã1,ùëã2, and ùëã3 and
then prove that ùëã1 ‚àßùëã2 ‚àßùëã3 implies ùëã. (Recall that ‚àßdenotes the
logical AND operator.)
Proofs by case distinction:
This is a special case of the above, where to
prove a statement ùëãwe split into several cases ùê∂1, ‚Ä¶ , ùê∂ùëò, and prove
that (a) the cases are exhaustive, in the sense that one of the cases ùê∂ùëñ
must happen and (b) go one by one and prove that each one of the
cases ùê∂ùëñimplies the result ùëãthat we are after.


--- Page 68 ---

68
introduction to theoretical computer science
Proofs by induction:
We discuss induction and give an example in
Section 1.6.1 below. We can think of such proofs as a variant of the
above, where we have an unbounded number of intermediate claims
ùëã0, ùëã2, ‚Ä¶ , ùëãùëò, and we prove that ùëã0 is true, as well as that ùëã0 implies
ùëã1, and that ùëã0 ‚àßùëã1 implies ùëã2, and so on and so forth. The website
for CMU course 15-251 contains a useful handout on potential pitfalls
when making proofs by induction.
‚ÄùWithout loss of generality (w.l.o.g)‚Äù:
This term can be initially quite con-
fusing. It is essentially a way to simplify proofs by case distinctions.
The idea is that if Case 1 is equal to Case 2 up to a change of variables
or a similar transformation, then the proof of Case 1 will also imply
the proof of Case 2. It is always a statement that should be viewed
with suspicion. Whenever you see it in a proof, ask yourself if you
understand why the assumption made is truly without loss of gen-
erality, and when you use it, try to see if the use is indeed justified.
When writing a proof, sometimes it might be easiest to simply repeat
the proof of the second case (adding a remark that the proof is very
similar to the first one).
R
Remark 1.20 ‚Äî Hierarchical Proofs (optional). Mathe-
matical proofs are ultimately written in English prose.
The well-known computer scientist Leslie Lamport
argues that this is a problem, and proofs should be
written in a more formal and rigorous way. In his
manuscript he proposes an approach for structured
hierarchical proofs, that have the following form:
‚Ä¢ A proof for a statement of the form ‚ÄúIf ùê¥then ùêµ‚Äù
is a sequence of numbered claims, starting with
the assumption that ùê¥is true, and ending with the
claim that ùêµis true.
‚Ä¢ Every claim is followed by a proof showing how
it is derived from the previous assumptions or
claims.
‚Ä¢ The proof for each claim is itself a sequence of
subclaims.
The advantage of Lamport‚Äôs format is that the role
that every sentence in the proof plays is very clear.
It is also much easier to transform such proofs into
machine-checkable forms. The disadvantage is that
such proofs can be tedious to read and write, with
less differentiation between the important parts of the
arguments versus the more routine ones.


--- Page 69 ---

mathematical background
69
1.6 EXTENDED EXAMPLE: TOPOLOGICAL SORTING
In this section we will prove the following: every directed acyclic
graph (DAG, see Definition 1.9) can be arranged in layers so that for
all directed edges ùë¢‚Üíùë£, the layer of ùë£is larger than the layer of ùë¢.
This result is known as topological sorting and is used in many appli-
cations, including task scheduling, build systems, software package
management, spreadsheet cell calculations, and many others (see
Fig. 1.8). In fact, we will also use it ourselves later on in this book.
Figure 1.8: An example of topological sorting. We con-
sider a directed graph corresponding to a prerequisite
graph of the courses in some Computer Science pro-
gram. The edge ùë¢‚Üíùë£means that the course ùë¢is a
prerequisite for the course ùë£. A layering or ‚Äútopologi-
cal sorting‚Äù of this graph is the same as mapping the
courses to semesters so that if we decide to take the
course ùë£in semester ùëì(ùë£), then we have already taken
all the prerequisites for ùë£(i.e., its in-neighbors) in
prior semesters.
We start with the following definition. A layering of a directed
graph is a way to assign for every vertex ùë£a natural number
(corresponding to its layer), such that ùë£‚Äôs in-neighbors are in
lower-numbered layers than ùë£, and ùë£‚Äôs out-neighbors are in
higher-numbered layers. The formal definition is as follows:
Definition 1.21 ‚Äî Layering of a DAG. Let ùê∫= (ùëâ, ùê∏) be a directed graph.
A layering of ùê∫is a function ùëì
‚à∂ùëâ
‚Üí‚Ñïsuch that for every edge
ùë¢‚Üíùë£of ùê∫, ùëì(ùë¢) < ùëì(ùë£).
In this section we prove that a directed graph is acyclic if and only if
it has a valid layering.
Theorem 1.22 ‚Äî Topological Sort. Let ùê∫be a directed graph. Then ùê∫is
acyclic if and only if there exists a layering ùëìof ùê∫.
To prove such a theorem, we need to first understand what it
means. Since it is an ‚Äúif and only if‚Äù statement, Theorem 1.22 corre-
sponds to two statements:
Lemma 1.23 For every directed graph ùê∫, if ùê∫is acyclic then it has a
layering.
Lemma 1.24 For every directed graph ùê∫, if ùê∫has a layering, then it is
acyclic.


--- Page 70 ---

70
introduction to theoretical computer science
Figure 1.9: Some examples of DAGs of one, two and
three vertices, and valid ways to assign layers to the
vertices.
To prove Theorem 1.22 we need to prove both Lemma 1.23 and
Lemma 1.24. Lemma 1.24 is actually not that hard to prove. Intuitively,
if ùê∫contains a cycle, then it cannot be the case that all edges on the
cycle increase in layer number, since if we travel along the cycle at
some point we must come back to the place we started from. The
formal proof is as follows:
Proof. Let ùê∫= (ùëâ, ùê∏) be a directed graph and let ùëì‚à∂ùëâ‚Üí‚Ñïbe a
layering of ùê∫as per Definition 1.21 . Suppose, towards a contradiction,
that ùê∫is not acyclic, and hence there exists some cycle ùë¢0, ùë¢1, ‚Ä¶ , ùë¢ùëò
such that ùë¢0 = ùë¢ùëòand for every ùëñ‚àà[ùëò] the edge ùë¢ùëñ‚Üíùë¢ùëñ+1 is present in
ùê∫. Since ùëìis a layering, for every ùëñ‚àà[ùëò], ùëì(ùë¢ùëñ) < ùëì(ùë¢ùëñ+1), which means
that
ùëì(ùë¢0) < ùëì(ùë¢1) < ‚ãØ< ùëì(ùë¢ùëò)
(1.19)
but this is a contradiction since ùë¢0 = ùë¢ùëòand hence ùëì(ùë¢0) = ùëì(ùë¢ùëò).
‚ñ†
Lemma 1.23 corresponds to the more difficult (and useful) direc-
tion. To prove it, we need to show how given an arbitrary DAG ùê∫, we
can come up with a layering of the vertices of ùê∫so that all edges ‚Äúgo
up‚Äù.
P
If you have not seen the proof of this theorem before
(or don‚Äôt remember it), this would be an excellent
point to pause and try to prove it yourself. One way
to do it would be to describe an algorithm that given as
input a directed acyclic graph ùê∫on ùëõvertices and ùëõ‚àí2
or fewer edges, constructs an array ùêπof length ùëõsuch
that for every edge ùë¢‚Üíùë£in the graph ùêπ[ùë¢] < ùêπ[ùë£].
1.6.1 Mathematical induction
There are several ways to prove Lemma 1.23. One approach to do is
to start by proving it for small graphs, such as graphs with 1, 2 or 3
vertices (see Fig. 1.9, for which we can check all the cases, and then try
to extend the proof for larger graphs. The technical term for this proof
approach is proof by induction.
Induction is simply an application of the self-evident Modus Ponens
rule that says that if
(a)
ùëÉis true
and
(b)
ùëÉimplies ùëÑ
then ùëÑis true.


--- Page 71 ---

mathematical background
71
In the setting of proofs by induction we typically have a statement
ùëÑ(ùëò) that is parameterized by some integer ùëò, and we prove that (a)
ùëÑ(0) is true, and (b) For every ùëò> 0, if ùëÑ(0), ‚Ä¶ , ùëÑ(ùëò‚àí1) are all true
then ùëÑ(ùëò) is true. (Usually proving (b) is the hard part, though there
are examples where the ‚Äúbase case‚Äù (a) is quite subtle.) By applying
Modus Ponens, we can deduce from (a) and (b) that ùëÑ(1) is true.
Once we did so, since we now know that both ùëÑ(0) and ùëÑ(1) are true,
then we can use this and (b) to deduce (again using Modus Ponens)
that ùëÑ(2) is true. We can repeat the same reasoning again and again
to obtain that ùëÑ(ùëò) is true for every ùëò. The statement (a) is called the
‚Äúbase case‚Äù, while (b) is called the ‚Äúinductive step‚Äù. The assumption
in (b) that ùëÑ(ùëñ) holds for ùëñ< ùëòis called the ‚Äúinductive hypothesis‚Äù.
(The form of induction described here is sometimes called ‚Äústrong
induction‚Äù as opposed to ‚Äúweak induction‚Äù where we replace (b)
by the statement (b‚Äô) that if ùëÑ(ùëò‚àí1) is true then ùëÑ(ùëò) is true; weak
induction can be thought of as the special case of strong induction
where we don‚Äôt use the assumption that ùëÑ(0), ‚Ä¶ , ùëÑ(ùëò‚àí2) are true.)
R
Remark 1.25 ‚Äî Induction and recursion. Proofs by in-
duction are closely related to algorithms by recursion.
In both cases we reduce solving a larger problem to
solving a smaller instance of itself. In a recursive algo-
rithm to solve some problem P on an input of length
ùëòwe ask ourselves ‚Äúwhat if someone handed me a
way to solve P on instances smaller than ùëò?‚Äù. In an
inductive proof to prove a statement Q parameterized
by a number ùëò, we ask ourselves ‚Äúwhat if I already
knew that ùëÑ(ùëò‚Ä≤) is true for ùëò‚Ä≤
<
ùëò?‚Äù. Both induction
and recursion are crucial concepts for this course and
Computer Science at large (and even other areas of
inquiry, including not just mathematics but other
sciences as well). Both can be confusing at first, but
with time and practice they become clearer. For more
on proofs by induction and recursion, you might find
the following Stanford CS 103 handout, this MIT 6.00
lecture or this excerpt of the Lehman-Leighton book
useful.
1.6.2 Proving the result by induction
There are several ways to use induction to prove Lemma 1.23 by in-
duction. We will use induction on the number ùëõof vertices, and so we
will define the statement ùëÑ(ùëõ) as follows:
ùëÑ(ùëõ) is ‚ÄúFor every DAG ùê∫= (ùëâ, ùê∏) with ùëõvertices, there is a layering of ùê∫.‚Äù


--- Page 72 ---

72
introduction to theoretical computer science
5 QED stands for ‚Äúquod erat demonstrandum‚Äù, which
is Latin for ‚Äúwhat was to be demonstrated‚Äù or ‚Äúthe
very thing it was required to have shown‚Äù.
6 Using ùëõ= 0 as the base case is logically valid, but
can be confusing. If you find the trivial ùëõ= 0 case
to be confusing, you can always directly verify the
statement for ùëõ= 1 and then use both ùëõ= 0 and
ùëõ= 1 as the base cases.
The statement for ùëÑ(0) (where the graph contains no vertices) is
trivial. Thus it will suffice to prove the following: for every ùëõ> 0, if
ùëÑ(ùëõ‚àí1) is true then ùëÑ(ùëõ) is true.
To do so, we need to somehow find a way, given a graph ùê∫of ùëõ
vertices, to reduce the task of finding a layering for ùê∫into the task of
finding a layering for some other graph ùê∫‚Ä≤ of ùëõ‚àí1 vertices. The idea is
that we will find a source of ùê∫: a vertex ùë£that has no in-neighbors. We
can then assign to ùë£the layer 0, and layer the remaining vertices using
the inductive hypothesis in layers 1, 2, ‚Ä¶.
The above is the intuition behind the proof of Lemma 1.23, but
when writing the formal proof below, we use the benefit of hind-
sight, and try to streamline what was a messy journey into a linear
and easy-to-follow flow of logic that starts with the word ‚ÄúProof:‚Äù
and ends with ‚ÄúQED‚Äù or the symbol ‚ñ†.5 Discussions, examples and
digressions can be very insightful, but we keep them outside the space
delimited between these two words, where (as described by this ex-
cellent handout) ‚Äúevery sentence must be load bearing‚Äù. Just like we
do in programming, we can break the proof into little ‚Äúsubroutines‚Äù
or ‚Äúfunctions‚Äù (known as lemmas or claims in math language), which
will be smaller statements that help us prove the main result. How-
ever, the proof should be structured in a way that ensures that it is
always crystal-clear to the reader in what stage we are of the proof.
The reader should be able to tell what is the role of every sentence in
the proof and which part it belongs to. We now present the formal
proof of Lemma 1.23.
Proof of Lemma 1.23. Let ùê∫= (ùëâ, ùê∏) be a DAG and ùëõ= |ùëâ| be the
number of its vertices. We prove the lemma by induction on ùëõ. The
base case is ùëõ= 0 where there are no vertices, and so the statement is
trivially true.6 For the case of ùëõ> 0, we make the inductive hypothesis
that every DAG ùê∫‚Ä≤ of at most ùëõ‚àí1 vertices has a layering.
We make the following claim:
Claim: ùê∫must contain a vertex ùë£of in-degree zero.
Proof of Claim: Suppose otherwise that every vertex ùë£‚ààùëâhas an
in-neighbor. Let ùë£0 be some vertex of ùê∫, let ùë£1 be an in-neighbor of ùë£0,
ùë£2 be an in-neighbor of ùë£1, and continue in this way for ùëõsteps until
we construct a list ùë£0, ùë£1, ‚Ä¶ , ùë£ùëõsuch that for every ùëñ‚àà[ùëõ], ùë£ùëñ+1 is an
in-neighbor of ùë£ùëñ, or in other words the edge ùë£ùëñ+1 ‚Üíùë£ùëñis present in the
graph. Since there are only ùëõvertices in this graph, one of the ùëõ+ 1
vertices in this sequence must repeat itself, and so there exists ùëñ< ùëó
such that ùë£ùëñ= ùë£ùëó. But then the sequence ùë£ùëó‚Üíùë£ùëó‚àí1 ‚Üí‚ãØ‚Üíùë£ùëñis a cycle
in ùê∫, contradicting our assumption that it is acyclic. (QED Claim)
Given the claim, we can let ùë£0 be some vertex of in-degree zero in
ùê∫, and let ùê∫‚Ä≤ be the graph obtained by removing ùë£0 from ùê∫. ùê∫‚Ä≤ has


--- Page 73 ---

mathematical background
73
ùëõ‚àí1 vertices and hence per the inductive hypothesis has a layering
ùëì‚Ä≤ ‚à∂(ùëâ‚ßµ{ùë£0}) ‚Üí‚Ñï. We define ùëì‚à∂ùëâ‚Üí‚Ñïas follows:
ùëì(ùë£) =
‚éß
{
‚é®
{
‚é©
ùëì‚Ä≤(ùë£) + 1
ùë£‚â†ùë£0
0
ùë£= ùë£0
.
(1.20)
We claim that ùëìis a valid layering, namely that for every edge ùë¢‚Üí
ùë£, ùëì(ùë¢) < ùëì(ùë£). To prove this, we split into cases:
‚Ä¢ Case 1: ùë¢‚â†ùë£0, ùë£‚â†ùë£0. In this case the edge ùë¢‚Üíùë£exists in the
graph ùê∫‚Ä≤ and hence by the inductive hypothesis ùëì‚Ä≤(ùë¢) < ùëì‚Ä≤(ùë£)
which implies that ùëì‚Ä≤(ùë¢) + 1 < ùëì‚Ä≤(ùë£) + 1.
‚Ä¢ Case 2: ùë¢= ùë£0, ùë£‚â†ùë£0. In this case ùëì(ùë¢) = 0 and ùëì(ùë£) = ùëì‚Ä≤(ùë£) + 1 >
0.
‚Ä¢ Case 3: ùë¢‚â†ùë£0, ùë£= ùë£0. This case can‚Äôt happen since ùë£0 does not
have in-neighbors.
‚Ä¢ Case 4: ùë¢= ùë£0, ùë£= ùë£0. This case again can‚Äôt happen since it means
that ùë£0 is its own-neighbor ‚Äî it is involved in a self loop which is a
form cycle that is disallowed in an acyclic graph.
Thus, ùëìis a valid layering for ùê∫which completes the proof.
‚ñ†
P
Reading a proof is no less of an important skill than
producing one. In fact, just like understanding code,
it is a highly non-trivial skill in itself. Therefore I
strongly suggest that you re-read the above proof, ask-
ing yourself at every sentence whether the assumption
it makes is justified, and whether this sentence truly
demonstrates what it purports to achieve. Another
good habit is to ask yourself when reading a proof for
every variable you encounter (such as ùë¢, ùëñ, ùê∫‚Ä≤, ùëì‚Ä≤, etc.
in the above proof) the following questions: (1) What
type of variable is it? is it a number? a graph? a vertex?
a function? and (2) What do we know about it? Is it
an arbitrary member of the set? Have we shown some
facts about it?, and (3) What are we trying to show
about it?.
1.6.3 Minimality and uniqueness
Theorem 1.22 guarantees that for every DAG ùê∫= (ùëâ, ùê∏) there exists
some layering ùëì‚à∂ùëâ‚Üí‚Ñïbut this layering is not necessarily unique.
For example, if ùëì‚à∂ùëâ‚Üí‚Ñïis a valid layering of the graph then so is
the function ùëì‚Ä≤ defined as ùëì‚Ä≤(ùë£) = 2 ‚ãÖùëì(ùë£). However, it turns out that


--- Page 74 ---

74
introduction to theoretical computer science
the minimal layering is unique. A minimal layering is one where every
vertex is given the smallest layer number possible. We now formally
define minimality and state the uniqueness theorem:
Theorem 1.26 ‚Äî Minimal layering is unique. Let ùê∫= (ùëâ, ùê∏) be a DAG. We
say that a layering ùëì‚à∂ùëâ‚Üí‚Ñïis minimal if for every vertex ùë£‚ààùëâ, if
ùë£has no in-neighbors then ùëì(ùë£) = 0 and if ùë£has in-neighbors then
there exists an in-neighbor ùë¢of ùë£such that ùëì(ùë¢) = ùëì(ùë£) ‚àí1.
For every layering ùëì, ùëî‚à∂ùëâ‚Üí‚Ñïof ùê∫, if both ùëìand ùëîare minimal
then ùëì= ùëî.
The definition of minimality in Theorem 1.26 implies that for every
vertex ùë£‚ààùëâ, we cannot move it to a lower layer without making
the layering invalid. If ùë£is a source (i.e., has in-degree zero) then
a minimal layering ùëìmust put it in layer 0, and for every other ùë£, if
ùëì(ùë£) = ùëñ, then we cannot modify this to set ùëì(ùë£) ‚â§ùëñ‚àí1 since there
is an-neighbor ùë¢of ùë£satisfying ùëì(ùë¢) = ùëñ‚àí1. What Theorem 1.26
says is that a minimal layering ùëìis unique in the sense that every other
minimal layering is equal to ùëì.
Proof Idea:
The idea is to prove the theorem by induction on the layers. If ùëìand
ùëîare minimal then they must agree on the source vertices, since both
ùëìand ùëîshould assign these vertices to layer 0. We can then show that
if ùëìand ùëîagree up to layer ùëñ‚àí1, then the minimality property implies
that they need to agree in layer ùëñas well. In the actual proof we use
a small trick to save on writing. Rather than proving the statement
that ùëì= ùëî(or in other words that ùëì(ùë£) = ùëî(ùë£) for every ùë£‚ààùëâ),
we prove the weaker statement that ùëì(ùë£) ‚â§ùëî(ùë£) for every ùë£‚ààùëâ.
(This is a weaker statement since the condition that ùëì(ùë£) is lesser or
equal than to ùëî(ùë£) is implied by the condition that ùëì(ùë£) is equal to
ùëî(ùë£).) However, since ùëìand ùëîare just labels we give to two minimal
layerings, by simply changing the names ‚Äúùëì‚Äù and ‚Äúùëî‚Äù the same proof
also shows that ùëî(ùë£) ‚â§ùëì(ùë£) for every ùë£‚ààùëâand hence that ùëì= ùëî.
‚ãÜ
Proof of Theorem 1.26. Let ùê∫= (ùëâ, ùê∏) be a DAG and ùëì, ùëî‚à∂ùëâ‚Üí‚Ñïbe
two minimal valid layering of ùê∫. We will prove that for every ùë£‚ààùëâ,
ùëì(ùë£) ‚â§ùëî(ùë£). Since we didn‚Äôt assume anything about ùëì, ùëîexcept their
minimality, the same proof will imply that for every ùë£‚ààùëâ, ùëî(ùë£) ‚â§ùëì(ùë£)
and hence that ùëì(ùë£) = ùëî(ùë£) for every ùë£‚ààùëâ, which is what we needed
to show.
We will prove that ùëì(ùë£) ‚â§ùëî(ùë£) for every ùë£‚ààùëâby induction on
ùëñ= ùëì(ùë£). The case ùëñ= 0 is immediate: since in this case ùëì(ùë£) = 0,
ùëî(ùë£) must be at least ùëì(ùë£). For the case ùëñ> 0, by the minimality of ùëì,


--- Page 75 ---

mathematical background
75
if ùëì(ùë£) = ùëñthen there must exist some in-neighbor ùë¢of ùë£such that
ùëì(ùë¢) = ùëñ‚àí1. By the induction hypothesis we get that ùëî(ùë¢) ‚â•ùëñ‚àí1, and
since ùëîis a valid layering it must hold that ùëî(ùë£) > ùëî(ùë¢) which means
that ùëî(ùë£) ‚â•ùëñ= ùëì(ùë£).
‚ñ†
P
The proof of Theorem 1.26 is fully rigorous, but is
written in a somewhat terse manner. Make sure that
you read through it and understand why this is indeed
an airtight proof of the Theorem‚Äôs statement.
1.7 THIS BOOK: NOTATION AND CONVENTIONS
Most of the notation we use in this book is standard and is used in
most mathematical texts. The main points where we diverge are:
‚Ä¢ We index the natural numbers ‚Ñïstarting with 0 (though many
other texts, especially in computer science, do the same).
‚Ä¢ We also index the set [ùëõ] starting with 0, and hence define it as
{0, ‚Ä¶ , ùëõ‚àí1}. In other texts it is often defined as {1, ‚Ä¶ , ùëõ}. Similarly,
we index our strings starting with 0, and hence a string ùë•‚àà{0, 1}ùëõ
is written as ùë•0ùë•1 ‚ãØùë•ùëõ‚àí1.
‚Ä¢ If ùëõis a natural number then 1ùëõdoes not equal the number 1 but
rather this is the length ùëõstring 11 ‚ãØ1 (that is a string of ùëõones).
Similarly, 0ùëõrefers to the length ùëõstring 00 ‚ãØ0.
‚Ä¢ Partial functions are functions that are not necessarily defined on
all inputs. When we write ùëì‚à∂ùê¥‚Üíùêµthis means that ùëìis a total
function unless we say otherwise. When we want to emphasize that
ùëìcan be a partial function, we will sometimes write ùëì‚à∂ùê¥‚Üíùëùùêµ.
‚Ä¢ As we will see later on in the course, we will mostly describe our
computational problems in terms of computing a Boolean function
ùëì‚à∂{0, 1}‚àó‚Üí{0, 1}. In contrast, many other textbooks refer to the
same task as deciding a language ùêø‚äÜ{0, 1}‚àó. These two viewpoints
are equivalent, since for every set ùêø‚äÜ{0, 1}‚àóthere is a correspond-
ing function ùêπsuch that ùêπ(ùë•) = 1 if and only if ùë•‚ààùêø. Computing
partial functions corresponds to the task known in the literature
as a solving a promise problem. Because the language notation is
so prevalent in other textbooks, we will occasionally remind the
reader of this correspondence.
‚Ä¢ We use ‚åàùë•‚åâand ‚åäùë•‚åãfor the ‚Äúceiling‚Äù and ‚Äúfloor‚Äù operators that
correspond to ‚Äúrounding up‚Äù or ‚Äúrounding down‚Äù a number to the


--- Page 76 ---

76
introduction to theoretical computer science
nearest integer. We use (ùë•mod ùë¶) to denote the ‚Äúremainder‚Äù of ùë•
when divided by ùë¶. That is, (ùë•mod ùë¶) = ùë•‚àíùë¶‚åäùë•/ùë¶‚åã. In context
when an integer is expected we‚Äôll typically ‚Äúsilently round‚Äù the
quantities to an integer. For example, if we say that ùë•is a string of
length ‚àöùëõthen this means that ùë•is of length ‚åà‚àöùëõ‚åâ. (We round up
for the sake of convention, but in most such cases, it will not make a
difference whether we round up or down.)
‚Ä¢ Like most Computer Science texts, we default to the logarithm in
base two. Thus, log ùëõis the same as log2 ùëõ.
‚Ä¢ We will also use the notation ùëì(ùëõ) = ùëùùëúùëôùë¶(ùëõ) as a shorthand for
ùëì(ùëõ) = ùëõùëÇ(1) (i.e., as shorthand for saying that there are some
constants ùëé, ùëèsuch that ùëì(ùëõ) ‚â§ùëé‚ãÖùëõùëèfor every sufficiently large
ùëõ). Similarly, we will use ùëì(ùëõ) = ùëùùëúùëôùë¶ùëôùëúùëî(ùëõ) as shorthand for
ùëì(ùëõ) = ùëùùëúùëôùë¶(log ùëõ) (i.e., as shorthand for saying that there are
some constants ùëé, ùëèsuch that ùëì(ùëõ) ‚â§ùëé‚ãÖ(log ùëõ)ùëèfor every sufficiently
large ùëõ).
‚Ä¢ As in often the case in mathematical literature, we use the apostro-
phe character to enrich our set of identifiers. Typically if ùë•denotes
some object, then ùë•‚Ä≤, ùë•‚Ä≥, etc. will denote other objects of the same
type.
‚Ä¢ To save on ‚Äúcognitive load‚Äù we will often use round constants such
as 10, 100, 1000 in the statements of both theorems and problem
set questions. When you see such a ‚Äúround‚Äù constant, you can
typically assume that it has no special significance and was just
chosen arbitrarily. For example, if you see a theorem of the form
‚ÄúAlgorithm ùê¥takes at most 1000 ‚ãÖùëõ2 steps to compute function ùêπ
on inputs of length ùëõ‚Äù then probably the number 1000 is an abitrary
sufficiently large constant, and one could prove the same theorem
with a bound of the form ùëê‚ãÖùëõ2 for a constant ùëêthat is smaller than
1000. Similarly, if a problem asks you to prove that some quantity is
at least ùëõ/100, it is quite possible that in truth the quantity is at least
ùëõ/ùëëfor some constant ùëëthat is smaller than 100.
1.7.1 Variable name conventions
Like programming, mathematics is full of variables. Whenever you see
a variable, it is always important to keep track of what is its type (e.g.,
whether the variable is a number, a string, a function, a graph, etc.).
To make this easier, we try to stick to certain conventions and consis-
tently use certain identifiers for variables of the same type. Some of
these conventions are listed in Section 1.7.1 below. These conventions
are not immutable laws and we might occasionally deviate from them.


--- Page 77 ---

mathematical background
77
Also, such conventions do not replace the need to explicitly declare for
each new variable the type of object that it denotes.
Table 1.2: Conventions for identifiers in this book
Identifier
Often denotes object of type
ùëñ,ùëó,ùëò,‚Ñì,ùëö,ùëõNatural numbers (i.e., in ‚Ñï= {0, 1, 2, ‚Ä¶})
ùúñ, ùõø
Small positive real numbers (very close to 0)
ùë•, ùë¶, ùëß, ùë§
Typically strings in {0, 1}‚àóthough sometimes numbers or
other objects. We often identify an object with its
representation as a string.
ùê∫
A graph. The set of ùê∫‚Äôs vertices is typically denoted by ùëâ.
Often ùëâ= [ùëõ]. The set of ùê∫‚Äôs edges is typically denoted by
ùê∏.
ùëÜ
Set
ùëì, ùëî, ‚Ñé
Functions. We often (though not always) use lowercase
identifiers for finite functions, which map {0, 1}ùëõto {0, 1}ùëö
(often ùëö= 1).
ùêπ, ùê∫, ùêª
Infinite (unbounded input) functions mapping {0, 1}‚àóto
{0, 1}‚àóor {0, 1}‚àóto {0, 1}ùëöfor some ùëö. Based on context,
the identifiers ùê∫, ùêªare sometimes used to denote
functions and sometimes graphs.
ùê¥, ùêµ, ùê∂
Boolean circuits
ùëÄ, ùëÅ
Turing machines
ùëÉ, ùëÑ
Programs
ùëá
A function mapping ‚Ñïto ‚Ñïthat corresponds to a time
bound.
ùëê
A positive number (often an unspecified constant; e.g.,
ùëá(ùëõ) = ùëÇ(ùëõ) corresponds to the existence of ùëês.t.
ùëá(ùëõ) ‚â§ùëê‚ãÖùëõevery ùëõ> 0). We sometimes use ùëé, ùëèin a
similar way.
Œ£
Finite set (often used as the alphabet for a set of strings).
1.7.2 Some idioms
Mathematical texts often employ certain conventions or ‚Äúidioms‚Äù.
Some examples of such idioms that we use in this text include the
following:
‚Ä¢ ‚ÄúLet ùëãbe ‚Ä¶‚Äù, ‚Äúlet ùëãdenote ‚Ä¶‚Äù, or ‚Äúlet ùëã= ‚Ä¶‚Äù: These are all
different ways for us to say that we are defining the symbol ùëãto
stand for whatever expression is in the ‚Ä¶. When ùëãis a property of
some objects we might define ùëãby writing something along the
lines of ‚ÄúWe say that ‚Ä¶ has the property ùëãif ‚Ä¶.‚Äù. While we often


--- Page 78 ---

78
introduction to theoretical computer science
try to define terms before they are used, sometimes a mathematical
sentence reads easier if we use a term before defining it, in which
case we add ‚ÄúWhere ùëãis ‚Ä¶‚Äù to explain how ùëãis defined in the
preceding expression.
‚Ä¢ Quantifiers: Mathematical texts involve many quantifiers such as
‚Äúfor all‚Äù and ‚Äúexists‚Äù. We sometimes spell these in words as in ‚Äúfor
all ùëñ‚àà‚Ñï‚Äù or ‚Äúthere is ùë•‚àà{0, 1}‚àó‚Äù, and sometimes use the formal
symbols ‚àÄand ‚àÉ. It is important to keep track on which variable is
quantified in what way the dependencies between the variables. For
example, a sentence fragment such as ‚Äúfor every ùëò> 0 there exists
ùëõ‚Äù means that ùëõcan be chosen in a way that depends on ùëò. The order
of quantifiers is important. For example, the following is a true
statement: ‚Äúfor every natural number ùëò> 1 there exists a prime number
ùëõsuch that ùëõdivides ùëò.‚Äù In contrast, the following statement is false:
‚Äúthere exists a prime number ùëõsuch that for every natural number ùëò> 1,
ùëõdivides ùëò.‚Äù
‚Ä¢ Numbered equations, theorems, definitions: To keep track of all
the terms we define and statements we prove, we often assign them
a (typically numeric) label, and then refer back to them in other
parts of the text.
‚Ä¢ (i.e.,), (e.g.,): Mathematical texts tend to contain quite a few of
these expressions. We use ùëã(i.e., ùëå) in cases where ùëåis equivalent
to ùëãand ùëã(e.g., ùëå) in cases where ùëåis an example of ùëã(e.g., one
can use phrases such as ‚Äúa natural number (i.e., a non-negative
integer)‚Äù or ‚Äúa natural number (e.g., 7)‚Äù).
‚Ä¢ ‚ÄúThus‚Äù, ‚ÄúTherefore‚Äù , ‚ÄúWe get that‚Äù: This means that the following
sentence is implied by the preceding one, as in ‚ÄúThe ùëõ-vertex graph
ùê∫is connected. Therefore it contains at least ùëõ‚àí1 edges.‚Äù We
sometimes use ‚Äúindeed‚Äù to indicate that the following text justifies
the claim that was made in the preceding sentence as in ‚ÄúThe ùëõ-
vertex graph ùê∫has at least ùëõ‚àí1 edges. Indeed, this follows since ùê∫is
connected.‚Äù
‚Ä¢ Constants: In Computer Science, we typically care about how our
algorithms‚Äô resource consumption (such as running time) scales
with certain quantities (such as the length of the input). We refer to
quantities that do not depend on the length of the input as constants
and so often use statements such as ‚Äúthere exists a constant ùëê> 0 such
that for every ùëõ‚àà‚Ñï, Algorithm ùê¥runs in at most ùëê‚ãÖùëõ2 steps on inputs of
length ùëõ.‚Äù The qualifier ‚Äúconstant‚Äù for ùëêis not strictly needed but is
added to emphasize that ùëêhere is a fixed number independent of ùëõ.
In fact sometimes, to reduce cognitive load, we will simply replace ùëê


--- Page 79 ---

mathematical background
79
by a sufficiently large round number such as 10, 100, or 1000, or use
ùëÇ-notation and write ‚ÄúAlgorithm ùê¥runs in ùëÇ(ùëõ2) time.‚Äù
‚úì
Chapter Recap
‚Ä¢ The basic ‚Äúmathematical data structures‚Äù we‚Äôll
need are numbers, sets, tuples, strings, graphs and
functions.
‚Ä¢ We can use basic objects to define more complex
notions. For example, graphs can be defined as a list
of pairs.
‚Ä¢ Given precise definitions of objects, we can state
unambiguous and precise statements. We can then
use mathematical proofs to determine whether these
statements are true or false.
‚Ä¢ A mathematical proof is not a formal ritual but
rather a clear, precise and ‚Äúbulletproof‚Äù argument
certifying the truth of a certain statement.
‚Ä¢ Big-ùëÇnotation is an extremely useful formalism
to suppress less significant details and allows us
to focus on the high level behavior of quantities of
interest.
‚Ä¢ The only way to get comfortable with mathematical
notions is to apply them in the contexts of solving
problems. You should expect to need to go back
time and again to the definitions and notation in
this chapter as you work through problems in this
course.
1.8 EXERCISES
Exercise 1.1 ‚Äî Logical expressions. a. Write a logical expression ùúë(ùë•)
involving the variables ùë•0, ùë•1, ùë•2 and the operators ‚àß(AND), ‚à®
(OR), and ¬¨ (NOT), such that ùúë(ùë•) is true if the majority of the
inputs are True.
b. Write a logical expression ùúë(ùë•) involving the variables ùë•0, ùë•1, ùë•2
and the operators ‚àß(AND), ‚à®(OR), and ¬¨ (NOT), such that ùúë(ùë•)
is true if the sum ‚àë
2
ùëñ=0 ùë•ùëñ(identifying ‚Äútrue‚Äù with 1 and ‚Äúfalse‚Äù
with 0) is odd.
‚ñ†
Exercise 1.2 ‚Äî Quantifiers. Use the logical quantifiers ‚àÄ(for all), ‚àÉ(there
exists), as well as ‚àß, ‚à®, ¬¨ and the arithmetic operations +, √ó, =, >, < to
write the following:
a. An expression ùúë(ùëõ, ùëò) such that for every natural numbers ùëõ, ùëò,
ùúë(ùëõ, ùëò) is true if and only if ùëòdivides ùëõ.
b. An expression ùúë(ùëõ) such that for every natural number ùëõ, ùúë(ùëõ) is
true if and only if ùëõis a power of three.


--- Page 80 ---

80
introduction to theoretical computer science
‚ñ†
Exercise 1.3 Describe the following statement in English words:
‚àÄùëõ‚àà‚Ñï‚àÉùëù>ùëõ‚àÄùëé, ùëè‚àà‚Ñï(ùëé√ó ùëè‚â†ùëù) ‚à®(ùëé= 1).
‚ñ†
Exercise 1.4 ‚Äî Set construction notation. Describe in words the following
sets:
a. ùëÜ= {ùë•‚àà{0, 1}100 ‚à∂‚àÄùëñ‚àà{0,‚Ä¶,99}ùë•ùëñ= ùë•99‚àíùëñ}
b. ùëá= {ùë•‚àà{0, 1}‚àó‚à∂‚àÄùëñ,ùëó‚àà{2,‚Ä¶,|ùë•|‚àí1}ùëñ‚ãÖùëó‚â†|ùë•|}
‚ñ†
Exercise 1.5 ‚Äî Existence of one to one mappings. For each one of the fol-
lowing pairs of sets (ùëÜ, ùëá), prove or disprove the following statement:
there is a one to one function ùëìmapping ùëÜto ùëá.
a. Let ùëõ> 10. ùëÜ= {0, 1}ùëõand ùëá= [ùëõ] √ó [ùëõ] √ó [ùëõ].
b. Let ùëõ> 10. ùëÜis the set of all functions mapping {0, 1}ùëõto {0, 1}.
ùëá= {0, 1}ùëõ3.
c. Let ùëõ> 100. ùëÜ= {ùëò‚àà[ùëõ] | ùëòis prime}, ùëá= {0, 1}‚åàlog ùëõ‚àí1‚åâ.
‚ñ†
Exercise 1.6 ‚Äî Inclusion Exclusion. a. Let ùê¥, ùêµbe finite sets. Prove that
|ùê¥‚à™ùêµ| = |ùê¥| + |ùêµ| ‚àí|ùê¥‚à©ùêµ|.
b. Let ùê¥0, ‚Ä¶ , ùê¥ùëò‚àí1 be finite sets. Prove that |ùê¥0 ‚à™‚ãØ‚à™ùê¥ùëò‚àí1| ‚â•
‚àë
ùëò‚àí1
ùëñ=0 |ùê¥ùëñ| ‚àí‚àë0‚â§ùëñ<ùëó<ùëò|ùê¥ùëñ‚à©ùê¥ùëó|.
c. Let ùê¥0, ‚Ä¶ , ùê¥ùëò‚àí1 be finite subsets of {1, ‚Ä¶ , ùëõ}, such that |ùê¥ùëñ| = ùëöfor
every ùëñ‚àà[ùëò]. Prove that if ùëò> 100ùëõ, then there exist two distinct
sets ùê¥ùëñ, ùê¥ùëós.t. |ùê¥ùëñ‚à©ùê¥ùëó| ‚â•ùëö2/(10ùëõ).
‚ñ†
Exercise 1.7 Prove that if ùëÜ, ùëáare finite and ùêπ‚à∂ùëÜ‚Üíùëáis one to one
then |ùëÜ| ‚â§|ùëá|.
‚ñ†
Exercise 1.8 Prove that if ùëÜ, ùëáare finite and ùêπ‚à∂ùëÜ‚Üíùëáis onto then
|ùëÜ| ‚â•|ùëá|.
‚ñ†
Exercise 1.9 Prove that for every finite ùëÜ, ùëá, there are (|ùëá| + 1)|ùëÜ| partial
functions from ùëÜto ùëá.
‚ñ†
Exercise 1.10 Suppose that {ùëÜùëõ}ùëõ‚àà‚Ñïis a sequence such that ùëÜ0 ‚â§10 and
for ùëõ> 1 ùëÜùëõ‚â§5ùëÜ‚åäùëõ
5 ‚åã+ 2ùëõ. Prove by induction that ùëÜùëõ‚â§100ùëõlog ùëõfor
every ùëõ.


--- Page 81 ---

mathematical background
81
7 one way to do this is to use Stirling‚Äôs approximation
for the factorial function..
‚ñ†
Exercise 1.11 Prove that for every undirected graph ùê∫of 100 vertices,
if every vertex has degree at most 4, then there exists a subset ùëÜof at
least 20 vertices such that no two vertices in ùëÜare neighbors of one
another.
‚ñ†
Exercise 1.12 ‚Äî ùëÇ-notation. For every pair of functions ùêπ, ùê∫below, deter-
mine which of the following relations holds: ùêπ= ùëÇ(ùê∫), ùêπ= Œ©(ùê∫),
ùêπ= ùëú(ùê∫) or ùêπ= ùúî(ùê∫).
a. ùêπ(ùëõ) = ùëõ, ùê∫(ùëõ) = 100ùëõ.
b. ùêπ(ùëõ) = ùëõ, ùê∫(ùëõ) = ‚àöùëõ.
c. ùêπ(ùëõ) = ùëõlog ùëõ, ùê∫(ùëõ) = 2(log(ùëõ))2.
d. ùêπ(ùëõ) = ‚àöùëõ, ùê∫(ùëõ) = 2‚àölog ùëõ
e. ùêπ(ùëõ) = (
ùëõ
‚åà0.2ùëõ‚åâ) , ùê∫(ùëõ) = 20.1ùëõ(where (ùëõ
ùëò) is the number of ùëò-sized
subsets of a set of size ùëõ) and ùëî(ùëõ) = 20.1ùëõ. See footnote for hint.7
‚ñ†
Exercise 1.13 Give an example of a pair of functions ùêπ, ùê∫‚à∂‚Ñï‚Üí‚Ñïsuch
that neither ùêπ= ùëÇ(ùê∫) nor ùê∫= ùëÇ(ùêπ) holds.
‚ñ†
Exercise 1.14 Prove that for every undirected graph ùê∫on ùëõvertices, if ùê∫
has at least ùëõedges then ùê∫contains a cycle.
‚ñ†
Exercise 1.15 Prove that for every undirected graph ùê∫of 1000 vertices,
if every vertex has degree at most 4, then there exists a subset ùëÜof at
least 200 vertices such that no two vertices in ùëÜare neighbors of one
another.
‚ñ†
1.9 BIBLIOGRAPHICAL NOTES
The heading ‚ÄúA Mathematician‚Äôs Apology‚Äù, refers to Hardy‚Äôs classic
book [Har41]. Even when Hardy is wrong, he is very much worth
reading.
There are many online sources for the mathematical background
needed for this book. In particular, the lecture notes for MIT 6.042
‚ÄúMathematics for Computer Science‚Äù [LLM18] are extremely com-
prehensive, and videos and assignments for this course are available
online. Similarly, Berkeley CS 70: ‚ÄúDiscrete Mathematics and Proba-
bility Theory‚Äù has extensive lecture notes online.


--- Page 82 ---

82
introduction to theoretical computer science
Other sources for discrete mathematics are Rosen [Ros19] and
Jim Aspens‚Äô online book [Asp18]. Lewis and Zax [LZ19], as well
as the online book of Fleck [Fle18], give a more gentle overview of
the much of the same material. Solow [Sol14] is a good introduction
to proof reading and writing. Kun [Kun18] gives an introduction
to mathematics aimed at readers with programming background.
Stanford‚Äôs CS 103 course has a wonderful collections of handouts on
mathematical proof techniques and discrete mathematics.
The word graph in the sense of Definition 1.3 was coined by the
mathematician Sylvester in 1878 in analogy with the chemical graphs
used to visualize molecules. There is an unfortunate confusion be-
tween this term and the more common usage of the word ‚Äúgraph‚Äù as
a way to plot data, and in particular a plot of some function ùëì(ùë•) as a
function of ùë•. One way to relate these two notions is to identify every
function ùëì‚à∂ùê¥‚Üíùêµwith the directed graph ùê∫ùëìover the vertex set
ùëâ= ùê¥‚à™ùêµsuch that ùê∫ùëìcontains the edge ùë•‚Üíùëì(ùë•) for every ùë•‚ààùê¥. In
a graph ùê∫ùëìconstructed in this way, every vertex in ùê¥has out-degree
equal to one. If the function ùëìis one to one then every vertex in ùêµhas
in-degree at most one. If the function ùëìis onto then every vertex in ùêµ
has in-degree at least one. If ùëìis a bijection then every vertex in ùêµhas
in-degree exactly equal to one.
Carl Pomerance‚Äôs quote is taken from the home page of Doron
Zeilberger.


--- Page 83 ---

Figure 2.1: Our basic notion of computation is some
process that maps an input to an output
2
Computation and Representation
‚ÄúThe alphabet (sic) was a great invention, which enabled men (sic) to store
and to learn with little effort what others had learned the hard way ‚Äì that is, to
learn from books rather than from direct, possibly painful, contact with the real
world.‚Äù, B.F. Skinner
‚ÄúThe name of the song is called ‚ÄòHADDOCK‚ÄôS EYES.‚Äù‚Äô [said the Knight]
‚ÄúOh, that‚Äôs the name of the song, is it?‚Äù Alice said, trying to feel interested.
‚ÄúNo, you don‚Äôt understand,‚Äù the Knight said, looking a little vexed. ‚ÄúThat‚Äôs
what the name is CALLED. The name really is ‚ÄòTHE AGED AGED MAN.‚Äù‚Äô
‚ÄúThen I ought to have said ‚ÄòThat‚Äôs what the SONG is called‚Äô?‚Äù Alice cor-
rected herself.
‚ÄúNo, you oughtn‚Äôt: that‚Äôs quite another thing! The SONG is called ‚ÄòWAYS
AND MEANS‚Äô: but that‚Äôs only what it‚Äôs CALLED, you know!‚Äù
‚ÄúWell, what IS the song, then?‚Äù said Alice, who was by this time com-
pletely bewildered.
‚ÄúI was coming to that,‚Äù the Knight said. ‚ÄúThe song really IS ‚ÄòA-SITTING ON
A GATE‚Äô: and the tune‚Äôs my own invention.‚Äù
Lewis Carroll, Through the Looking-Glass
To a first approximation, computation is a process that maps an input
to an output.
When discussing computation, it is essential to separate the ques-
tion of what is the task we need to perform (i.e., the specification) from
the question of how we achieve this task (i.e., the implementation).
For example, as we‚Äôve seen, there is more than one way to achieve the
computational task of computing the product of two integers.
In this chapter we focus on the what part, namely defining compu-
tational tasks. For starters, we need to define the inputs and outputs.
Capturing all the potential inputs and outputs that we might ever
want to compute on seems challenging, since computation today is
applied to a wide variety of objects. We do not compute merely on
numbers, but also on texts, images, videos, connection graphs of social
Compiled on 8.26.2020 18:10
Learning Objectives:
‚Ä¢ Distinguish between specification and
implementation, or equivalently between
mathematical functions and
algorithms/programs.
‚Ä¢ Representing an object as a string (often of
zeroes and ones).
‚Ä¢ Examples of representations for common
objects such as numbers, vectors, lists, graphs.
‚Ä¢ Prefix-free representations.
‚Ä¢ Cantor‚Äôs Theorem: The real numbers cannot
be represented exactly as finite strings.


--- Page 84 ---

84
introduction to theoretical computer science
Figure 2.2: We represent numbers, texts, images, net-
works and many other objects using strings of zeroes
and ones. Writing the zeroes and ones themselves in
green font over a black background is optional.
networks, MRI scans, gene data, and even other programs. We will
represent all these objects as strings of zeroes and ones, that is objects
such as 0011101 or 1011 or any other finite list of 1‚Äôs and 0‚Äôs. (This
choice is for convenience: there is nothing ‚Äúholy‚Äù about zeroes and
ones, and we could have used any other finite collection of symbols.)
Today, we are so used to the notion of digital representation that
we are not surprised by the existence of such an encoding. But it is
actually a deep insight with significant implications. Many animals
can convey a particular fear or desire, but what is unique about hu-
mans is language: we use a finite collection of basic symbols to describe
a potentially unlimited range of experiences. Language allows trans-
mission of information over both time and space and enables soci-
eties that span a great many people and accumulate a body of shared
knowledge over time.
Over the last several decades, we have seen a revolution in what we
can represent and convey in digital form. We can capture experiences
with almost perfect fidelity, and disseminate it essentially instanta-
neously to an unlimited audience. Moreover, once information is in
digital form, we can compute over it, and gain insights from data that
were not accessible in prior times. At the heart of this revolution is the
simple but profound observation that we can represent an unbounded
variety of objects using a finite set of symbols (and in fact using only
the two symbols 0 and 1).
In later chapters, we will typically take such representations for
granted, and hence use expressions such as ‚Äúprogram ùëÉtakes ùë•as
input‚Äù when ùë•might be a number, a vector, a graph, or any other
object, when we really mean that ùëÉtakes as input the representation of
ùë•as a binary string. However, in this chapter we will dwell a bit more
on how we can construct such representations.
This chapter: A non-mathy overview
The main takeaways from this chapter are:
‚Ä¢ We can represent all kinds of objects we want to use as in-
puts and outputs using binary strings. For example, we can
use the binary basis to represent integers and rational num-
bers as binary strings (see Section 2.1.1 and Section 2.2).
‚Ä¢ We can use compose the representations of simple objects
to represent more complex objects. In this way, we can
represent lists of integers or rational numbers, and use
that to represent objects such as matrices, images, and
graphs. Prefix-free encoding is one way to achieve such a
composition (see Section 2.5.2).


--- Page 85 ---

computation and representation
85
‚Ä¢ A computational task specifies a map between an input to an
output‚Äî a function. It is crucially important to distinguish
between the ‚Äúwhat‚Äù and the ‚Äúhow‚Äù, or the specification
and implementation (see Section 2.6.1). A function simply
defines which output corresponds to which input. It does
not specify how to compute the output from the input, and
as we‚Äôve seen in the context of multiplication, there can be
more than one way to compute the same function.
‚Ä¢ While the set of all possible binary strings is infinite, it
still cannot represent everything. In particular, there is no
representation of the real numbers (with absolute accuracy)
as binary strings. This result is also known as ‚ÄúCantor‚Äôs
Theorem‚Äù (see Section 2.4) and is typically referred to as
the result that the ‚Äúreals are uncountable‚Äù. It is also im-
plies that there are different levels of infinity though we will
not get into this topic in this book (see Remark 2.10).
The two ‚Äúbig ideas‚Äô ‚Äô we discuss are Big Idea 1 - we can com-
pose representations for simple objects to represent more
complex objects and Big Idea 2 - it is crucial to distinguish
between functions (‚Äúwhat‚Äù) and programs (‚Äúhow‚Äù). The latter
will be a theme we will come back to time and again in this
book.
2.1 DEFINING REPRESENTATIONS
Every time we store numbers, images, sounds, databases, or other ob-
jects on a computer, what we actually store in the computer‚Äôs memory
is the representation of these objects. Moreover, the idea of representa-
tion is not restricted to digital computers. When we write down text or
make a drawing we are representing ideas or experiences as sequences
of symbols (which might as well be strings of zeroes and ones). Even
our brain does not store the actual sensory inputs we experience, but
rather only a representation of them.
To use objects such as numbers, images, graphs, or others as inputs
for computation, we need to define precisely how to represent these
objects as binary strings. A representation scheme is a way to map an ob-
ject ùë•to a binary string ùê∏(ùë•) ‚àà{0, 1}‚àó. For example, a representation
scheme for natural numbers is a function ùê∏‚à∂‚Ñï‚Üí{0, 1}‚àó. Of course,
we cannot merely represent all numbers as the string ‚Äú0011‚Äù (for ex-
ample). A minimal requirement is that if two numbers ùë•and ùë•‚Ä≤ are
different then they would be represented by different strings. Another
way to say this is that we require the encoding function ùê∏to be one to
one.


--- Page 86 ---

86
introduction to theoretical computer science
Figure 2.3: Representing each one the digits
0, 1, 2, ‚Ä¶ , 9 as a 12 √ó 8 bitmap image, which can be
thought of as a string in {0, 1}96. Using this scheme
we can represent a natural number ùë•of ùëõdecimal
digits as a string in {0, 1}96ùëõ. Image taken from blog
post of A. C. Andersen.
2.1.1 Representing natural numbers
We now show how we can represent natural numbers as binary
strings. Over the years people have represented numbers in a variety
of ways, including Roman numerals, tally marks, our own Hindu-
Arabic decimal system, and many others. We can use any one of
those as well as many others to represent a number as a string (see
Fig. 2.3). However, for the sake of concreteness, we use the binary
basis as our default representation of natural numbers as strings.
For example, we represent the number six as the string 110 since
1 ‚ãÖ22 + 1 ‚ãÖ21 + 0 ‚ãÖ20 = 6, and similarly we represent the number thirty-
five as the string ùë¶= 100011 which satisfies ‚àë
5
ùëñ=0 ùë¶ùëñ‚ãÖ2|ùë¶|‚àíùëñ‚àí1 = 35.
Some more examples are given in the table below.
Table 2.1: Representing numbers in the binary basis. The lefthand
column contains representations of natural numbers in the deci-
mal basis, while the righthand column contains representations
of the same numbers in the binary basis.
Number (decimal representation)
Number (binary representation)
0
0
1
1
2
10
5
101
16
10000
40
101000
53
110101
389
110000101
3750
111010100110
If ùëõis even, then the least significant digit of ùëõ‚Äôs binary representa-
tion is 0, while if ùëõis odd then this digit equals 1. Just like the number
‚åäùëõ/10‚åãcorresponds to ‚Äúchopping off‚Äù the least significant decimal
digit (e.g., ‚åä457/10‚åã= ‚åä45.7‚åã= 45), the number ‚åäùëõ/2‚åãcorresponds
to the ‚Äúchopping off‚Äù the least significant binary digit. Hence the bi-
nary representation can be formally defined as the following function
ùëÅùë°ùëÜ‚à∂‚Ñï‚Üí{0, 1}‚àó(ùëÅùë°ùëÜstands for ‚Äúnatural numbers to strings‚Äù):
ùëÅùë°ùëÜ(ùëõ) =
‚éß
{
{
‚é®
{
{
‚é©
0
ùëõ= 0
1
ùëõ= 1
ùëÅùë°ùëÜ(‚åäùëõ/2‚åã)ùëùùëéùëüùëñùë°ùë¶(ùëõ)
ùëõ> 1
(2.1)
where ùëùùëéùëüùëñùë°ùë¶‚à∂‚Ñï‚Üí{0, 1} is the function defined as ùëùùëéùëüùëñùë°ùë¶(ùëõ) = 0
if ùëõis even and ùëùùëéùëüùëñùë°ùë¶(ùëõ) = 1 if ùëõis odd, and as usual, for strings
ùë•, ùë¶‚àà{0, 1}‚àó, ùë•ùë¶denotes the concatenation of ùë•and ùë¶. The function


--- Page 87 ---

computation and representation
87
ùëÅùë°ùëÜis defined recursively: for every ùëõ> 0 we define ùëüùëíùëù(ùëõ) in terms
of the representation of the smaller number ‚åäùëõ/2‚åã. It is also possible to
define ùëÅùë°ùëÜnon-recursively, see Exercise 2.2.
Throughout most of this book, the particular choices of represen-
tation of numbers as binary strings would not matter much: we just
need to know that such a representation exists. In fact, for many of our
purposes we can even use the simpler representation of mapping a
natural number ùëõto the length-ùëõall-zero string 0ùëõ.
R
Remark 2.1 ‚Äî Binary representation in python (optional).
We can implement the binary representation in Python
as follows:
def NtS(n):# natural numbers to strings
if n > 1:
return NtS(n // 2) + str(n % 2)
else:
return str(n % 2)
print(NtS(236))
# 11101100
print(NtS(19))
# 10011
We can also use Python to implement the inverse
transformation, mapping a string back to the natural
number it represents.
def StN(x):# String to number
k = len(x)-1
return sum(int(x[i])*(2**(k-i)) for i in
range(k+1))
‚Ü™
print(StN(NtS(236)))
# 236
R
Remark 2.2 ‚Äî Programming examples. In this book,
we sometimes use code examples as in Remark 2.1.
The point is always to emphasize that certain com-
putations can be achieved concretely, rather than
illustrating the features of Python or any other pro-
gramming language. Indeed, one of the messages of
this book is that all programming languages are in
a certain precise sense equivalent to one another, and
hence we could have just as well used JavaScript, C,
COBOL, Visual Basic or even BrainF*ck. This book
is not about programming, and it is absolutely OK if


--- Page 88 ---

88
introduction to theoretical computer science
1 While the Babylonians already invented a positional
system much earlier, the decimal positional system
we use today was invented by Indian mathematicians
around the third century. Arab mathematicians took
it up in the 8th century. It first received significant
attention in Europe with the publication of the 1202
book ‚ÄúLiber Abaci‚Äù by Leonardo of Pisa, also known as
Fibonacci, but it did not displace Roman numerals in
common usage until the 15th century.
you are not familiar with Python or do not follow code
examples such as those in Remark 2.1.
2.1.2 Meaning of representations (discussion)
It is natural for us to think of 236 as the ‚Äúactual‚Äù number, and of
11101100 as ‚Äúmerely‚Äù its representation. However, for most Euro-
peans in the middle ages CCXXXVI would be the ‚Äúactual‚Äù number and
236 (if they have even heard about it) would be the weird Hindu-
Arabic positional representation.1 When our AI robot overlords ma-
terialize, they will probably think of 11101100 as the ‚Äúactual‚Äù number
and of 236 as ‚Äúmerely‚Äù a representation that they need to use when
they give commands to humans.
So what is the ‚Äúactual‚Äù number? This is a question that philoso-
phers of mathematics have pondered over throughout history. Plato
argued that mathematical objects exist in some ideal sphere of exis-
tence (that to a certain extent is more ‚Äúreal‚Äù than the world we per-
ceive via our senses, as this latter world is merely the shadow of this
ideal sphere). In Plato‚Äôs vision, the symbols 236 are merely notation
for some ideal object, that, in homage to the late musician, we can
refer to as ‚Äúthe number commonly represented by 236‚Äù.
The Austrian philosopher Ludwig Wittgenstein, on the other hand,
argued that mathematical objects do not exist at all, and the only
things that exist are the actual marks on paper that make up 236,
11101100 or CCXXXVI. In Wittgenstein‚Äôs view, mathematics is merely
about formal manipulation of symbols that do not have any inherent
meaning. You can think of the ‚Äúactual‚Äù number as (somewhat recur-
sively) ‚Äúthat thing which is common to 236, 11101100 and CCXXXVI
and all other past and future representations that are meant to capture
the same object‚Äù.
While reading this book, you are free to choose your own phi-
losophy of mathematics, as long as you maintain the distinction be-
tween the mathematical objects themselves and the various particular
choices of representing them, whether as splotches of ink, pixels on a
screen, zeroes and one, or any other form.
2.2 REPRESENTATIONS BEYOND NATURAL NUMBERS
We have seen that natural numbers can be represented as binary
strings. We now show that the same is true for other types of objects,
including (potentially negative) integers, rational numbers, vectors,
lists, graphs and many others. In many instances, choosing the ‚Äúright‚Äù
string representation for a piece of data is highly nontrivial, and find-
ing the ‚Äúbest‚Äù one (e.g., most compact, best fidelity, most efficiently
manipulable, robust to errors, most informative features, etc.) is the


--- Page 89 ---

computation and representation
89
object of intense research. But for now, we focus on presenting some
simple representations for various objects that we would like to use as
inputs and outputs for computation.
2.2.1 Representing (potentially negative) integers
Since we can represent natural numbers as strings, we can
represent the full set of integers (i.e., members of the set
‚Ñ§= {‚Ä¶ , ‚àí3, ‚àí2, ‚àí1, 0, +1, +2, +3, ‚Ä¶} ) by adding one more bit
that represents the sign. To represent a (potentially negative) number
ùëö, we prepend to the representation of the natural number |ùëö| a bit ùúé
that equals 0 if ùëö‚â•0 and equals 1 if ùëö< 0. Formally, we define the
function ùëçùë°ùëÜ‚à∂‚Ñ§‚Üí{0, 1}‚àóas follows
ùëçùë°ùëÜ(ùëö) =
‚éß
{
‚é®
{
‚é©
0 ùëÅùë°ùëÜ(ùëö)
ùëö‚â•0
1 ùëÅùë°ùëÜ(‚àíùëö)
ùëö< 0
(2.2)
where ùëÅùë°ùëÜis defined as in (2.1).
While the encoding function of a representation needs to be one
to one, it does not have to be onto. For example, in the representation
above there is no number that is represented by the empty string
but it is still a fine representation, since every integer is represented
uniquely by some string.
R
Remark 2.3 ‚Äî Interpretation and context. Given a string
ùë¶
‚àà
{0, 1}‚àó, how do we know if it‚Äôs ‚Äúsupposed‚Äù to
represent a (nonnegative) natural number or a (po-
tentially negative) integer? For that matter, even if
we know ùë¶is ‚Äúsupposed‚Äù to be an integer, how do
we know what representation scheme it uses? The
short answer is that we do not necessarily know this
information, unless it is supplied from the context. (In
programming languages, the compiler or interpreter
determines the representation of the sequence of bits
corresponding to a variable based on the variable‚Äôs
type.) We can treat the same string ùë¶as representing a
natural number, an integer, a piece of text, an image,
or a green gremlin. Whenever we say a sentence such
as ‚Äúlet ùëõbe the number represented by the string ùë¶,‚Äù
we will assume that we are fixing some canonical rep-
resentation scheme such as the ones above. The choice
of the particular representation scheme will rarely
matter, except that we want to make sure to stick with
the same one for consistency.
2.2.2 Two‚Äôs complement representation (optional)
Section 2.2.1‚Äôs approach of representing an integer using a specific
‚Äúsign bit‚Äù is known as the Signed Magnitude Representation and was


--- Page 90 ---

90
introduction to theoretical computer science
Figure 2.4: In the two‚Äôs complement representation
we represent a potentially negative integer ùëò‚àà
{‚àí2ùëõ, ‚Ä¶ , 2ùëõ‚àí1} as an ùëõ+ 1 length string using the
binary representation of the integer ùëòmod 2ùëõ+1. On
the lefthand side: this representation for ùëõ= 3 (the
red integers are the numbers being represented by
the blue binary strings). If a microprocessor does not
check for overflows, adding the two positive numbers
6 and 5 might result in the negative number ‚àí5 (since
‚àí5 mod 16 = 11. The righthand side is a C program
that will on some 32 bit architecture print a negative
number after adding two positive numbers. (Integer
overflow in C is considered undefined behavior which
means the result of this program, including whether
it runs or crashes, could differ depending on the
architecture, compiler, and even compiler options and
version.)
used in some early computers. However, the two‚Äôs complement rep-
resentation is much more common in practice. The two‚Äôs complement
representation of an integer ùëòin the set {‚àí2ùëõ, ‚àí2ùëõ+ 1, ‚Ä¶ , 2ùëõ‚àí1} is the
string ùëçùë°ùëÜùëõ(ùëò) of length ùëõ+ 1 defined as follows:
ùëçùë°ùëÜùëõ(ùëò) =
‚éß
{
‚é®
{
‚é©
ùëÅùë°ùëÜùëõ+1(ùëò)
0 ‚â§ùëò‚â§2ùëõ‚àí1
ùëÅùë°ùëÜùëõ+1(2ùëõ+1 + ùëò)
‚àí2ùëõ‚â§ùëò‚â§‚àí1
,
(2.3)
where ùëÅùë°ùëÜ‚Ñì(ùëö) demotes the standard binary representation of a num-
ber ùëö‚àà{0, ‚Ä¶ , 2‚Ñì} as string of length ‚Ñì, padded with leading zeros
as needed. For example, if ùëõ= 3 then ùëçùë°ùëÜ3(1) = ùëÅùë°ùëÜ4(1) = 0001,
ùëçùë°ùëÜ3(2) = ùëÅùë°ùëÜ4(2) = 0010, ùëçùë°ùëÜ3(‚àí1) = ùëÅùë°ùëÜ4(16 ‚àí1) = 1111, and
ùëçùë°ùëÜ3(‚àí8) = ùëÅùë°ùëÜ4(16 ‚àí8) = 1000. If ùëòis a negative number larger than
or equal to ‚àí2ùëõthen 2ùëõ+1 + ùëòis a number between 2ùëõand 2ùëõ+1 ‚àí1.
Hence the two‚Äôs complement representation of such a number ùëòis a
string of length ùëõ+ 1 with its first digit equal to 1.
Another way to say this is that we represent a potentially negative
number ùëò‚àà{‚àí2ùëõ, ‚Ä¶ , 2ùëõ‚àí1} as the non-negative number ùëòmod 2ùëõ+1
(see also Fig. 2.4). This means that if two (potentially negative) num-
bers ùëòand ùëò‚Ä≤ are not too large (i.e., |ùëò| + |ùëò‚Ä≤| < 2ùëõ+1), then we can
compute the representation of ùëò+ ùëò‚Ä≤ by adding modulo 2ùëõ+1 the rep-
resentations of ùëòand ùëò‚Ä≤ as if they were non-negative integers. This
property of the two‚Äôs complement representation is its main attraction
since, depending on their architectures, microprocessors can often
perform arithmetic operations modulo 2ùë§very efficiently (for certain
values of ùë§such as 32 and 64). Many systems leave it to the pro-
grammer to check that values are not too large and will carry out this
modular arithmetic regardless of the size of the numbers involved. For
this reason, in some systems adding two large positive numbers can
result in a negative number (e.g., adding 2ùëõ‚àí100 and 2ùëõ‚àí200 might
result in ‚àí300 since (2ùëõ+1 ‚àí300) mod 2ùëõ+1 = ‚àí300, see also Fig. 2.4).
2.2.3 Rational numbers, and representing pairs of strings
We can represent a rational number of the form ùëé/ùëèby represent-
ing the two numbers ùëéand ùëè. However, merely concatenating the
representations of ùëéand ùëèwill not work. For example, the binary rep-
resentation of 4 is 100 and the binary representation of 43 is 101011,
but the concatenation 100101011 of these strings is also the concatena-
tion of the representation 10010 of 18 and the representation 1011 of
11. Hence, if we used such simple concatenation then we would not
be able to tell if the string 100101011 is supposed to represent 4/43 or
18/11.
We tackle this by giving a general representation for pairs of strings.
If we were using a pen and paper, we would just use a separator sym-
bol such as ‚Äñ to represent, for example, the pair consisting of the num-


--- Page 91 ---

computation and representation
91
bers represented by 10 and 110001 as the length-9 string ‚Äú01‚Äñ110001‚Äù.
In other words, there is a one to one map ùêπfrom pairs of strings
ùë•, ùë¶‚àà{0, 1}‚àóinto a single string ùëßover the alphabet Œ£ = {0, 1, ‚Äñ}
(in other words, ùëß‚ààŒ£‚àó). Using such separators is similar to the
way we use spaces and punctuation to separate words in English. By
adding a little redundancy, we achieve the same effect in the digital
domain. We can map the three-element set Œ£ to the three-element set
{00, 11, 01} ‚äÇ{0, 1}2 in a one-to-one fashion, and hence encode a
length ùëõstring ùëß‚ààŒ£‚àóas a length 2ùëõstring ùë§‚àà{0, 1}‚àó.
Our final representation for rational numbers is obtained by com-
posing the following steps:
1. Representing a non-negative rational number as a pair of natural
numbers.
2. Representing a natural number by a string via the binary represen-
tation.
3. Combining 1 and 2 to obtain a representation of a rational number
as a pair of strings.
4. Representing a pair of strings over {0, 1} as a single string over
Œ£ = {0, 1, ‚Äñ}.
5. Representing a string over Œ£ as a longer string over {0, 1}.
‚ñ†Example 2.4 ‚Äî Representing a rational number as a string. Consider the
rational number ùëü
=
‚àí5/8. We represent ‚àí5 as 1101 and +8 as
01000, and so we can represent ùëüas the pair of strings (1101, 01000)
and represent this pair as the length 10 string 1101‚Äñ01000 over
the alphabet {0, 1, ‚Äñ}. Now, applying the map 0
‚Ü¶
00, 1
‚Ü¶
11,
‚Äñ
‚Ü¶
01, we can represent the latter string as the length 20 string
ùë†= 11110011010011000000 over the alphabet {0, 1}.
The same idea can be used to represent triples of strings, quadru-
ples, and so on as a string. Indeed, this is one instance of a very gen-
eral principle that we use time and again in both the theory and prac-
tice of computer science (for example, in Object Oriented program-
ming):
ÔÉ´Big Idea 1 If we can represent objects of type ùëáas strings, then we
can represent tuples of objects of type ùëáas strings as well.
Repeating the same idea, once we can represent objects of type ùëá,
we can also represent lists of lists of such objects, and even lists of lists
of lists and so on and so forth. We will come back to this point when
we discuss prefix free encoding in Section 2.5.2.


--- Page 92 ---

92
introduction to theoretical computer science
Figure 2.6: XKCD cartoon on floating-point arithmetic.
2.3 REPRESENTING REAL NUMBERS
The set of real numbers ‚Ñùcontains all numbers including positive,
negative, and fractional, as well as irrational numbers such as ùúãor ùëí.
Every real number can be approximated by a rational number, and
thus we can represent every real number ùë•by a rational number ùëé/ùëè
that is very close to ùë•. For example, we can represent ùúãby 22/7 within
an error of about 10‚àí3. If we want a smaller error (e.g., about 10‚àí4)
then we can use 311/99, and so on and so forth.
Figure 2.5: The floating point representation of a real
number ùë•‚àà‚Ñùis its approximation as a number of
the form ùúéùëè‚ãÖ2ùëíwhere ùúé‚àà{¬±1}, ùëíis an (potentially
negative) integer, and ùëèis a rational number between
1 and 2 expressed as a binary fraction 1.ùëè0ùëè1ùëè2 ‚Ä¶ ùëèùëò
for some ùëè1, ‚Ä¶ , ùëèùëò‚àà{0, 1} (that is ùëè= 1 + ùëè1/2 +
ùëè2/4 + ‚Ä¶ + ùëèùëò/2ùëò). Commonly-used floating point
representations fix the numbers ‚Ñìand ùëòof bits to
represent ùëíand ùëèrespectively. In the example above,
assuming we use two‚Äôs complement representation
for ùëí, the number represented is ‚àí1 √ó 25 √ó (1 + 1/2 +
1/4 + 1/64 + 1/512) = ‚àí56.5625.
The above representation of real numbers via rational numbers
that approximate them is a fine choice for a representation scheme.
However, typically in computing applications, it is more common to
use the floating point representation scheme (see Fig. 2.5) to represent
real numbers. In the floating point representation scheme we rep-
resent ùë•‚àà‚Ñùby the pair (ùëè, ùëí) of (positive or negative) integers of
some prescribed sizes (determined by the desired accuracy) such that
ùëè√ó 2ùëíis closest to ùë•. Floating point representation is the base-two
version of scientific notation, where one represents a number ùë¶‚ààùëÖ
as its approximation of the form ùëè√ó 10ùëífor ùëè, ùëí. It is called ‚Äúfloating
point‚Äù because we can think of the number ùëèas specifying a sequence
of binary digits, and ùëías describing the location of the ‚Äúbinary point‚Äù
within this sequence. The use of floating representation is the reason
why in many programming systems, printing the expression 0.1+0.2
will result in 0.30000000000000004 and not 0.3, see here, here and
here for more.
The reader might be (rightly) worried about the fact that the float-
ing point representation (or the rational number one) can only ap-
proximately represent real numbers. In many (though not all) com-
putational applications, one can make the accuracy tight enough so
that this does not affect the final result, though sometimes we do need
to be careful. Indeed, floating-point bugs can sometimes be no jok-
ing matter. For example, floating point rounding errors have been
implicated in the failure of a U.S. Patriot missile to intercept an Iraqi
Scud missile, costing 28 lives, as well as a 100 million pound error in
computing payouts to British pensioners.


--- Page 93 ---

computation and representation
93
2 ùëÖùë°ùëÜstands for ‚Äúreal numbers to strings‚Äù.
2.4 CANTOR‚ÄôS THEOREM, COUNTABLE SETS, AND STRING REP-
RESENTATIONS OF THE REAL NUMBERS
‚ÄúFor any collection of fruits, we can make more fruit salads than there are
fruits. If not, we could label each salad with a different fruit, and consider the
salad of all fruits not in their salad. The label of this salad is in it if and only if
it is not.‚Äù, Martha Storey.
Given the issues with floating point approximations for real num-
bers, a natural question is whether it is possible to represent real num-
bers exactly as strings. Unfortunately, the following theorem shows
that this cannot be done:
Theorem 2.5 ‚Äî Cantor‚Äôs Theorem. There does not exist a one-to-one
function ùëÖùë°ùëÜ‚à∂‚Ñù‚Üí{0, 1}‚àó. 2
Countable sets.
We say that a set ùëÜis countable if there is an onto
map ùê∂‚à∂‚Ñï‚ÜíùëÜ, or in other words, we can write ùëÜas the sequence
ùê∂(0), ùê∂(1), ùê∂(2), ‚Ä¶. Since the binary representation yields an onto
map from {0, 1}‚àóto ‚Ñï, and the composition of two onto maps is onto,
a set ùëÜis countable iff there is an onto map from {0, 1}‚àóto ùëÜ. Using
the basic properties of functions (see Section 1.4.3), a set is countable
if and only if there is a one-to-one function from ùëÜto {0, 1}‚àó. Hence,
we can rephrase Theorem 2.5 as follows:
Theorem 2.6 ‚Äî Cantor‚Äôs Theorem (equivalent statement). The reals are un-
countable. That is, there does not exist an onto function ùëÅùë°ùëÖ‚à∂‚Ñï‚Üí
‚Ñù.
Theorem 2.6 was proven by Georg Cantor in 1874. This result (and
the theory around it) was quite shocking to mathematicians at the
time. By showing that there is no one-to-one map from ‚Ñùto {0, 1}‚àó(or
‚Ñï), Cantor showed that these two infinite sets have ‚Äúdifferent forms of
infinity‚Äù and that the set of real numbers ‚Ñùis in some sense ‚Äúbigger‚Äù
than the infinite set {0, 1}‚àó. The notion that there are ‚Äúshades of infin-
ity‚Äù was deeply disturbing to mathematicians and philosophers at the
time. The philosopher Ludwig Wittgenstein (whom we mentioned be-
fore) called Cantor‚Äôs results ‚Äúutter nonsense‚Äù and ‚Äúlaughable.‚Äù Others
thought they were even worse than that. Leopold Kronecker called
Cantor a ‚Äúcorrupter of youth,‚Äù while Henri Poincar√© said that Can-
tor‚Äôs ideas ‚Äúshould be banished from mathematics once and for all.‚Äù
The tide eventually turned, and these days Cantor‚Äôs work is univer-
sally accepted as the cornerstone of set theory and the foundations of
mathematics. As David Hilbert said in 1925, ‚ÄúNo one shall expel us from
the paradise which Cantor has created for us.‚Äù As we will see later in this


--- Page 94 ---

94
introduction to theoretical computer science
3 ùêπùë°ùëÜstands for ‚Äúfunctions to strings‚Äù.
4 ùêπùë°ùëÖstands for ‚Äúfunctions to reals.‚Äù
book, Cantor‚Äôs ideas also play a huge role in the theory of computa-
tion.
Now that we have discussed Theorem 2.5‚Äôs importance, let us see
the proof. It is achieved in two steps:
1. Define some infinite set ùí≥for which it is easier for us to prove that
ùí≥is not countable (namely, it‚Äôs easier for us to prove that there is
no one-to-one function from ùí≥to {0, 1}‚àó).
2. Prove that there is a one-to-one function ùê∫mapping ùí≥to ‚Ñù.
We can use a proof by contradiction to show that these two facts
together imply Theorem 2.5. Specifically, if we assume (towards the
sake of contradiction) that there exists some one-to-one ùêπmapping ‚Ñù
to {0, 1}‚àóthen the function ùë•‚Ü¶ùêπ(ùê∫(ùë•)) obtained by composing ùêπ
with the function ùê∫from Step 2 above would be a one-to-one function
from ùí≥to {0, 1}‚àó, which contradicts what we proved in Step 1!
To turn this idea into a full proof of Theorem 2.5 we need to:
‚Ä¢ Define the set ùí≥.
‚Ä¢ Prove that there is no one-to-one function from ùí≥to {0, 1}‚àó
‚Ä¢ Prove that there is a one-to-one function from ùí≥to ‚Ñù.
We now proceed to do precisely that. That is, we will define the set
{0, 1}‚àû, which will play the role of ùí≥, and then state and prove two
lemmas that show that this set satisfies our two desired properties.
Definition 2.7 We denote by {0, 1}‚àûthe set {ùëì| ùëì‚à∂‚Ñï‚Üí{0, 1}}.
That is, {0, 1}‚àûis a set of functions, and a function ùëìis in {0, 1}‚àû
iff its domain is ‚Ñïand its codomain is {0, 1}. We can also think of
{0, 1}‚àûas the set of all infinite sequences of bits, since a function ùëì‚à∂
‚Ñï‚Üí{0, 1} can be identified with the sequence (ùëì(0), ùëì(1), ùëì(2), ‚Ä¶).
The following two lemmas show that {0, 1}‚àûcan play the role of ùí≥to
establish Theorem 2.5.
Lemma 2.8 There does not exist a one-to-one map ùêπùë°ùëÜ‚à∂{0, 1}‚àû‚Üí
{0, 1}‚àó.3
Lemma 2.9 There does exist a one-to-one map ùêπùë°ùëÖ‚à∂{0, 1}‚àû‚Üí‚Ñù.4
As we‚Äôve seen above, Lemma 2.8 and Lemma 2.9 together imply
Theorem 2.5. To repeat the argument more formally, suppose, for
the sake of contradiction, that there did exist a one-to-one function
ùëÖùë°ùëÜ‚à∂‚Ñù‚Üí{0, 1}‚àó. By Lemma 2.9, there exists a one-to-one function
ùêπùë°ùëÖ‚à∂{0, 1}‚àû‚Üí‚Ñù. Thus, under this assumption, since the composi-
tion of two one-to-one functions is one-to-one (see Exercise 2.12), the


--- Page 95 ---

computation and representation
95
function ùêπùë°ùëÜ‚à∂{0, 1}‚àû‚Üí{0, 1}‚àódefined as ùêπùë°ùëÜ(ùëì) = ùëÖùë°ùëÜ(ùêπùë°ùëÖ(ùëì))
will be one to one, contradicting Lemma 2.8. See Fig. 2.7 for a graphi-
cal illustration of this argument.
Figure 2.7: We prove Theorem 2.5 by combining
Lemma 2.8 and Lemma 2.9. Lemma 2.9, which uses
standard calculus tools, shows the existence of a
one-to-one map ùêπùë°ùëÖfrom the set {0, 1}‚àûto the real
numbers. So, if a hypothetical one-to-one map ùëÖùë°ùëÜ‚à∂
‚Ñù‚Üí{0, 1}‚àóexisted, then we could compose them
to get a one-to-one map ùêπùë°ùëÜ‚à∂{0, 1}‚àû‚Üí{0, 1}‚àó.
Yet this contradicts Lemma 2.8- the heart of the proof-
which rules out the existence of such a map.
Now all that is left is to prove these two lemmas. We start by prov-
ing Lemma 2.8 which is really the heart of Theorem 2.5.
Figure 2.8: We construct a function ùëësuch that ùëë‚â†
ùëÜùë°ùêπ(ùë•) for every ùë•‚àà{0, 1}‚àóby ensuring that
ùëë(ùëõ(ùë•)) ‚â†ùëÜùë°ùêπ(ùë•)(ùëõ(ùë•)) for every ùë•‚àà{0, 1}‚àó
with lexicographic order ùëõ(ùë•). We can think of this
as building a table where the columns correspond
to numbers ùëö‚àà‚Ñïand the rows correspond to
ùë•‚àà{0, 1}‚àó(sorted according to ùëõ(ùë•)). If the entry
in the ùë•-th row and the ùëö-th column corresponds to
ùëî(ùëö)) where ùëî= ùëÜùë°ùêπ(ùë•) then ùëëis obtained by going
over the ‚Äúdiagonal‚Äù elements in this table (the entries
corresponding to the ùë•-th row and ùëõ(ùë•)-th column)
and ensuring that ùëë(ùë•)(ùëõ(ùë•)) ‚â†ùëÜùë°ùêπ(ùë•)(ùë•).
Warm-up: ‚ÄùBaby Cantor‚Äù.
The proof of Lemma 2.8 is rather subtle. One
way to get intution for it is to consider the following finite statement
‚Äùthere is no onto function ùëì‚à∂{0, ‚Ä¶ , 99} ‚Üí{0, 1}100. Of course we
know it‚Äôs true since the set {0, 1}100 is bigger than the set [100], but
let‚Äôs see a direct proof. For every ùëì‚à∂{0, ‚Ä¶ , 99} ‚Üí{0, 1}100, we
can define the string ùëë‚àà{0, 1}100 as follows: ùëë= (1 ‚àíùëì(0)0, 1 ‚àí
ùëì(1)1, ‚Ä¶ , 1 ‚àíùëì(99)99). If ùëìwas onto, then there would exist some
ùëõ‚àà[100] such that ùëì(ùëõ) = ùëë(ùëõ), but we claim that no such ùëõexists.


--- Page 96 ---

96
introduction to theoretical computer science
Indeed, if there was such ùëõ, then the ùëõ-th coordinate of ùëëwould equal
ùëì(ùëõ)ùëõbut by definition this coordinate equals 1 ‚àíùëì(ùëõ)ùëõ. See also a
‚Äúproof by code‚Äù of this statement.
Proof of Lemma 2.8. We will prove that there does not exist an onto
function ùëÜùë°ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}‚àû. This implies the lemma since
for every two sets ùê¥and ùêµ, there exists an onto function from ùê¥to
ùêµif and only if there exists a one-to-one function from ùêµto ùê¥(see
Lemma 1.2).
The technique of this proof is known as the ‚Äúdiagonal argument‚Äù
and is illustrated in Fig. 2.8. We assume, towards a contradiction, that
there exists such a function ùëÜùë°ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}‚àû. We will show
that ùëÜùë°ùêπis not onto by demonstrating a function ùëë‚àà{0, 1}‚àûsuch that
ùëë‚â†ùëÜùë°ùêπ(ùë•) for every ùë•‚àà{0, 1}‚àó. Consider the lexicographic ordering
of binary strings (i.e., "",0,1,00,01,‚Ä¶). For every ùëõ‚àà‚Ñï, we let ùë•ùëõbe the
ùëõ-th string in this order. That is ùë•0 = "", ùë•1 = 0, ùë•2 = 1 and so on and
so forth. We define the function ùëë‚àà{0, 1}‚àûas follows:
ùëë(ùëõ) = 1 ‚àíùëÜùë°ùêπ(ùë•ùëõ)(ùëõ)
(2.4)
for every ùëõ‚àà‚Ñï. That is, to compute ùëëon input ùëõ‚àà‚Ñï, we first com-
pute ùëî= ùëÜùë°ùêπ(ùë•ùëõ), where ùë•ùëõ‚àà{0, 1}‚àóis the ùëõ-th string in the lexico-
graphical ordering. Since ùëî‚àà{0, 1}‚àû, it is a function mapping ‚Ñïto
{0, 1}. The value ùëë(ùëõ) is defined to be the negation of ùëî(ùëõ).
The definition of the function ùëëis a bit subtle. One way to think
about it is to imagine the function ùëÜùë°ùêπas being specified by an in-
finitely long table, in which every row corresponds to a string ùë•‚àà
{0, 1}‚àó(with strings sorted in lexicographic order), and contains the
sequence ùëÜùë°ùêπ(ùë•)(0), ùëÜùë°ùêπ(ùë•)(1), ùëÜùë°ùêπ(ùë•)(2), ‚Ä¶. The diagonal elements in
this table are the values
ùëÜùë°ùêπ("")(0), ùëÜùë°ùêπ(0)(1), ùëÜùë°ùêπ(1)(2), ùëÜùë°ùêπ(00)(3), ùëÜùë°ùêπ(01)(4), ‚Ä¶
(2.5)
which correspond to the elements ùëÜùë°ùêπ(ùë•ùëõ)(ùëõ) in the ùëõ-th row and
ùëõ-th column of this table for ùëõ= 0, 1, 2, ‚Ä¶. The function ùëëwe defined
above maps every ùëõ‚àà‚Ñïto the negation of the ùëõ-th diagonal value.
To complete the proof that ùëÜùë°ùêπis not onto we need to show that
ùëë‚â†ùëÜùë°ùêπ(ùë•) for every ùë•‚àà{0, 1}‚àó. Indeed, let ùë•‚àà{0, 1}‚àóbe some string
and let ùëî= ùëÜùë°ùêπ(ùë•). If ùëõis the position of ùë•in the lexicographical
order then by construction ùëë(ùëõ) = 1 ‚àíùëî(ùëõ) ‚â†ùëî(ùëõ) which means that
ùëî‚â†ùëëwhich is what we wanted to prove.
‚ñ†


--- Page 97 ---

computation and representation
97
R
Remark 2.10 ‚Äî Generalizing beyond strings and reals.
Lemma 2.8 doesn‚Äôt really have much to do with the
natural numbers or the strings. An examination of
the proof shows that it really shows that for every
set ùëÜ, there is no one-to-one map ùêπ
‚à∂
{0, 1}ùëÜ
‚Üí
ùëÜ
where {0, 1}ùëÜdenotes the set {ùëì
| ùëì
‚à∂ùëÜ‚Üí{0, 1}}
of all Boolean functions with domain ùëÜ. Since we can
identify a subset ùëâ‚äÜùëÜwith its characteristic function
ùëì
= 1ùëâ(i.e., 1ùëâ(ùë•) = 1 iff ùë•‚ààùëâ), we can think of
{0, 1}ùëÜalso as the set of all subsets of ùëÜ. This subset
is sometimes called the power set of ùëÜand denoted by
ùí´(ùëÜ) or 2ùëÜ.
The proof of Lemma 2.8 can be generalized to show
that there is no one-to-one map between a set and its
power set. In particular, it means that the set {0, 1}‚Ñùis
‚Äúeven bigger‚Äù than ‚Ñù. Cantor used these ideas to con-
struct an infinite hierarchy of shades of infinity. The
number of such shades turns out to be much larger
than |‚Ñï| or even |‚Ñù|. He denoted the cardinality of ‚Ñï
by ‚Ñµ0 and denoted the next largest infinite number
by ‚Ñµ1. (‚Ñµis the first letter in the Hebrew alphabet.)
Cantor also made the continuum hypothesis that
|‚Ñù|
=
‚Ñµ1. We will come back to the fascinating story
of this hypothesis later on in this book. This lecture of
Aaronson mentions some of these issues (see also this
Berkeley CS 70 lecture).
To complete the proof of Theorem 2.5, we need to show Lemma 2.9.
This requires some calculus background but is otherwise straightfor-
ward. If you have not had much experience with limits of a real series
before, then the formal proof below might be a little hard to follow.
This part is not the core of Cantor‚Äôs argument, nor are such limits
important to the remainder of this book, so you can feel free to take
Lemma 2.9 on faith and skip the proof.
Proof Idea:
We define ùêπùë°ùëÖ(ùëì) to be the number between 0 and 2 whose dec-
imal expansion is ùëì(0).ùëì(1)ùëì(2) ‚Ä¶, or in other words ùêπùë°ùëÖ(ùëì) =
‚àë
‚àû
ùëñ=0 ùëì(ùëñ) ‚ãÖ10‚àíùëñ. If ùëìand ùëîare two distinct functions in {0, 1}‚àû, then
there must be some input ùëòin which they disagree. If we take the
minimum such ùëò, then the numbers ùëì(0).ùëì(1)ùëì(2) ‚Ä¶ ùëì(ùëò‚àí1)ùëì(ùëò) ‚Ä¶
and ùëî(0).ùëî(1)ùëî(2) ‚Ä¶ ùëî(ùëò) ‚Ä¶ agree with each other all the way up to the
ùëò‚àí1-th digit after the decimal point, and disagree on the ùëò-th digit.
But then these numbers must be distinct. Concretely, if ùëì(ùëò) = 1 and
ùëî(ùëò) = 0 then the first number is larger than the second, and otherwise
(ùëì(ùëò) = 0 and ùëî(ùëò) = 1) the first number is smaller than the second.
In the proof we have to be a little careful since these are numbers with
infinite expansions. For example, the number one half has two decimal


--- Page 98 ---

98
introduction to theoretical computer science
expansions 0.5 and 0.49999 ‚ãØ. However, this issue does not come up
here, since we restrict attention only to numbers with decimal expan-
sions that do not involve the digit 9.
‚ãÜ
Proof of Lemma 2.9. For every ùëì‚àà{0, 1}‚àû, we define ùêπùë°ùëÖ(ùëì) to be the
number whose decimal expansion is ùëì(0).ùëì(1)ùëì(2)ùëì(3) ‚Ä¶. Formally,
ùêπùë°ùëÖ(ùëì) =
‚àû
‚àë
ùëñ=0
ùëì(ùëñ) ‚ãÖ10‚àíùëñ
(2.6)
It is a known result in calculus (whose proof we will not repeat here)
that the series on the righthand side of (2.6) converges to a definite
limit in ‚Ñù.
We now prove that ùêπùë°ùëÖis one to one. Let ùëì, ùëîbe two distinct func-
tions in {0, 1}‚àû. Since ùëìand ùëîare distinct, there must be some input
on which they differ, and we define ùëòto be the smallest such input
and assume without loss of generality that ùëì(ùëò) = 0 and ùëî(ùëò) = 1.
(Otherwise, if ùëì(ùëò) = 1 and ùëî(ùëò) = 0, then we can simply switch the
roles of ùëìand ùëî.) The numbers ùêπùë°ùëÖ(ùëì) and ùêπùë°ùëÖ(ùëî) agree with each
other up to the ùëò‚àí1-th digit up after the decimal point. Since this digit
equals 0 for ùêπùë°ùëÖ(ùëì) and equals 1 for ùêπùë°ùëÖ(ùëî), we claim that ùêπùë°ùëÖ(ùëî) is
bigger than ùêπùë°ùëÖ(ùëì) by at least 0.5 ‚ãÖ10‚àíùëò. To see this note that the dif-
ference ùêπùë°ùëÖ(ùëî) ‚àíùêπùë°ùëÖ(ùëì) will be minimized if ùëî(‚Ñì) = 0 for every ‚Ñì> ùëò
and ùëì(‚Ñì) = 1 for every ‚Ñì> ùëò, in which case (since ùëìand ùëîagree up to
the ùëò‚àí1-th digit)
ùêπùë°ùëÖ(ùëî) ‚àíùêπùë°ùëÖ(ùëì) = 10‚àíùëò‚àí10‚àíùëò‚àí1 ‚àí10‚àíùëò‚àí2 ‚àí10‚àíùëò‚àí3 ‚àí‚ãØ
(2.7)
Since the infinite series ‚àë
‚àû
ùëó=0 10‚àíùëñconverges to 10/9, it follows that
for every such ùëìand ùëî, ùêπùë°ùëÖ(ùëî) ‚àíùêπùë°ùëÖ(ùëì) ‚â•10‚àíùëò‚àí10‚àíùëò‚àí1 ‚ãÖ(10/9) > 0.
In particular we see that for every distinct ùëì, ùëî‚àà{0, 1}‚àû, ùêπùë°ùëÖ(ùëì) ‚â†
ùêπùë°ùëÖ(ùëî), implying that the function ùêπùë°ùëÖis one to one.
‚ñ†
R
Remark 2.11 ‚Äî Using decimal expansion (op-
tional). In the proof above we used the fact that
1
+
1/10
+
1/100
+
‚ãØconverges to 10/9, which
plugging into (2.7) yields that the difference between
ùêπùë°ùëÖ(ùëî) and ùêπùë°ùëÖ(‚Ñé) is at least 10‚àíùëò‚àí10‚àíùëò‚àí1‚ãÖ(10/9) > 0.
While the choice of the decimal representation for ùêπùë°ùëÖ
was arbitrary, we could not have used the binary
representation in its place. Had we used the binary
expansion instead of decimal, the corresponding se-
quence 1 + 1/2 + 1/4 + ‚ãØconverges to 2/1
=
2,


--- Page 99 ---

computation and representation
99
and since 2‚àíùëò
=
2‚àíùëò‚àí1 ‚ãÖ2, we could not have de-
duced that ùêπùë°ùëÖis one to one. Indeed there do exist
pairs of distinct sequences ùëì, ùëî
‚àà
{0, 1}‚àûsuch that
‚àë
‚àû
ùëñ=0 ùëì(ùëñ)2‚àíùëñ
=
‚àë
‚àû
ùëñ=0 ùëî(ùëñ)2‚àíùëñ. (For example, the se-
quence 1, 0, 0, 0, ‚Ä¶ and the sequence 0, 1, 1, 1, ‚Ä¶ have
this property.)
2.4.1 Corollary: Boolean functions are uncountable
Cantor‚Äôs Theorem yields the following corollary that we will use
several times in this book: the set of all Boolean functions (mapping
{0, 1}‚àóto {0, 1}) is not countable:
Theorem 2.12 ‚Äî Boolean functions are uncountable. Let ALL be the set of
all functions ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}. Then ALL is uncountable. Equiv-
alently, there does not exist an onto map ùëÜùë°ùê¥ùêøùêø‚à∂{0, 1}‚àó‚ÜíALL.
Proof Idea:
This is a direct consequence of Lemma 2.8, since we can use the
binary representation to show a one-to-one map from {0, 1}‚àûto ALL.
Hence the uncountability of {0, 1}‚àûimplies the uncountability of
ALL.
‚ãÜ
Proof of Theorem 2.12. Since {0, 1}‚àûis uncountable, the result will
follow by showing a one-to-one map from {0, 1}‚àûto ALL. The reason
is that the existence of such a map implies that if ALL was countable,
and hence there was a one-to-one map from ALL to ‚Ñï, then there
would have been a one-to-one map from {0, 1}‚àûto ‚Ñï, contradicting
Lemma 2.8.
We now show this one-to-one map. We simply map a function
ùëì‚àà{0, 1}‚àûto the function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1} as follows. We let
ùêπ(0) = ùëì(0), ùêπ(1) = ùëì(1), ùêπ(10) = ùêπ(2), ùêπ(11) = ùêπ(3) and so on and
so forth. That is, for every ùë•‚àà{0, 1}‚àóthat represents a natural number
ùëõin the binary basis, we define ùêπ(ùë•) = ùëì(ùëõ). If ùë•does not represent
such a number (e.g., it has a leading zero), then we set ùêπ(ùë•) = 0.
This map is one-to-one since if ùëì‚â†ùëîare two distinct elements in
{0, 1}‚àû, then there must be some input ùëõ‚àà‚Ñïon which ùëì(ùëõ) ‚â†ùëî(ùëõ).
But then if ùë•‚àà{0, 1}‚àóis the string representing ùëõ, we see that ùêπ(ùë•) ‚â†
ùê∫(ùë•) where ùêπis the function in ALL that ùëìmapped to, and ùê∫is the
function that ùëîis mapped to.
‚ñ†


--- Page 100 ---

100
introduction to theoretical computer science
2.4.2 Equivalent conditions for countability
The results above establish many equivalent ways to phrase the fact
that a set is countable. Specifically, the following statements are all
equivalent:
1. The set ùëÜis countable
2. There exists an onto map from ‚Ñïto ùëÜ
3. There exists an onto map from {0, 1}‚àóto ùëÜ.
4. There exists a one-to-one map from ùëÜto ‚Ñï
5. There exists a one-to-one map from ùëÜto {0, 1}‚àó.
6. There exists an onto map from some countable set ùëáto ùëÜ.
7. There exists a one-to-one map from ùëÜto some countable set ùëá.
P
Make sure you know how to prove the equivalence of
all the results above.
2.5 REPRESENTING OBJECTS BEYOND NUMBERS
Numbers are of course by no means the only objects that we can repre-
sent as binary strings. A representation scheme for representing objects
from some set ùí™consists of an encoding function that maps an object in
ùí™to a string, and a decoding function that decodes a string back to an
object in ùí™. Formally, we make the following definition:
Definition 2.13 ‚Äî String representation. Let ùí™be any set. A representation
scheme for ùí™is a pair of functions ùê∏, ùê∑where ùê∏‚à∂ùí™‚Üí{0, 1}‚àóis a
total one-to-one function, ùê∑‚à∂{0, 1}‚àó‚Üíùëùùí™is a (possibly partial)
function, and such that ùê∑and ùê∏satisfy that ùê∑(ùê∏(ùëú)) = ùëúfor every
ùëú‚ààùí™. ùê∏is known as the encoding function and ùê∑is known as the
decoding function.
Note that the condition ùê∑(ùê∏(ùëú)) = ùëúfor every ùëú‚ààùí™implies
that ùê∑is onto (can you see why?). It turns out that to construct a
representation scheme we only need to find an encoding function. That
is, every one-to-one encoding function has a corresponding decoding
function, as shown in the following lemma:
Lemma 2.14 Suppose that ùê∏‚à∂ùí™‚Üí{0, 1}‚àóis one-to-one. Then there
exists a function ùê∑‚à∂{0, 1}‚àó‚Üíùí™such that ùê∑(ùê∏(ùëú)) = ùëúfor every
ùëú‚ààùí™.


--- Page 101 ---

computation and representation
101
Proof. Let ùëú0 be some arbitrary element of ùí™. For every ùë•‚àà{0, 1}‚àó,
there exists either zero or a single ùëú‚ààùí™such that ùê∏(ùëú) = ùë•(otherwise
ùê∏would not be one-to-one). We will define ùê∑(ùë•) to equal ùëú0 in the
first case and this single object ùëúin the second case. By definition
ùê∑(ùê∏(ùëú)) = ùëúfor every ùëú‚ààùí™.
‚ñ†
R
Remark 2.15 ‚Äî Total decoding functions. While the
decoding function of a representation scheme can in
general be a partial function, the proof of Lemma 2.14
implies that every representation scheme has a total
decoding function. This observation can sometimes be
useful.
2.5.1 Finite representations
If ùí™is finite, then we can represent every object in ùí™as a string of
length at most some number ùëõ. What is the value of ùëõ? Let us denote
by {0, 1}‚â§ùëõthe set {ùë•‚àà{0, 1}‚àó‚à∂|ùë•| ‚â§ùëõ} of strings of length at most ùëõ.
The size of {0, 1}‚â§ùëõis equal to
|{0, 1}0| + |{0, 1}1| + |{0, 1}2| + ‚ãØ+ |{0, 1}ùëõ| =
ùëõ
‚àë
ùëñ=0
2ùëñ= 2ùëõ+1 ‚àí1 (2.8)
using the standard formula for summing a geometric progression.
To obtain a representation of objects in ùí™as strings of length at
most ùëõwe need to come up with a one-to-one function from ùí™to
{0, 1}‚â§ùëõ. We can do so, if and only if |ùí™| ‚â§2ùëõ+1 ‚àí1 as is implied by
the following lemma:
Lemma 2.16 For every two finite sets ùëÜ, ùëá, there exists a one-to-one
ùê∏‚à∂ùëÜ‚Üíùëáif and only if |ùëÜ| ‚â§|ùëá|.
Proof. Let ùëò= |ùëÜ| and ùëö= |ùëá| and so write the elements of ùëÜand
ùëáas ùëÜ= {ùë†0, ùë†1, ‚Ä¶ , ùë†ùëò‚àí1} and ùëá= {ùë°0, ùë°1, ‚Ä¶ , ùë°ùëö‚àí1}. We need to
show that there is a one-to-one function ùê∏‚à∂ùëÜ‚Üíùëáiff ùëò‚â§ùëö. For
the ‚Äúif‚Äù direction, if ùëò‚â§ùëöwe can simply define ùê∏(ùë†ùëñ) = ùë°ùëñfor every
ùëñ‚àà[ùëò]. Clearly for ùëñ‚â†ùëó, ùë°ùëñ= ùê∏(ùë†ùëñ) ‚â†ùê∏(ùë†ùëó) = ùë°ùëó, and hence this
function is one-to-one. In the other direction, suppose that ùëò> ùëöand
ùê∏‚à∂ùëÜ‚Üíùëáis some function. Then ùê∏cannot be one-to-one. Indeed, for
ùëñ= 0, 1, ‚Ä¶ , ùëö‚àí1 let us ‚Äúmark‚Äù the element ùë°ùëó= ùê∏(ùë†ùëñ) in ùëá. If ùë°ùëówas
marked before, then we have found two objects in ùëÜmapping to the
same element ùë°ùëó. Otherwise, since ùëáhas ùëöelements, when we get to
ùëñ= ùëö‚àí1 we mark all the objects in ùëá. Hence, in this case, ùê∏(ùë†ùëö) must
map to an element that was already marked before. (This observation
is sometimes known as the ‚ÄúPigeon Hole Principle‚Äù: the principle that


--- Page 102 ---

102
introduction to theoretical computer science
if you have a pigeon coop with ùëöholes, and ùëò> ùëöpigeons, then there
must be two pigeons in the same hole. )
‚ñ†
2.5.2 Prefix-free encoding
When showing a representation scheme for rational numbers, we used
the ‚Äúhack‚Äù of encoding the alphabet {0, 1, ‚Äñ} to represent tuples of
strings as a single string. This is a special case of the general paradigm
of prefix-free encoding. The idea is the following: if our representation
has the property that no string ùë•representing an object ùëúis a prefix
(i.e., an initial substring) of a string ùë¶representing a different object
ùëú‚Ä≤, then we can represent a list of objects by merely concatenating the
representations of all the list members. For example, because in En-
glish every sentence ends with a punctuation mark such as a period,
exclamation, or question mark, no sentence can be a prefix of another
and so we can represent a list of sentences by merely concatenating
the sentences one after the other. (English has some complications
such as periods used for abbreviations (e.g., ‚Äúe.g.‚Äù) or sentence quotes
containing punctuation, but the high level point of a prefix-free repre-
sentation for sentences still holds.)
It turns out that we can transform every representation to a prefix-
free form. This justifies Big Idea 1, and allows us to transform a repre-
sentation scheme for objects of a type ùëáto a representation scheme of
lists of objects of the type ùëá. By repeating the same technique, we can
also represent lists of lists of objects of type ùëá, and so on and so forth.
But first let us formally define prefix-freeness:
Definition 2.17 ‚Äî Prefix free encoding. For two strings ùë¶, ùë¶‚Ä≤, we say that ùë¶
is a prefix of ùë¶‚Ä≤ if |ùë¶| ‚â§|ùë¶‚Ä≤| and for every ùëñ< |ùë¶|, ùë¶‚Ä≤
ùëñ= ùë¶ùëñ.
Let ùí™be a non-empty set and ùê∏‚à∂ùí™‚Üí{0, 1}‚àóbe a function. We
say that ùê∏is prefix-free if ùê∏(ùëú) is non-empty for every ùëú
‚àà
ùí™and
there does not exist a distinct pair of objects ùëú, ùëú‚Ä≤
‚àà
ùí™such that
ùê∏(ùëú) is a prefix of ùê∏(ùëú‚Ä≤).
Recall that for every set ùí™, the set ùí™‚àóconsists of all finite length
tuples (i.e., lists) of elements in ùí™. The following theorem shows that
if ùê∏is a prefix-free encoding of ùí™then by concatenating encodings we
can obtain a valid (i.e., one-to-one) representation of ùí™‚àó:
Theorem 2.18 ‚Äî Prefix-free implies tuple encoding. Suppose that ùê∏‚à∂ùí™‚Üí
{0, 1}‚àóis prefix-free. Then the following map ùê∏‚à∂ùí™‚àó‚Üí{0, 1}‚àóis
one to one, for every ùëú0, ‚Ä¶ , ùëúùëò‚àí1 ‚ààùí™‚àó, we define
ùê∏(ùëú0, ‚Ä¶ , ùëúùëò‚àí1) = ùê∏(ùëú0)ùê∏(ùëú1) ‚ãØùê∏(ùëúùëò‚àí1) .
(2.9)


--- Page 103 ---

computation and representation
103
Figure 2.9: If we have a prefix-free representation of
each object then we can concatenate the representa-
tions of ùëòobjects to obtain a representation for the
tuple (ùëú0, ‚Ä¶ , ùëúùëò‚àí1).
P
Theorem 2.18 is an example of a theorem that is a little
hard to parse, but in fact is fairly straightforward to
prove once you understand what it means. Therefore,
I highly recommend that you pause here to make
sure you understand the statement of this theorem.
You should also try to prove it on your own before
proceeding further.
Proof Idea:
The idea behind the proof is simple. Suppose that for example
we want to decode a triple (ùëú0, ùëú1, ùëú2) from its representation ùë•=
ùê∏(ùëú0, ùëú1, ùëú2) = ùê∏(ùëú0)ùê∏(ùëú1)ùê∏(ùëú2). We will do so by first finding the
first prefix ùë•0 of ùë•that is a representation of some object. Then we
will decode this object, remove ùë•0 from ùë•to obtain a new string ùë•‚Ä≤,
and continue onwards to find the first prefix ùë•1 of ùë•‚Ä≤ and so on and so
forth (see Exercise 2.9). The prefix-freeness property of ùê∏will ensure
that ùë•0 will in fact be ùê∏(ùëú0), ùë•1 will be ùê∏(ùëú1), etc.
‚ãÜ
Proof of Theorem 2.18. We now show the formal proof. Suppose, to-
wards the sake of contradiction, that there exist two distinct tuples
(ùëú0, ‚Ä¶ , ùëúùëò‚àí1) and (ùëú‚Ä≤
0, ‚Ä¶ , ùëú‚Ä≤
ùëò‚Ä≤‚àí1) such that
ùê∏(ùëú0, ‚Ä¶ , ùëúùëò‚àí1) = ùê∏(ùëú‚Ä≤
0, ‚Ä¶ , ùëú‚Ä≤
ùëò‚Ä≤‚àí1) .
(2.10)
We will denote the string ùê∏(ùëú0, ‚Ä¶ , ùëúùëò‚àí1) by ùë•.
Let ùëñbe the first index such that ùëúùëñ‚â†ùëú‚Ä≤
ùëñ. (If ùëúùëñ= ùëú‚Ä≤
ùëñfor all ùëñthen,
since we assume the two tuples are distinct, one of them must be
larger than the other. In this case we assume without loss of generality
that ùëò‚Ä≤ > ùëòand let ùëñ= ùëò.) In the case that ùëñ< ùëò, we see that the string
ùë•can be written in two different ways:
ùë•= ùê∏(ùëú0, ‚Ä¶ , ùëúùëò‚àí1) = ùë•0 ‚ãØùë•ùëñ‚àí1ùê∏(ùëúùëñ)ùê∏(ùëúùëñ+1) ‚ãØùê∏(ùëúùëò‚àí1)
(2.11)
and
ùë•= ùê∏(ùëú‚Ä≤
0, ‚Ä¶ , ùëú‚Ä≤
ùëò‚Ä≤‚àí1) = ùë•0 ‚ãØùë•ùëñ‚àí1ùê∏(ùëú‚Ä≤
ùëñ)ùê∏(ùëú‚Ä≤
ùëñ+1) ‚ãØùê∏(ùëú‚Ä≤
ùëò‚àí1)
(2.12)
where ùë•ùëó= ùê∏(ùëúùëó) = ùê∏(ùëú‚Ä≤
ùëó) for all ùëó< ùëñ. Let ùë¶be the string obtained
after removing the prefix ùë•0 ‚ãØùë•ùëñ‚àíùëñfrom ùë•. We see that ùë¶can be writ-
ten as both ùë¶= ùê∏(ùëúùëñ)ùë†for some string ùë†‚àà{0, 1}‚àóand as ùë¶= ùê∏(ùëú‚Ä≤
ùëñ)ùë†‚Ä≤
for some ùë†‚Ä≤ ‚àà{0, 1}‚àó. But this means that one of ùê∏(ùëúùëñ) and ùê∏(ùëú‚Ä≤
ùëñ) must
be a prefix of the other, contradicting the prefix-freeness of ùê∏.


--- Page 104 ---

104
introduction to theoretical computer science
In the case that ùëñ= ùëòand ùëò‚Ä≤ > ùëò, we get a contradiction in the
following way. In this case
ùë•= ùê∏(ùëú0) ‚ãØùê∏(ùëúùëò‚àí1) = ùê∏(ùëú0) ‚ãØùê∏(ùëúùëò‚àí1)ùê∏(ùëú‚Ä≤
ùëò) ‚ãØùê∏(ùëú‚Ä≤
ùëò‚Ä≤‚àí1)
(2.13)
which means that ùê∏(ùëú‚Ä≤
ùëò) ‚ãØùê∏(ùëú‚Ä≤
ùëò‚Ä≤‚àí1) must correspond to the empty
string "". But in such a case ùê∏(ùëú‚Ä≤
ùëò) must be the empty string, which in
particular is the prefix of any other string, contradicting the prefix-
freeness of ùê∏.
‚ñ†
R
Remark 2.19 ‚Äî Prefix freeness of list representation.
Even if the representation ùê∏of objects in ùí™is prefix
free, this does not mean that our representation ùê∏
of lists of such objects will be prefix free as well. In
fact, it won‚Äôt be: for every three objects ùëú, ùëú‚Ä≤, ùëú‚Ä≥ the
representation of the list (ùëú, ùëú‚Ä≤) will be a prefix of the
representation of the list (ùëú, ùëú‚Ä≤, ùëú‚Ä≥). However, as we see
in Lemma 2.20 below, we can transform every repre-
sentation into prefix-free form, and so will be able to
use that transformation if needed to represent lists of
lists, lists of lists of lists, and so on and so forth.
2.5.3 Making representations prefix-free
Some natural representations are prefix-free. For example, every fixed
output length representation (i.e., one-to-one function ùê∏‚à∂ùí™‚Üí{0, 1}ùëõ)
is automatically prefix-free, since a string ùë•can only be a prefix of
an equal-length ùë•‚Ä≤ if ùë•and ùë•‚Ä≤ are identical. Moreover, the approach
we used for representing rational numbers can be used to show the
following:
Lemma 2.20 Let ùê∏‚à∂ùí™‚Üí{0, 1}‚àóbe a one-to-one function. Then there is
a one-to-one prefix-free encoding ùê∏such that |ùê∏(ùëú)| ‚â§2|ùê∏(ùëú)| + 2 for
every ùëú‚ààùí™.
P
For the sake of completeness, we will include the
proof below, but it is a good idea for you to pause
here and try to prove it on your own, using the same
technique we used for representing rational numbers.
Proof of Lemma 2.20. The idea behind the proof is to use the map 0 ‚Ü¶
00, 1 ‚Ü¶11 to ‚Äúdouble‚Äù every bit in the string ùë•and then mark the
end of the string by concatenating to it the pair 01. If we encode a


--- Page 105 ---

computation and representation
105
string ùë•in this way, it ensures that the encoding of ùë•is never a prefix
of the encoding of a distinct string ùë•‚Ä≤. Formally, we define the function
PF ‚à∂{0, 1}‚àó‚Üí{0, 1}‚àóas follows:
PF(ùë•) = ùë•0ùë•0ùë•1ùë•1 ‚Ä¶ ùë•ùëõ‚àí1ùë•ùëõ‚àí101
(2.14)
for every ùë•‚àà{0, 1}‚àó. If ùê∏‚à∂ùí™‚Üí{0, 1}‚àóis the (potentially not
prefix-free) representation for ùí™, then we transform it into a prefix-
free representation ùê∏‚à∂ùí™‚Üí{0, 1}‚àóby defining ùê∏(ùëú) = PF(ùê∏(ùëú)).
To prove the lemma we need to show that (1) ùê∏is one-to-one and
(2) ùê∏is prefix-free. In fact, prefix freeness is a stronger condition than
one-to-one (if two strings are equal then in particular one of them is a
prefix of the other) and hence it suffices to prove (2), which we now
do.
Let ùëú‚â†ùëú‚Ä≤ in ùí™be two distinct objects. We will prove that ùê∏(ùëú)
is a not a prefix of ùê∏(ùëú‚Ä≤). Define ùë•= ùê∏(ùëú) and ùë•‚Ä≤ = ùê∏(ùëú‚Ä≤). Since
ùê∏is one-to-one, ùë•‚â†ùë•‚Ä≤. Under our assumption, PF(ùë•) is a prefix
of PF(ùë•‚Ä≤). If |ùë•| < |ùë•‚Ä≤| then the two bits in positions 2|ùë•|, 2|ùë•| + 1
in PF(ùë•) have the value 01 but the corresponding bits in PF(ùë•‚Ä≤) will
equal either 00 or 11 (depending on the |ùë•|-th bit of ùë•‚Ä≤) and hence
PF(ùë•) cannot be a prefix of PF(ùë•‚Ä≤). If |ùë•| = |ùë•‚Ä≤| then, since ùë•‚â†ùë•‚Ä≤,
there must be a coordinate ùëñin which they differ, meaning that the
strings PF(ùë•) and PF(ùë•‚Ä≤) differ in the coordinates 2ùëñ, 2ùëñ+ 1, which
again means that PF(ùë•) cannot be a prefix of PF(ùë•‚Ä≤). If |ùë•| > |ùë•‚Ä≤|
then |PF(ùë•)| = 2|ùë•| + 2 > |PF(ùë•‚Ä≤)| = 2|ùë•‚Ä≤| + 2 and hence PF(ùë•) is
longer than (and cannot be a prefix of) PF(ùë•‚Ä≤). In all cases we see that
PF(ùë•) = ùê∏(ùëú) is not a prefix of PF(ùë•‚Ä≤) = ùê∏(ùëú‚Ä≤), hence completing the
proof.
‚ñ†
The proof of Lemma 2.20 is not the only or even the best way to
transform an arbitrary representation into prefix-free form. Exer-
cise 2.10 asks you to construct a more efficient prefix-free transforma-
tion satisfying |ùê∏(ùëú)| ‚â§|ùê∏(ùëú)| + ùëÇ(log |ùê∏(ùëú)|).
2.5.4 ‚ÄúProof by Python‚Äù (optional)
The proofs of Theorem 2.18 and Lemma 2.20 are constructive in the
sense that they give us:
‚Ä¢ A way to transform the encoding and decoding functions of any
representation of an object ùëÇto encoding and decoding functions
that are prefix-free, and
‚Ä¢ A way to extend prefix-free encoding and decoding of single objects
to encoding and decoding of lists of objects by concatenation.


--- Page 106 ---

106
introduction to theoretical computer science
Specifically, we could transform any pair of Python functions en-
code and decode to functions pfencode and pfdecode that correspond
to a prefix-free encoding and decoding. Similarly, given pfencode and
pfdecode for single objects, we can extend them to encoding of lists.
Let us show how this works for the case of the NtS and StN functions
we defined above.
We start with the ‚ÄúPython proof‚Äù of Lemma 2.20: a way to trans-
form an arbitrary representation into one that is prefix free. The func-
tion prefixfree below takes as input a pair of encoding and decoding
functions, and returns a triple of functions containing prefix-free encod-
ing and decoding functions, as well as a function that checks whether
a string is a valid encoding of an object.
# takes functions encode and decode mapping
# objects to lists of bits and vice versa,
# and returns functions pfencode and pfdecode that
# maps objects to lists of bits and vice versa
# in a prefix-free way.
# Also returns a function pfvalid that says
# whether a list is a valid encoding
def prefixfree(encode, decode):
def pfencode(o):
L = encode(o)
return [L[i//2] for i in range(2*len(L))]+[0,1]
def pfdecode(L):
return decode([L[j] for j in range(0,len(L)-2,2)])
def pfvalid(L):
return (len(L) % 2 == 0 ) and all(L[2*i]==L[2*i+1]
for i in range((len(L)-2)//2)) and
L[-2:]==[0,1]
‚Ü™
‚Ü™
return pfencode, pfdecode, pfvalid
pfNtS, pfStN , pfvalidN = prefixfree(NtS,StN)
NtS(234)
# 11101010
pfNtS(234)
# 111111001100110001
pfStN(pfNtS(234))
# 234
pfvalidM(pfNtS(234))
# true


--- Page 107 ---

computation and representation
107
P
Note that the Python function prefixfree above
takes two Python functions as input and outputs
three Python functions as output. (When it‚Äôs not
too awkward, we use the term ‚ÄúPython function‚Äù or
‚Äúsubroutine‚Äù to distinguish between such snippets of
Python programs and mathematical functions.) You
don‚Äôt have to know Python in this course, but you do
need to get comfortable with the idea of functions as
mathematical objects in their own right, that can be
used as inputs and outputs of other functions.
We now show a ‚ÄúPython proof‚Äù of Theorem 2.18. Namely, we show
a function represlists that takes as input a prefix-free representation
scheme (implemented via encoding, decoding, and validity testing
functions) and outputs a representation scheme for lists of such ob-
jects. If we want to make this representation prefix-free then we could
fit it into the function prefixfree above.
def represlists(pfencode,pfdecode,pfvalid):
"""
Takes functions pfencode, pfdecode and pfvalid,
and returns functions encodelists, decodelists
that can encode and decode lists of the objects
respectively.
"""
def encodelist(L):
"""Gets list of objects, encodes it as list of
bits"""
‚Ü™
return "".join([pfencode(obj) for obj in L])
def decodelist(S):
"""Gets lists of bits, returns lists of objects"""
i=0; j=1 ; res = []
while j<=len(S):
if pfvalid(S[i:j]):
res += [pfdecode(S[i:j])]
i=j
j+= 1
return res
return encodelist,decodelist
LtS , StL = represlists(pfNtS,pfStN,pfvalidN)


--- Page 108 ---

108
introduction to theoretical computer science
Figure 2.10: The word ‚ÄúBinary‚Äù in ‚ÄúGrade 1‚Äù or
‚Äúuncontracted‚Äù Unified English Braille. This word is
encoded using seven symbols since the first one is a
modifier indicating that the first letter is capitalized.
LtS([234,12,5])
# 111111001100110001111100000111001101
StL(LtS([234,12,5]))
# [234, 12, 5]
2.5.5 Representing letters and text
We can represent a letter or symbol by a string, and then if this rep-
resentation is prefix-free, we can represent a sequence of symbols by
merely concatenating the representation of each symbol. One such
representation is the ASCII that represents 128 letters and symbols as
strings of 7 bits. Since the ASCII representation is fixed-length, it is
automatically prefix-free (can you see why?). Unicode is representa-
tion of (at the time of this writing) about 128,000 symbols as numbers
(known as code points) between 0 and 1, 114, 111. There are several
types of prefix-free representations of the code points, a popular one
being UTF-8 that encodes every codepoint into a string of length be-
tween 8 and 32.
‚ñ†Example 2.21 ‚Äî The Braille representation. The Braille system is another
way to encode letters and other symbols as binary strings. Specifi-
cally, in Braille, every letter is encoded as a string in {0, 1}6, which
is written using indented dots arranged in two columns and three
rows, see Fig. 2.10. (Some symbols require more than one six-bit
string to encode, and so Braille uses a more general prefix-free
encoding.)
The Braille system was invented in 1821 by Louis Braille when
he was just 12 years old (though he continued working on it and
improving it throughout his life). Braille was a French boy who
lost his eyesight at the age of 5 as the result of an accident.
‚ñ†Example 2.22 ‚Äî Representing objects in C (optional). We can use pro-
gramming languages to probe how our computing environment
represents various values. This is easiest to do in ‚Äúunsafe‚Äù pro-
gramming languages such as C that allow direct access to the
memory.
Using a simple C program we have produced the following
representations of various values. One can see that for integers,
multiplying by 2 corresponds to a ‚Äúleft shift‚Äù inside each byte. In
contrast, for floating point numbers, multiplying by two corre-
sponds to adding one to the exponent part of the representation.
In the architecture we used, a negative number is represented


--- Page 109 ---

computation and representation
109
using the two‚Äôs complement approach. C represents strings in a
prefix-free form by ensuring that a zero byte is at their end.
int
2
: 00000010 00000000 00000000 00000000
int
4
: 00000100 00000000 00000000 00000000
int
513
: 00000001 00000010 00000000 00000000
long
513
: 00000001 00000010 00000000 00000000
00000000 00000000 00000000 00000000
‚Ü™
int
-1
: 11111111 11111111 11111111 11111111
int
-2
: 11111110 11111111 11111111 11111111
string
Hello: 01001000 01100101 01101100 01101100
01101111 00000000
‚Ü™
string
abcd : 01100001 01100010 01100011 01100100
00000000
‚Ü™
float
33.0 : 00000000 00000000 00000100 01000010
float
66.0 : 00000000 00000000 10000100 01000010
float
132.0: 00000000 00000000 00000100 01000011
double
132.0: 00000000 00000000 00000000 00000000
00000000 10000000 01100000 01000000
‚Ü™
2.5.6 Representing vectors, matrices, images
Once we can represent numbers and lists of numbers, then we can also
represent vectors (which are just lists of numbers). Similarly, we can
represent lists of lists, and thus, in particular, can represent matrices.
To represent an image, we can represent the color at each pixel by a
list of three numbers corresponding to the intensity of Red, Green and
Blue. (We can restrict to three primary colors since most humans only
have three types of cones in their retinas; we would have needed 16
primary colors to represent colors visible to the Mantis Shrimp.) Thus
an image of ùëõpixels would be represented by a list of ùëõsuch length-
three lists. A video can be represented as a list of images. Of course
these representations are rather wasteful and much more compact
representations are typically used for images and videos, though this
will not be our concern in this book.
2.5.7 Representing graphs
A graph on ùëõvertices can be represented as an ùëõ√ó ùëõadjacency matrix
whose (ùëñ, ùëó)ùë°‚Ñéentry is equal to 1 if the edge (ùëñ, ùëó) is present and is
equal to 0 otherwise. That is, we can represent an ùëõvertex directed
graph ùê∫= (ùëâ, ùê∏) as a string ùê¥‚àà{0, 1}ùëõ2 such that ùê¥ùëñ,ùëó= 1 iff the
edge ‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó
ùëñùëó‚ààùê∏. We can transform an undirected graph to a directed
graph by replacing every edge {ùëñ, ùëó} with both edges ‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó
ùëñùëóand ‚Éñ‚Éñ‚Éñ‚Éñ‚Éñ‚Éñ‚Éñ‚Éñ
ùëñùëó
Another representation for graphs is the adjacency list representa-
tion. That is, we identify the vertex set ùëâof a graph with the set [ùëõ]


--- Page 110 ---

110
introduction to theoretical computer science
Figure 2.11: Representing the graph ùê∫=
({0, 1, 2, 3, 4}, {(1, 0), (4, 0), (1, 4), (4, 1), (2, 1), (3, 2), (4, 3)})
in the adjacency matrix and adjacency list representa-
tions.
where ùëõ= |ùëâ|, and represent the graph ùê∫= (ùëâ, ùê∏) as a list of ùëõ
lists, where the ùëñ-th list consists of the out-neighbors of vertex ùëñ. The
difference between these representations can be significant for some
applications, though for us would typically be immaterial.
2.5.8 Representing lists and nested lists
If we have a way of representing objects from a set ùí™as binary strings,
then we can represent lists of these objects by applying a prefix-free
transformation. Moreover, we can use a trick similar to the above to
handle nested lists. The idea is that if we have some representation
ùê∏‚à∂ùí™‚Üí{0, 1}‚àó, then we can represent nested lists of items from
ùí™using strings over the five element alphabet Œ£ = { 0,1,[ , ] , , }.
For example, if ùëú1 is represented by 0011, ùëú2 is represented by 10011,
and ùëú3 is represented by 00111, then we can represent the nested list
(ùëú1, (ùëú2, ùëú3)) as the string "[0011,[10011,00111]]" over the alphabet
Œ£. By encoding every element of Œ£ itself as a three-bit string, we can
transform any representation for objects ùí™into a representation that
enables representing (potentially nested) lists of these objects.
2.5.9 Notation
We will typically identify an object with its representation as a string.
For example, if ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}‚àóis some function that maps
strings to strings and ùëõis an integer, we might make statements such
as ‚Äúùêπ(ùëõ) + 1 is prime‚Äù to mean that if we represent ùëõas a string ùë•,
then the integer ùëörepresented by the string ùêπ(ùë•) satisfies that ùëö+ 1
is prime. (You can see how this convention of identifying objects with
their representation can save us a lot of cumbersome formalism.)
Similarly, if ùë•, ùë¶are some objects and ùêπis a function that takes strings
as inputs, then by ùêπ(ùë•, ùë¶) we will mean the result of applying ùêπto the
representation of the ordered pair (ùë•, ùë¶). We use the same notation to
invoke functions on ùëò-tuples of objects for every ùëò.
This convention of identifying an object with its representation as
a string is one that we humans follow all the time. For example, when
people say a statement such as ‚Äú17 is a prime number‚Äù, what they
really mean is that the integer whose decimal representation is the
string ‚Äú17‚Äù, is prime.
When we say
ùê¥is an algorithm that computes the multiplication function on natural num-
bers.
what we really mean is that
ùê¥is an algorithm that computes the function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}‚àósuch that
for every pair ùëé, ùëè‚àà‚Ñï, if ùë•‚àà{0, 1}‚àóis a string representing the pair (ùëé, ùëè)
then ùêπ(ùë•) will be a string representing their product ùëé‚ãÖùëè.


--- Page 111 ---

computation and representation
111
2.6 DEFINING COMPUTATIONAL TASKS AS MATHEMATICAL FUNC-
TIONS
Abstractly, a computational process is some process that takes an input
which is a string of bits and produces an output which is a string
of bits. This transformation of input to output can be done using a
modern computer, a person following instructions, the evolution of
some natural system, or any other means.
In future chapters, we will turn to mathematically defining com-
putational processes, but, as we discussed above, at the moment we
focus on computational tasks. That is, we focus on the specification and
not the implementation. Again, at an abstract level, a computational
task can specify any relation that the output needs to have with the in-
put. However, for most of this book, we will focus on the simplest and
most common task of computing a function. Here are some examples:
‚Ä¢ Given (a representation of) two integers ùë•, ùë¶, compute the product
ùë•√ó ùë¶. Using our representation above, this corresponds to com-
puting a function from {0, 1}‚àóto {0, 1}‚àó. We have seen that there is
more than one way to solve this computational task, and in fact, we
still do not know the best algorithm for this problem.
‚Ä¢ Given (a representation of) an integer ùëß> 1, compute its factoriza-
tion; i.e., the list of primes ùëù1 ‚â§‚ãØ‚â§ùëùùëòsuch that ùëß= ùëù1 ‚ãØùëùùëò. This
again corresponds to computing a function from {0, 1}‚àóto {0, 1}‚àó.
The gaps in our knowledge of the complexity of this problem are
even larger.
‚Ä¢ Given (a representation of) a graph ùê∫and two vertices ùë†and ùë°,
compute the length of the shortest path in ùê∫between ùë†and ùë°, or do
the same for the longest path (with no repeated vertices) between
ùë†and ùë°. Both these tasks correspond to computing a function from
{0, 1}‚àóto {0, 1}‚àó, though it turns out that there is a vast difference in
their computational difficulty.
‚Ä¢ Given the code of a Python program, determine whether there is an
input that would force it into an infinite loop. This task corresponds
to computing a partial function from {0, 1}‚àóto {0, 1} since not every
string corresponds to a syntactically valid Python program. We will
see that we do understand the computational status of this problem,
but the answer is quite surprising.
‚Ä¢ Given (a representation of) an image ùêº, decide if ùêºis a photo of a
cat or a dog. This corresponds to computing some (partial) func-
tion from {0, 1}‚àóto {0, 1}.


--- Page 112 ---

112
introduction to theoretical computer science
Figure 2.12: A subset ùêø‚äÜ{0, 1}‚àócan be identified
with the function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1} such that
ùêπ(ùë•) = 1 if ùë•‚ààùêøand ùêπ(ùë•) = 0 if ùë•‚àâùêø. Functions
with a single bit of output are called Boolean functions,
while subsets of strings are called languages. The
above shows that the two are essentially the same
object, and we can identify the task of deciding
membership in ùêø(known as deciding a language in the
literature) with the task of computing the function ùêπ.
R
Remark 2.23 ‚Äî Boolean functions and languages. An
important special case of computational tasks corre-
sponds to computing Boolean functions, whose output
is a single bit {0, 1}. Computing such functions corre-
sponds to answering a YES/NO question, and hence
this task is also known as a decision problem. Given any
function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1} and ùë•‚àà{0, 1}‚àó, the task
of computing ùêπ(ùë•) corresponds to the task of deciding
whether or not ùë•‚ààùêøwhere ùêø= {ùë•‚à∂ùêπ(ùë•) = 1} is
known as the language that corresponds to the function
ùêπ. (The language terminology is due to historical
connections between the theory of computation and
formal linguistics as developed by Noam Chomsky.)
Hence many texts refer to such a computational task
as deciding a language.
For every particular function ùêπ, there can be several possible algo-
rithms to compute ùêπ. We will be interested in questions such as:
‚Ä¢ For a given function ùêπ, can it be the case that there is no algorithm to
compute ùêπ?
‚Ä¢ If there is an algorithm, what is the best one? Could it be that ùêπis
‚Äúeffectively uncomputable‚Äù in the sense that every algorithm for
computing ùêπrequires a prohibitively large amount of resources?
‚Ä¢ If we cannot answer this question, can we show equivalence be-
tween different functions ùêπand ùêπ‚Ä≤ in the sense that either they are
both easy (i.e., have fast algorithms) or they are both hard?
‚Ä¢ Can a function being hard to compute ever be a good thing? Can we
use it for applications in areas such as cryptography?
In order to do that, we will need to mathematically define the no-
tion of an algorithm, which is what we will do in Chapter 3.
2.6.1 Distinguish functions from programs!
You should always watch out for potential confusions between speci-
fications and implementations or equivalently between mathematical
functions and algorithms/programs. It does not help that program-
ming languages (Python included) use the term ‚Äúfunctions‚Äù to denote
(parts of) programs. This confusion also stems from thousands of years
of mathematical history, where people typically defined functions by
means of a way to compute them.
For example, consider the multiplication function on natural num-
bers. This is the function MULT ‚à∂‚Ñï√ó ‚Ñï‚Üí‚Ñïthat maps a pair (ùë•, ùë¶)
of natural numbers to the number ùë•‚ãÖùë¶. As we mentioned, it can be
implemented in more than one way:


--- Page 113 ---

computation and representation
113
Figure 2.13: A function is a mapping of inputs to
outputs. A program is a set of instructions on how
to obtain an output given an input. A program
computes a function, but it is not the same as a func-
tion, popular programming language terminology
notwithstanding.
def mult1(x,y):
res = 0
while y>0:
res += x
y
-= 1
return res
def mult2(x,y):
a = str(x) # represent x as string in decimal notation
b = str(y) # represent y as string in decimal notation
res = 0
for i in range(len(a)):
for j in range(len(b)):
res += int(a[len(a)-i])*int(b[len(b)-
j])*(10**(i+j))
‚Ü™
return res
print(mult1(12,7))
# 84
print(mult2(12,7))
# 84
Both mult1 and mult2 produce the same output given the same
pair of natural number inputs. (Though mult1 will take far longer to
do so when the numbers become large.) Hence, even though these are
two different programs, they compute the same mathematical function.
This distinction between a program or algorithm ùê¥, and the function ùêπ
that ùê¥computes will be absolutely crucial for us in this course (see also
Fig. 2.13).
ÔÉ´Big Idea 2 A function is not the same as a program. A program
computes a function.
Distinguishing functions from programs (or other ways for comput-
ing, including circuits and machines) is a crucial theme for this course.
For this reason, this is often a running theme in questions that I (and
many other instructors) assign in homework and exams (hint, hint).
R
Remark 2.24 ‚Äî Computation beyond functions (advanced,
optional). Functions capture quite a lot of compu-
tational tasks, but one can consider more general
settings as well. For starters, we can and will talk
about partial functions, that are not defined on all
inputs. When computing a partial function, we only


--- Page 114 ---

114
introduction to theoretical computer science
need to worry about the inputs on which the function
is defined. Another way to say it is that we can design
an algorithm for a partial function ùêπunder the as-
sumption that someone ‚Äúpromised‚Äù us that all inputs
ùë•would be such that ùêπ(ùë•) is defined (as otherwise,
we do not care about the result). Hence such tasks are
also known as promise problems.
Another generalization is to consider relations that may
have more than one possible admissible output. For
example, consider the task of finding any solution for
a given set of equations. A relation ùëÖmaps a string
ùë•
‚àà
{0, 1}‚àóinto a set of strings ùëÖ(ùë•) (for example, ùë•
might describe a set of equations, in which case ùëÖ(ùë•)
would correspond to the set of all solutions to ùë•). We
can also identify a relation ùëÖwith the set of pairs of
strings (ùë•, ùë¶) where ùë¶
‚àà
ùëÖ(ùë•). A computational pro-
cess solves a relation if for every ùë•‚àà{0, 1}‚àó, it outputs
some string ùë¶‚ààùëÖ(ùë•).
Later in this book, we will consider even more general
tasks, including interactive tasks, such as finding a
good strategy in a game, tasks defined using proba-
bilistic notions, and others. However, for much of this
book, we will focus on the task of computing a func-
tion, and often even a Boolean function, that has only a
single bit of output. It turns out that a great deal of the
theory of computation can be studied in the context of
this task, and the insights learned are applicable in the
more general settings.
‚úì
Chapter Recap
‚Ä¢ We can represent objects we want to compute on
using binary strings.
‚Ä¢ A representation scheme for a set of objects ùí™is a
one-to-one map from ùí™to {0, 1}‚àó.
‚Ä¢ We can use prefix-free encoding to ‚Äúboost‚Äù a repre-
sentation for a set ùí™into representations of lists of
elements in ùí™.
‚Ä¢ A basic computational task is the task of computing
a function ùêπ
‚à∂{0, 1}‚àó‚Üí{0, 1}‚àó. This task encom-
passes not just arithmetical computations such
as multiplication, factoring, etc. but a great many
other tasks arising in areas as diverse as scientific
computing, artificial intelligence, image processing,
data mining and many many more.
‚Ä¢ We will study the question of finding (or at least
giving bounds on) what the best algorithm for
computing ùêπfor various interesting functions ùêπis.


--- Page 115 ---

computation and representation
115
2.7 EXERCISES
Exercise 2.1 Which one of these objects can be represented by a binary
string?
a. An integer ùë•
b. An undirected graph ùê∫.
c. A directed graph ùêª
d. All of the above.
‚ñ†
Exercise 2.2 ‚Äî Binary representation. a. Prove that the function ùëÅùë°ùëÜ‚à∂‚Ñï‚Üí
{0, 1}‚àóof the binary representation defined in (2.1) satisfies that for
every ùëõ‚àà‚Ñï, if ùë•= ùëÅùë°ùëÜ(ùëõ) then |ùë•| = 1 + max(0, ‚åälog2 ùëõ‚åã) and
ùë•ùëñ= ‚åäùë•/2‚åälog2 ùëõ‚åã‚àíùëñ‚åãmod 2.
b. Prove that ùëÅùë°ùëÜis a one to one function by coming up with a func-
tion ùëÜùë°ùëÅ‚à∂{0, 1}‚àó‚Üí‚Ñïsuch that ùëÜùë°ùëÅ(ùëÅùë°ùëÜ(ùëõ)) = ùëõfor every
ùëõ‚àà‚Ñï.
‚ñ†
Exercise 2.3 ‚Äî More compact than ASCII representation. The ASCII encoding
can be used to encode a string of ùëõEnglish letters as a 7ùëõbit binary
string, but in this exercise, we ask about finding a more compact rep-
resentation for strings of English lowercase letters.
1. Prove that there exists a representation scheme (ùê∏, ùê∑) for strings
over the 26-letter alphabet {ùëé, ùëè, ùëê, ‚Ä¶ , ùëß} as binary strings such
that for every ùëõ> 0 and length-ùëõstring ùë•‚àà{ùëé, ùëè, ‚Ä¶ , ùëß}ùëõ, the
representation ùê∏(ùë•) is a binary string of length at most 4.8ùëõ+ 1000.
In other words, prove that for every ùëõ, there exists a one-to-one
function ùê∏‚à∂{ùëé, ùëè, ‚Ä¶ , ùëß}ùëõ‚Üí{0, 1}‚åä4.8ùëõ+1000‚åã.
2. Prove that there exists no representation scheme for strings over the
alphabet {ùëé, ùëè, ‚Ä¶ , ùëß} as binary strings such that for every length-ùëõ
string ùë•‚àà{ùëé, ùëè, ‚Ä¶ , ùëß}ùëõ, the representation ùê∏(ùë•) is a binary string of
length ‚åä4.6ùëõ+ 1000‚åã. In other words, prove that there exists some
ùëõ> 0 such that there is no one-to-one function ùê∏‚à∂{ùëé, ùëè, ‚Ä¶ , ùëß}ùëõ‚Üí
{0, 1}‚åä4.6ùëõ+1000‚åã.
3. Python‚Äôs bz2.compress function is a mapping from strings to
strings, which uses the lossless (and hence one to one) bzip2 algo-
rithm for compression. After converting to lowercase, and truncat-
ing spaces and numbers, the text of Tolstoy‚Äôs ‚ÄúWar and Peace‚Äù con-
tains ùëõ= 2, 517, 262. Yet, if we run bz2.compress on the string of
the text of ‚ÄúWar and Peace‚Äù we get a string of length ùëò= 6, 274, 768


--- Page 116 ---

116
introduction to theoretical computer science
5 Actually that particular fictional company uses a
metric that focuses more on compression speed then
ratio, see here and here.
bits, which is only 2.49ùëõ(and in particular much smaller than
4.6ùëõ). Explain why this does not contradict your answer to the
previous question.
4. Interestingly, if we try to apply bz2.compress on a random string,
we get much worse performance. In my experiments, I got a ratio
of about 4.78 between the number of bits in the output and the
number of characters in the input. However, one could imagine that
one could do better and that there exists a company called ‚ÄúPied
Piper‚Äù with an algorithm that can losslessly compress a string of ùëõ
random lowercase letters to fewer than 4.6ùëõbits.5 Show that this
is not the case by proving that for every ùëõ> 100 and one to one
function ùê∏ùëõùëêùëúùëëùëí‚à∂{ùëé, ‚Ä¶ , ùëß}ùëõ‚Üí{0, 1}‚àó, if we let ùëçbe the random
variable |ùê∏ùëõùëêùëúùëëùëí(ùë•)| (i.e., the length of ùê∏ùëõùëêùëúùëëùëí(ùë•)) for ùë•chosen
uniformly at random from the set {ùëé, ‚Ä¶ , ùëß}ùëõ, then the expected
value of ùëçis at least 4.6ùëõ.
‚ñ†
Exercise 2.4 ‚Äî Representing graphs: upper bound. Show that there is a string
representation of directed graphs with vertex set [ùëõ] and degree at
most 10 that uses at most 1000ùëõlog ùëõbits. More formally, show the
following: Suppose we define for every ùëõ‚àà‚Ñï, the set ùê∫ùëõas the set
containing all directed graphs (with no self loops) over the vertex
set [ùëõ] where every vertex has degree at most 10. Then, prove that for
every sufficiently large ùëõ, there exists a one-to-one function ùê∏‚à∂ùê∫ùëõ‚Üí
{0, 1}‚åä1000ùëõlog ùëõ‚åã.
‚ñ†
Exercise 2.5 ‚Äî Representing graphs: lower bound. 1. Define ùëÜùëõto be the
set of one-to-one and onto functions mapping [ùëõ] to [ùëõ]. Prove that
there is a one-to-one mapping from ùëÜùëõto ùê∫2ùëõ, where ùê∫2ùëõis the set
defined in Exercise 2.4 above.
2. In this question you will show that one cannot improve the rep-
resentation of Exercise 2.4 to length ùëú(ùëõlog ùëõ). Specifically, prove
for every sufficiently large ùëõ‚àà‚Ñïthere is no one-to-one function
ùê∏‚à∂ùê∫ùëõ‚Üí{0, 1}‚åä0.001ùëõlog ùëõ‚åã+1000.
‚ñ†
Exercise 2.6 ‚Äî Multiplying in different representation. Recall that the grade-
school algorithm for multiplying two numbers requires ùëÇ(ùëõ2) oper-
ations. Suppose that instead of using decimal representation, we use
one of the following representations ùëÖ(ùë•) to represent a number ùë•
between 0 and 10ùëõ‚àí1. For which one of these representations you can
still multiply the numbers in ùëÇ(ùëõ2) operations?


--- Page 117 ---

computation and representation
117
a. The standard binary representation: ùêµ(ùë•) = (ùë•0, ‚Ä¶ , ùë•ùëò) where
ùë•= ‚àë
ùëò
ùëñ=0 ùë•ùëñ2ùëñand ùëòis the largest number s.t. ùë•‚â•2ùëò.
b. The reverse binary representation: ùêµ(ùë•) = (ùë•ùëò, ‚Ä¶ , ùë•0) where ùë•ùëñis
defined as above for ùëñ= 0, ‚Ä¶ , ùëò‚àí1.
c. Binary coded decimal representation: ùêµ(ùë•) = (ùë¶0, ‚Ä¶ , ùë¶ùëõ‚àí1) where
ùë¶ùëñ‚àà{0, 1}4 represents the ùëñùë°‚Ñédecimal digit of ùë•mapping 0 to 0000,
1 to 0001, 2 to 0010, etc. (i.e. 9 maps to 1001)
d. All of the above.
‚ñ†
Exercise 2.7 Suppose that ùëÖ‚à∂‚Ñï‚Üí{0, 1}‚àócorresponds to representing a
number ùë•as a string of ùë•1‚Äôs, (e.g., ùëÖ(4) = 1111, ùëÖ(7) = 1111111, etc.).
If ùë•, ùë¶are numbers between 0 and 10ùëõ‚àí1, can we still multiply ùë•and
ùë¶using ùëÇ(ùëõ2) operations if we are given them in the representation
ùëÖ(‚ãÖ)?
‚ñ†
Exercise 2.8 Recall that if ùêπis a one-to-one and onto function mapping
elements of a finite set ùëàinto a finite set ùëâthen the sizes of ùëàand ùëâ
are the same. Let ùêµ‚à∂‚Ñï‚Üí{0, 1}‚àóbe the function such that for every
ùë•‚àà‚Ñï, ùêµ(ùë•) is the binary representation of ùë•.
1. Prove that ùë•< 2ùëòif and only if |ùêµ(ùë•)| ‚â§ùëò.
2. Use 1. to compute the size of the set {ùë¶‚àà{0, 1}‚àó‚à∂|ùë¶| ‚â§ùëò} where |ùë¶|
denotes the length of the string ùë¶.
3. Use 1. and 2. to prove that 2ùëò‚àí1 = 1 + 2 + 4 + ‚ãØ+ 2ùëò‚àí1.
‚ñ†
Exercise 2.9 ‚Äî Prefix-free encoding of tuples. Suppose that ùêπ‚à∂‚Ñï‚Üí{0, 1}‚àó
is a one-to-one function that is prefix-free in the sense that there is no
ùëé‚â†ùëès.t. ùêπ(ùëé) is a prefix of ùêπ(ùëè).
a. Prove that ùêπ2 ‚à∂‚Ñï√ó ‚Ñï‚Üí{0, 1}‚àó, defined as ùêπ2(ùëé, ùëè) = ùêπ(ùëé)ùêπ(ùëè) (i.e.,
the concatenation of ùêπ(ùëé) and ùêπ(ùëè)) is a one-to-one function.
b. Prove that ùêπ‚àó‚à∂‚Ñï‚àó‚Üí{0, 1}‚àódefined as ùêπ‚àó(ùëé1, ‚Ä¶ , ùëéùëò) =
ùêπ(ùëé1) ‚ãØùêπ(ùëéùëò) is a one-to-one function, where ‚Ñï‚àódenotes the set of
all finite-length lists of natural numbers.
‚ñ†
Exercise 2.10 ‚Äî More efficient prefix-free transformation. Suppose that ùêπ‚à∂
ùëÇ‚Üí{0, 1}‚àóis some (not necessarily prefix-free) representation of the
objects in the set ùëÇ, and ùê∫‚à∂‚Ñï‚Üí{0, 1}‚àóis a prefix-free representa-
tion of the natural numbers. Define ùêπ‚Ä≤(ùëú) = ùê∫(|ùêπ(ùëú)|)ùêπ(ùëú) (i.e., the
concatenation of the representation of the length ùêπ(ùëú) and ùêπ(ùëú)).


--- Page 118 ---

118
introduction to theoretical computer science
6 Hint: Think recursively how to represent the length
of the string.
a. Prove that ùêπ‚Ä≤ is a prefix-free representation of ùëÇ.
b. Show that we can transform any representation to a prefix-free one
by a modification that takes a ùëòbit string into a string of length at
most ùëò+ ùëÇ(log ùëò).
c. Show that we can transform any representation to a prefix-free one
by a modification that takes a ùëòbit string into a string of length at
most ùëò+ log ùëò+ ùëÇ(log log ùëò).6
‚ñ†
Exercise 2.11 ‚Äî Kraft‚Äôs Inequality. Suppose that ùëÜ‚äÜ{0, 1}ùëõis some finite
prefix-free set.
a. For every ùëò‚â§ùëõand length-ùëòstring ùë•‚ààùëÜ, let ùêø(ùë•) ‚äÜ{0, 1}ùëõdenote
all the length-ùëõstrings whose first ùëòbits are ùë•0, ‚Ä¶ , ùë•ùëò‚àí1. Prove that
(1) |ùêø(ùë•)| = 2ùëõ‚àí|ùë•| and (2) If ùë•‚â†ùë•‚Ä≤ then ùêø(ùë•) is disjoint from
ùêø(ùë•‚Ä≤).
b. Prove that ‚àëùë•‚ààùëÜ2‚àí|ùë•| ‚â§1.
c. Prove that there is no prefix-free encoding of strings with less than
logarithmic overhead. That is, prove that there is no function PF ‚à∂
{0, 1}‚àó‚Üí{0, 1}‚àós.t. |PF(ùë•)| ‚â§|ùë•| + 0.9 log |ùë•| for every ùë•‚àà{0, 1}‚àó
and such that the set {PF(ùë•) ‚à∂ùë•‚àà{0, 1}‚àó} is prefix-free. The factor
0.9 is arbitrary; all that matters is that it is less than 1.
‚ñ†
Exercise 2.12 ‚Äî Composition of one-to-one functions. Prove that for every
two one-to-one functions ùêπ‚à∂ùëÜ‚Üíùëáand ùê∫‚à∂ùëá‚Üíùëà, the function
ùêª‚à∂ùëÜ‚Üíùëàdefined as ùêª(ùë•) = ùê∫(ùêπ(ùë•)) is one to one.
‚ñ†
Exercise 2.13 ‚Äî Natural numbers and strings. 1. We have shown that
the natural numbers can be represented as strings. Prove that
the other direction holds as well: that there is a one-to-one map
ùëÜùë°ùëÅ‚à∂{0, 1}‚àó‚Üí‚Ñï. (ùëÜùë°ùëÅstands for ‚Äústrings to numbers.‚Äù)
2. Recall that Cantor proved that there is no one-to-one map ùëÖùë°ùëÅ‚à∂
‚Ñù‚Üí‚Ñï. Show that Cantor‚Äôs result implies Theorem 2.5.
‚ñ†
Exercise 2.14 ‚Äî Map lists of integers to a number. Recall that for every set
ùëÜ, the set ùëÜ‚àóis defined as the set of all finite sequences of mem-
bers of ùëÜ(i.e., ùëÜ‚àó= {(ùë•0, ‚Ä¶ , ùë•ùëõ‚àí1) | ùëõ‚àà‚Ñï, ‚àÄùëñ‚àà[ùëõ]ùë•ùëñ‚ààùëÜ} ).
Prove that there is a one-one-map from ‚Ñ§‚àóto ‚Ñïwhere ‚Ñ§is the set of
{‚Ä¶ , ‚àí3, ‚àí2, ‚àí1, 0, +1, +2, +3, ‚Ä¶} of all integers.
‚ñ†


--- Page 119 ---

computation and representation
119
2.8 BIBLIOGRAPHICAL NOTES
The study of representing data as strings, including issues such as
compression and error corrections falls under the purview of information
theory, as covered in the classic textbook of Cover and Thomas [CT06].
Representations are also studied in the field of data structures design, as
covered in texts such as [Cor+09].
The question of whether to represent integers with the most signif-
icant digit first or last is known as Big Endian vs. Little Endian repre-
sentation. This terminology comes from Cohen‚Äôs [Coh81] entertaining
and informative paper about the conflict between adherents of both
schools which he compared to the warring tribes in Jonathan Swift‚Äôs
‚ÄúGulliver‚Äôs Travels‚Äù. The two‚Äôs complement representation of signed
integers was suggested in von Neumann‚Äôs classic report [Neu45]
that detailed the design approaches for a stored-program computer,
though similar representations have been used even earlier in abacus
and other mechanical computation devices.
The idea that we should separate the definition or specification of
a function from its implementation or computation might seem ‚Äúobvi-
ous,‚Äù but it took quite a lot of time for mathematicians to arrive at this
viewpoint. Historically, a function ùêπwas identified by rules or formu-
las showing how to derive the output from the input. As we discuss
in greater depth in Chapter 9, in the 1800s this somewhat informal
notion of a function started ‚Äúbreaking at the seams,‚Äù and eventually
mathematicians arrived at the more rigorous definition of a function
as an arbitrary assignment of input to outputs. While many functions
may be described (or computed) by one or more formulas, today we
do not consider that to be an essential property of functions, and also
allow functions that do not correspond to any ‚Äúnice‚Äù formula.
We have mentioned that all representations of the real numbers
are inherently approximate. Thus an important endeavor is to under-
stand what guarantees we can offer on the approximation quality of
the output of an algorithm, as a function of the approximation quality
of the inputs. This question is known as the question of determining
the numerical stability of given equations. The Floating Points Guide
website contains an extensive description of the floating point repre-
sentation, as well the many ways in which it could subtly fail, see also
the website 0.30000000000000004.com.
Dauben [Dau90] gives a biography of Cantor with emphasis on
the development of his mathematical ideas. [Hal60] is a classic text-
book on set theory, including also Cantor‚Äôs theorem. Cantor‚Äôs Theo-
rem is also covered in many texts on discrete mathematics, including
[LLM18; LZ19].


--- Page 120 ---

120
introduction to theoretical computer science
The adjacency matrix representation of graphs is not merely a con-
venient way to map a graph into a binary string, but it turns out that
many natural notions and operations on matrices are useful for graphs
as well. (For example, Google‚Äôs PageRank algorithm relies on this
viewpoint.) The notes of Spielman‚Äôs course are an excellent source for
this area, known as spectral graph theory. We will return to this view
much later in this book when we talk about random walks.


--- Page 121 ---

I
FINITE COMPUTATION


--- Page 122 ---



--- Page 123 ---

Figure 3.1: Calculating wheels by Charles Babbage.
Image taken from the Mark I ‚Äòoperating manual‚Äô
Figure 3.2: A 1944 Popular Mechanics article on the
Harvard Mark I computer.
3
Defining computation
‚Äúthere is no reason why mental as well as bodily labor should not be economized
by the aid of machinery‚Äù, Charles Babbage, 1852
‚ÄúIf, unwarned by my example, any man shall undertake and shall succeed
in constructing an engine embodying in itself the whole of the executive de-
partment of mathematical analysis upon different principles or by simpler
mechanical means, I have no fear of leaving my reputation in his charge, for he
alone will be fully able to appreciate the nature of my efforts and the value of
their results.‚Äù, Charles Babbage, 1864
‚ÄúTo understand a program you must become both the machine and the pro-
gram.‚Äù, Alan Perlis, 1982
People have been computing for thousands of years, with aids
that include not just pen and paper, but also abacus, slide rules, vari-
ous mechanical devices, and modern electronic computers. A priori,
the notion of computation seems to be tied to the particular mech-
anism that you use. You might think that the ‚Äúbest‚Äù algorithm for
multiplying numbers will differ if you implement it in Python on a
modern laptop than if you use pen and paper. However, as we saw
in the introduction (Chapter 0), an algorithm that is asymptotically
better would eventually beat a worse one regardless of the underly-
ing technology. This gives us hope for a technology independent way
of defining computation. This is what we do in this chapter. We will
define the notion of computing an output from an input by applying a
sequence of basic operations (see Fig. 3.3). Using this, we will be able
to precisely define statements such as ‚Äúfunction ùëìcan be computed
by model ùëã‚Äù or ‚Äúfunction ùëìcan be computed by model ùëãusing ùë†
operations‚Äù.
This chapter: A non-mathy overview
The main takeaways from this chapter are:
Compiled on 8.26.2020 18:10
Learning Objectives:
‚Ä¢ See that computation can be precisely
modeled.
‚Ä¢ Learn the computational model of Boolean
circuits / straight-line programs.
‚Ä¢ Equivalence of circuits and straight-line
programs.
‚Ä¢ Equivalence of AND/OR/NOT and NAND.
‚Ä¢ Examples of computing in the physical world.


--- Page 124 ---

124
introduction to theoretical computer science
Figure 3.3: A function mapping strings to strings
specifies a computational task, i.e., describes what the
desired relation between the input and the output
is. In this chapter we define models for implementing
computational processes that achieve the desired
relation, i.e., describe how to compute the output
from the input. We will see several examples of such
models using both Boolean circuits and straight-line
programming languages.
‚Ä¢ We can use logical operations such as AND, OR, and NOT to
compute an output from an input (see Section 3.2).
‚Ä¢ A Boolean circuit is a way to compose the basic logical
operations to compute a more complex function (see Sec-
tion 3.3). We can think of Boolean circuits as both a mathe-
matical model (which is based on directed acyclic graphs)
as well as physical devices we can construct in the real
world in a variety of ways, including not just silicon-based
semi-conductors but also mechanical and even biological
mechanisms (see Section 3.4).
‚Ä¢ We can describe Boolean circuits also as straight-line pro-
grams, which are programs that do not have any looping
constructs (i.e., no while / for/ do .. until etc.), see
Section 3.3.2.
‚Ä¢ It is possible to implement the AND, OR, and NOT oper-
ations using the NAND operation (as well as vice versa).
This means that circuits with AND/OR/NOT gates can
compute the same functions (i.e., are equivalent in power)
to circuits with NAND gates, and we can use either model
to describe computation based on our convenience, see
Section 3.5. To give out a ‚Äúspoiler‚Äù, we will see in Chap-
ter 4 that such circuits can compute all finite functions.
One ‚Äúbig idea‚Äù of this chapter is the notion of equivalence
between models (Big Idea 3). Two computational models
are equivalent if they can compute the same set of functions.


--- Page 125 ---

defining computation
125
Figure 3.4: Text pages from Algebra manuscript with
geometrical solutions to two quadratic equations.
Shelfmark: MS. Huntington 214 fol. 004v-005r
Figure 3.5: An explanation for children of the two digit
addition algorithm
Boolean circuits with AND/OR/NOT gates are equivalent to
circuits with NAND gates, but this is just one example of the
more general phenomenon that we will see many times in
this book.
3.1 DEFINING COMPUTATION
The name ‚Äúalgorithm‚Äù is derived from the Latin transliteration of
Muhammad ibn Musa al-Khwarizmi‚Äôs name. Al-Khwarizmi was a
Persian scholar during the 9th century whose books introduced the
western world to the decimal positional numeral system, as well as to
the solutions of linear and quadratic equations (see Fig. 3.4). However
Al-Khwarizmi‚Äôs descriptions of algorithms were rather informal by
today‚Äôs standards. Rather than use ‚Äúvariables‚Äù such as ùë•, ùë¶, he used
concrete numbers such as 10 and 39, and trusted the reader to be
able to extrapolate from these examples, much as algorithms are still
taught to children today.
Here is how Al-Khwarizmi described the algorithm for solving an
equation of the form ùë•2 + ùëèùë•= ùëê:
[How to solve an equation of the form ] ‚Äúroots and squares are equal to num-
bers‚Äù: For instance ‚Äúone square , and ten roots of the same, amount to thirty-
nine dirhems‚Äù that is to say, what must be the square which, when increased
by ten of its own root, amounts to thirty-nine? The solution is this: you halve
the number of the roots, which in the present instance yields five. This you
multiply by itself; the product is twenty-five. Add this to thirty-nine‚Äô the sum
is sixty-four. Now take the root of this, which is eight, and subtract from it half
the number of roots, which is five; the remainder is three. This is the root of the
square which you sought for; the square itself is nine.
For the purposes of this book, we will need a much more precise
way to describe algorithms. Fortunately (or is it unfortunately?), at
least at the moment, computers lag far behind school-age children
in learning from examples. Hence in the 20th century, people came
up with exact formalisms for describing algorithms, namely program-
ming languages. Here is al-Khwarizmi‚Äôs quadratic equation solving
algorithm described in the Python programming language:
from math import sqrt
#Pythonspeak to enable use of the sqrt function to compute
square roots.
‚Ü™
def solve_eq(b,c):
# return solution of x^2 + bx = c following Al
Khwarizmi's instructions
‚Ü™
# Al Kwarizmi demonstrates this for the case b=10 and
c= 39
‚Ü™


--- Page 126 ---

126
introduction to theoretical computer science
val1 = b / 2.0 # "halve the number of the roots"
val2 = val1 * val1 # "this you multiply by itself"
val3 = val2 + c # "Add this to thirty-nine"
val4 = sqrt(val3) # "take the root of this"
val5 = val4 - val1 # "subtract from it half the number
of roots"
‚Ü™
return val5
# "This is the root of the square which
you sought for"
‚Ü™
# Test: solve x^2 + 10*x = 39
print(solve_eq(10,39))
# 3.0
We can define algorithms informally as follows:
Informal definition of an algorithm: An algorithm is a set of instruc-
tions for how to compute an output from an input by following a se-
quence of ‚Äúelementary steps‚Äù.
An algorithm ùê¥computes a function ùêπif for every input ùë•, if we follow
the instructions of ùê¥on the input ùë•, we obtain the output ùêπ(ùë•).
In this chapter we will make this informal definition precise using
the model of Boolean Circuits. We will show that Boolean Circuits
are equivalent in power to straight line programs that are written in
‚Äúultra simple‚Äù programming languages that do not even have loops.
We will also see that the particular choice of elementary operations is
immaterial and many different choices yield models with equivalent
power (see Fig. 3.6). However, it will take us some time to get there.
We will start by discussing what are ‚Äúelementary operations‚Äù and how
we map a description of an algorithm into an actual physical process
that produces an output from an input in the real world.
Figure 3.6: An overview of the computational models
defined in this chapter. We will show several equiv-
alent ways to represent a recipe for performing a
finite computation. Specifically we will show that we
can model such a computation using either a Boolean
circuit or a straight line program, and these two repre-
sentations are equivalent to one another. We will also
show that we can choose as our basic operations ei-
ther the set {AND, OR, NOT} or the set {NAND} and
these two choices are equivalent in power. By making
the choice of whether to use circuits or programs,
and whether to use {AND, OR, NOT} or {NAND} we
obtain four equivalent ways of modeling finite com-
putation. Moreover, there are many other choices of
sets of basic operations that are equivalent in power.


--- Page 127 ---

defining computation
127
3.2 COMPUTING USING AND, OR, AND NOT.
An algorithm breaks down a complex calculation into a series of sim-
pler steps. These steps can be executed in a variety of different ways,
including:
‚Ä¢ Writing down symbols on a piece of paper.
‚Ä¢ Modifying the current flowing on electrical wires.
‚Ä¢ Binding a protein to a strand of DNA.
‚Ä¢ Responding to a stimulus by a member of a collection (e.g., a bee in
a colony, a trader in a market).
To formally define algorithms, let us try to ‚Äúerr on the side of sim-
plicity‚Äù and model our ‚Äúbasic steps‚Äù as truly minimal. For example,
here are some very simple functions:
‚Ä¢ OR ‚à∂{0, 1}2 ‚Üí{0, 1} defined as
OR(ùëé, ùëè) =
‚éß
{
‚é®
{
‚é©
0
ùëé= ùëè= 0
1
otherwise
(3.1)
‚Ä¢ AND ‚à∂{0, 1}2 ‚Üí{0, 1} defined as
AND(ùëé, ùëè) =
‚éß
{
‚é®
{
‚é©
1
ùëé= ùëè= 1
0
otherwise
(3.2)
‚Ä¢ NOT ‚à∂{0, 1} ‚Üí{0, 1} defined as
NOT(ùëé) =
‚éß
{
‚é®
{
‚é©
0
ùëé= 1
1
ùëé= 0
(3.3)
The functions AND, OR and NOT, are the basic logical operators
used in logic and many computer systems. In the context of logic, it is
common to use the notation ùëé‚àßùëèfor AND(ùëé, ùëè), ùëé‚à®ùëèfor OR(ùëé, ùëè) and
ùëéand ¬¨ùëéfor NOT(ùëé), and we will use this notation as well.
Each one of the functions AND, OR, NOT takes either one or two
single bits as input, and produces a single bit as output. Clearly, it
cannot get much more basic than that. However, the power of compu-
tation comes from composing such simple building blocks together.


--- Page 128 ---

128
introduction to theoretical computer science
‚ñ†Example 3.1 ‚Äî Majority from ùê¥ùëÅùê∑,ùëÇùëÖand ùëÅùëÇùëá. Consider the func-
tion MAJ ‚à∂{0, 1}3 ‚Üí{0, 1} that is defined as follows:
MAJ(ùë•) =
‚éß
{
‚é®
{
‚é©
1
ùë•0 + ùë•1 + ùë•2 ‚â•2
0
otherwise
.
(3.4)
That is, for every ùë•‚àà{0, 1}3, MAJ(ùë•) = 1 if and only if the ma-
jority (i.e., at least two out of the three) of ùë•‚Äôs elements are equal
to 1. Can you come up with a formula involving AND, OR and
NOT to compute MAJ? (It would be useful for you to pause at this
point and work out the formula for yourself. As a hint, although
the NOT operator is needed to compute some functions, you will
not need to use it to compute MAJ.)
Let us first try to rephrase MAJ(ùë•) in words: ‚ÄúMAJ(ùë•) = 1 if and
only if there exists some pair of distinct elements ùëñ, ùëósuch that both
ùë•ùëñand ùë•ùëóare equal to 1.‚Äù In other words it means that MAJ(ùë•) = 1
iff either both ùë•0 = 1 and ùë•1 = 1, or both ùë•1 = 1 and ùë•2 = 1, or both
ùë•0 = 1 and ùë•2 = 1. Since the OR of three conditions ùëê0, ùëê1, ùëê2 can
be written as OR(ùëê0, OR(ùëê1, ùëê2)), we can now translate this into a
formula as follows:
MAJ(ùë•0, ùë•1, ùë•2) = OR ( AND(ùë•0, ùë•1) , OR(AND(ùë•1, ùë•2) , AND(ùë•0, ùë•2)) ) .
(3.5)
Recall that we can also write ùëé
‚à®
ùëèfor OR(ùëé, ùëè) and ùëé
‚àß
ùëèfor
AND(ùëé, ùëè). With this notation, (3.5) can also be written as
MAJ(ùë•0, ùë•1, ùë•2) = ((ùë•0 ‚àßùë•1) ‚à®(ùë•1 ‚àßùë•2)) ‚à®(ùë•0 ‚àßùë•2) .
(3.6)
We can also write (3.5) in a ‚Äúprogramming language‚Äù form,
expressing it as a set of instructions for computing MAJ given the
basic operations AND, OR, NOT:
def MAJ(X[0],X[1],X[2]):
firstpair
= AND(X[0],X[1])
secondpair = AND(X[1],X[2])
thirdpair
= AND(X[0],X[2])
temp
= OR(secondpair,thirdpair)
return OR(firstpair,temp)
3.2.1 Some properties of AND and OR
Like standard addition and multiplication, the functions AND and OR
satisfy the properties of commutativity: ùëé‚à®ùëè= ùëè‚à®ùëéand ùëé‚àßùëè= ùëè‚àßùëé
and associativity: (ùëé‚à®ùëè)‚à®ùëê= ùëé‚à®(ùëè‚à®ùëê) and (ùëé‚àßùëè)‚àßùëê= ùëé‚àß(ùëè‚àßùëê). As in


--- Page 129 ---

defining computation
129
the case of addition and multiplication, we often drop the parenthesis
and write ùëé‚à®ùëè‚à®ùëê‚à®ùëëfor ((ùëé‚à®ùëè)‚à®ùëê)‚à®ùëë, and similarly OR‚Äôs and AND‚Äôs
of more terms. They also satisfy a variant of the distributive law:
Solved Exercise 3.1 ‚Äî Distributive law for AND and OR. Prove that for every
ùëé, ùëè, ùëê‚àà{0, 1}, ùëé‚àß(ùëè‚à®ùëê) = (ùëé‚àßùëè) ‚à®(ùëé‚àßùëê).
‚ñ†
Solution:
We can prove this by enumerating over all the 8 possible values
for ùëé, ùëè, ùëê‚àà{0, 1} but it also follows from the standard distributive
law. Suppose that we identify any positive integer with ‚Äútrue‚Äù and
the value zero with ‚Äúfalse‚Äù. Then for every numbers ùë¢, ùë£‚àà‚Ñï, ùë¢+ ùë£
is positive if and only if ùë¢‚à®ùë£is true and ùë¢‚ãÖùë£is positive if and only
if ùë¢‚àßùë£is true. This means that for every ùëé, ùëè, ùëê‚àà{0, 1}, the expres-
sion ùëé‚àß(ùëè‚à®ùëê) is true if and only if ùëé‚ãÖ(ùëè+ ùëê) is positive, and the
expression (ùëé‚àßùëè) ‚à®(ùëé‚àßùëê) is true if and only if ùëé‚ãÖùëè+ ùëé‚ãÖùëêis positive,
But by the standard distributive law ùëé‚ãÖ(ùëè+ ùëê) = ùëé‚ãÖùëè+ ùëé‚ãÖùëêand
hence the former expression is true if and only if the latter one is.
‚ñ†
3.2.2 Extended example: Computing XOR from AND, OR, and NOT
Let us see how we can obtain a different function from the same
building blocks. Define XOR ‚à∂{0, 1}2 ‚Üí{0, 1} to be the function
XOR(ùëé, ùëè) = ùëé+ ùëèmod 2. That is, XOR(0, 0) = XOR(1, 1) = 0 and
XOR(1, 0) = XOR(0, 1) = 1. We claim that we can construct XOR
using only AND, OR, and NOT.
P
As usual, it is a good exercise to try to work out the
algorithm for XOR using AND, OR and NOT on your
own before reading further.
The following algorithm computes XOR using AND, OR, and NOT:
Algorithm 3.2 ‚Äî ùëãùëÇùëÖfrom ùê¥ùëÅùê∑/ùëÇùëÖ/ùëÅùëÇùëá.
Input: ùëé, ùëè‚àà{0, 1}.
Output: ùëãùëÇùëÖ(ùëé, ùëè)
1: ùë§1 ‚Üêùê¥ùëÅùê∑(ùëé, ùëè)
2: ùë§2 ‚ÜêùëÅùëÇùëá(ùë§1)
3: ùë§3 ‚ÜêùëÇùëÖ(ùëé, ùëè)
4: return ùê¥ùëÅùê∑(ùë§2, ùë§3)
Lemma 3.3 For every ùëé, ùëè‚àà{0, 1}, on input ùëé, ùëè, Algorithm 3.2 outputs
ùëé+ ùëèmod 2.


--- Page 130 ---

130
introduction to theoretical computer science
Proof. For every ùëé, ùëè, XOR(ùëé, ùëè) = 1 if and only if ùëéis different from
ùëè. On input ùëé, ùëè‚àà{0, 1}, Algorithm 3.2 outputs AND(ùë§2, ùë§3) where
ùë§2 = NOT(AND(ùëé, ùëè)) and ùë§3 = OR(ùëé, ùëè).
‚Ä¢ If ùëé= ùëè= 0 then ùë§3 = OR(ùëé, ùëè) = 0 and so the output will be 0.
‚Ä¢ If ùëé= ùëè= 1 then AND(ùëé, ùëè) = 1 and so ùë§2 = NOT(AND(ùëé, ùëè)) = 0
and the output will be 0.
‚Ä¢ If ùëé= 1 and ùëè= 0 (or vice versa) then both ùë§3 = OR(ùëé, ùëè) = 1
and ùë§1 = AND(ùëé, ùëè) = 0, in which case the algorithm will output
OR(NOT(ùë§1), ùë§3) = 1.
‚ñ†
We can also express Algorithm 3.2 using a programming language.
Specifically, the following is a Python program that computes the XOR
function:
def AND(a,b): return a*b
def OR(a,b):
return 1-(1-a)*(1-b)
def NOT(a):
return 1-a
def XOR(a,b):
w1 = AND(a,b)
w2 = NOT(w1)
w3 = OR(a,b)
return AND(w2,w3)
# Test out the code
print([f"XOR({a},{b})={XOR(a,b)}" for a in [0,1] for b in
[0,1]])
‚Ü™
# ['XOR(0,0)=0', 'XOR(0,1)=1', 'XOR(1,0)=1', 'XOR(1,1)=0']
Solved Exercise 3.2 ‚Äî Compute ùëãùëÇùëÖon three bits of input. Let XOR3 ‚à∂
{0, 1}3 ‚Üí{0, 1} be the function defined as XOR3(ùëé, ùëè, ùëê) = ùëé+ ùëè+ ùëê
mod 2. That is, XOR3(ùëé, ùëè, ùëê) = 1 if ùëé+ùëè+ùëêis odd, and XOR3(ùëé, ùëè, ùëê) =
0 otherwise. Show that you can compute XOR3 using AND, OR, and
NOT. You can express it as a formula, use a programming language
such as Python, or use a Boolean circuit.
‚ñ†
Solution:
Addition modulo two satisfies the same properties of associativ-
ity ((ùëé+ ùëè) + ùëê= ùëé+ (ùëè+ ùëê)) and commutativity (ùëé+ ùëè= ùëè+ ùëé) as
standard addition. This means that, if we define ùëé‚äïùëèto equal ùëé+ ùëè


--- Page 131 ---

defining computation
131
mod 2, then
XOR3(ùëé, ùëè, ùëê) = (ùëé‚äïùëè) ‚äïùëê
(3.7)
or in other words
XOR3(ùëé, ùëè, ùëê) = XOR(XOR(ùëé, ùëè), ùëê) .
(3.8)
Since we know how to compute XOR using AND, OR, and
NOT, we can compose this to compute XOR3 using the same build-
ing blocks. In Python this corresponds to the following program:
def XOR3(a,b,c):
w1 = AND(a,b)
w2 = NOT(w1)
w3 = OR(a,b)
w4 = AND(w2,w3)
w5 = AND(w4,c)
w6 = NOT(w5)
w7 = OR(w4,c)
return AND(w6,w7)
# Let's test this out
print([f"XOR3({a},{b},{c})={XOR3(a,b,c)}" for a in [0,1]
for b in [0,1] for c in [0,1]])
‚Ü™
# ['XOR3(0,0,0)=0', 'XOR3(0,0,1)=1', 'XOR3(0,1,0)=1',
'XOR3(0,1,1)=0', 'XOR3(1,0,0)=1', 'XOR3(1,0,1)=0',
'XOR3(1,1,0)=0', 'XOR3(1,1,1)=1']
‚Ü™
‚Ü™
‚ñ†
P
Try to generalize the above examples to obtain a way
to compute XORùëõ‚à∂{0, 1}ùëõ‚Üí{0, 1} for every ùëõus-
ing at most 4ùëõbasic steps involving applications of a
function in {AND, OR, NOT} to outputs or previously
computed values.
3.2.3 Informally defining ‚Äúbasic operations‚Äù and ‚Äúalgorithms‚Äù
We have seen that we can obtain at least some examples of interesting
functions by composing together applications of AND, OR, and NOT.
This suggests that we can use AND, OR, and NOT as our ‚Äúbasic opera-
tions‚Äù, hence obtaining the following definition of an ‚Äúalgorithm‚Äù:
Semi-formal definition of an algorithm: An algorithm consists of a
sequence of steps of the form ‚Äúcompute a new value by applying AND,
OR, or NOT to previously computed values‚Äù.


--- Page 132 ---

132
introduction to theoretical computer science
An algorithm ùê¥computes a function ùêπif for every input ùë•to ùêπ, if we
feed ùë•as input to the algorithm, the value computed in its last step is
ùêπ(ùë•).
There are several concerns that are raised by this definition:
1. First and foremost, this definition is indeed too informal. We do not
specify exactly what each step does, nor what it means to ‚Äúfeed ùë•as
input‚Äù.
2. Second, the choice of AND, OR or NOT seems rather arbitrary.
Why not XOR and MAJ? Why not allow operations like addition
and multiplication? What about any other logical constructions
such if/then or while?
3. Third, do we even know that this definition has anything to do
with actual computing? If someone gave us a description of such an
algorithm, could we use it to actually compute the function in the
real world?
P
These concerns will to a large extent guide us in the
upcoming chapters. Thus you would be well advised
to re-read the above informal definition and see what
you think about these issues.
A large part of this book will be devoted to addressing the above
issues. We will see that:
1. We can make the definition of an algorithm fully formal, and so
give a precise mathematical meaning to statements such as ‚ÄúAlgo-
rithm ùê¥computes function ùëì‚Äù.
2. While the choice of AND/OR/NOT is arbitrary, and we could just
as well have chosen other functions, we will also see this choice
does not matter much. We will see that we would obtain the same
computational power if we instead used addition and multiplica-
tion, and essentially every other operation that could be reasonably
thought of as a basic step.
3. It turns out that we can and do compute such ‚ÄúAND/OR/NOT
based algorithms‚Äù in the real world. First of all, such an algorithm
is clearly well specified, and so can be executed by a human with a
pen and paper. Second, there are a variety of ways to mechanize this
computation. We‚Äôve already seen that we can write Python code
that corresponds to following such a list of instructions. But in fact
we can directly implement operations such as AND, OR, and NOT


--- Page 133 ---

defining computation
133
Figure 3.7: Standard symbols for the logical operations
or ‚Äúgates‚Äù of AND, OR, NOT, as well as the operation
NAND discussed in Section 3.5.
Figure 3.8: A circuit with AND, OR and NOT gates for
computing the XOR function.
via electronic signals using components known as transistors. This is
how modern electronic computers operate.
In the remainder of this chapter, and the rest of this book, we will
begin to answer some of these questions. We will see more examples
of the power of simple operations to compute more complex opera-
tions including addition, multiplication, sorting and more. We will
also discuss how to physically implement simple operations such as
AND, OR and NOT using a variety of technologies.
3.3 BOOLEAN CIRCUITS
Boolean circuits provide a precise notion of ‚Äúcomposing basic opera-
tions together‚Äù. A Boolean circuit (see Fig. 3.9) is composed of gates
and inputs that are connected by wires. The wires carry a signal that
represents either the value 0 or 1. Each gate corresponds to either the
OR, AND, or NOT operation. An OR gate has two incoming wires,
and one or more outgoing wires. If these two incoming wires carry
the signals ùëéand ùëè(for ùëé, ùëè‚àà{0, 1}), then the signal on the outgoing
wires will be OR(ùëé, ùëè). AND and NOT gates are defined similarly. The
inputs have only outgoing wires. If we set a certain input to a value
ùëé‚àà{0, 1}, then this value is propagated on all the wires outgoing
from it. We also designate some gates as output gates, and their value
corresponds to the result of evaluating the circuit. For example, ??
gives such a circuit for the XOR function, following Section 3.2.2. We
evaluate an ùëõ-input Boolean circuit ùê∂on an input ùë•‚àà{0, 1}ùëõby plac-
ing the bits of ùë•on the inputs, and then propagating the values on the
wires until we reach an output, see Fig. 3.9.
R
Remark 3.4 ‚Äî Physical realization of Boolean circuits.
Boolean circuits are a mathematical model that does not
necessarily correspond to a physical object, but they
can be implemented physically. In physical imple-
mentation of circuits, the signal is often implemented
by electric potential, or voltage, on a wire, where for
example voltage above a certain level is interpreted
as a logical value of 1, and below a certain level is
interpreted as a logical value of 0. Section 3.4 dis-
cusses physical implementation of Boolean circuits
(with examples including using electrical signals such
as in silicon-based circuits, but also biological and
mechanical implementations as well).
Solved Exercise 3.3 ‚Äî All equal function. Define ALLEQ ‚à∂{0, 1}4 ‚Üí{0, 1}
to be the function that on input ùë•‚àà{0, 1}4 outputs 1 if and only if
ùë•0 = ùë•1 = ùë•2 = ùë•3. Give a Boolean circuit for computing ALLEQ.


--- Page 134 ---

134
introduction to theoretical computer science
Figure 3.9: A Boolean Circuit consists of gates that are
connected by wires to one another and the inputs. The
left side depicts a circuit with 2 inputs and 5 gates,
one of which is designated the output gate. The right
side depicts the evaluation of this circuit on the input
ùë•‚àà{0, 1}2 with ùë•0 = 1 and ùë•1 = 0. The value of
every gate is obtained by applying the corresponding
function (AND, OR, or NOT) to values on the wire(s)
that enter it. The output of the circuit on a given
input is the value of the output gate(s). In this case,
the circuit computes the XOR function and hence it
outputs 1 on the input 10.
Figure 3.10: A Boolean circuit for computing the all
equal function ALLEQ ‚à∂{0, 1}4 ‚Üí{0, 1} that outputs
1 on ùë•‚àà{0, 1}4 if and only if ùë•0 = ùë•1 = ùë•2 = ùë•3.
‚ñ†
Solution:
Another way to describe the function ALLEQ is that it outputs
1 on an input ùë•‚àà{0, 1}4 if and only if ùë•= 04 or ùë•= 14. We can
phrase the condition ùë•
=
14 as ùë•0 ‚àßùë•1 ‚àßùë•2 ‚àßùë•3 which can be
computed using three AND gates. Similarly we can phrase the con-
dition ùë•= 04 as ùë•0 ‚àßùë•1 ‚àßùë•2 ‚àßùë•3 which can be computed using four
NOT gates and three AND gates. The output of ALLEQ is the OR
of these two conditions, which results in the circuit of 4 NOT gates,
6 AND gates, and one OR gate presented in Fig. 3.10.
‚ñ†
3.3.1 Boolean circuits: a formal definition
We defined Boolean circuits informally as obtained by connecting
AND, OR, and NOT gates via wires so as to produce an output from
an input. However, to be able to prove theorems about the existence or
non-existence of Boolean circuits for computing various functions we
need to:
1. Formally define a Boolean circuit as a mathematical object.
2. Formally define what it means for a circuit ùê∂to compute a function
ùëì.
We now proceed to do so. We will define a Boolean circuit as a
labeled Directed Acyclic Graph (DAG). The vertices of the graph corre-
spond to the gates and inputs of the circuit, and the edges of the graph
correspond to the wires. A wire from an input or gate ùë¢to a gate ùë£in
the circuit corresponds to a directed edge between the corresponding
vertices. The inputs are vertices with no incoming edges, while each
gate has the appropriate number of incoming edges based on the func-
tion it computes. (That is, AND and OR gates have two in-neighbors,
while NOT gates have one in-neighbor.) The formal definition is as
follows (see also Fig. 3.11):


--- Page 135 ---

defining computation
135
Figure 3.11: A Boolean Circuit is a labeled directed
acyclic graph (DAG). It has ùëõinput vertices, which are
marked with X[0],‚Ä¶, X[ùëõ‚àí1] and have no incoming
edges, and the rest of the vertices are gates. AND,
OR, and NOT gates have two, two, and one incoming
edges, respectively. If the circuit has ùëöoutputs, then
ùëöof the gates are known as outputs and are marked
with Y[0],‚Ä¶,Y[ùëö‚àí1]. When we evaluate a circuit
ùê∂on an input ùë•‚àà{0, 1}ùëõ, we start by setting the
value of the input vertices to ùë•0, ‚Ä¶ , ùë•ùëõ‚àí1 and then
propagate the values, assigning to each gate ùëîthe
result of applying the operation of ùëîto the values of
ùëî‚Äôs in-neighbors. The output of the circuit is the value
assigned to the output gates.
1 Having parallel edges means an AND or OR gate
ùë¢can have both its in-neighbors be the same gate
ùë£. Since AND(ùëé, ùëé) = OR(ùëé, ùëé) = ùëéfor every ùëé‚àà
{0, 1}, such parallel edges don‚Äôt help in computing
new values in circuits with AND/OR/NOT gates.
However, we will see circuits with more general sets
of gates later on.
Definition 3.5 ‚Äî Boolean Circuits. Let ùëõ, ùëö, ùë†be positive integers with
ùë†‚â•ùëö. A Boolean circuit with ùëõinputs, ùëöoutputs, and ùë†gates, is a
labeled directed acyclic graph (DAG) ùê∫= (ùëâ, ùê∏) with ùë†+ùëõvertices
satisfying the following properties:
‚Ä¢ Exactly ùëõof the vertices have no in-neighbors. These vertices
are known as inputs and are labeled with the ùëõlabels X[0], ‚Ä¶,
X[ùëõ‚àí1]. Each input has at least one out-neighbor.
‚Ä¢ The other ùë†vertices are known as gates. Each gate is labeled with
‚àß, ‚à®or ¬¨. Gates labeled with ‚àß(AND) or ‚à®(OR) have two in-
neighbors. Gates labeled with ¬¨ (NOT) have one in-neighbor.
We will allow parallel edges. 1
‚Ä¢ Exactly ùëöof the gates are also labeled with the ùëölabels Y[0], ‚Ä¶,
Y[ùëö‚àí1] (in addition to their label ‚àß/‚à®/¬¨). These are known as
outputs.
The size of a Boolean circuit is the number of gates it contains.
P
This is a non-trivial mathematical definition, so it is
worth taking the time to read it slowly and carefully.
As in all mathematical definitions, we are using a
known mathematical object ‚Äî a directed acyclic graph
(DAG) ‚Äî to define a new object, a Boolean circuit.
This might be a good time to review some of the basic


--- Page 136 ---

136
introduction to theoretical computer science
properties of DAGs and in particular the fact that they
can be topologically sorted, see Section 1.6.
If ùê∂is a circuit with ùëõinputs and ùëöoutputs, and ùë•‚àà{0, 1}ùëõ, then
we can compute the output of ùê∂on the input ùë•in the natural way:
assign the input vertices X[0], ‚Ä¶, X[ùëõ‚àí1] the values ùë•0, ‚Ä¶ , ùë•ùëõ‚àí1,
apply each gate on the values of its in-neighbors, and then output the
values that correspond to the output vertices. Formally, this is defined
as follows:
Definition 3.6 ‚Äî Computing a function via a Boolean circuit. Let ùê∂be a
Boolean circuit with ùëõinputs and ùëöoutputs. For every ùë•‚àà{0, 1}ùëõ,
the output of ùê∂on the input ùë•, denoted by ùê∂(ùë•), is defined as the
result of the following process:
We let ‚Ñé‚à∂ùëâ
‚Üí‚Ñïbe the minimal layering of ùê∂(aka topological
sorting, see Theorem 1.26). We let ùêøbe the maximum layer of ‚Ñé,
and for ‚Ñì= 0, 1, ‚Ä¶ , ùêøwe do the following:
‚Ä¢ For every ùë£in the ‚Ñì-th layer (i.e., ùë£such that ‚Ñé(ùë£) = ‚Ñì) do:
‚Äì If ùë£is an input vertex labeled with X[ùëñ] for some ùëñ‚àà[ùëõ], then
we assign to ùë£the value ùë•ùëñ.
‚Äì If ùë£is a gate vertex labeled with ‚àßand with two in-neighbors
ùë¢, ùë§then we assign to ùë£the AND of the values assigned to
ùë¢and ùë§. (Since ùë¢and ùë§are in-neighbors of ùë£, they are in a
lower layer than ùë£, and hence their values have already been
assigned.)
‚Äì If ùë£is a gate vertex labeled with ‚à®and with two in-neighbors
ùë¢, ùë§then we assign to ùë£the OR of the values assigned to ùë¢
and ùë§.
‚Äì If ùë£is a gate vertex labeled with ¬¨ and with one in-neighbor ùë¢
then we assign to ùë£the negation of the value assigned to ùë¢.
‚Ä¢ The result of this process is the value ùë¶
‚àà
{0, 1}ùëösuch that for
every ùëó
‚àà
[ùëö], ùë¶ùëóis the value assigned to the vertex with label
Y[ùëó].
Let ùëì‚à∂{0, 1}ùëõ‚Üí{0, 1}ùëö. We say that the circuit ùê∂computes ùëìif
for every ùë•‚àà{0, 1}ùëõ, ùê∂(ùë•) = ùëì(ùë•).
R
Remark 3.7 ‚Äî Boolean circuits nitpicks (optional). In
phrasing Definition 3.5, we‚Äôve made some technical


--- Page 137 ---

defining computation
137
choices that are not very important, but will be con-
venient for us later on. Having parallel edges means
an AND or OR gate ùë¢can have both its in-neighbors
be the same gate ùë£. Since AND(ùëé, ùëé) = OR(ùëé, ùëé) = ùëé
for every ùëé‚àà{0, 1}, such parallel edges don‚Äôt help in
computing new values in circuits with AND/OR/NOT
gates. However, we will see circuits with more gen-
eral sets of gates later on. The condition that every
input vertex has at least one out-neighbor is also not
very important because we can always add ‚Äúdummy
gates‚Äù that touch these inputs. However, it is conve-
nient since it guarantees that (since every gate has at
most two in-neighbors) that the number of inputs in a
circuit is never larger than twice its size.
3.3.2 Equivalence of circuits and straight-line programs
We have seen two ways to describe how to compute a function ùëìusing
AND, OR and NOT:
‚Ä¢ A Boolean circuit, defined in Definition 3.5, computes ùëìby connect-
ing via wires AND, OR, and NOT gates to the inputs.
‚Ä¢ We can also describe such a computation using a straight-line
program that has lines of the form foo = AND(bar,blah), foo =
OR(bar,blah) and foo = NOT(bar) where foo, bar and blah are
variable names. (We call this a straight-line program since it contains
no loops or branching (e.g., if/then) statements.)
We now formally define the AON-CIRC programming language
(‚ÄúAON‚Äù stands for AND/OR/NOT; ‚ÄúCIRC‚Äù stands for circuit) which
has the above operations, and show that it is equivalent to Boolean
circuits.
Definition 3.8 ‚Äî AON-CIRC Programming language. An AON-CIRC pro-
gram is a string of lines of the form foo = AND(bar,blah), foo
= OR(bar,blah) and foo = NOT(bar) where foo, bar and blah
are variable names. 2 Variables of the form X[ùëñ] are known as
input variables, and variables of the form Y[ùëó] are known as output
variables. In every line, the variables on the righthand side of the
assignment operators must either be input variables or variables
that have already been assigned a value.
A valid AON-CIRC program ùëÉincludes input variables of the
form X[0],‚Ä¶,X[ùëõ
‚àí
1] and output variables of the form Y[0],‚Ä¶,
Y[ùëö‚àí1] for some ùëõ, ùëö‚â•1. If ùëÉis valid AON-CIRC program and
ùë•
‚àà
{0, 1}ùëõ, then we define the output of ùëÉon input ùë•, denoted by
ùëÉ(ùë•), to be the string ùë¶
‚àà
{0, 1}ùëöcorresponding to the values of
the output variables Y[0] ,‚Ä¶, Y[ùëö‚àí1] in the execution of ùëÉwhere


--- Page 138 ---

138
introduction to theoretical computer science
2 We follow the common programming languages
convention of using names such as foo, bar, baz,
blah as stand-ins for generic identifiers. A variable
identifier in our programming language can be
any combination of letters, numbers, underscores,
and brackets. The appendix contains a full formal
specification of our programming language.
we initialize the input variables X[0],‚Ä¶,X[ùëõ
‚àí
1] to the values
ùë•0, ‚Ä¶ , ùë•ùëõ‚àí1.
We say that such an AON-CIRC program ùëÉcomputes a function
ùëì‚à∂{0, 1}ùëõ‚Üí{0, 1}ùëöif ùëÉ(ùë•) = ùëì(ùë•) for every ùë•‚àà{0, 1}ùëõ.
AON-CIRC is not a practical programming language: it was de-
signed for pedagogical purposes only, as a way to model computation
as composition of AND, OR, and NOT. However, AON-CIRC can still
be easily implemented on a computer. The following solved exercise
gives an example of an AON-CIRC program.
Solved Exercise 3.4 Consider the following function CMP ‚à∂{0, 1}4 ‚Üí
{0, 1} that on four input bits ùëé, ùëè, ùëê, ùëë‚àà{0, 1}, outputs 1 iff the number
represented by (ùëé, ùëè) is larger than the number represented by (ùëê, ùëë).
That is CMP(ùëé, ùëè, ùëê, ùëë) = 1 iff 2ùëé+ ùëè> 2ùëê+ ùëë.
Write an AON-CIRC program to compute CMP.
‚ñ†
Solution:
Writing such a program is tedious but not truly hard. To com-
pare two numbers we first compare their most significant digit,
and then go down to the next digit and so on and so forth. In this
case where the numbers have just two binary digits, these compar-
isons are particularly simple. The number represented by (ùëé, ùëè) is
larger than the number represented by (ùëê, ùëë) if and only if one of
the following conditions happens:
1. The most significant bit ùëéof (ùëé, ùëè) is larger than the most signifi-
cant bit ùëêof (ùëê, ùëë).
or
2. The two most significant bits ùëéand ùëêare equal, but ùëè> ùëë.
Another way to express the same condition is the following: the
number (ùëé, ùëè) is larger than (ùëê, ùëë) iff ùëé> ùëêOR (ùëé‚â•ùëêAND ùëè> ùëë).
For binary digits ùõº, ùõΩ, the condition ùõº> ùõΩis simply that ùõº= 1
and ùõΩ= 0 or AND(ùõº, NOT(ùõΩ)) = 1, and the condition ùõº‚â•ùõΩis sim-
ply OR(ùõº, NOT(ùõΩ)) = 1. Together these observations can be used to
give the following AON-CIRC program to compute CMP:
temp_1 = NOT(X[2])
temp_2 = AND(X[0],temp_1)
temp_3 = OR(X[0],temp_1)
temp_4 = NOT(X[3])


--- Page 139 ---

defining computation
139
Figure 3.12: A circuit for computing the CMP function.
The evaluation of this circuit on (1, 1, 1, 0) yields the
output 1, since the number 3 (represented in binary
as 11) is larger than the number 2 (represented in
binary as 10).
temp_5 = AND(X[1],temp_4)
temp_6 = AND(temp_5,temp_3)
Y[0] = OR(temp_2,temp_6)
We can also present this 8-line program as a circuit with 8 gates,
see Fig. 3.12.
‚ñ†
It turns out that AON-CIRC programs and Boolean circuits have
exactly the same power:
Theorem 3.9 ‚Äî Equivalence of circuits and straight-line programs. Let
ùëì
‚à∂
{0, 1}ùëõ
‚Üí
{0, 1}ùëöand ùë†
‚â•
ùëöbe some number. Then ùëìis
computable by a Boolean circuit of ùë†gates if and only if ùëìis com-
putable by an AON-CIRC program of ùë†lines.
Proof Idea:
The idea is simple - AON-CIRC programs and Boolean circuits
are just different ways of describing the exact same computational
process. For example, an AND gate in a Boolean circuit corresponding
to computing the AND of two previously-computed values. In a AON-
CIRC program this will correspond to the line that stores in a variable
the AND of two previously-computed variables.
‚ãÜ
P
This proof of Theorem 3.9 is simple at heart, but all
the details it contains can make it a little cumbersome
to read. You might be better off trying to work it out
yourself before reading it. Our GitHub repository con-
tains a ‚Äúproof by Python‚Äù of Theorem 3.9: implemen-
tation of functions circuit2prog and prog2circuits
mapping Boolean circuits to AON-CIRC programs and
vice versa.
Proof of Theorem 3.9. Let ùëì‚à∂{0, 1}ùëõ‚Üí{0, 1}ùëö. Since the theorem is an
‚Äúif and only if‚Äù statement, to prove it we need to show both directions:
translating an AON-CIRC program that computes ùëìinto a circuit that
computes ùëì, and translating a circuit that computes ùëìinto an AON-
CIRC program that does so.
We start with the first direction. Let ùëÉbe an ùë†line AON-CIRC that
computes ùëì. We define a circuit ùê∂as follows: the circuit will have ùëõ
inputs and ùë†gates. For every ùëñ‚àà[ùë†], if the ùëñ-th line has the form foo
= AND(bar,blah) then the ùëñ-th gate in the circuit will be an AND
gate that is connected to gates ùëóand ùëòwhere ùëóand ùëòcorrespond to
the last lines before ùëñwhere the variables bar and blah (respectively)


--- Page 140 ---

140
introduction to theoretical computer science
Figure 3.13: Two equivalent descriptions of the same
AND/OR/NOT computation as both an AON pro-
gram and a Boolean circuit.
where written to. (For example, if ùëñ= 57 and the last line bar was
written to is 35 and the last line blah was written to is 17 then the two
in-neighbors of gate 57 will be gates 35 and 17.) If either bar or blah is
an input variable then we connect the gate to the corresponding input
vertex instead. If foo is an output variable of the form Y[ùëó] then we
add the same label to the corresponding gate to mark it as an output
gate. We do the analogous operations if the ùëñ-th line involves an OR
or a NOT operation (except that we use the corresponding OR or NOT
gate, and in the latter case have only one in-neighbor instead of two).
For every input ùë•‚àà{0, 1}ùëõ, if we run the program ùëÉon ùë•, then the
value written that is computed in the ùëñ-th line is exactly the value
that will be assigned to the ùëñ-th gate if we evaluate the circuit ùê∂on ùë•.
Hence ùê∂(ùë•) = ùëÉ(ùë•) for every ùë•‚àà{0, 1}ùëõ.
For the other direction, let ùê∂be a circuit of ùë†gates and ùëõinputs that
computes the function ùëì. We sort the gates according to a topological
order and write them as ùë£0, ‚Ä¶ , ùë£ùë†‚àí1. We now can create a program
ùëÉof ùë†lines as follows. For every ùëñ‚àà[ùë†], if ùë£ùëñis an AND gate with
in-neighbors ùë£ùëó, ùë£ùëòthen we will add a line to ùëÉof the form temp_ùëñ
= AND(temp_ùëó,temp_ùëò), unless one of the vertices is an input vertex
or an output gate, in which case we change this to the form X[.] or
Y[.] appropriately. Because we work in topological ordering, we are
guaranteed that the in-neighbors ùë£ùëóand ùë£ùëòcorrespond to variables
that have already been assigned a value. We do the same for OR and
NOT gates. Once again, one can verify that for every input ùë•, the
value ùëÉ(ùë•) will equal ùê∂(ùë•) and hence the program computes the
same function as the circuit. (Note that since ùê∂is a valid circuit, per
Definition 3.5, every input vertex of ùê∂has at least one out-neighbor
and there are exactly ùëöoutput gates labeled 0, ‚Ä¶ , ùëö‚àí1; hence all the
variables X[0], ‚Ä¶, X[ùëõ‚àí1] and Y[0] ,‚Ä¶, Y[ùëö‚àí1] will appear in the
program ùëÉ.)
‚ñ†
3.4 PHYSICAL IMPLEMENTATIONS OF COMPUTING DEVICES (DI-
GRESSION)
Computation is an abstract notion that is distinct from its physical
implementations. While most modern computing devices are obtained
by mapping logical gates to semiconductor based transistors, over
history people have computed using a huge variety of mechanisms,
including mechanical systems, gas and liquid (known as fluidics),
biological and chemical processes, and even living creatures (e.g., see
Fig. 3.14 or this video for how crabs or slime mold can be used to do
computations).


--- Page 141 ---

defining computation
141
Figure 3.14: Crab-based logic gates from the paper
‚ÄúRobust soldier-crab ball gate‚Äù by Gunji, Nishiyama
and Adamatzky. This is an example of an AND gate
that relies on the tendency of two swarms of crabs
arriving from different directions to combine to a
single swarm that continues in the average of the
directions.
Figure 3.15: We can implement the logic of transistors
using water. The water pressure from the gate closes
or opens a faucet between the source and the sink.
In this section we will review some of these implementations, both
so you can get an appreciation of how it is possible to directly translate
Boolean circuits to the physical world, without going through the en-
tire stack of architecture, operating systems, and compilers, as well as
to emphasize that silicon-based processors are by no means the only
way to perform computation. Indeed, as we will see in Chapter 23,
a very exciting recent line of work involves using different media for
computation that would allow us to take advantage of quantum me-
chanical effects to enable different types of algorithms.
Such a cool way to explain logic gates. pic.twitter.com/6Wgu2ZKFCx
‚Äî Lionel Page (@page_eco) October 28, 2019
3.4.1 Transistors
A transistor can be thought of as an electric circuit with two inputs,
known as the source and the gate and an output, known as the sink.
The gate controls whether current flows from the source to the sink. In
a standard transistor, if the gate is ‚ÄúON‚Äù then current can flow from the
source to the sink and if it is ‚ÄúOFF‚Äù then it can‚Äôt. In a complementary
transistor this is reversed: if the gate is ‚ÄúOFF‚Äù then current can flow
from the source to the sink and if it is ‚ÄúON‚Äù then it can‚Äôt.
There are several ways to implement the logic of a transistor. For
example, we can use faucets to implement it using water pressure
(e.g. Fig. 3.15). This might seem as merely a curiosity, but there is
a field known as fluidics concerned with implementing logical op-
erations using liquids or gasses. Some of the motivations include
operating in extreme environmental conditions such as in space or a
battlefield, where standard electronic equipment would not survive.
The standard implementations of transistors use electrical current.
One of the original implementations used vacuum tubes. As its name
implies, a vacuum tube is a tube containing nothing (i.e., a vacuum)
and where a priori electrons could freely flow from the source (a
wire) to the sink (a plate). However, there is a gate (a grid) between
the two, where modulating its voltage can block the flow of electrons.
Early vacuum tubes were roughly the size of lightbulbs (and
looked very much like them too). In the 1950‚Äôs they were supplanted
by transistors, which implement the same logic using semiconduc-
tors which are materials that normally do not conduct electricity but
whose conductivity can be modified and controlled by inserting impu-
rities (‚Äúdoping‚Äù) and applying an external electric field (this is known
as the field effect). In the 1960‚Äôs computers started to be implemented
using integrated circuits which enabled much greater density. In 1965,
Gordon Moore predicted that the number of transistors per integrated
circuit would double every year (see Fig. 3.16), and that this would
lead to ‚Äúsuch wonders as home computers ‚Äîor at least terminals con-


--- Page 142 ---

142
introduction to theoretical computer science
Figure 3.16: The number of transistors per integrated
circuits from 1959 till 1965 and a prediction that ex-
ponential growth will continue for at least another
decade. Figure taken from ‚ÄúCramming More Com-
ponents onto Integrated Circuits‚Äù, Gordon Moore,
1965
Figure 3.17: Cartoon from Gordon Moore‚Äôs article
‚Äúpredicting‚Äù the implications of radically improving
transistor density.
Figure 3.18: The exponential growth in computing
power over the last 120 years. Graph by Steve Jurvet-
son, extending a prior graph of Ray Kurzweil.
Figure 3.19: Implementing logical gates using transis-
tors. Figure taken from Rory Mangles‚Äô website.
nected to a central computer‚Äî automatic controls for automobiles,
and personal portable communications equipment‚Äù. Since then, (ad-
justed versions of) this so-called ‚ÄúMoore‚Äôs law‚Äù have been running
strong, though exponential growth cannot be sustained forever, and
some physical limitations are already becoming apparent.
3.4.2 Logical gates from transistors
We can use transistors to implement various Boolean functions such as
AND, OR, and NOT. For each a two-input gate ùê∫‚à∂{0, 1}2 ‚Üí{0, 1},
such an implementation would be a system with two input wires ùë•, ùë¶
and one output wire ùëß, such that if we identify high voltage with ‚Äú1‚Äù
and low voltage with ‚Äú0‚Äù, then the wire ùëßwill equal to ‚Äú1‚Äù if and only
if applying ùê∫to the values of the wires ùë•and ùë¶is 1 (see Fig. 3.19 and
Fig. 3.20). This means that if there exists a AND/OR/NOT circuit to
compute a function ùëî‚à∂{0, 1}ùëõ‚Üí{0, 1}ùëö, then we can compute ùëîin
the physical world using transistors as well.
3.4.3 Biological computing
Computation can be based on biological or chemical systems. For ex-
ample the lac operon produces the enzymes needed to digest lactose
only if the conditions ùë•‚àß(¬¨ùë¶) hold where ùë•is ‚Äúlactose is present‚Äù and
ùë¶is ‚Äúglucose is present‚Äù. Researchers have managed to create transis-
tors, and from them logic gates, based on DNA molecules (see also
Fig. 3.21). One motivation for DNA computing is to achieve increased
parallelism or storage density; another is to create ‚Äúsmart biological
agents‚Äù that could perhaps be injected into bodies, replicate them-
selves, and fix or kill cells that were damaged by a disease such as
cancer. Computing in biological systems is not restricted, of course, to
DNA: even larger systems such as flocks of birds can be considered as
computational processes.
3.4.4 Cellular automata and the game of life
Cellular automata is a model of a system composed of a sequence of
cells, each of which can have a finite state. At each step, a cell updates
its state based on the states of its neighboring cells and some simple
rules. As we will discuss later in this book (see Section 8.4), cellular
automata such as Conway‚Äôs ‚ÄúGame of Life‚Äù can be used to simulate
computation gates .
3.4.5 Neural networks
One computation device that we all carry with us is our own brain.
Brains have served humanity throughout history, doing computations
that range from distinguishing prey from predators, through making
scientific discoveries and artistic masterpieces, to composing witty 280
character messages. The exact working of the brain is still not fully


--- Page 143 ---

defining computation
143
Figure 3.20: Implementing a NAND gate (see Sec-
tion 3.5) using transistors.
Figure 3.21: Performance of DNA-based logic gates.
Figure taken from paper of Bonnet et al, Science, 2013.
Figure 3.22: An AND gate using a ‚ÄúGame of Life‚Äù
configuration. Figure taken from Jean-Philippe
Rennard‚Äôs paper.
understood, but one common mathematical model for it is a (very
large) neural network.
A neural network can be thought of as a Boolean circuit that instead
of AND/OR/NOT uses some other gates as the basic basis. For exam-
ple, one particular basis we can use are threshold gates. For every vector
ùë§= (ùë§0, ‚Ä¶ , ùë§ùëò‚àí1) of integers and integer ùë°(some or all of which
could be negative), the threshold function corresponding to ùë§, ùë°is the
function ùëáùë§,ùë°‚à∂{0, 1}ùëò‚Üí{0, 1} that maps ùë•‚àà{0, 1}ùëòto 1 if and only if
‚àë
ùëò‚àí1
ùëñ=0 ùë§ùëñùë•ùëñ‚â•ùë°. For example, the threshold function ùëáùë§,ùë°correspond-
ing to ùë§= (1, 1, 1, 1, 1) and ùë°= 3 is simply the majority function MAJ5
on {0, 1}5. Threshold gates can be thought of as an approximation for
neuron cells that make up the core of human and animal brains. To a
first approximation, a neuron has ùëòinputs and a single output, and
the neurons ‚Äúfires‚Äù or ‚Äúturns on‚Äù its output when those signals pass
some threshold.
Many machine learning algorithms use artificial neural networks
whose purpose is not to imitate biology but rather to perform some
computational tasks, and hence are not restricted to threshold or
other biologically-inspired gates. Generally, a neural network is often
described as operating on signals that are real numbers, rather than
0/1 values, and where the output of a gate on inputs ùë•0, ‚Ä¶ , ùë•ùëò‚àí1 is
obtained by applying ùëì(‚àëùëñùë§ùëñùë•ùëñ) where ùëì‚à∂‚Ñù‚Üí‚Ñùis an activation
function such as rectified linear unit (ReLU), Sigmoid, or many others
(see Fig. 3.23). However, for the purposes of our discussion, all of
the above are equivalent (see also Exercise 3.13). In particular we can
reduce the setting of real inputs to binary inputs by representing a
real number in the binary basis, and multiplying the weight of the bit
corresponding to the ùëñùë°‚Ñédigit by 2ùëñ.
3.4.6 A computer made from marbles and pipes
We can implement computation using many other physical media,
without any electronic, biological, or chemical components. Many
suggestions for mechanical computers have been put forward, going
back at least to Gottfried Leibniz‚Äô computing machines from the 1670s
and Charles Babbage‚Äôs 1837 plan for a mechanical ‚ÄúAnalytical Engine‚Äù.
As one example, Fig. 3.24 shows a simple implementation of a NAND
(negation of AND, see Section 3.5) gate using marbles going through
pipes. We represent a logical value in {0, 1} by a pair of pipes, such
that there is a marble flowing through exactly one of the pipes. We
call one of the pipes the ‚Äú0 pipe‚Äù and the other the ‚Äú1 pipe‚Äù, and so
the identity of the pipe containing the marble determines the logical
value. A NAND gate corresponds to a mechanical object with two
pairs of incoming pipes and one pair of outgoing pipes, such that for
every ùëé, ùëè‚àà{0, 1}, if two marbles are rolling toward the object in the


--- Page 144 ---

144
introduction to theoretical computer science
Figure 3.23: Common activation functions used in
Neural Networks, including rectified linear units
(ReLU), sigmoids, and hyperbolic tangent. All of
those can be thought of as continuous approximations
to simple the step function. All of these can be used
to compute the NAND gate (see Exercise 3.13). This
property enables neural networks to (approximately)
compute any function that can be computed by a
Boolean circuit.
Figure 3.24: A physical implementation of a NAND
gate using marbles. Each wire in a Boolean circuit is
modeled by a pair of pipes representing the values
0 and 1 respectively, and hence a gate has four input
pipes (two for each logical input) and two output
pipes. If one of the input pipes representing the value
0 has a marble in it then that marble will flow to the
output pipe representing the value 1. (The dashed
line represents a gadget that will ensure that at most
one marble is allowed to flow onward in the pipe.)
If both the input pipes representing the value 1 have
marbles in them, then the first marble will be stuck
but the second one will flow onwards to the output
pipe representing the value 0.
Figure 3.25: A ‚Äúgadget‚Äù in a pipe that ensures that at
most one marble can pass through it. The first marble
that passes causes the barrier to lift and block new
ones.
ùëépipe of the first pair and the ùëèpipe of the second pair, then a marble
will roll out of the object in the NAND(ùëé, ùëè)-pipe of the outgoing pair.
In fact, there is even a commercially-available educational game that
uses marbles as a basis of computing, see Fig. 3.26.
3.5 THE NAND FUNCTION
The NAND function is another simple function that is extremely use-
ful for defining computation. It is the function mapping {0, 1}2 to
{0, 1} defined by:
NAND(ùëé, ùëè) =
‚éß
{
‚é®
{
‚é©
0
ùëé= ùëè= 1
1
otherwise
.
(3.9)
As its name implies, NAND is the NOT of AND (i.e., NAND(ùëé, ùëè) =
NOT(AND(ùëé, ùëè))), and so we can clearly compute NAND using AND
and NOT. Interestingly, the opposite direction holds as well:
Theorem 3.10 ‚Äî NAND computes AND,OR,NOT. We can compute AND,
OR, and NOT by composing only the NAND function.
Proof. We start with the following observation. For every ùëé‚àà{0, 1},
AND(ùëé, ùëé) = ùëé. Hence, NAND(ùëé, ùëé) = NOT(AND(ùëé, ùëé)) = NOT(ùëé).
This means that NAND can compute NOT. By the principle of ‚Äúdou-
ble negation‚Äù, AND(ùëé, ùëè) = NOT(NOT(AND(ùëé, ùëè))), and hence
we can use NAND to compute AND as well. Once we can compute
AND and NOT, we can compute OR using ‚ÄúDe Morgan‚Äôs Law‚Äù:
OR(ùëé, ùëè) = NOT(AND(NOT(ùëé), NOT(ùëè))) (which can also be writ-
ten as ùëé‚à®ùëè= ùëé‚àßùëè) for every ùëé, ùëè‚àà{0, 1}.
‚ñ†
P
Theorem 3.10‚Äôs proof is very simple, but you should
make sure that (i) you understand the statement of
the theorem, and (ii) you follow its proof. In partic-
ular, you should make sure you understand why De
Morgan‚Äôs law is true.
We can use NAND to compute many other functions, as demon-
strated in the following exercise.
Solved Exercise 3.5 ‚Äî Compute majority with NAND. Let MAJ ‚à∂{0, 1}3 ‚Üí
{0, 1} be the function that on input ùëé, ùëè, ùëêoutputs 1 iff ùëé+ ùëè+ ùëê‚â•2.
Show how to compute MAJ using a composition of NAND‚Äôs.
‚ñ†


--- Page 145 ---

defining computation
145
Figure 3.26: The game ‚ÄúTuring Tumble‚Äù contains an
implementation of logical gates using marbles.
Figure 3.27: A circuit with NAND gates to compute
the Majority function on three bits
Solution:
Recall that (3.5) states that
MAJ(ùë•0, ùë•1, ùë•2) = OR ( AND(ùë•0, ùë•1) , OR(AND(ùë•1, ùë•2) , AND(ùë•0, ùë•2)) ) .
(3.10)
We can use Theorem 3.10 to replace all the occurrences of AND
and OR with NAND‚Äôs. Specifically, we can use the equivalence
AND(ùëé, ùëè) = NOT(NAND(ùëé, ùëè)), OR(ùëé, ùëè) = NAND(NOT(ùëé), NOT(ùëè)),
and NOT(ùëé)
=
NAND(ùëé, ùëé) to replace the righthand side of
(3.10) with an expression involving only NAND, yielding that
MAJ(ùëé, ùëè, ùëê) is equivalent to the (somewhat unwieldy) expression
NAND( NAND( NAND(NAND(ùëé, ùëè), NAND(ùëé, ùëê)),
NAND(NAND(ùëé, ùëè), NAND(ùëé, ùëê)) ),
NAND(ùëè, ùëê) )
(3.11)
The same formula can also be expressed as a circuit with NAND
gates, see Fig. 3.27.
‚ñ†
3.5.1 NAND Circuits
We define NAND Circuits as circuits in which all the gates are NAND
operations. Such a circuit again corresponds to a directed acyclic
graph (DAG) since all the gates correspond to the same function (i.e.,
NAND), we do not even need to label them, and all gates have in-
degree exactly two. Despite their simplicity, NAND circuits can be
quite powerful.
‚ñ†Example 3.11 ‚Äî ùëÅùê¥ùëÅùê∑circuit for ùëãùëÇùëÖ. Recall the XOR function
which maps ùë•0, ùë•1
‚àà
{0, 1} to ùë•0 + ùë•1 mod 2. We have seen in
Section 3.2.2 that we can compute XOR using AND, OR, and NOT,
and so by Theorem 3.10 we can compute it using only NAND‚Äôs.
However, the following is a direct construction of computing XOR
by a sequence of NAND operations:
1. Let ùë¢= NAND(ùë•0, ùë•1).
2. Let ùë£= NAND(ùë•0, ùë¢)
3. Let ùë§= NAND(ùë•1, ùë¢).
4. The XOR of ùë•0 and ùë•1 is ùë¶0 = NAND(ùë£, ùë§).


--- Page 146 ---

146
introduction to theoretical computer science
Figure 3.28: A circuit with NAND gates to compute
the XOR of two bits.
One can verify that this algorithm does indeed compute XOR
by enumerating all the four choices for ùë•0, ùë•1 ‚àà{0, 1}. We can also
represent this algorithm graphically as a circuit, see Fig. 3.28.
In fact, we can show the following theorem:
Theorem 3.12 ‚Äî NAND is a universal operation. For every Boolean circuit
ùê∂of ùë†gates, there exists a NAND circuit ùê∂‚Ä≤ of at most 3ùë†gates that
computes the same function as ùê∂.
Proof Idea:
The idea of the proof is to just replace every AND, OR and NOT
gate with their NAND implementation following the proof of Theo-
rem 3.10.
‚ãÜ
Proof of Theorem 3.12. If ùê∂is a Boolean circuit, then since, as we‚Äôve
seen in the proof of Theorem 3.10, for every ùëé, ùëè‚àà{0, 1}
‚Ä¢ NOT(ùëé) = NAND(ùëé, ùëé)
‚Ä¢ AND(ùëé, ùëè) = NAND(NAND(ùëé, ùëè), NAND(ùëé, ùëè))
‚Ä¢ OR(ùëé, ùëè) = NAND(NAND(ùëé, ùëé), NAND(ùëè, ùëè))
we can replace every gate of ùê∂with at most three NAND gates to
obtain an equivalent circuit ùê∂‚Ä≤. The resulting circuit will have at most
3ùë†gates.
‚ñ†
ÔÉ´Big Idea 3 Two models are equivalent in power if they can be used
to compute the same set of functions.
3.5.2 More examples of NAND circuits (optional)
Here are some more sophisticated examples of NAND circuits:
Incrementing integers.
Consider the task of computing, given as input
a string ùë•‚àà{0, 1}ùëõthat represents a natural number ùëã‚àà‚Ñï, the
representation of ùëã+ 1. That is, we want to compute the function
INCùëõ‚à∂{0, 1}ùëõ‚Üí{0, 1}ùëõ+1 such that for every ùë•0, ‚Ä¶ , ùë•ùëõ‚àí1, INCùëõ(ùë•) =
ùë¶which satisfies ‚àë
ùëõ
ùëñ=0 ùë¶ùëñ2ùëñ= (‚àë
ùëõ‚àí1
ùëñ=0 ùë•ùëñ2ùëñ) + 1. (For simplicity of
notation, in this example we use the representation where the least
significant digit is first rather than last.)
The increment operation can be very informally described as fol-
lows: ‚ÄúAdd 1 to the least significant bit and propagate the carry‚Äù. A little


--- Page 147 ---

defining computation
147
Figure 3.29: NAND circuit with computing the incre-
ment function on 4 bits.
more precisely, in the case of the binary representation, to obtain the
increment of ùë•, we scan ùë•from the least significant bit onwards, and
flip all 1‚Äôs to 0‚Äôs until we encounter a bit equal to 0, in which case we
flip it to 1 and stop.
Thus we can compute the increment of ùë•0, ‚Ä¶ , ùë•ùëõ‚àí1 by doing the
following:
Algorithm 3.13 ‚Äî Compute Increment Function.
Input: ùë•0, ùë•1, ‚Ä¶ , ùë•ùëõ‚àí1 representing the number ‚àë
ùëõ‚àí1
ùëñ=0 ùë•ùëñ‚ãÖ2ùëñ
# we use LSB-first representation
Output: ùë¶‚àà{0, 1}ùëõ+1 such that ‚àë
ùëõ
ùëñ=0 ùë¶ùëñ‚ãÖ2ùëñ= ‚àë
ùëõ‚àí1
ùëñ=0 ùë•ùëñ‚ãÖ2ùëñ+1
1: Let ùëê0 ‚Üê1
# we pretend we have a ‚Äùcarry‚Äù of 1 initially
2: for ùëñ= 0, ‚Ä¶ , ùëõ‚àí1 do
3:
Let ùë¶ùëñ‚ÜêùëãùëÇùëÖ(ùë•ùëñ, ùëêùëñ).
4:
if ùëêùëñ= ùë•ùëñ= 1 then
5:
ùëêùëñ+1 = 1
6:
else
7:
ùëêùëñ+1 = 0
8:
end if
9: end for
10: Let ùë¶ùëõ‚Üêùëêùëõ.
Algorithm 3.13 describes precisely how to compute the increment
operation, and can be easily transformed into Python code that per-
forms the same computation, but it does not seem to directly yield
a NAND circuit to compute this. However, we can transform this
algorithm line by line to a NAND circuit. For example, since for ev-
ery ùëé, NAND(ùëé, NOT(ùëé)) = 1, we can replace the initial statement
ùëê0 = 1 with ùëê0 = NAND(ùë•0, NAND(ùë•0, ùë•0)). We already know
how to compute XOR using NAND and so we can use this to im-
plement the operation ùë¶ùëñ‚ÜêXOR(ùë•ùëñ, ùëêùëñ). Similarly, we can write
the ‚Äúif‚Äù statement as saying ùëêùëñ+1 ‚ÜêAND(ùëêùëñ, ùë•ùëñ), or in other words
ùëêùëñ+1 ‚ÜêNAND(NAND(ùëêùëñ, ùë•ùëñ), NAND(ùëêùëñ, ùë•ùëñ)). Finally, the assignment
ùë¶ùëõ= ùëêùëõcan be written as ùë¶ùëõ= NAND(NAND(ùëêùëõ, ùëêùëõ), NAND(ùëêùëõ, ùëêùëõ)).
Combining these observations yields for every ùëõ‚àà‚Ñï, a NAND circuit
to compute INCùëõ. For example, Fig. 3.29 shows what this circuit looks
like for ùëõ= 4.
From increment to addition.
Once we have the increment operation,
we can certainly compute addition by repeatedly incrementing (i.e.,
compute ùë•+ùë¶by performing INC(ùë•) ùë¶times). However, that would be
quite inefficient and unnecessary. With the same idea of keeping track
of carries we can implement the ‚Äúgrade-school‚Äù addition algorithm
and compute the function ADDùëõ‚à∂{0, 1}2ùëõ‚Üí{0, 1}ùëõ+1 that on


--- Page 148 ---

148
introduction to theoretical computer science
input ùë•‚àà{0, 1}2ùëõoutputs the binary representation of the sum of the
numbers represented by ùë•0, ‚Ä¶ , ùë•ùëõ‚àí1 and ùë•ùëõ+1, ‚Ä¶ , ùë•ùëõ:
Algorithm 3.14 ‚Äî Addition using NAND.
Input: ùë¢‚àà{0, 1}ùëõ, ùë£‚àà{0, 1}ùëõrepresenting numbers in
LSB-first binary representation.
Output: LSB-first binary representation of ùë•+ ùë¶.
1: Let ùëê0 ‚Üê0
2: for ùëñ= 0, ‚Ä¶ , ùëõ‚àí1 do
3:
Let ùë¶ùëñ‚Üêùë¢ùëñ+ ùë£ùëñmod 2
4:
if ùë¢ùëñ+ ùë£ùëñ+ ùëêùëñ‚â•2 then
5:
ùëêùëñ+1 ‚Üê1
6:
else
7:
ùëêùëñ+1 ‚Üê0
8:
end if
9: end for
10: Let ùë¶ùëõ‚Üêùëêùëõ
Once again, Algorithm 3.14 can be translated into a NAND cir-
cuit. The crucial observation is that the ‚Äúif/then‚Äù statement simply
corresponds to ùëêùëñ+1 ‚ÜêMAJ3(ùë¢ùëñ, ùë£ùëñ, ùë£ùëñ) and we have seen in Solved
Exercise 3.5 that the function MAJ3 ‚à∂{0, 1}3 ‚Üí{0, 1} can be computed
using NANDs.
3.5.3 The NAND-CIRC Programming language
Just like we did for Boolean circuits, we can define a programming-
language analog of NAND circuits. It is even simpler than the AON-
CIRC language since we only have a single operation. We define the
NAND-CIRC Programming Language to be a programming language
where every line has the following form:
foo = NAND(bar,blah)
where foo, bar and blah are variable identifiers.
‚ñ†Example 3.15 ‚Äî Our first NAND-CIRC program. Here is an example of a
NAND-CIRC program:
u = NAND(X[0],X[1])
v = NAND(X[0],u)
w = NAND(X[1],u)
Y[0] = NAND(v,w)


--- Page 149 ---

defining computation
149
Figure 3.30: A NAND program and the corresponding
circuit. Note how every line in the program corre-
sponds to a gate in the circuit.
P
Do you know what function this program computes?
Hint: you have seen it before.
Formally, just like we did in Definition 3.8 for AON-CIRC, we can
define the notion of computation by a NAND-CIRC program in the
natural way:
Definition 3.16 ‚Äî Computing by a NAND-CIRC program. Let ùëì‚à∂{0, 1}ùëõ‚Üí
{0, 1}ùëöbe some function, and let ùëÉbe a NAND-CIRC program.
We say that ùëÉcomputes the function ùëìif:
1. ùëÉhas ùëõinput variables X[0], ‚Ä¶ ,X[ùëõ‚àí1] and ùëöoutput variables
Y[0],‚Ä¶,Y[ùëö‚àí1].
2. For every ùë•
‚àà
{0, 1}ùëõ, if we execute ùëÉwhen we assign to
X[0], ‚Ä¶ ,X[ùëõ
‚àí
1] the values ùë•0, ‚Ä¶ , ùë•ùëõ‚àí1, then at the end of
the execution, the output variables Y[0],‚Ä¶,Y[ùëö
‚àí
1] have the
values ùë¶0, ‚Ä¶ , ùë¶ùëö‚àí1 where ùë¶= ùëì(ùë•).
As before we can show that NAND circuits are equivalent to
NAND-CIRC programs (see Fig. 3.30):
Theorem 3.17 ‚Äî NAND circuits and straight-line program equivalence. For
every ùëì
‚à∂
{0, 1}ùëõ
‚Üí
{0, 1}ùëöand ùë†
‚â•
ùëö, ùëìis computable by a
NAND-CIRC program of ùë†lines if and only if ùëìis computable by a
NAND circuit of ùë†gates.
We omit the proof of Theorem 3.17 since it follows along exactly
the same lines as the equivalence of Boolean circuits and AON-CIRC
program (Theorem 3.9). Given Theorem 3.17 and Theorem 3.12, we
know that we can translate every ùë†-line AON-CIRC program ùëÉinto
an equivalent NAND-CIRC program of at most 3ùë†lines. In fact, this
translation can be easily done by replacing every line of the form
foo = AND(bar,blah), foo = OR(bar,blah) or foo = NOT(bar)
with the equivalent 1-3 lines that use the NAND operation. Our GitHub
repository contains a ‚Äúproof by code‚Äù: a simple Python program
AON2NAND that transforms an AON-CIRC into an equivalent NAND-
CIRC program.
R
Remark 3.18 ‚Äî Is the NAND-CIRC programming language
Turing Complete? (optional note). You might have heard
of a term called ‚ÄúTuring Complete‚Äù that is sometimes


--- Page 150 ---

150
introduction to theoretical computer science
used to describe programming languages. (If you
haven‚Äôt, feel free to ignore the rest of this remark: we
define this term precisely in Chapter 8.) If so, you
might wonder if the NAND-CIRC programming lan-
guage has this property. The answer is no, or perhaps
more accurately, the term ‚ÄúTuring Completeness‚Äù is
not really applicable for the NAND-CIRC program-
ming language. The reason is that, by design, the
NAND-CIRC programming language can only com-
pute finite functions ùêπ‚à∂{0, 1}ùëõ‚Üí{0, 1}ùëöthat take a
fixed number of input bits and produce a fixed num-
ber of outputs bits. The term ‚ÄúTuring Complete‚Äù is
only applicable to programming languages for infinite
functions that can take inputs of arbitrary length. We
will come back to this distinction later on in this book.
3.6 EQUIVALENCE OF ALL THESE MODELS
If we put together Theorem 3.9, Theorem 3.12, and Theorem 3.17, we
obtain the following result:
Theorem 3.19 ‚Äî Equivalence between models of finite computation. For
every sufficiently large ùë†, ùëõ, ùëöand ùëì
‚à∂
{0, 1}ùëõ
‚Üí
{0, 1}ùëö, the
following conditions are all equivalent to one another:
‚Ä¢ ùëìcan be computed by a Boolean circuit (with ‚àß, ‚à®, ¬¨ gates) of at
most ùëÇ(ùë†) gates.
‚Ä¢ ùëìcan be computed by an AON-CIRC straight-line program of at
most ùëÇ(ùë†) lines.
‚Ä¢ ùëìcan be computed by a NAND circuit of at most ùëÇ(ùë†) gates.
‚Ä¢ ùëìcan be computed by a NAND-CIRC straight-line program of at
most ùëÇ(ùë†) lines.
By ‚ÄúùëÇ(ùë†)‚Äù we mean that the bound is at most ùëê‚ãÖùë†where ùëêis a con-
stant that is independent of ùëõ. For example, if ùëìcan be computed by a
Boolean circuit of ùë†gates, then it can be computed by a NAND-CIRC
program of at most 3ùë†lines, and if ùëìcan be computed by a NAND
circuit of ùë†gates, then it can be computed by an AON-CIRC program
of at most 2ùë†lines.
Proof Idea:
We omit the formal proof, which is obtained by combining Theo-
rem 3.9, Theorem 3.12, and Theorem 3.17. The key observation is that
the results we have seen allow us to translate a program/circuit that
computes ùëìin one of the above models into a program/circuit that


--- Page 151 ---

defining computation
151
computes ùëìin another model by increasing the lines/gates by at most
a constant factor (in fact this constant factor is at most 3).
‚ãÜ
Theorem 3.9 is a special case of a more general result. We can con-
sider even more general models of computation, where instead of
AND/OR/NOT or NAND, we use other operations (see Section 3.6.1
below). It turns out that Boolean circuits are equivalent in power to
such models as well. The fact that all these different ways to define
computation lead to equivalent models shows that we are ‚Äúon the
right track‚Äù. It justifies the seemingly arbitrary choices that we‚Äôve
made of using AND/OR/NOT or NAND as our basic operations,
since these choices do not affect the power of our computational
model. Equivalence results such as Theorem 3.19 mean that we can
easily translate between Boolean circuits, NAND circuits, NAND-
CIRC programs and the like. We will use this ability later on in this
book, often shifting to the most convenient formulation without mak-
ing a big deal about it. Hence we will not worry too much about the
distinction between, for example, Boolean circuits and NAND-CIRC
programs.
In contrast, we will continue to take special care to distinguish
between circuits/programs and functions (recall Big Idea 2). A func-
tion corresponds to a specification of a computational task, and it is
a fundamentally different object than a program or a circuit, which
corresponds to the implementation of the task.
3.6.1 Circuits with other gate sets
There is nothing special about AND/OR/NOT or NAND. For every
set of functions ùí¢= {ùê∫0, ‚Ä¶ , ùê∫ùëò‚àí1}, we can define a notion of circuits
that use elements of ùí¢as gates, and a notion of a ‚Äúùí¢programming
language‚Äù where every line involves assigning to a variable foo the re-
sult of applying some ùê∫ùëñ‚ààùí¢to previously defined or input variables.
Specifically, we can make the following definition:
Definition 3.20 ‚Äî General straight-line programs. Let ‚Ñ±
=
{ùëì0, ‚Ä¶ , ùëìùë°‚àí1}
be a finite collection of Boolean functions, such that ùëìùëñ‚à∂{0, 1}ùëòùëñ‚Üí
{0, 1} for some ùëòùëñ‚àà‚Ñï. An ‚Ñ±program is a sequence of lines, each of
which assigns to some variable the result of applying some ùëìùëñ‚àà‚Ñ±
to ùëòùëñother variables. As above, we use X[ùëñ] and Y[ùëó] to denote the
input and output variables.
We say that ‚Ñ±is a universal set of operations (also known as a uni-
versal gate set) if there exists a ‚Ñ±program to compute the function
NAND.


--- Page 152 ---

152
introduction to theoretical computer science
3 One can also define these functions as taking a
length zero input. This makes no difference for the
computational power of the model.
AON-CIRC programs correspond to {ùê¥ùëÅùê∑, OR, NOT} programs,
NAND-CIRC programs corresponds to ‚Ñ±programs for the set
‚Ñ±that only contains the NAND function, but we can also define
{IF, ZERO, ONE} programs (see below), or use any other set.
We can also define ‚Ñ±circuits, which will be directed graphs in
which each gate corresponds to applying a function ùëìùëñ‚àà‚Ñ±, and will
each have ùëòùëñincoming wires and a single outgoing wire. (If the func-
tion ùëìùëñis not symmetric, in the sense that the order of its input matters
then we need to label each wire entering a gate as to which parameter
of the function it corresponds to.) As in Theorem 3.9, we can show
that ‚Ñ±circuits and ‚Ñ±programs are equivalent. We have seen that for
‚Ñ±= {AND, OR, NOT}, the resulting circuits/programs are equivalent
in power to the NAND-CIRC programming language, as we can com-
pute NAND using AND/OR/NOT and vice versa. This turns out to be
a special case of a general phenomena‚Äî the universality of NAND and
other gate sets ‚Äî that we will explore more in depth later in this book.
‚ñ†Example 3.21 ‚Äî IF,ZERO,ONE circuits. Let ‚Ñ±
=
{IF, ZERO, ONE}
where ZERO ‚à∂{0, 1} ‚Üí{0} and ONE ‚à∂{0, 1} ‚Üí{1} are the
constant zero and one functions, 3 and IF ‚à∂{0, 1}3 ‚Üí{0, 1} is the
function that on input (ùëé, ùëè, ùëê) outputs ùëèif ùëé
=
1 and ùëêotherwise.
Then ‚Ñ±is universal.
Indeed, we can demonstrate that {IF, ZERO, ONE} is universal
using the following formula for NAND:
NAND(ùëé, ùëè) = IF(ùëé, IF(ùëè, ZERO, ONE), ONE) .
(3.12)
There are also some sets ‚Ñ±that are more restricted in power. For
example it can be shown that if we use only AND or OR gates (with-
out NOT) then we do not get an equivalent model of computation.
The exercises cover several examples of universal and non-universal
gate sets.
3.6.2 Specification vs. implementation (again)
As we discussed in Section 2.6.1, one of the most important distinc-
tions in this book is that of specification versus implementation or sep-
arating ‚Äúwhat‚Äù from ‚Äúhow‚Äù (see Fig. 3.31). A function corresponds
to the specification of a computational task, that is what output should
be produced for every particular input. A program (or circuit, or any
other way to specify algorithms) corresponds to the implementation of
how to compute the desired output from the input. That is, a program
is a set of instructions how to compute the output from the input.
Even within the same computational model there can be many differ-
ent ways to compute the same function. For example, there is more


--- Page 153 ---

defining computation
153
Figure 3.31: It is crucial to distinguish between the
specification of a computational task, namely what is
the function that is to be computed and the implemen-
tation of it, namely the algorithm, program, or circuit
that contains the instructions defining how to map
an input to an output. The same function could be
computed in many different ways.
than one NAND-CIRC program that computes the majority function,
more than one Boolean circuit to compute the addition function, and
so on and so forth.
Confusing specification and implementation (or equivalently func-
tions and programs) is a common mistake, and one that is unfortu-
nately encouraged by the common programming-language termi-
nology of referring to parts of programs as ‚Äúfunctions‚Äù. However, in
both the theory and practice of computer science, it is important to
maintain this distinction, and it is particularly important for us in this
book.
‚úì
Chapter Recap
‚Ä¢ An algorithm is a recipe for performing a compu-
tation as a sequence of ‚Äúelementary‚Äù or ‚Äúsimple‚Äù
operations.
‚Ä¢ One candidate definition for ‚Äúelementary‚Äù opera-
tions is the set AND, OR and NOT.
‚Ä¢ Another candidate definition for an ‚Äúelementary‚Äù
operation is the NAND operation. It is an operation
that is easily implementable in the physical world
in a variety of methods including by electronic
transistors.
‚Ä¢ We can use NAND to compute many other func-
tions, including majority, increment, and others.
‚Ä¢ There are other equivalent choices, including the
sets {ùê¥ùëÅùê∑, OR, NOT} and {IF, ZERO, ONE}.
‚Ä¢ We can formally define the notion of a function
ùêπ‚à∂{0, 1}ùëõ‚Üí{0, 1}ùëöbeing computable using the
NAND-CIRC Programming language.
‚Ä¢ For every set of basic operations, the notions of be-
ing computable by a circuit and being computable
by a straight-line program are equivalent.


--- Page 154 ---

154
introduction to theoretical computer science
4 Hint: Use the fact that MAJ(ùëé, ùëè, ùëê) = ùëÄùê¥ùêΩ(ùëé, ùëè, ùëê)
to prove that every ùëì‚à∂{0, 1}ùëõ‚Üí{0, 1} computable
by a circuit with only MAJ and NOT gates satisfies
ùëì(0, 0, ‚Ä¶ , 0) ‚â†ùëì(1, 1, ‚Ä¶ , 1). Thanks to Nathan
Brunelle and David Evans for suggesting this exercise.
3.7 EXERCISES
Exercise 3.1 ‚Äî Compare 4 bit numbers. Give a Boolean circuit
(with AND/OR/NOT gates) that computes the function
CMP8 ‚à∂{0, 1}8 ‚Üí{0, 1} such that CMP8(ùëé0, ùëé1, ùëé2, ùëé3, ùëè0, ùëè1, ùëè2, ùëè3) = 1
if and only if the number represented by ùëé0ùëé1ùëé2ùëé3 is larger than the
number represented by ùëè0ùëè1ùëè2ùëè3.
‚ñ†
Exercise 3.2 ‚Äî Compare ùëõbit numbers. Prove that there exists a constant ùëê
such that for every ùëõthere is a Boolean circuit (with AND/OR/NOT
gates) ùê∂of at most ùëê‚ãÖùëõgates that computes the function CMP2ùëõ‚à∂
{0, 1}2ùëõ‚Üí{0, 1} such that CMP2ùëõ(ùëé0 ‚ãØùëéùëõ‚àí1ùëè0 ‚ãØùëèùëõ‚àí1) = 1 if and
only if the number represented by ùëé0 ‚ãØùëéùëõ‚àí1 is larger than the number
represented by ùëè0 ‚ãØùëèùëõ‚àí1.
‚ñ†
Exercise 3.3 ‚Äî OR,NOT is universal. Prove that the set {OR, NOT} is univer-
sal, in the sense that one can compute NAND using these gates.
‚ñ†
Exercise 3.4 ‚Äî AND,OR is not universal. Prove that for every ùëõ-bit input
circuit ùê∂that contains only AND and OR gates, as well as gates that
compute the constant functions 0 and 1, ùê∂is monotone, in the sense
that if ùë•, ùë•‚Ä≤ ‚àà{0, 1}ùëõ, ùë•ùëñ‚â§ùë•‚Ä≤
ùëñfor every ùëñ‚àà[ùëõ], then ùê∂(ùë•) ‚â§ùê∂(ùë•‚Ä≤).
Conclude that the set {AND, OR, 0, 1} is not universal.
‚ñ†
Exercise 3.5 ‚Äî XOR is not universal. Prove that for every ùëõ-bit input circuit
ùê∂that contains only XOR gates, as well as gates that compute the
constant functions 0 and 1, ùê∂is affine or linear modulo two, in the sense
that there exists some ùëé‚àà{0, 1}ùëõand ùëè‚àà{0, 1} such that for every
ùë•‚àà{0, 1}ùëõ, ùê∂(ùë•) = ‚àë
ùëõ‚àí1
ùëñ=0 ùëéùëñùë•ùëñ+ ùëèmod 2.
Conclude that the set {XOR, 0, 1} is not universal.
‚ñ†
Exercise 3.6 ‚Äî MAJ,NOT, 1 is universal. Let MAJ ‚à∂{0, 1}3 ‚Üí{0, 1} be the
majority function. Prove that {MAJ, NOT, 1} is a universal set of gates.
‚ñ†
Exercise 3.7 ‚Äî MAJ,NOT is not universal. Prove that {MAJ, NOT} is not a
universal set. See footnote for hint.4
‚ñ†
Exercise 3.8 ‚Äî NOR is universal. Let NOR ‚à∂{0, 1}2 ‚Üí{0, 1} defined as
NOR(ùëé, ùëè) = NOT(OR(ùëé, ùëè)). Prove that {NOR} is a universal set of
gates.
‚ñ†


--- Page 155 ---

defining computation
155
5 Thanks to Alec Sun and Simon Fischer for comments
on this problem.
6 Hint: Use the conditions of Definition 3.5 stipulating
that every input vertex has at least one out-neighbor
and there are exactly ùëöoutput gates. See also Re-
mark 3.7.
Exercise 3.9 ‚Äî Lookup is universal. Prove that {LOOKUP1, 0, 1} is a uni-
versal set of gates where 0 and 1 are the constant functions and
LOOKUP1 ‚à∂{0, 1}3 ‚Üí{0, 1} satisfies LOOKUP1(ùëé, ùëè, ùëê) equals ùëéif
ùëê= 0 and equals ùëèif ùëê= 1.
‚ñ†
Exercise 3.10 ‚Äî Bound on universal basis size (challenge). Prove that for ev-
ery subset ùêµof the functions from {0, 1}ùëòto {0, 1}, if ùêµis universal
then there is a ùêµ-circuit of at most ùëÇ(1) gates to compute the NAND
function (you can start by showing that there is a ùêµcircuit of at most
ùëÇ(ùëò16) gates).5
‚ñ†
Exercise 3.11 ‚Äî Size and inputs / outputs. Prove that for every NAND cir-
cuit of size ùë†with ùëõinputs and ùëöoutputs, ùë†‚â•min{ùëõ/2, ùëö}. See
footnote for hint.6
‚ñ†
Exercise 3.12 ‚Äî Threshold using NANDs. Prove that there is some constant
ùëêsuch that for every ùëõ> 1, and integers ùëé0, ‚Ä¶ , ùëéùëõ‚àí1, ùëè‚àà{‚àí2ùëõ, ‚àí2ùëõ+
1, ‚Ä¶ , ‚àí1, 0, +1, ‚Ä¶ , 2ùëõ}, there is a NAND circuit with at most ùëêùëõ4 gates
that computes the threshold function ùëìùëé0,‚Ä¶,ùëéùëõ‚àí1,ùëè‚à∂{0, 1}ùëõ‚Üí{0, 1} that
on input ùë•‚àà{0, 1}ùëõoutputs 1 if and only if ‚àë
ùëõ‚àí1
ùëñ=0 ùëéùëñùë•ùëñ> ùëè.
‚ñ†
Exercise 3.13 ‚Äî NANDs from activation functions. We say that a function
ùëì‚à∂‚Ñù2 ‚Üí‚Ñùis a NAND approximator if it has the following property: for
every ùëé, ùëè‚àà‚Ñù, if min{|ùëé|, |1 ‚àíùëé|} ‚â§1/3 and min{|ùëè|, |1 ‚àíùëè|} ‚â§1/3 then
|ùëì(ùëé, ùëè) ‚àíNAND(‚åäùëé‚åâ, ‚åäùëè‚åâ)| ‚â§1/3 where we denote by ‚åäùë•‚åâthe integer
closest to ùë•. That is, if ùëé, ùëèare within a distance 1/3 to {0, 1} then we
want ùëì(ùëé, ùëè) to equal the NAND of the values in {0, 1} that are closest
to ùëéand ùëèrespectively. Otherwise, we do not care what the output of
ùëìis on ùëéand ùëè.
In this exercise you will show that you can construct a NAND ap-
proximator from many common activation functions used in deep
neural networks. As a corollary you will obtain that deep neural net-
works can simulate NAND circuits. Since NAND circuits can also
simulate deep neural networks, these two computational models are
equivalent to one another.
1. Show that there is a NAND approximator ùëìdefined as ùëì(ùëé, ùëè) =
ùêø(ùëÖùëíùêøùëà(ùêø‚Ä≤(ùëé, ùëè))) where ùêø‚Ä≤ ‚à∂‚Ñù2 ‚Üí‚Ñùis an affine function (of
the form ùêø‚Ä≤(ùëé, ùëè) = ùõºùëé+ ùõΩùëè+ ùõæfor some ùõº, ùõΩ, ùõæ‚àà‚Ñù), ùêøis an
affine function (of the form ùêø(ùë¶) = ùõºùë¶+ ùõΩfor ùõº, ùõΩ‚àà‚Ñù), and
ùëÖùëíùêøùëà‚à∂‚Ñù‚Üí‚Ñù, is the function defined as ùëÖùëíùêøùëà(ùë•) = max{0, ùë•}.


--- Page 156 ---

156
introduction to theoretical computer science
7 One approach to solve this is using recursion and
analyzing it using the so called ‚ÄúMaster Theorem‚Äù.
8 Hint: Vertices in layers beyond the output can be
safely removed without changing the functionality of
the circuit.
2. Show that there is a NAND approximator ùëìdefined as ùëì(ùëé, ùëè) =
ùêø(ùë†ùëñùëîùëöùëúùëñùëë(ùêø‚Ä≤(ùëé, ùëè))) where ùêø‚Ä≤, ùêøare affine as above and ùë†ùëñùëîùëöùëúùëñùëë‚à∂
‚Ñù‚Üí‚Ñùis the function defined as ùë†ùëñùëîùëöùëúùëñùëë(ùë•) = ùëíùë•/(ùëíùë•+ 1).
3. Show that there is a NAND approximator ùëìdefined as ùëì(ùëé, ùëè) =
ùêø(ùë°ùëéùëõ‚Ñé(ùêø‚Ä≤(ùëé, ùëè))) where ùêø‚Ä≤, ùêøare affine as above and ùë°ùëéùëõ‚Ñé‚à∂‚Ñù‚Üí‚Ñù
is the function defined as ùë°ùëéùëõ‚Ñé(ùë•) = (ùëíùë•‚àíùëí‚àíùë•)/(ùëíùë•+ ùëí‚àíùë•).
4. Prove that for every NAND-circuit ùê∂with ùëõinputs and one output
that computes a function ùëî‚à∂{0, 1}ùëõ‚Üí{0, 1}, if we replace every
gate of ùê∂with a NAND-approximator and then invoke the result-
ing circuit on some ùë•‚àà{0, 1}ùëõ, the output will be a number ùë¶such
that |ùë¶‚àíùëî(ùë•)| ‚â§1/3.
‚ñ†
Exercise 3.14 ‚Äî Majority with NANDs efficiently. Prove that there is some
constant ùëêsuch that for every ùëõ> 1, there is a NAND circuit of at
most ùëê‚ãÖùëõgates that computes the majority function on ùëõinput bits
MAJùëõ‚à∂{0, 1}ùëõ‚Üí{0, 1}. That is MAJùëõ(ùë•) = 1 iff ‚àë
ùëõ‚àí1
ùëñ=0 ùë•ùëñ> ùëõ/2. See
footnote for hint.7
‚ñ†
Exercise 3.15 ‚Äî Output at last layer. Prove that for every ùëì‚à∂{0, 1}ùëõ‚Üí
{0, 1}, if there is a Boolean circuit ùê∂of ùë†gates that computes ùëìthen
there is a Boolean circuit ùê∂‚Ä≤ of at most ùë†gates such that in the minimal
layering of ùê∂‚Ä≤, the output gate of ùê∂‚Ä≤ is placed in the last layer. See
footnote for hint.8
‚ñ†
3.8 BIOGRAPHICAL NOTES
The excerpt from Al-Khwarizmi‚Äôs book is from ‚ÄúThe Algebra of Ben-
Musa‚Äù, Fredric Rosen, 1831.
Charles Babbage (1791-1871) was a visionary scientist, mathemati-
cian, and inventor (see [Swa02; CM00]). More than a century before
the invention of modern electronic computers, Babbage realized that
computation can be in principle mechanized. His first design for a
mechanical computer was the difference engine that was designed to do
polynomial interpolation. He then designed the analytical engine which
was a much more general machine and the first prototype for a pro-
grammable general purpose computer. Unfortunately, Babbage was
never able to complete the design of his prototypes. One of the earliest
people to realize the engine‚Äôs potential and far reaching implications
was Ada Lovelace (see the notes for Chapter 7).
Boolean algebra was first investigated by Boole and DeMorgan
in the 1840‚Äôs [Boo47; De 47]. The definition of Boolean circuits and


--- Page 157 ---

defining computation
157
connection to electrical relay circuits was given in Shannon‚Äôs Masters
Thesis [Sha38]. (Howard Gardener called Shannon‚Äôs thesis ‚Äúpossibly
the most important, and also the most famous, master‚Äôs thesis of the
[20th] century‚Äù.) Savage‚Äôs book [Sav98], like this one, introduces
the theory of computation starting with Boolean circuits as the first
model. Jukna‚Äôs book [Juk12] contains a modern in-depth exposition of
Boolean circuits, see also [Weg87].
The NAND function was shown to be universal by Sheffer [She13],
though this also appears in the earlier work of Peirce, see [Bur78].
Whitehead and Russell used NAND as the basis for their logic in
their magnum opus Principia Mathematica [WR12]. In her Ph.D thesis,
Ernst [Ern09] investigates empirically the minimal NAND circuits
for various functions. Nisan and Shocken‚Äôs book [NS05] builds a
computing system starting from NAND gates and ending with high
level programs and games (‚ÄúNAND to Tetris‚Äù); see also the website
nandtotetris.org.


--- Page 158 ---



--- Page 159 ---

4
Syntactic sugar, and computing every function
‚Äú[In 1951] I had a running compiler and nobody would touch it because,
they carefully told me, computers could only do arithmetic; they could not do
programs.‚Äù, Grace Murray Hopper, 1986.
‚ÄúSyntactic sugar causes cancer of the semicolon.‚Äù, Alan Perlis, 1982.
The computational models we considered thus far are as ‚Äúbare
bones‚Äù as they come. For example, our NAND-CIRC ‚Äúprogramming
language‚Äù has only the single operation foo = NAND(bar,blah). In
this chapter we will see that these simple models are actually equiv-
alent to more sophisticated ones. The key observation is that we can
implement more complex features using our basic building blocks,
and then use these new features themselves as building blocks for
even more sophisticated features. This is known as ‚Äúsyntactic sugar‚Äù
in the field of programming language design since we are not modi-
fying the underlying programming model itself, but rather we merely
implement new features by syntactically transforming a program that
uses such features into one that doesn‚Äôt.
This chapter provides a ‚Äútoolkit‚Äù that can be used to show that
many functions can be computed by NAND-CIRC programs, and
hence also by Boolean circuits. We will also use this toolkit to prove
a fundamental theorem: every finite function ùëì‚à∂{0, 1}ùëõ‚Üí{0, 1}ùëö
can be computed by a Boolean circuit, see Theorem 4.13 below. While
the syntactic sugar toolkit is important in its own right, Theorem 4.13
can also be proven directly without using this toolkit. We present this
alternative proof in Section 4.5. See Fig. 4.1 for an outline of the results
of this chapter.
This chapter: A non-mathy overview
In this chapter we will see our first major result: every fi-
nite function can be computed some Boolean circuit (see
Theorem 4.13 and Big Idea 5). This is sometimes known as
Compiled on 8.26.2020 18:10
Learning Objectives:
‚Ä¢ Get comfortable with syntactic sugar or
automatic translation of higher level logic to
low level gates.
‚Ä¢ Learn proof of major result: every finite
function can be computed by a Boolean
circuit.
‚Ä¢ Start thinking quantitatively about number
of lines required for computation.


--- Page 160 ---

160
introduction to theoretical computer science
Figure 4.1: An outline of the results of this chapter. In
Section 4.1 we give a toolkit of ‚Äúsyntactic sugar‚Äù trans-
formations showing how to implement features such
as programmer-defined functions and conditional
statements in NAND-CIRC. We use these tools in
Section 4.3 to give a NAND-CIRC program (or alter-
natively a Boolean circuit) to compute the LOOKUP
function. We then build on this result to show in Sec-
tion 4.4 that NAND-CIRC programs (or equivalently,
Boolean circuits) can compute every finite function.
An alternative direct proof of the same result is given
in Section 4.5.
the ‚Äúuniversality‚Äù of AND, OR, and NOT (and, using the
equivalence of Chapter 3, of NAND as well)
Despite being an important result, Theorem 4.13 is actually
not that hard to prove. Section 4.5 presents a relatively sim-
ple direct proof of this result. However, in Section 4.1 and
Section 4.3 we derive this result using the concept of ‚Äúsyntac-
tic sugar‚Äù (see Big Idea 4). This is an important concept for
programming languages theory and practice. The idea be-
hind ‚Äúsyntactic sugar‚Äù is that we can extend a programming
language by implementing advanced features from its basic
components. For example, we can take the AON-CIRC and
NAND-CIRC programming languages we saw in Chapter 3,
and extend them to achieve features such as user-defined
functions (e.g., def Foo(...)), condtional statements (e.g.,
if blah ...), and more. Once we have these features, it
is not that hard to show that we can take the ‚Äútruth table‚Äù
(table of all inputs and outputs) of any function, and use that
to create an AON-CIRC or NAND-CIRC program that maps
each input to its corresponding output.
We will also get our first glimpse of quantitative measures in
this chapter. While Theorem 4.13 tells us that every func-
tion can be computed by some circuit, the number of gates
in this circuit can be exponentially large. (We are not using
here ‚Äúexponentially‚Äù as some colloquial term for ‚Äúvery very
big‚Äù but in a very precise mathematical sense, which also
happens to coincide with being very very big.) It turns out
that some functions (for example, integer addition and multi-


--- Page 161 ---

syntactic sugar, and computing every function
161
plication) can be in fact computed using far fewer gates. We
will explore this issue of ‚Äúgate complexity‚Äù more deeply in
Chapter 5 and following chapters.
4.1 SOME EXAMPLES OF SYNTACTIC SUGAR
We now present some examples of ‚Äúsyntactic sugar‚Äù transformations
that we can use in constructing straightline programs or circuits. We
focus on the straight-line programming language view of our computa-
tional models, and specifically (for the sake of concreteness) on the
NAND-CIRC programming language. This is convenient because
many of the syntactic sugar transformations we present are easiest to
think about in terms of applying ‚Äúsearch and replace‚Äù operations to
the source code of a program. However, by Theorem 3.19, all of our
results hold equally well for circuits, whether ones using NAND gates
or Boolean circuits that use the AND, OR, and NOT operations. Enu-
merating the examples of such syntactic sugar transformations can be
a little tedious, but we do it for two reasons:
1. To convince you that despite their seeming simplicity and limita-
tions, simple models such as Boolean circuits or the NAND-CIRC
programming language are actually quite powerful.
2. So you can realize how lucky you are to be taking a theory of com-
putation course and not a compilers course‚Ä¶ :)
4.1.1 User-defined procedures
One staple of almost any programming language is the ability to
define and then execute procedures or subroutines. (These are often
known as functions in some programming languages, but we prefer
the name procedures to avoid confusion with the function that a pro-
gram computes.) The NAND-CIRC programming language does
not have this mechanism built in. However, we can achieve the same
effect using the time honored technique of ‚Äúcopy and paste‚Äù. Specifi-
cally, we can replace code which defines a procedure such as
def Proc(a,b):
proc_code
return c
some_code
f = Proc(d,e)
some_more_code
with the following code where we ‚Äúpaste‚Äù the code of Proc


--- Page 162 ---

162
introduction to theoretical computer science
some_code
proc_code'
some_more_code
and where proc_code' is obtained by replacing all occurrences
of a with d, b with e, and c with f. When doing that we will need to
ensure that all other variables appearing in proc_code' don‚Äôt interfere
with other variables. We can always do so by renaming variables to
new names that were not used before. The above reasoning leads to
the proof of the following theorem:
Theorem 4.1 ‚Äî Procedure definition synctatic sugar. Let NAND-CIRC-
PROC be the programming language NAND-CIRC augmented
with the syntax above for defining procedures. Then for every
NAND-CIRC-PROC program ùëÉ, there exists a standard (i.e.,
‚Äúsugar free‚Äù) NAND-CIRC program ùëÉ‚Ä≤ that computes the same
function as ùëÉ.
R
Remark 4.2 ‚Äî No recursive procedure. NAND-CIRC-
PROC only allows non recursive procedures. In particu-
lar, the code of a procedure Proc cannot call Proc but
only use procedures that were defined before it. With-
out this restriction, the above ‚Äúsearch and replace‚Äù
procedure might never terminate and Theorem 4.1
would not be true.
Theorem 4.1 can be proven using the transformation above, but
since the formal proof is somewhat long and tedious, we omit it here.
‚ñ†Example 4.3 ‚Äî Computing Majority from NAND using syntactic sugar. Pro-
cedures allow us to express NAND-CIRC programs much more
cleanly and succinctly. For example, because we can compute
AND, OR, and NOT using NANDs, we can compute the Majority
function as follows:
def NOT(a):
return NAND(a,a)
def AND(a,b):
temp = NAND(a,b)
return NOT(temp)
def OR(a,b):
temp1 = NOT(a)
temp2 = NOT(b)


--- Page 163 ---

syntactic sugar, and computing every function
163
return NAND(temp1,temp2)
def MAJ(a,b,c):
and1 = AND(a,b)
and2 = AND(a,c)
and3 = AND(b,c)
or1 = OR(and1,and2)
return OR(or1,and3)
print(MAJ(0,1,1))
# 1
Fig. 4.2 presents the ‚Äúsugar free‚Äù NAND-CIRC program (and
the corresponding circuit) that is obtained by ‚Äúexpanding out‚Äù this
program, replacing the calls to procedures with their definitions.
ÔÉ´Big Idea 4 Once we show that a computational model ùëãis equiv-
alent to a model that has feature ùëå, we can assume we have ùëåwhen
showing that a function ùëìis computable by ùëã.
Figure 4.2: A standard (i.e., ‚Äúsugar free‚Äù) NAND-
CIRC program that is obtained by expanding out the
procedure definitions in the program for Majority
of Example 4.3. The corresponding circuit is on
the right. Note that this is not the most efficient
NAND circuit/program for majority: we can save on
some gates by ‚Äúshort cutting‚Äù steps where a gate ùë¢
computes NAND(ùë£, ùë£) and then a gate ùë§computes
NAND(ùë¢, ùë¢) (as indicated by the dashed green
arrows in the above figure).
R
Remark 4.4 ‚Äî Counting lines. While we can use syn-
tactic sugar to present NAND-CIRC programs in more
readable ways, we did not change the definition of
the language itself. Therefore, whenever we say that
some function ùëìhas an ùë†-line NAND-CIRC program
we mean a standard ‚Äúsugar free‚Äù NAND-CIRC pro-
gram, where all syntactic sugar has been expanded
out. For example, the program of Example 4.3 is a
12-line program for computing the MAJ function,


--- Page 164 ---

164
introduction to theoretical computer science
even though it can be written in fewer lines using
NAND-CIRC-PROC.
4.1.2 Proof by Python (optional)
We can write a Python program that implements the proof of Theo-
rem 4.1. This is a Python program that takes a NAND-CIRC-PROC
program ùëÉthat includes procedure definitions and uses simple
‚Äúsearch and replace‚Äù to transform ùëÉinto a standard (i.e., ‚Äúsugar
free‚Äù) NAND-CIRC program ùëÉ‚Ä≤ that computes the same function
as ùëÉwithout using any procedures. The idea is simple: if the program
ùëÉcontains a definition of a procedure Proc of two arguments x and y,
then whenever we see a line of the form foo = Proc(bar,blah), we
can replace this line by:
1. The body of the procedure Proc (replacing all occurrences of x and
y with bar and blah respectively).
2. A line foo = exp, where exp is the expression following the re-
turn statement in the definition of the procedure Proc.
To make this more robust we add a prefix to the internal variables
used by Proc to ensure they don‚Äôt conflict with the variables of ùëÉ;
for simplicity we ignore this issue in the code below though it can be
easily added.
The code of the Python function desugar below achieves such a
transformation.
Fig. 4.2 shows the result of applying desugar to the program of Ex-
ample 4.3 that uses syntactic sugar to compute the Majority function.
Specifically, we first apply desugar to remove usage of the OR func-
tion, then apply it to remove usage of the AND function, and finally
apply it a third time to remove usage of the NOT function.
R
Remark 4.5 ‚Äî Parsing function definitions (optional). The
function desugar in Fig. 4.3 assumes that it is given
the procedure already split up into its name, argu-
ments, and body. It is not crucial for our purposes to
describe precisely how to scan a definition and split it
up into these components, but in case you are curious,
it can be achieved in Python via the following code:
def parse_func(code):
"""Parse a function definition into name,
arguments and body"""
‚Ü™
lines = [l.strip() for l in code.split('\n')]
regexp = r'def\s+([a-zA-Z\_0-9]+)\(([\sa-zA-
Z0-9\_,]+)\)\s*:\s*'
‚Ü™


--- Page 165 ---

syntactic sugar, and computing every function
165
Figure 4.3: Python code for transforming NAND-CIRC-PROC programs into standard sugar free NAND-CIRC programs.
def desugar(code, func_name, func_args,func_body):
"""
Replaces all occurences of
foo = func_name(func_args)
with
func_body[x->a,y->b]
foo = [result returned in func_body]
"""
# Uses Python regular expressions to simplify the search and replace,
# see https://docs.python.org/3/library/re.html and Chapter 9 of the book
# regular expression for capturing a list of variable names separated by commas
arglist = ",".join([r"([a-zA-Z0-9\_\[\]]+)" for i in range(len(func_args))])
# regular expression for capturing a statement of the form
# "variable = func_name(arguments)"
regexp = fr'([a-zA-Z0-9\_\[\]]+)\s*=\s*{func_name}\({arglist}\)\s*$'
while True:
m = re.search(regexp, code, re.MULTILINE)
if not m: break
newcode = func_body
# replace function arguments by the variables from the function invocation
for i in range(len(func_args)):
newcode = newcode.replace(func_args[i], m.group(i+2))
# Splice the new code inside
newcode = newcode.replace('return', m.group(1) + " = ")
code = code[:m.start()] + newcode + code[m.end()+1:]
return code


--- Page 166 ---

166
introduction to theoretical computer science
m = re.match(regexp,lines[0])
return m.group(1), m.group(2).split(','),
'\n'.join(lines[1:])
‚Ü™
4.1.3 Conditional statements
Another sorely missing feature in NAND-CIRC is a conditional
statement such as the if/then constructs that are found in many
programming languages. However, using procedures, we can ob-
tain an ersatz if/then construct. First we can compute the function
IF ‚à∂{0, 1}3 ‚Üí{0, 1} such that IF(ùëé, ùëè, ùëê) equals ùëèif ùëé= 1 and ùëêif ùëé= 0.
P
Before reading onward, try to see how you could com-
pute the IF function using NAND‚Äôs. Once you do that,
see how you can use that to emulate if/then types of
constructs.
The IF function can be implemented from NANDs as follows (see
Exercise 4.2):
def IF(cond,a,b):
notcond = NAND(cond,cond)
temp = NAND(b,notcond)
temp1 = NAND(a,cond)
return NAND(temp,temp1)
The IF function is also known as a multiplexing function, since ùëêùëúùëõùëë
can be thought of as a switch that controls whether the output is con-
nected to ùëéor ùëè. Once we have a procedure for computing the IF func-
tion, we can implement conditionals in NAND. The idea is that we
replace code of the form
if (condition):
assign blah to variable foo
with code of the form
foo
= IF(condition, blah, foo)
that assigns to foo its old value when condition equals 0, and
assign to foo the value of blah otherwise. More generally we can
replace code of the form
if (cond):
a = ...
b = ...
c = ...


--- Page 167 ---

syntactic sugar, and computing every function
167
with code of the form
temp_a = ...
temp_b = ...
temp_c = ...
a = IF(cond,temp_a,a)
b = IF(cond,temp_b,b)
c = IF(cond,temp_c,c)
Using such transformations, we can prove the following theorem.
Once again we omit the (not too insightful) full formal proof, though
see Section 4.1.2 for some hints on how to obtain it.
Theorem 4.6 ‚Äî Conditional statements synctatic sugar. Let NAND-CIRC-
IF be the programming language NAND-CIRC augmented with
if/then/else statements for allowing code to be conditionally
executed based on whether a variable is equal to 0 or 1.
Then for every NAND-CIRC-IF program ùëÉ, there exists a stan-
dard (i.e., ‚Äúsugar free‚Äù) NAND-CIRC program ùëÉ‚Ä≤ that computes
the same function as ùëÉ.
4.2 EXTENDED EXAMPLE: ADDITION AND MULTIPLICATION (OP-
TIONAL)
Using ‚Äúsyntactic sugar‚Äù, we can write the integer addition function as
follows:
# Add two n-bit integers
# Use LSB first notation for simplicity
def ADD(A,B):
Result = [0]*(n+1)
Carry
= [0]*(n+1)
Carry[0] = zero(A[0])
for i in range(n):
Result[i] = XOR(Carry[i],XOR(A[i],B[i]))
Carry[i+1] = MAJ(Carry[i],A[i],B[i])
Result[n] = Carry[n]
return Result
ADD([1,1,1,0,0],[1,0,0,0,0]);;
# [0, 0, 0, 1, 0, 0]
where zero is the constant zero function, and MAJ and XOR corre-
spond to the majority and XOR functions respectively. While we use
Python syntax for convenience, in this example ùëõis some fixed integer
and so for every such ùëõ, ADD is a finite function that takes as input 2ùëõ


--- Page 168 ---

168
introduction to theoretical computer science
1 The value of ùëêcan be improved to 9, see Exercise 4.5.
Figure 4.5: The number of lines in our NAND-CIRC
program to add two ùëõbit numbers, as a function of
ùëõ, for ùëõ‚Äôs between 1 and 100. This is not the most
efficient program for this task, but the important point
is that it has the form ùëÇ(ùëõ).
bits and outputs ùëõ+ 1 bits. In particular for every ùëõwe can remove
the loop construct for i in range(n) by simply repeating the code ùëõ
times, replacing the value of i with 0, 1, 2, ‚Ä¶ , ùëõ‚àí1. By expanding out
all the features, for every value of ùëõwe can translate the above pro-
gram into a standard (‚Äúsugar free‚Äù) NAND-CIRC program. Fig. 4.4
depicts what we get for ùëõ= 2.
Figure 4.4: The NAND-CIRC program and corre-
sponding NAND circuit for adding two-digit binary
numbers that are obtained by ‚Äúexpanding out‚Äù all the
syntactic sugar. The program/circuit has 43 lines/-
gates which is by no means necessary. It is possible
to add ùëõbit numbers using 9ùëõNAND gates, see
Exercise 4.5.
By going through the above program carefully and accounting for
the number of gates, we can see that it yields a proof of the following
theorem (see also Fig. 4.5):
Theorem 4.7 ‚Äî Addition using NAND-CIRC programs. For every ùëõ
‚àà
‚Ñï,
let ADDùëõ
‚à∂
{0, 1}2ùëõ
‚Üí
{0, 1}ùëõ+1 be the function that, given
ùë•, ùë•‚Ä≤ ‚àà{0, 1}ùëõcomputes the representation of the sum of the num-
bers that ùë•and ùë•‚Ä≤ represent. Then there is a constant ùëê
‚â§
30 such
that for every ùëõthere is a NAND-CIRC program of at most ùëêùëõlines
computing ADDùëõ. 1
Once we have addition, we can use the grade-school algorithm to
obtain multiplication as well, thus obtaining the following theorem:
Theorem 4.8 ‚Äî Multiplication using NAND-CIRC programs. For every ùëõ,
let MULTùëõ
‚à∂
{0, 1}2ùëõ
‚Üí
{0, 1}2ùëõbe the function that, given
ùë•, ùë•‚Ä≤
‚àà
{0, 1}ùëõcomputes the representation of the product of the
numbers that ùë•and ùë•‚Ä≤ represent. Then there is a constant ùëêsuch
that for every ùëõ, there is a NAND-CIRC program of at most ùëêùëõ2
that computes the function MULTùëõ.
We omit the proof, though in Exercise 4.7 we ask you to supply
a ‚Äúconstructive proof‚Äù in the form of a program (in your favorite


--- Page 169 ---

syntactic sugar, and computing every function
169
programming language) that on input a number ùëõ, outputs the code
of a NAND-CIRC program of at most 1000ùëõ2 lines that computes the
MULTùëõfunction. In fact, we can use Karatsuba‚Äôs algorithm to show
that there is a NAND-CIRC program of ùëÇ(ùëõlog2 3) lines to compute
MULTùëõ(and can get even further asymptotic improvements using
better algorithms).
4.3 THE LOOKUP FUNCTION
The LOOKUP function will play an important role in this chapter and
later. It is defined as follows:
Definition 4.9 ‚Äî Lookup function. For every ùëò, the lookup function of
order ùëò, LOOKUPùëò‚à∂{0, 1}2ùëò+ùëò‚Üí{0, 1} is defined as follows: For
every ùë•‚àà{0, 1}2ùëòand ùëñ‚àà{0, 1}ùëò,
LOOKUPùëò(ùë•, ùëñ) = ùë•ùëñ
(4.1)
where ùë•ùëñdenotes the ùëñùë°‚Ñéentry of ùë•, using the binary representation
to identify ùëñwith a number in {0, ‚Ä¶ , 2ùëò‚àí1}.
Figure 4.6: The LOOKUPùëòfunction takes an input
in {0, 1}2ùëò+ùëò, which we denote by ùë•, ùëñ(with ùë•‚àà
{0, 1}2ùëòand ùëñ‚àà{0, 1}ùëò). The output is ùë•ùëñ: the ùëñ-th
coordinate of ùë•, where we identify ùëñas a number
in [ùëò] using the binary representation. In the above
example ùë•‚àà{0, 1}16 and ùëñ‚àà{0, 1}4. Since ùëñ= 0110
is the binary representation of the number 6, the
output of LOOKUP4(ùë•, ùëñ) in this case is ùë•6 = 1.
See Fig. 4.6 for an illustration of the LOOKUP function. It turns
out that for every ùëò, we can compute LOOKUPùëòusing a NAND-CIRC
program:
Theorem 4.10 ‚Äî Lookup function. For every ùëò
>
0, there is a NAND-
CIRC program that computes the function LOOKUPùëò‚à∂{0, 1}2ùëò+ùëò‚Üí
{0, 1}. Moreover, the number of lines in this program is at most
4 ‚ãÖ2ùëò.
An immediate corollary of Theorem 4.10 is that for every ùëò> 0,
LOOKUPùëòcan be computed by a Boolean circuit (with AND, OR and
NOT gates) of at most 8 ‚ãÖ2ùëògates.
4.3.1 Constructing a NAND-CIRC program for LOOKUP
We prove Theorem 4.10 by induction. For the case ùëò= 1, LOOKUP1
maps (ùë•0, ùë•1, ùëñ) ‚àà{0, 1}3 to ùë•ùëñ. In other words, if ùëñ= 0 then it outputs


--- Page 170 ---

170
introduction to theoretical computer science
ùë•0 and otherwise it outputs ùë•1, which (up to reordering variables) is
the same as the IF function presented in Section 4.1.3, which can be
computed by a 4-line NAND-CIRC program.
As a warm-up for the case of general ùëò, let us consider the case
of ùëò= 2. Given input ùë•= (ùë•0, ùë•1, ùë•2, ùë•3) for LOOKUP2 and an
index ùëñ= (ùëñ0, ùëñ1), if the most significant bit ùëñ0 of the index is 0 then
LOOKUP2(ùë•, ùëñ) will equal ùë•0 if ùëñ1 = 0 and equal ùë•1 if ùëñ1 = 1. Similarly,
if the most significant bit ùëñ0 is 1 then LOOKUP2(ùë•, ùëñ) will equal ùë•2 if
ùëñ1 = 0 and will equal ùë•3 if ùëñ1 = 1. Another way to say this is that we
can write LOOKUP2 as follows:
def LOOKUP2(X[0],X[1],X[2],X[3],i[0],i[1]):
if i[0]==1:
return LOOKUP1(X[2],X[3],i[1])
else:
return LOOKUP1(X[0],X[1],i[1])
or in other words,
def LOOKUP2(X[0],X[1],X[2],X[3],i[0],i[1]):
a = LOOKUP1(X[2],X[3],i[1])
b = LOOKUP1(X[0],X[1],i[1])
return IF( i[0],a,b)
More generally, as shown in the following lemma, we can compute
LOOKUPùëòusing two invocations of LOOKUPùëò‚àí1 and one invocation
of IF:
Lemma 4.11 ‚Äî Lookup recursion. For every ùëò‚â•2, LOOKUPùëò(ùë•0, ‚Ä¶ , ùë•2ùëò‚àí1, ùëñ0, ‚Ä¶ , ùëñùëò‚àí1)
is equal to
IF (ùëñ0, LOOKUPùëò‚àí1(ùë•2ùëò‚àí1, ‚Ä¶ , ùë•2ùëò‚àí1, ùëñ1, ‚Ä¶ , ùëñùëò‚àí1), LOOKUPùëò‚àí1(ùë•0, ‚Ä¶ , ùë•2ùëò‚àí1‚àí1, ùëñ1, ‚Ä¶ , ùëñùëò‚àí1))
(4.2)
Proof. If the most significant bit ùëñ0 of ùëñis zero, then the index ùëñis
in {0, ‚Ä¶ , 2ùëò‚àí1 ‚àí1} and hence we can perform the lookup on the
‚Äúfirst half‚Äù of ùë•and the result of LOOKUPùëò(ùë•, ùëñ) will be the same as
ùëé= LOOKUPùëò‚àí1(ùë•0, ‚Ä¶ , ùë•2ùëò‚àí1‚àí1, ùëñ1, ‚Ä¶ , ùëñùëò‚àí1). On the other hand, if this
most significant bit ùëñ0 is equal to 1, then the index is in {2ùëò‚àí1, ‚Ä¶ , 2ùëò‚àí
1}, in which case the result of LOOKUPùëò(ùë•, ùëñ) is the same as ùëè=
LOOKUPùëò‚àí1(ùë•2ùëò‚àí1, ‚Ä¶ , ùë•2ùëò‚àí1, ùëñ1, ‚Ä¶ , ùëñùëò‚àí1). Thus we can compute
LOOKUPùëò(ùë•, ùëñ) by first computing ùëéand ùëèand then outputting
IF(ùëñùëò‚àí1, ùëé, ùëè).
‚ñ†
Proof of Theorem 4.10 from Lemma 4.11.
Now that we have Lemma 4.11,
we can complete the proof of Theorem 4.10. We will prove by induc-
tion on ùëòthat there is a NAND-CIRC program of at most 4 ‚ãÖ(2ùëò‚àí1)


--- Page 171 ---

syntactic sugar, and computing every function
171
Figure 4.7: The number of lines in our implementation
of the LOOKUP_k function as a function of ùëò(i.e., the
length of the index). The number of lines in our
implementation is roughly 3 ‚ãÖ2ùëò.
lines for LOOKUPùëò. For ùëò= 1 this follows by the four line program for
IF we‚Äôve seen before. For ùëò> 1, we use the following pseudocode:
a = LOOKUP_(k-1)(X[0],...,X[2^(k-1)-1],i[1],...,i[k-1])
b = LOOKUP_(k-1)(X[2^(k-1)],...,Z[2^(k-1)],i[1],...,i[k-
1])
‚Ü™
return IF(i[0],b,a)
If we let ùêø(ùëò) be the number of lines required for LOOKUPùëò, then
the above pseudo-code shows that
ùêø(ùëò) ‚â§2ùêø(ùëò‚àí1) + 4 .
(4.3)
Since under our induction hypothesis ùêø(ùëò‚àí1) ‚â§4(2ùëò‚àí1 ‚àí1), we get
that ùêø(ùëò) ‚â§2 ‚ãÖ4(2ùëò‚àí1 ‚àí1) + 4 = 4(2ùëò‚àí1) which is what we wanted
to prove. See Fig. 4.7 for a plot of the actual number of lines in our
implementation of LOOKUPùëò.
4.4 COMPUTING EVERY FUNCTION
At this point we know the following facts about NAND-CIRC pro-
grams (and so equivalently about Boolean circuits and our other
equivalent models):
1. They can compute at least some non trivial functions.
2. Coming up with NAND-CIRC programs for various functions is a
very tedious task.
Thus I would not blame the reader if they were not particularly
looking forward to a long sequence of examples of functions that can
be computed by NAND-CIRC programs. However, it turns out we are
not going to need this, as we can show in one fell swoop that NAND-
CIRC programs can compute every finite function:
Theorem 4.12 ‚Äî Universality of NAND. There exists some constant ùëê> 0
such that for every ùëõ, ùëö> 0 and function ùëì‚à∂{0, 1}ùëõ‚Üí{0, 1}ùëö,
there is a NAND-CIRC program with at most ùëê‚ãÖùëö2ùëõlines that com-
putes the function ùëì.
By Theorem 3.19, the models of NAND circuits, NAND-CIRC pro-
grams, AON-CIRC programs, and Boolean circuits, are all equivalent
to one another, and hence Theorem 4.12 holds for all these models. In
particular, the following theorem is equivalent to Theorem 4.12:
Theorem 4.13 ‚Äî Universality of Boolean circuits. There exists some
constant ùëê
>
0 such that for every ùëõ, ùëö
>
0 and function


--- Page 172 ---

172
introduction to theoretical computer science
2 In case you are curious, this is the function on input
ùëñ‚àà{0, 1}4 (which we interpret as a number in [16]),
that outputs the ùëñ-th digit of ùúãin the binary basis.
ùëì
‚à∂
{0, 1}ùëõ
‚Üí
{0, 1}ùëö, there is a Boolean circuit with at most
ùëê‚ãÖùëö2ùëõgates that computes the function ùëì.
ÔÉ´Big Idea 5 Every finite function can be computed by a large
enough Boolean circuit.
Improved bounds. Though it will not be of great importance to us, it
is possible to improve on the proof of Theorem 4.12 and shave an extra
factor of ùëõ, as well as optimize the constant ùëê, and so prove that for
every ùúñ> 0, ùëö‚àà‚Ñïand sufficiently large ùëõ, if ùëì‚à∂{0, 1}ùëõ‚Üí{0, 1}ùëö
then ùëìcan be computed by a NAND circuit of at most (1 + ùúñ) ùëö‚ãÖ2ùëõ
ùëõ
gates. The proof of this result is beyond the scope of this book, but we
do discuss how to obtain a bound of the form ùëÇ( ùëö‚ãÖ2ùëõ
ùëõ) in Section 4.4.2;
see also the biographical notes.
4.4.1 Proof of NAND‚Äôs Universality
To prove Theorem 4.12, we need to give a NAND circuit, or equiva-
lently a NAND-CIRC program, for every possible function. We will
restrict our attention to the case of Boolean functions (i.e., ùëö= 1).
Exercise 4.9 asks you to extend the proof for all values of ùëö. A func-
tion ùêπ‚à∂{0, 1}ùëõ‚Üí{0, 1} can be specified by a table of its values for
each one of the 2ùëõinputs. For example, the table below describes one
particular function ùê∫‚à∂{0, 1}4 ‚Üí{0, 1}:2
Table 4.1: An example of a function ùê∫‚à∂{0, 1}4 ‚Üí{0, 1}.
Input (ùë•)
Output (ùê∫(ùë•))
0000
1
0001
1
0010
0
0011
0
0100
1
0101
0
0110
0
0111
1
1000
0
1001
0
1010
0
1011
0
1100
1
1101
1
1110
1
1111
1


--- Page 173 ---

syntactic sugar, and computing every function
173
For every ùë•‚àà{0, 1}4, ùê∫(ùë•) = LOOKUP4(1100100100001111, ùë•), and
so the following is NAND-CIRC ‚Äúpseudocode‚Äù to compute ùê∫using
syntactic sugar for the LOOKUP_4 procedure.
G0000 = 1
G1000 = 1
G0100 = 0
...
G0111 = 1
G1111 = 1
Y[0] = LOOKUP_4(G0000,G1000,...,G1111,
X[0],X[1],X[2],X[3])
We can translate this pseudocode into an actual NAND-CIRC pro-
gram by adding three lines to define variables zero and one that are
initialized to 0 and 1 respectively, and then replacing a statement such
as Gxxx = 0 with Gxxx = NAND(one,one) and a statement such as
Gxxx = 1 with Gxxx = NAND(zero,zero). The call to LOOKUP_4 will
be replaced by the NAND-CIRC program that computes LOOKUP4,
plugging in the appropriate inputs.
There was nothing about the above reasoning that was particular to
the function ùê∫above. Given every function ùêπ‚à∂{0, 1}ùëõ‚Üí{0, 1}, we
can write a NAND-CIRC program that does the following:
1. Initialize 2ùëõvariables of the form F00...0 till F11...1 so that for
every ùëß‚àà{0, 1}ùëõ, the variable corresponding to ùëßis assigned the
value ùêπ(ùëß).
2. Compute LOOKUPùëõon the 2ùëõvariables initialized in the previ-
ous step, with the index variable being the input variables X[0
],‚Ä¶,X[2ùëõ‚àí1 ]. That is, just like in the pseudocode for G above, we
use Y[0] = LOOKUP(F00..00,...,F11..1,X[0],..,x[ùëõ‚àí1])
The total number of lines in the resulting program is 3 + 2ùëõlines for
initializing the variables plus the 4 ‚ãÖ2ùëõlines that we pay for computing
LOOKUPùëõ. This completes the proof of Theorem 4.12.
R
Remark 4.14 ‚Äî Result in perspective. While Theo-
rem 4.12 seems striking at first, in retrospect, it is
perhaps not that surprising that every finite function
can be computed with a NAND-CIRC program. After
all, a finite function ùêπ
‚à∂
{0, 1}ùëõ
‚Üí
{0, 1}ùëöcan be
represented by simply the list of its outputs for each
one of the 2ùëõinput values. So it makes sense that we
could write a NAND-CIRC program of similar size
to compute it. What is more interesting is that some
functions, such as addition and multiplication, have


--- Page 174 ---

174
introduction to theoretical computer science
3 The constant ùëêin this theorem is at most 10 and in
fact can be arbitrarily close to 1, see Section 4.8.
Figure 4.8: We can compute ùëì‚à∂{0, 1}ùëõ‚Üí{0, 1} on
input ùë•= ùëéùëèwhere ùëé‚àà{0, 1}ùëòand ùëè‚àà{0, 1}ùëõ‚àíùëò
by first computing the 2ùëõ‚àíùëòlong string ùëî(ùëé) that
corresponds to all ùëì‚Äôs values on inputs that begin with
ùëé, and then outputting the ùëè-th coordinate of this
string.
a much more efficient representation: one that only
requires ùëÇ(ùëõ2) or even fewer lines.
4.4.2 Improving by a factor of ùëõ(optional)
By being a little more careful, we can improve the bound of Theo-
rem 4.12 and show that every function ùêπ‚à∂{0, 1}ùëõ‚Üí{0, 1}ùëöcan be
computed by a NAND-CIRC program of at most ùëÇ(ùëö2ùëõ/ùëõ) lines. In
other words, we can prove the following improved version:
Theorem 4.15 ‚Äî Universality of NAND circuits, improved bound. There ex-
ists a constant ùëê
>
0 such that for every ùëõ, ùëö
>
0 and function
ùëì‚à∂{0, 1}ùëõ‚Üí{0, 1}ùëö, there is a NAND-CIRC program with at most
ùëê‚ãÖùëö2ùëõ/ùëõlines that computes the function ùëì. 3
Proof. As before, it is enough to prove the case that ùëö= 1. Hence
we let ùëì‚à∂{0, 1}ùëõ‚Üí{0, 1}, and our goal is to prove that there exists
a NAND-CIRC program of ùëÇ(2ùëõ/ùëõ) lines (or equivalently a Boolean
circuit of ùëÇ(2ùëõ/ùëõ) gates) that computes ùëì.
We let ùëò= log(ùëõ‚àí2 log ùëõ) (the reasoning behind this choice will
become clear later on). We define the function ùëî‚à∂{0, 1}ùëò‚Üí{0, 1}2ùëõ‚àíùëò
as follows:
ùëî(ùëé) = ùëì(ùëé0ùëõ‚àíùëò)ùëì(ùëé0ùëõ‚àíùëò‚àí11) ‚ãØùëì(ùëé1ùëõ‚àíùëò) .
(4.4)
In other words, if we use the usual binary representation to identify
the numbers {0, ‚Ä¶ , 2ùëõ‚àíùëò‚àí1} with the strings {0, 1}ùëõ‚àíùëò, then for every
ùëé‚àà{0, 1}ùëòand ùëè‚àà{0, 1}ùëõ‚àíùëò
ùëî(ùëé)ùëè= ùëì(ùëéùëè) .
(4.5)
(4.5) means that for every ùë•‚àà{0, 1}ùëõ, if we write ùë•= ùëéùëèwith
ùëé‚àà{0, 1}ùëòand ùëè‚àà{0, 1}ùëõ‚àíùëòthen we can compute ùëì(ùë•) by first
computing the string ùëá= ùëî(ùëé) of length 2ùëõ‚àíùëò, and then computing
LOOKUPùëõ‚àíùëò(ùëá, ùëè) to retrieve the element of ùëáat the position cor-
responding to ùëè(see Fig. 4.8). The cost to compute the LOOKUPùëõ‚àíùëò
is ùëÇ(2ùëõ‚àíùëò) lines/gates and the cost in NAND-CIRC lines (or Boolean
gates) to compute ùëìis at most
ùëêùëúùë†ùë°(ùëî) + ùëÇ(2ùëõ‚àíùëò) ,
(4.6)
where ùëêùëúùë†ùë°(ùëî) is the number of operations (i.e., lines of NAND-CIRC
programs or gates in a circuit) needed to compute ùëî.
To complete the proof we need to give a bound on ùëêùëúùë†ùë°(ùëî). Since ùëî
is a function mapping {0, 1}ùëòto {0, 1}2ùëõ‚àíùëò, we can also think of it as a
collection of 2ùëõ‚àíùëòfunctions ùëî0, ‚Ä¶ , ùëî2ùëõ‚àíùëò‚àí1 ‚à∂{0, 1}ùëò‚Üí{0, 1}, where


--- Page 175 ---

syntactic sugar, and computing every function
175
Figure 4.9: If ùëî0, ‚Ä¶ , ùëîùëÅ‚àí1 is a collection of functions
each mapping {0, 1}ùëòto {0, 1} such that at most ùëÜ
of them are distinct then for every ùëé‚àà{0, 1}ùëò, we
can compute all the values ùëî0(ùëé), ‚Ä¶ , ùëîùëÅ‚àí1(ùëé) using
at most ùëÇ(ùëÜ‚ãÖ2ùëò+ ùëÅ) operations by first computing
the distinct functions and then copying the resulting
values.
ùëîùëñ(ùë•) = ùëî(ùëé)ùëñfor every ùëé‚àà{0, 1}ùëòand ùëñ‚àà[2ùëõ‚àíùëò]. (That is, ùëîùëñ(ùëé) is
the ùëñ-th bit of ùëî(ùëé).) Naively, we could use Theorem 4.12 to compute
each ùëîùëñin ùëÇ(2ùëò) lines, but then the total cost is ùëÇ(2ùëõ‚àíùëò‚ãÖ2ùëò) = ùëÇ(2ùëõ)
which does not save us anything. However, the crucial observation
is that there are only 22ùëòdistinct functions mapping {0, 1}ùëòto {0, 1}.
For example, if ùëî17 is an identical function to ùëî67 that means that if
we already computed ùëî17(ùëé) then we can compute ùëî67(ùëé) using only
a constant number of operations: simply copy the same value! In
general, if you have a collection of ùëÅfunctions ùëî0, ‚Ä¶ , ùëîùëÅ‚àí1 mapping
{0, 1}ùëòto {0, 1}, of which at most ùëÜare distinct then for every value
ùëé‚àà{0, 1}ùëòwe can compute the ùëÅvalues ùëî0(ùëé), ‚Ä¶ , ùëîùëÅ‚àí1(ùëé) using at
most ùëÇ(ùëÜ‚ãÖ2ùëò+ ùëÅ) operations (see Fig. 4.9).
In our case, because there are at most 22ùëòdistinct functions map-
ping {0, 1}ùëòto {0, 1}, we can compute the function ùëî(and hence by
(4.5) also ùëì) using at most
ùëÇ(22ùëò‚ãÖ2ùëò+ 2ùëõ‚àíùëò)
(4.7)
operations. Now all that is left is to plug into (4.7) our choice of ùëò=
log(ùëõ‚àí2 log ùëõ). By definition, 2ùëò= ùëõ‚àí2 log ùëõ, which means that (4.7)
can be bounded
ùëÇ(2ùëõ‚àí2 log ùëõ‚ãÖ(ùëõ‚àí2 log ùëõ) + 2ùëõ‚àílog(ùëõ‚àí2 log ùëõ)) ‚â§
(4.8)
ùëÇ( 2ùëõ
ùëõ2 ‚ãÖùëõ+
2ùëõ
ùëõ‚àí2 log ùëõ) ‚â§ùëÇ( 2ùëõ
ùëõ+
2ùëõ
0.5ùëõ) = ùëÇ( 2ùëõ
ùëõ)
(4.9)
which is what we wanted to prove. (We used above the fact that ùëõ‚àí
2 log ùëõ‚â•0.5 log ùëõfor sufficiently large ùëõ.)
‚ñ†
Using the connection between NAND-CIRC programs and Boolean
circuits, an immediate corollary of Theorem 4.15 is the following
improvement to Theorem 4.13:
Theorem 4.16 ‚Äî Universality of Boolean circuits, improved bound. There
exists some constant ùëê> 0 such that for every ùëõ, ùëö> 0 and func-
tion ùëì‚à∂{0, 1}ùëõ‚Üí{0, 1}ùëö, there is a Boolean circuit with at most
ùëê‚ãÖùëö2ùëõ/ùëõgates that computes the function ùëì.
4.5 COMPUTING EVERY FUNCTION: AN ALTERNATIVE PROOF
Theorem 4.13 is a fundamental result in the theory (and practice!) of
computation. In this section we present an alternative proof of this
basic fact that Boolean circuits can compute every finite function. This
alternative proof gives a somewhat worse quantitative bound on the
number of gates but it has the advantage of being simpler, working


--- Page 176 ---

176
introduction to theoretical computer science
directly with circuits and avoiding the usage of all the syntactic sugar
machinery. (However, that machinery is useful in its own right, and
will find other applications later on.)
Theorem 4.17 ‚Äî Universality of Boolean circuits (alternative phrasing). There
exists some constant ùëê> 0 such that for every ùëõ, ùëö> 0 and func-
tion ùëì‚à∂{0, 1}ùëõ‚Üí{0, 1}ùëö, there is a Boolean circuit with at most
ùëê‚ãÖùëö‚ãÖùëõ2ùëõgates that computes the function ùëì.
Figure 4.10: Given a function ùëì‚à∂{0, 1}ùëõ‚Üí{0, 1},
we let {ùë•0, ùë•1, ‚Ä¶ , ùë•ùëÅ‚àí1} ‚äÜ{0, 1}ùëõbe the set of
inputs such that ùëì(ùë•ùëñ) = 1, and note that ùëÅ‚â§2ùëõ.
We can express ùëìas the OR of ùõøùë•ùëñfor ùëñ‚àà[ùëÅ] where
the function ùõøùõº‚à∂{0, 1}ùëõ‚Üí{0, 1} (for ùõº‚àà{0, 1}ùëõ)
is defined as follows: ùõøùõº(ùë•) = 1 iff ùë•= ùõº. We can
compute the OR of ùëÅvalues using ùëÅtwo-input OR
gates. Therefore if we have a circuit of size ùëÇ(ùëõ) to
compute ùõøùõºfor every ùõº‚àà{0, 1}ùëõ, we can compute ùëì
using a circuit of size ùëÇ(ùëõ‚ãÖùëÅ) = ùëÇ(ùëõ‚ãÖ2ùëõ).
Proof Idea:
The idea of the proof is illustrated in Fig. 4.10. As before, it is
enough to focus on the case that ùëö= 1 (the function ùëìhas a sin-
gle output), since we can always extend this to the case of ùëö> 1
by looking at the composition of ùëöcircuits each computing a differ-
ent output bit of the function ùëì. We start by showing that for every
ùõº‚àà{0, 1}ùëõ, there is an ùëÇ(ùëõ) sized circuit that computes the function
ùõøùõº‚à∂{0, 1}ùëõ‚Üí{0, 1} defined as follows: ùõøùõº(ùë•) = 1 iff ùë•= ùõº(that is,
ùõøùõºoutputs 0 on all inputs except the input ùõº). We can then write any
function ùëì‚à∂{0, 1}ùëõ‚Üí{0, 1} as the OR of at most 2ùëõfunctions ùõøùõºfor
the ùõº‚Äôs on which ùëì(ùõº) = 1.
‚ãÜ
Proof of Theorem 4.17. We prove the theorem for the case ùëö= 1. The
result can be extended for ùëö> 1 as before (see also Exercise 4.9). Let
ùëì‚à∂{0, 1}ùëõ‚Üí{0, 1}. We will prove that there is an ùëÇ(ùëõ‚ãÖ2ùëõ)-sized
Boolean circuit to compute ùëìin the following steps:


--- Page 177 ---

syntactic sugar, and computing every function
177
Figure 4.11: For every string ùõº‚àà{0, 1}ùëõ, there is a
Boolean circuit of ùëÇ(ùëõ) gates to compute the function
ùõøùõº‚à∂{0, 1}ùëõ‚Üí{0, 1} such that ùõøùõº(ùë•) = 1 if and
only if ùë•= ùõº. The circuit is very simple. Given input
ùë•0, ‚Ä¶ , ùë•ùëõ‚àí1 we compute the AND of ùëß0, ‚Ä¶ , ùëßùëõ‚àí1
where ùëßùëñ= ùë•ùëñif ùõºùëñ= 1 and ùëßùëñ= NOT(ùë•ùëñ) if ùõºùëñ= 0.
While formally Boolean circuits only have a gate for
computing the AND of two inputs, we can implement
an AND of ùëõinputs by composing ùëõtwo-input
ANDs.
1. We show that for every ùõº‚àà{0, 1}ùëõ, there is an ùëÇ(ùëõ) sized circuit
that computes the function ùõøùõº‚à∂{0, 1}ùëõ‚Üí{0, 1}, where ùõøùõº(ùë•) = 1 iff
ùë•= ùõº.
2. We then show that this implies the existence of an ùëÇ(ùëõ‚ãÖ2ùëõ)-sized
circuit that computes ùëì, by writing ùëì(ùë•) as the OR of ùõøùõº(ùë•) for all
ùõº‚àà{0, 1}ùëõsuch that ùëì(ùõº) = 1.
We start with Step 1:
CLAIM: For ùõº‚àà{0, 1}ùëõ, define ùõøùõº‚à∂{0, 1}ùëõas follows:
ùõøùõº(ùë•) =
‚éß
{
‚é®
{
‚é©
1
ùë•= ùõº
0
otherwise
.
(4.10)
then there is a Boolean circuit using at most 2ùëõgates that computes ùõøùõº.
PROOF OF CLAIM: The proof is illustrated in Fig. 4.11. As an
example, consider the function ùõø011 ‚à∂{0, 1}3 ‚Üí{0, 1}. This function
outputs 1 on ùë•if and only if ùë•0 = 0, ùë•1 = 1 and ùë•2 = 1, and so we can
write ùõø011(ùë•) = ùë•0 ‚àßùë•1 ‚àßùë•2, which translates into a Boolean circuit
with one NOT gate and two AND gates. More generally, for every
ùõº‚àà{0, 1}ùëõ, we can express ùõøùõº(ùë•) as (ùë•0 = ùõº0)‚àß(ùë•1 = ùõº1)‚àß‚ãØ‚àß(ùë•ùëõ‚àí1 =
ùõºùëõ‚àí1), where if ùõºùëñ= 0 we replace ùë•ùëñ= ùõºùëñwith ùë•ùëñand if ùõºùëñ= 1 we
replace ùë•ùëñ= ùõºùëñby simply ùë•ùëñ. This yields a circuit that computes ùõøùõº
using ùëõAND gates and at most ùëõNOT gates, so a total of at most 2ùëõ
gates.
Now for every function ùëì‚à∂{0, 1}ùëõ‚Üí{0, 1}, we can write
ùëì(ùë•) = ùõøùë•0(ùë•) ‚à®ùõøùë•1(ùë•) ‚à®‚ãØ‚à®ùõøùë•ùëÅ‚àí1(ùë•)
(4.11)
where ùëÜ= {ùë•0, ‚Ä¶ , ùë•ùëÅ‚àí1} is the set of inputs on which ùëìoutputs 1.
(To see this, you can verify that the right-hand side of (4.11) evaluates
to 1 on ùë•‚àà{0, 1}ùëõif and only if ùë•is in the set ùëÜ.)
Therefore we can compute ùëìusing a Boolean circuit of at most 2ùëõ
gates for each of the ùëÅfunctions ùõøùë•ùëñand combine that with at most ùëÅ
OR gates, thus obtaining a circuit of at most 2ùëõ‚ãÖùëÅ+ ùëÅgates. Since
ùëÜ‚äÜ{0, 1}ùëõ, its size ùëÅis at most 2ùëõand hence the total number of
gates in this circuit is ùëÇ(ùëõ‚ãÖ2ùëõ).
‚ñ†
4.6 THE CLASS SIZE(ùëá)
We have seen that every function ùëì‚à∂{0, 1}ùëõ‚Üí{0, 1}ùëöcan be com-
puted by a circuit of size ùëÇ(ùëö‚ãÖ2ùëõ), and some functions (such as ad-
dition and multiplication) can be computed by much smaller circuits.
We define SIZE(ùë†) to be the set of functions that can be computed by
NAND circuits of at most ùë†gates (or equivalently, by NAND-CIRC
programs of at most ùë†lines). Formally, the definition is as follows:


--- Page 178 ---

178
introduction to theoretical computer science
4 The restriction that ùëö, ùëõ‚â§2ùë†makes no difference;
see Exercise 3.11.
Definition 4.18 ‚Äî Size class of functions. For every ùëõ, ùëö
‚àà
{1, ‚Ä¶ , 2ùë†},
we let set SIZEùëõ,ùëö(ùë†) denotes the set of all functions ùëì‚à∂{0, 1}ùëõ‚Üí
{0, 1}ùëösuch that ùëì
‚àà
SIZE(ùë†). 4 We denote by SIZEùëõ(ùë†) the set
SIZEùëõ,1(ùë†). For every integer ùë†‚â•1, we let SIZE(ùë†) = ‚à™ùëõ,ùëö‚â§2ùë†SIZEùëõ,ùëö(ùë†)
be the set of all functions ùëìfor which there exists a NAND circuit
of at most ùë†gates that compute ùëì.
Fig. 4.12 depicts the set SIZEùëõ,1(ùë†). Note that SIZEùëõ,ùëö(ùë†) is a set of
functions, not of programs! (Asking if a program or a circuit is a mem-
ber of SIZEùëõ,ùëö(ùë†) is a category error as in the sense of Fig. 4.13.) As
we discussed in Section 3.6.2 (and Section 2.6.1), the distinction be-
tween programs and functions is absolutely crucial. You should always
remember that while a program computes a function, it is not equal to
a function. In particular, as we‚Äôve seen, there can be more than one
program to compute the same function.
Figure 4.12: There are 22ùëõfunctions mapping {0, 1}ùëõ
to {0, 1}, and an infinite number of circuits with ùëõbit
inputs and a single bit of output. Every circuit com-
putes one function, but every function can be com-
puted by many circuits. We say that ùëì‚ààSIZEùëõ,1(ùë†)
if the smallest circuit that computes ùëìhas ùë†or fewer
gates. For example XORùëõ‚ààSIZEùëõ,1(4ùëõ). Theo-
rem 4.12 shows that every function ùëîis computable
by some circuit of at most ùëê‚ãÖ2ùëõ/ùëõgates, and hence
SIZEùëõ,1(ùëê‚ãÖ2ùëõ/ùëõ) corresponds to the set of all func-
tions from {0, 1}ùëõto {0, 1}.
While we defined SIZE(ùë†) with respect to NAND gates, we
would get essentially the same class if we defined it with respect to
AND/OR/NOT gates:
Lemma 4.19 Let SIZEùê¥ùëÇùëÅ
ùëõ,ùëö(ùë†) denote the set of all functions ùëì‚à∂{0, 1}ùëõ‚Üí
{0, 1}ùëöthat can be computed by an AND/OR/NOT Boolean circuit of
at most ùë†gates. Then,
SIZEùëõ,ùëö(ùë†/2) ‚äÜSIZEùê¥ùëÇùëÅ
ùëõ,ùëö(ùë†) ‚äÜSIZEùëõ,ùëö(3ùë†)
(4.12)
Proof. If ùëìcan be computed by a NAND circuit of at most ùë†/2 gates,
then by replacing each NAND with the two gates NOT and AND, we
can obtain an AND/OR/NOT Boolean circuit of at most ùë†gates that
computes ùëì. On the other hand, if ùëìcan be computed by a Boolean


--- Page 179 ---

syntactic sugar, and computing every function
179
Figure 4.13: A ‚Äúcategory error‚Äù is a question such as
‚Äúis a cucumber even or odd?‚Äù which does not even
make sense. In this book one type of category error
you should watch out for is confusing functions and
programs (i.e., confusing specifications and implemen-
tations). If ùê∂is a circuit or program, then asking if
ùê∂‚ààSIZEùëõ,1(ùë†) is a category error, since SIZEùëõ,1(ùë†) is
a set of functions and not programs or circuits.
AND/OR/NOT circuit of at most ùë†gates, then by Theorem 3.12 it can
be computed by a NAND circuit of at most 3ùë†gates.
‚ñ†
The results we have seen in this chapter can be phrased as showing
that ADDùëõ‚ààSIZE2ùëõ,ùëõ+1(100ùëõ) and MULTùëõ‚ààSIZE2ùëõ,2ùëõ(10000ùëõlog2 3).
Theorem 4.12 shows that for some constant ùëê, SIZEùëõ,ùëö(ùëêùëö2ùëõ) is equal
to the set of all functions from {0, 1}ùëõto {0, 1}ùëö.
R
Remark 4.20 ‚Äî Finite vs infinite functions. A NAND-
CIRC program ùëÉcan only compute a function with
a certain number ùëõof inputs and a certain number
ùëöof outputs. Hence, for example, there is no single
NAND-CIRC program that can compute the incre-
ment function INC
‚à∂
{0, 1}‚àó
‚Üí
{0, 1}‚àóthat maps a
string ùë•(which we identify with a number via the
binary representation) to the string that represents
ùë•+ 1. Rather for every ùëõ> 0, there is a NAND-CIRC
program ùëÉùëõthat computes the restriction INCùëõof
the function INC to inputs of length ùëõ. Since it can be
shown that for every ùëõ
>
0 such a program ùëÉùëõexists
of length at most 10ùëõ, INCùëõ
‚àà
SIZE(10ùëõ) for every
ùëõ> 0.
If ùëá
‚à∂‚Ñï‚Üí‚Ñïand ùêπ
‚à∂{0, 1}‚àó
‚Üí{0, 1}‚àó, we will
write ùêπ
‚àà
SIZE‚àó(ùëá(ùëõ)) (or sometimes slightly abuse
notation and write simply ùêπ
‚àà
SIZE(ùëá(ùëõ))) to in-
dicate that for every ùëõthe restriction ùêπ‚Üæùëõof ùêπto
inputs in {0, 1}ùëõis in SIZE(ùëá(ùëõ)). Hence we can write
INC ‚ààSIZE‚àó(10ùëõ). We will come back to this issue of
finite vs infinite functions later in this course.
Solved Exercise 4.1 ‚Äî ùëÜùêºùëçùê∏closed under complement.. In this exercise we
prove a certain ‚Äúclosure property‚Äù of the class SIZEùëõ(ùë†). That is, we
show that if ùëìis in this class then (up to some small additive term) so
is the complement of ùëì, which is the function ùëî(ùë•) = 1 ‚àíùëì(ùë•).
Prove that there is a constant ùëêsuch that for every ùëì‚à∂{0, 1}ùëõ‚Üí
{0, 1} and ùë†‚àà‚Ñï, if ùëì‚ààSIZEùëõ(ùë†) then 1 ‚àíùëì‚ààSIZEùëõ(ùë†+ ùëê).
‚ñ†
Solution:
If ùëì
‚àà
SIZE(ùë†) then there is an ùë†-line NAND-CIRC program
ùëÉthat computes ùëì. We can rename the variable Y[0] in ùëÉto a
variable temp and add the line
Y[0] = NAND(temp,temp)
at the very end to obtain a program ùëÉ‚Ä≤ that computes 1 ‚àíùëì.
‚ñ†


--- Page 180 ---

180
introduction to theoretical computer science
‚úì
Chapter Recap
‚Ä¢ We can define the notion of computing a function
via a simplified ‚Äúprogramming language‚Äù, where
computing a function ùêπin ùëásteps would corre-
spond to having a ùëá-line NAND-CIRC program
that computes ùêπ.
‚Ä¢ While the NAND-CIRC programming only has one
operation, other operations such as functions and
conditional execution can be implemented using it.
‚Ä¢ Every function ùëì‚à∂{0, 1}ùëõ‚Üí{0, 1}ùëöcan be com-
puted by a circuit of at most ùëÇ(ùëö2ùëõ) gates (and in
fact at most ùëÇ(ùëö2ùëõ/ùëõ) gates).
‚Ä¢ Sometimes (or maybe always?) we can translate an
efficient algorithm to compute ùëìinto a circuit that
computes ùëìwith a number of gates comparable to
the number of steps in this algorithm.
4.7 EXERCISES
Exercise 4.1 ‚Äî Pairing. This exercise asks you to give a one-to-one map
from ‚Ñï2 to ‚Ñï. This can be useful to implement two-dimensional arrays
as ‚Äúsyntactic sugar‚Äù in programming languages that only have one-
dimensional array.
1. Prove that the map ùêπ(ùë•, ùë¶) = 2ùë•3ùë¶is a one-to-one map from ‚Ñï2 to
‚Ñï.
2. Show that there is a one-to-one map ùêπ‚à∂‚Ñï2 ‚Üí‚Ñïsuch that for every
ùë•, ùë¶, ùêπ(ùë•, ùë¶) ‚â§100 ‚ãÖmax{ùë•, ùë¶}2 + 100.
3. For every ùëò, show that there is a one-to-one map ùêπ‚à∂‚Ñïùëò‚Üí‚Ñïsuch
that for every ùë•0, ‚Ä¶ , ùë•ùëò‚àí1 ‚àà‚Ñï, ùêπ(ùë•0, ‚Ä¶ , ùë•ùëò‚àí1) ‚â§100 ‚ãÖ(ùë•0 + ùë•1 + ‚Ä¶ +
ùë•ùëò‚àí1 + 100ùëò)ùëò.
‚ñ†
Exercise 4.2 ‚Äî Computing MUX. Prove that the NAND-CIRC program be-
low computes the function MUX (or LOOKUP1) where MUX(ùëé, ùëè, ùëê)
equals ùëéif ùëê= 0 and equals ùëèif ùëê= 1:
t = NAND(X[2],X[2])
u = NAND(X[0],t)
v = NAND(X[1],X[2])
Y[0] = NAND(u,v)
‚ñ†
Exercise 4.3 ‚Äî At least two / Majority. Give a NAND-CIRC program of at
most 6 lines to compute the function MAJ ‚à∂{0, 1}3 ‚Üí{0, 1} where
MAJ(ùëé, ùëè, ùëê) = 1 iff ùëé+ ùëè+ ùëê‚â•2.


--- Page 181 ---

syntactic sugar, and computing every function
181
5 You can start by transforming ùëÉinto a NAND-CIRC-
PROC program that uses procedure statements, and
then use the code of Fig. 4.3 to transform the latter
into a ‚Äúsugar free‚Äù NAND-CIRC program.
6 Use a ‚Äúcascade‚Äù of adding the bits one after the
other, starting with the least significant digit, just like
in the elementary-school algorithm.
‚ñ†
Exercise 4.4 ‚Äî Conditional statements. In this exercise we will explore The-
orem 4.6: transforming NAND-CIRC-IF programs that use code such
as if .. then .. else .. to standard NAND-CIRC programs.
1. Give a ‚Äúproof by code‚Äù of Theorem 4.6: a program in a program-
ming language of your choice that transforms a NAND-CIRC-IF
program ùëÉinto a ‚Äúsugar free‚Äù NAND-CIRC program ùëÉ‚Ä≤ that com-
putes the same function. See footnote for hint.5
2. Prove the following statement, which is the heart of Theorem 4.6:
suppose that there exists an ùë†-line NAND-CIRC program to com-
pute ùëì‚à∂{0, 1}ùëõ‚Üí{0, 1} and an ùë†‚Ä≤-line NAND-CIRC program
to compute ùëî‚à∂{0, 1}ùëõ‚Üí{0, 1}. Prove that there exist a NAND-
CIRC program of at most ùë†+ ùë†‚Ä≤ + 10 lines to compute the func-
tion ‚Ñé‚à∂{0, 1}ùëõ+1 ‚Üí{0, 1} where ‚Ñé(ùë•0, ‚Ä¶ , ùë•ùëõ‚àí1, ùë•ùëõ) equals
ùëì(ùë•0, ‚Ä¶ , ùë•ùëõ‚àí1) if ùë•ùëõ= 0 and equals ùëî(ùë•0, ‚Ä¶ , ùë•ùëõ‚àí1) otherwise.
(All programs in this item are standard ‚Äúsugar-free‚Äù NAND-CIRC
programs.)
‚ñ†
Exercise 4.5 ‚Äî Half and full adders. 1. A half adder is the function HA ‚à∂
{0, 1}2 ‚à∂‚Üí{0, 1}2 that corresponds to adding two binary bits. That
is, for every ùëé, ùëè‚àà{0, 1}, HA(ùëé, ùëè) = (ùëí, ùëì) where 2ùëí+ ùëì= ùëé+ ùëè.
Prove that there is a NAND circuit of at most five NAND gates that
computes HA.
2. A full adder is the function FA ‚à∂{0, 1}3 ‚Üí{0, 1}2 that takes in
two bits and a ‚Äúcarry‚Äù bit and outputs their sum. That is, for every
ùëé, ùëè, ùëê‚àà{0, 1}, FA(ùëé, ùëè, ùëê) = (ùëí, ùëì) such that 2ùëí+ ùëì= ùëé+ ùëè+ ùëê.
Prove that there is a NAND circuit of at most nine NAND gates that
computes FA.
3. Prove that if there is a NAND circuit of ùëêgates that computes FA,
then there is a circuit of ùëêùëõgates that computes ADDùëõwhere (as
in Theorem 4.7) ADDùëõ‚à∂{0, 1}2ùëõ‚Üí{0, 1}ùëõ+1 is the function that
outputs the addition of two input ùëõ-bit numbers. See footnote for
hint.6
4. Show that for every ùëõthere is a NAND-CIRC program to compute
ADDùëõwith at most 9ùëõlines.
‚ñ†
Exercise 4.6 ‚Äî Addition. Write a program using your favorite program-
ming language that on input of an integer ùëõ, outputs a NAND-CIRC
program that computes ADDùëõ. Can you ensure that the program it
outputs for ADDùëõhas fewer than 10ùëõlines?


--- Page 182 ---

182
introduction to theoretical computer science
7 Hint: Use Karatsuba‚Äôs algorithm.
‚ñ†
Exercise 4.7 ‚Äî Multiplication. Write a program using your favorite pro-
gramming language that on input of an integer ùëõ, outputs a NAND-
CIRC program that computes MULTùëõ. Can you ensure that the pro-
gram it outputs for MULTùëõhas fewer than 1000 ‚ãÖùëõ2 lines?
‚ñ†
Exercise 4.8 ‚Äî Efficient multiplication (challenge). Write a program using
your favorite programming language that on input of an integer ùëõ,
outputs a NAND-CIRC program that computes MULTùëõand has at
most 10000ùëõ1.9 lines.7 What is the smallest number of lines you can
use to multiply two 2048 bit numbers?
‚ñ†
Exercise 4.9 ‚Äî Multibit function. In the text Theorem 4.12 is only proven
for the case ùëö= 1. In this exercise you will extend the proof for every
ùëö.
Prove that
1. If there is an ùë†-line NAND-CIRC program to compute
ùëì‚à∂{0, 1}ùëõ‚Üí{0, 1} and an ùë†‚Ä≤-line NAND-CIRC program
to compute ùëì‚Ä≤ ‚à∂{0, 1}ùëõ‚Üí{0, 1} then there is an ùë†+ ùë†‚Ä≤-line
program to compute the function ùëî‚à∂{0, 1}ùëõ‚Üí{0, 1}2 such that
ùëî(ùë•) = (ùëì(ùë•), ùëì‚Ä≤(ùë•)).
2. For every function ùëì‚à∂{0, 1}ùëõ‚Üí{0, 1}ùëö, there is a NAND-CIRC
program of at most 10ùëö‚ãÖ2ùëõlines that computes ùëì. (You can use the
ùëö= 1 case of Theorem 4.12, as well as Item 1.)
‚ñ†
Exercise 4.10 ‚Äî Simplifying using syntactic sugar. Let ùëÉbe the following
NAND-CIRC program:
Temp[0] = NAND(X[0],X[0])
Temp[1] = NAND(X[1],X[1])
Temp[2] = NAND(Temp[0],Temp[1])
Temp[3] = NAND(X[2],X[2])
Temp[4] = NAND(X[3],X[3])
Temp[5] = NAND(Temp[3],Temp[4])
Temp[6] = NAND(Temp[2],Temp[2])
Temp[7] = NAND(Temp[5],Temp[5])
Y[0] = NAND(Temp[6],Temp[7])
1. Write a program ùëÉ‚Ä≤ with at most three lines of code that uses both
NAND as well as the syntactic sugar OR that computes the same func-
tion as ùëÉ.


--- Page 183 ---

syntactic sugar, and computing every function
183
8 You can use the fact that (ùëé+ùëè)+ùëêmod 2 = ùëé+ùëè+ùëê
mod 2. In particular it means that if you have the
lines d = XOR(a,b) and e = XOR(d,c) then e gets the
sum modulo 2 of the variable a, b and c.
2. Draw a circuit that computes the same function as ùëÉand uses only
AND and NOT gates.
‚ñ†
In the following exercises you are asked to compare the power of
pairs of programming languages. By ‚Äúcomparing the power‚Äù of two
programming languages ùëãand ùëåwe mean determining the relation
between the set of functions that are computable using programs in ùëã
and ùëårespectively. That is, to answer such a question you need to do
both of the following:
1. Either prove that for every program ùëÉin ùëãthere is a program ùëÉ‚Ä≤
in ùëåthat computes the same function as ùëÉ, or give an example for
a function that is computable by an ùëã-program but not computable
by a ùëå-program.
and
2. Either prove that for every program ùëÉin ùëåthere is a program ùëÉ‚Ä≤
in ùëãthat computes the same function as ùëÉ, or give an example for a
function that is computable by a ùëå-program but not computable by
an ùëã-program.
When you give an example as above of a function that is com-
putable in one programming language but not the other, you need
to prove that the function you showed is (1) computable in the first
programming language and (2) not computable in the second program-
ming language.
Exercise 4.11 ‚Äî Compare IF and NAND. Let IF-CIRC be the programming
language where we have the following operations foo = 0, foo = 1,
foo = IF(cond,yes,no) (that is, we can use the constants 0 and 1,
and the IF ‚à∂{0, 1}3 ‚Üí{0, 1} function such that IF(ùëé, ùëè, ùëê) equals ùëèif
ùëé= 1 and equals ùëêif ùëé= 0). Compare the power of the NAND-CIRC
programming language and the IF-CIRC programming language.
‚ñ†
Exercise 4.12 ‚Äî Compare XOR and NAND. Let XOR-CIRC be the pro-
gramming language where we have the following operations foo
= XOR(bar,blah), foo = 1 and bar = 0 (that is, we can use the
constants 0, 1 and the XOR function that maps ùëé, ùëè‚àà{0, 1}2 to ùëé+ ùëè
mod 2). Compare the power of the NAND-CIRC programming
language and the XOR-CIRC programming language. See footnote for
hint.8
‚ñ†
Exercise 4.13 ‚Äî Circuits for majority. Prove that there is some constant ùëê
such that for every ùëõ> 1, MAJùëõ‚ààSIZE(ùëêùëõ) where MAJùëõ‚à∂{0, 1}ùëõ‚Üí


--- Page 184 ---

184
introduction to theoretical computer science
9 One approach to solve this is using recursion and the
so-called Master Theorem.
{0, 1} is the majority function on ùëõinput bits. That is MAJùëõ(ùë•) = 1 iff
‚àë
ùëõ‚àí1
ùëñ=0 ùë•ùëñ> ùëõ/2. See footnote for hint.9
‚ñ†
Exercise 4.14 ‚Äî Circuits for threshold. Prove that there is some constant ùëê
such that for every ùëõ> 1, and integers ùëé0, ‚Ä¶ , ùëéùëõ‚àí1, ùëè‚àà{‚àí2ùëõ, ‚àí2ùëõ+
1, ‚Ä¶ , ‚àí1, 0, +1, ‚Ä¶ , 2ùëõ}, there is a NAND circuit with at most ùëõùëêgates
that computes the threshold function ùëìùëé0,‚Ä¶,ùëéùëõ‚àí1,ùëè‚à∂{0, 1}ùëõ‚Üí{0, 1} that
on input ùë•‚àà{0, 1}ùëõoutputs 1 if and only if ‚àë
ùëõ‚àí1
ùëñ=0 ùëéùëñùë•ùëñ> ùëè.
‚ñ†
4.8 BIBLIOGRAPHICAL NOTES
See Jukna‚Äôs and Wegener‚Äôs books [Juk12; Weg87] for much more
extensive discussion on circuits. Shannon showed that every Boolean
function can be computed by a circuit of exponential size [Sha38]. The
improved bound of ùëê‚ãÖ2ùëõ/ùëõ(with the optimal value of ùëêfor many
bases) is due to Lupanov [Lup58]. An exposition of this for the case
of NAND (where ùëê= 1) is given in Chapter 4 of his book [Lup84].
(Thanks to Sasha Golovnev for tracking down this reference!)
The concept of ‚Äúsyntactic sugar‚Äù is also known as ‚Äúmacros‚Äù or
‚Äúmeta-programming‚Äù and is sometimes implemented via a prepro-
cessor or macro language in a programming language or a text editor.
One modern example is the Babel JavaScript syntax transformer, that
converts JavaScript programs written using the latest features into
a format that older Browsers can accept. It even has a plug-in ar-
chitecture, that allows users to add their own syntactic sugar to the
language.


--- Page 185 ---

5
Code as data, data as code
‚ÄúThe term code script is, of course, too narrow. The chromosomal structures
are at the same time instrumental in bringing about the development they
foreshadow. They are law-code and executive power - or, to use another simile,
they are architect‚Äôs plan and builder‚Äôs craft - in one.‚Äù , Erwin Schr√∂dinger,
1944.
‚ÄúA mathematician would hardly call a correspondence between the set of 64
triples of four units and a set of twenty other units,‚Äùuniversal‚Äú, while such
correspondence is, probably, the most fundamental general feature of life on
Earth‚Äù, Misha Gromov, 2013
A program is simply a sequence of symbols, each of which can be
encoded as a string of 0‚Äôs and 1‚Äôs using (for example) the ASCII stan-
dard. Therefore we can represent every NAND-CIRC program (and
hence also every Boolean circuit) as a binary string. This statement
seems obvious but it is actually quite profound. It means that we can
treat circuits or NAND-CIRC programs both as instructions to car-
rying computation and also as data that could potentially be used as
inputs to other computations.
ÔÉ´Big Idea 6 A program is a piece of text, and so it can be fed as input
to other programs.
This correspondence between code and data is one of the most fun-
damental aspects of computing. It underlies the notion of general
purpose computers, that are not pre-wired to compute only one task,
and also forms the basis of our hope for obtaining general artificial
intelligence. This concept finds immense use in all areas of comput-
ing, from scripting languages to machine learning, but it is fair to say
that we haven‚Äôt yet fully mastered it. Many security exploits involve
cases such as ‚Äúbuffer overflows‚Äù when attackers manage to inject code
where the system expected only ‚Äúpassive‚Äù data (see Fig. 5.1). The re-
lation between code and data reaches beyond the realm of electronic
Compiled on 8.26.2020 18:10
Learning Objectives:
‚Ä¢ See one of the most important concepts in
computing: duality between code and data.
‚Ä¢ Build up comfort in moving between
different representations of programs.
‚Ä¢ Follow the construction of a ‚Äúuniversal circuit
evaluator‚Äù that can evaluate other circuits
given their representation.
‚Ä¢ See major result that complements the result
of the last chapter: some functions require an
exponential number of gates to compute.
‚Ä¢ Discussion of Physical extended
Church-Turing thesis stating that Boolean
circuits capture all feasible computation in
the physical world, and its physical and
philosophical implications.


--- Page 186 ---

186
introduction to theoretical computer science
Figure 5.1: As illustrated in this xkcd cartoon, many
exploits, including buffer overflow, SQL injections,
and more, utilize the blurry line between ‚Äúactive
programs‚Äù and ‚Äústatic strings‚Äù.
computers. For example, DNA can be thought of as both a program
and data (in the words of Schr√∂dinger, who wrote before the discov-
ery of DNA‚Äôs structure a book that inspired Watson and Crick, DNA is
both ‚Äúarchitect‚Äôs plan and builder‚Äôs craft‚Äù).
This chapter: A non-mathy overview
In this chapter, we will begin to explore some of the many
applications of the correspondence between code and data.
We start by using the representation of programs/circuits
as strings to count the number of programs/circuits up to a
certain size, and use that to obtain a counterpart to the result
we proved in Chapter 4. There we proved that every function
can be computed by a circuit, but that circuit could be expo-
nentially large (see Theorem 4.16 for the precise bound) In
this chapter we will prove that there are some functions for
which we cannot do better: the smallest circuit that computes
them is exponentially large.
We will also use the notion of representing programs/cir-
cuits as strings to show the existence of a ‚Äúuniversal circuit‚Äù
- a circuit that can evaluate other circuits. In programming
languages, this is known as a ‚Äúmeta circular evaluator‚Äù - a
program in a certain programming language that can eval-
uate other programs in the same language. These results
do have an important restriction: the universal circuit will
have to be of bigger size than the circuits it evaluates. We will
show how to get around this restriction in Chapter 7 where
we introduce loops and Turing Machines.
See Fig. 5.2 for an overview of the results of this chapter.
Figure 5.2: Overview of the results in this chapter.
We use the representation of programs/circuits as
strings to derive two main results. First we show
the existence of a universal program/circuit, and
in fact (with more work) the existence of such a
program/circuit whose size is at most polynomial in
the size of the program/circuit it evaluates. We then
use the string representation to count the number
of programs/circuits of a given size, and use that to
establish that some functions require an exponential
number of lines/gates to compute.


--- Page 187 ---

code as data, data as code
187
5.1 REPRESENTING PROGRAMS AS STRINGS
We can represent programs or circuits as strings in a myriad of ways.
For example, since Boolean circuits are labeled directed acyclic graphs,
we can use the adjacency matrix or adjacency list representations for
them. However, since the code of a program is ultimately just a se-
quence of letters and symbols, arguably the conceptually simplest
representation of a program is as such a sequence. For example, the
following NAND-CIRC program ùëÉ
temp_0 = NAND(X[0],X[1])
temp_1 = NAND(X[0],temp_0)
temp_2 = NAND(X[1],temp_0)
Y[0] = NAND(temp_1,temp_2)
is simply a string of 107 symbols which include lower and upper
case letters, digits, the underscore character _ and equality sign =,
punctuation marks such as ‚Äú(‚Äù,‚Äú)‚Äù,‚Äú,‚Äù, spaces, and ‚Äúnew line‚Äù mark-
ers (often denoted as ‚Äú\n‚Äù or ‚Äú‚Üµ‚Äù). Each such symbol can be encoded
as a string of 7 bits using the ASCII encoding, and hence the program
ùëÉcan be encoded as a string of length 7 ‚ãÖ107 = 749 bits.
Nothing in the above discussion was specific to the program ùëÉ, and
hence we can use the same reasoning to prove that every NAND-CIRC
program can be represented as a string in {0, 1}‚àó. In fact, we can do a
bit better. Since the names of the working variables of a NAND-CIRC
program do not affect its functionality, we can always transform a pro-
gram to have the form of ùëÉ‚Ä≤ where all variables apart from the inputs
and outputs have the form temp_0, temp_1, temp_2, etc.. Moreover,
if the program has ùë†lines, then we will never need to use an index
larger than 3ùë†(since each line involves at most three variables), and
similarly the indices of the input and output variables will all be at
most 3ùë†. Since a number between 0 and 3ùë†can be expressed using
at most ‚åàlog10(3ùë†+ 1)‚åâ= ùëÇ(log ùë†) digits, each line in the program
(which has the form foo = NAND(bar,blah)), can be represented
using ùëÇ(1) + ùëÇ(log ùë†) = ùëÇ(log ùë†) symbols, each of which can be rep-
resented by 7 bits. Hence an ùë†line program can be represented as a
string of ùëÇ(ùë†log ùë†) bits, resulting in the following theorem:
Theorem 5.1 ‚Äî Representing programs as strings. There is a constant ùëê
such that for ùëì
‚àà
SIZE(ùë†), there exists a program ùëÉcomputing ùëì
whose string representation has length at most ùëêùë†log ùë†.
P


--- Page 188 ---

188
introduction to theoretical computer science
1 The implicit constant in the ùëÇ(‚ãÖ) notation is smaller
than 10. That is, for all sufficiently large ùë†, |SIZE(ùë†)| <
210ùë†log ùë†, see Remark 5.4. As discussed in Section 1.7,
we use the bound 10 simply because it is a round
number.
2 ‚ÄúAstronomical‚Äù here is an understatement: there are
much fewer than 2210 stars, or even particles, in the
observable universe.
We omit the formal proof of Theorem 5.1 but please
make sure that you understand why it follows from
the reasoning above.
5.2 COUNTING PROGRAMS, AND LOWER BOUNDS ON THE SIZE
OF NAND-CIRC PROGRAMS
One consequence of the representation of programs as strings is that
the number of programs of certain length is bounded by the number
of strings that represent them. This has consequences for the sets
SIZE(ùë†) that we defined in Section 4.6.
Theorem 5.2 ‚Äî Counting programs. For every ùë†‚àà‚Ñï,
|SIZE(ùë†)| ‚â§2ùëÇ(ùë†log ùë†).
(5.1)
That is, there are at most 2ùëÇ(ùë†log ùë†) functions computed by NAND-
CIRC programs of at most ùë†lines. 1
Proof. We will show a one-to-one map ùê∏from SIZE(ùë†) to the set of
strings of length ùëêùë†log ùë†for some constant ùëê. This will conclude the
proof, since it implies that |SIZE(ùë†)| is smaller than the size of the set
of all strings of length at most ‚Ñì, which equals 1+2+4+‚ãØ+2‚Ñì= 2‚Ñì+1‚àí1
by the formula for sums of geometric progressions.
The map ùê∏will simply map ùëìto the representation of the program
computing ùëì. Specifically, we let ùê∏(ùëì) be the representation of the
program ùëÉcomputing ùëìgiven by Theorem 5.1. This representation
has size at most ùëêùë†log ùë†, and moreover the map ùê∏is one to one, since
if ùëì‚â†ùëì‚Ä≤ then every two programs computing ùëìand ùëì‚Ä≤ respectively
must have different representations.
‚ñ†
A function mapping {0, 1}2 to {0, 1} can be identified with the
table of its four values on the inputs 00, 01, 10, 11. A function mapping
{0, 1}3 to {0, 1} can be identified with the table of its eight values on
the inputs 000, 001, 010, 011, 100, 101, 110, 111. More generally, every
function ùêπ‚à∂{0, 1}ùëõ‚Üí{0, 1} can be identified with the table of its 2ùëõ
values on the inputs {0, 1}ùëõ. Hence the number of functions mapping
{0, 1}ùëõto {0, 1} is equal to the number of such tables which (since
we can choose either 0 or 1 for every row) is exactly 22ùëõ. Note that
this is double exponential in ùëõ, and hence even for small values of ùëõ
(e.g., ùëõ= 10) the number of functions from {0, 1}ùëõto {0, 1} is truly
astronomical.2 This has the following important corollary:


--- Page 189 ---

code as data, data as code
189
3 The constant ùõøis at least 0.1 and in fact, can be im-
proved to be arbitrarily close to 1/2, see Exercise 5.7.
Theorem 5.3 ‚Äî Counting argument lower bound. There is a constant
ùõø
>
0, such that for every sufficiently large ùëõ, there is a function
ùëì‚à∂{0, 1}ùëõ‚Üí{0, 1} such that ùëì‚àâSIZE ( ùõø2ùëõ
ùëõ). That is, the shortest
NAND-CIRC program to compute ùëìrequires more than ùõø
‚ãÖ
2ùëõ/ùëõ
lines. 3
Proof. The proof is simple. If we let ùëêbe the constant such that
|SIZE(ùë†)| ‚â§2ùëêùë†log ùë†and ùõø= 1/ùëê, then setting ùë†= ùõø2ùëõ/ùëõwe see that
|SIZE( ùõø2ùëõ
ùëõ)| ‚â§2ùëêùõø2ùëõ
ùëõ
log ùë†< 2ùëêùõø2ùëõ= 22ùëõ
(5.2)
using the fact that since ùë†< 2ùëõ, log ùë†< ùëõand ùõø= 1/ùëê. But since
|SIZE(ùë†)| is smaller than the total number of functions mapping ùëõbits
to 1 bit, there must be at least one such function not in SIZE(ùë†), which
is what we needed to prove.
‚ñ†
We have seen before that every function mapping {0, 1}ùëõto {0, 1}
can be computed by an ùëÇ(2ùëõ/ùëõ) line program. Theorem 5.3 shows
that this is tight in the sense that some functions do require such an
astronomical number of lines to compute.
ÔÉ´Big Idea 7 Some functions ùëì‚à∂{0, 1}ùëõ‚Üí{0, 1} cannot be computed
by a Boolean circuit using fewer than exponential (in ùëõ) number of
gates.
In fact, as we explore in the exercises, this is the case for most func-
tions. Hence functions that can be computed in a small number of
lines (such as addition, multiplication, finding short paths in graphs,
or even the EVAL function) are the exception, rather than the rule.
R
Remark 5.4 ‚Äî More efficient representation (advanced,
optional). The ASCII representation is not the shortest
representation for NAND-CIRC programs. NAND-
CIRC programs are equivalent to circuits with NAND
gates, which means that a NAND-CIRC program of ùë†
lines, ùëõinputs, and ùëöoutputs can be represented by
a labeled directed graph of ùë†+ ùëõvertices, of which ùëõ
have in-degree zero, and the ùë†others have in-degree
at most two. Using the adjacency matrix represen-
tation for such graphs, we can reduce the implicit
constant in Theorem 5.2 to be arbitrarily close to 5, see
Exercise 5.6.


--- Page 190 ---

190
introduction to theoretical computer science
Figure 5.4: We prove Theorem 5.5 by coming up
with a list ùëì0, ‚Ä¶ , ùëì2ùëõof functions such that ùëì0 is the
all zero function, ùëì2ùëõis a function (obtained from
Theorem 5.3) outside of SIZE(0.1 ‚ãÖ2ùëõ/ùëõ) and such
that ùëìùëñ‚àí1 and ùëìùëñdiffer by one another on at most one
input. We can show that for every ùëñ, the number of
gates to compute ùëìùëñis at most 10ùëõlarger than the
number of gates to compute ùëìùëñ‚àí1 and so if we let ùëñ
be the smallest number such that ùëìùëñ‚àâSIZE(ùë†), then
ùëìùëñ‚ààSIZE(ùë†+ 10ùëõ).
5.2.1 Size hierarchy theorem (optional)
By Theorem 4.15 the class SIZEùëõ(10 ‚ãÖ2ùëõ/ùëõ) contains all functions
from {0, 1}ùëõto {0, 1}, while by Theorem 5.3, there is some function
ùëì‚à∂{0, 1}ùëõ‚Üí{0, 1} that is not contained in SIZEùëõ(0.1 ‚ãÖ2ùëõ/ùëõ). In other
words, for every sufficiently large ùëõ,
SIZEùëõ(0.1 2ùëõ
ùëõ) ‚ääSIZEùëõ(10 2ùëõ
ùëõ) .
(5.3)
It turns out that we can use Theorem 5.3 to show a more general re-
sult: whenever we increase our ‚Äúbudget‚Äù of gates we can compute
new functions.
Theorem 5.5 ‚Äî Size Hierarchy Theorem. For every sufficiently large ùëõ
and 10ùëõ< ùë†< 0.1 ‚ãÖ2ùëõ/ùëõ,
SIZEùëõ(ùë†) ‚ääSIZEùëõ(ùë†+ 10ùëõ) .
(5.4)
Proof Idea:
To prove the theorem we need to find a function ùëì‚à∂{0, 1}ùëõ‚Üí{0, 1}
such that ùëìcan be computed by a circuit of ùë†+ 10ùëõgates but it cannot
be computed by a circuit of ùë†gates. We will do so by coming up with
a sequence of functions ùëì0, ùëì1, ùëì2, ‚Ä¶ , ùëìùëÅwith the following properties:
(1) ùëì0 can be computed by a circuit of at most 10ùëõgates, (2) ùëìùëÅcannot
be computed by a circuit of 0.1 ‚ãÖ2ùëõ/ùëõgates, and (3) for every ùëñ‚àà
{0, ‚Ä¶ , ùëÅ}, if ùëìùëñcan be computed by a circuit of size ùë†, then ùëìùëñ+1 can be
computed by a circuit of size at most ùë†+10ùëõ. Together these properties
imply that if we let ùëñbe the smallest number such that ùëìùëñ‚àâSIZEùëõ(ùë†),
then since ùëìùëñ‚àí1 ‚ààSIZE(ùë†) it must hold that ùëìùëñ‚ààSIZE(ùë†+ 10ùëõ) which is
what we need to prove. See Fig. 5.4 for an illustration.
‚ãÜ
Proof of Theorem 5.5. Let ùëì‚àó‚à∂{0, 1}ùëõ‚Üí{0, 1} be the function
(whose existence we are guaranteed by Theorem 5.3) such that
ùëì‚àó‚àâSIZEùëõ(0.1 ‚ãÖ2ùëõ/ùëõ). We define the functions ùëì0, ùëì1, ‚Ä¶ , ùëì2ùëõmap-
ping {0, 1}ùëõto {0, 1} as follows. For every ùë•‚àà{0, 1}ùëõ, if ùëôùëíùë•(ùë•) ‚àà
{0, 1, ‚Ä¶ , 2ùëõ‚àí1} is ùë•‚Äôs order in the lexicographical order then
ùëìùëñ(ùë•) =
‚éß
{
‚é®
{
‚é©
ùëì‚àó(ùë•)
ùëôùëíùë•(ùë•) < ùëñ
0
otherwise
.
(5.5)
The function ùëì0 is simply the constant zero function, while the
function ùëì2ùëõis equal to ùëì‚àó. Moreover, for every ùëñ‚àà[2ùëõ], the functions
ùëìùëñand ùëìùëñ+1 differ on at most one input (i.e., the input ùë•‚àà{0, 1}ùëõsuch
that ùëôùëíùë•(ùë•) = ùëñ). Let 10ùëõ< ùë†< 0.1 ‚ãÖ2ùëõ/ùëõ, and let ùëñbe the first index
such that ùëìùëñ‚àâSIZEùëõ(ùë†). Since ùëì2ùëõ= ùëì‚àó‚àâSIZE(0.1 ‚ãÖ2ùëõ/ùëõ) there


--- Page 191 ---

code as data, data as code
191
must exist such an index ùëñ, and moreover ùëñ> 0 since the constant zero
function is a member of SIZEùëõ(10ùëõ).
By our choice of ùëñ, ùëìùëñ‚àí1 is a member of SIZEùëõ(ùë†). To complete the
proof, we need to show that ùëìùëñ‚ààSIZEùëõ(ùë†+ 10ùëõ). Let ùë•‚àóbe the string
such that ùëôùëíùë•(ùë•‚àó) = ùëñùëè‚àà{0, 1} is the value of ùëì‚àó(ùë•‚àó). Then we can
define ùëìùëñalso as follows
ùëìùëñ(ùë•) =
‚éß
{
‚é®
{
‚é©
ùëè
ùë•= ùë•‚àó
ùëìùëñ(ùë•)
ùë•‚â†ùë•‚àó
(5.6)
or in other words
ùëìùëñ(ùë•) = ùëìùëñ‚àí1(ùë•) ‚àßEQUAL(ùë•‚àó, ùë•) ‚à®ùëè‚àß¬¨EQUAL(ùë•‚àó, ùë•)
(5.7)
where EQUAL ‚à∂{0, 1}2ùëõ‚Üí{0, 1} is the function that maps ùë•, ùë•‚Ä≤ ‚àà
{0, 1}ùëõto 1 if they are equal and to 0 otherwise. Since (by our choice
of ùëñ), ùëìùëñ‚àí1 can be computed using at most ùë†gates and (as can be easily
verified) that EQUAL ‚ààSIZEùëõ(9ùëõ), we can compute ùëìùëñusing at most
ùë†+ 9ùëõ+ ùëÇ(1) ‚â§ùë†+ 10ùëõgates which is what we wanted to prove.
‚ñ†
Figure 5.5: An illustration of some of what we know
about the size complexity classes (not to scale!). This
figure depicts classes of the form SIZEùëõ,ùëõ(ùë†) but the
state of affairs for other size complexity classes such
as SIZEùëõ,1(ùë†) is similar. We know by Theorem 4.12
(with the improvement of Section 4.4.2) that all
functions mapping ùëõbits to ùëõbits can be computed
by a circuit of size ùëê‚ãÖ2ùëõfor ùëê‚â§10, while on the
other hand the counting lower bound (Theorem 5.3,
see also Exercise 5.4) shows that some such functions
will require 0.1 ‚ãÖ2ùëõ, and the size hierarchy theorem
(Theorem 5.5) shows the existence of functions in
SIZE(ùëÜ) ‚ßµSIZE(ùë†) whenever ùë†= ùëú(ùëÜ), see also
Exercise 5.5. We also consider some specific examples:
addition of two ùëõ/2 bit numbers can be done in ùëÇ(ùëõ)
lines, while we don‚Äôt know of such a program for
multiplying two ùëõbit numbers, though we do know
it can be done in ùëÇ(ùëõ2) and in fact even better size.
In the above, FACTORùëõcorresponds to the inverse
problem of multiplying- finding the prime factorization
of a given number. At the moment we do not know
of any circuit a polynomial (or even sub-exponential)
number of lines that can compute FACTORùëõ.
R
Remark 5.6 ‚Äî Explicit functions. While the size hierar-
chy theorem guarantees that there exists some function
that can be computed using, for example, ùëõ2 gates, but
not using 100ùëõgates, we do not know of any explicit
example of such a function. While we suspect that
integer multiplication is such an example, we do not
have any proof that this is the case.


--- Page 192 ---

192
introduction to theoretical computer science
5.3 THE TUPLES REPRESENTATION
ASCII is a fine presentation of programs, but for some applications
it is useful to have a more concrete representation of NAND-CIRC
programs. In this section we describe a particular choice, that will
be convenient for us later on. A NAND-CIRC program is simply a
sequence of lines of the form
blah = NAND(baz,boo)
There is of course nothing special about the particular names we
use for variables. Although they would be harder to read, we could
write all our programs using only working variables such as temp_0,
temp_1 etc. Therefore, our representation for NAND-CIRC programs
ignores the actual names of the variables, and just associate a number
with each variable. We encode a line of the program as a triple of
numbers. If the line has the form foo = NAND(bar,blah) then we
encode it with the triple (ùëñ, ùëó, ùëò) where ùëñis the number corresponding
to the variable foo and ùëóand ùëòare the numbers corresponding to bar
and blah respectively.
More concretely, we will associate every variable with a number
in the set [ùë°] = {0, 1, ‚Ä¶ , ùë°‚àí1}. The first ùëõnumbers {0, ‚Ä¶ , ùëõ‚àí1}
correspond to the input variables, the last ùëönumbers {ùë°‚àíùëö, ‚Ä¶ , ùë°‚àí1}
correspond to the output variables, and the intermediate numbers
{ùëõ, ‚Ä¶ , ùë°‚àíùëö‚àí1} correspond to the remaining ‚Äúworkspace‚Äù variables.
Formally, we define our representation as follows:
Definition 5.7 ‚Äî List of tuples representation. Let ùëÉbe a NAND-CIRC
program of ùëõinputs, ùëöoutputs, and ùë†lines, and let ùë°be the num-
ber of distinct variables used by ùëÉ. The list of tuples representation
of ùëÉis the triple (ùëõ, ùëö, ùêø) where ùêøis a list of triples of the form
(ùëñ, ùëó, ùëò) for ùëñ, ùëó, ùëò‚àà[ùë°].
We assign a number for variable of ùëÉas follows:
‚Ä¢ For every ùëñ‚àà[ùëõ], the variable X[ùëñ] is assigned the number ùëñ.
‚Ä¢ For every ùëó
‚àà
[ùëö], the variable Y[ùëó] is assigned the number
ùë°‚àíùëö+ ùëó.
‚Ä¢ Every other variable is assigned a number in {ùëõ, ùëõ+1, ‚Ä¶ , ùë°‚àíùëö‚àí
1} in the order in which the variable appears in the program ùëÉ.
The list of tuples representation is our default choice for represent-
ing NAND-CIRC programs. Since ‚Äúlist of tuples representation‚Äù is a
bit of a mouthful, we will often call it simply ‚Äúthe representation‚Äù for
a program ùëÉ. Sometimes, when the number ùëõof inputs and number


--- Page 193 ---

code as data, data as code
193
4 If you‚Äôre curious what these few lines are, see our
GitHub repository.
ùëöof outputs are known from the context, we will simply represent a
program as the list ùêøinstead of the triple (ùëõ, ùëö, ùêø).
‚ñ†Example 5.8 ‚Äî Representing the XOR program. Our favorite NAND-
CIRC program, the program
u = NAND(X[0],X[1])
v = NAND(X[0],u)
w = NAND(X[1],u)
Y[0] = NAND(v,w)
computing the XOR function is represented as the tuple (2, 1, ùêø)
where ùêø= ((2, 0, 1), (3, 0, 2), (4, 1, 2), (5, 3, 4)). That is, the variables
X[0] and X[1] are given the indices 0 and 1 respectively, the vari-
ables u,v,w are given the indices 2, 3, 4 respectively, and the variable
Y[0] is given the index 5.
Transforming a NAND-CIRC program from its representation as
code to the representation as a list of tuples is a fairly straightforward
programming exercise, and in particular can be done in a few lines of
Python.4 The list-of-tuples representation loses information such as the
particular names we used for the variables, but this is OK since these
names do not make a difference to the functionality of the program.
5.3.1 From tuples to strings
If ùëÉis a program of size ùë†, then the number ùë°of variables is at most 3ùë†
(as every line touches at most three variables). Hence we can encode
every variable index in [ùë°] as a string of length ‚Ñì= ‚åàlog(3ùë†)‚åâ, by adding
leading zeroes as needed. Since this is a fixed-length encoding, it is
prefix free, and so we can encode the list ùêøof ùë†triples (corresponding
to the encoding of the ùë†lines of the program) as simply the string of
length 3‚Ñìùë†obtained by concatenating all of these encodings.
We define ùëÜ(ùë†) to be the length of the string representing the list ùêø
corresponding to a size ùë†program. By the above we see that
ùëÜ(ùë†) = 3ùë†‚åàlog(3ùë†)‚åâ.
(5.8)
We can represent ùëÉ= (ùëõ, ùëö, ùêø) as a string by prepending a prefix
free representation of ùëõand ùëöto the list ùêø. Since ùëõ, ùëö‚â§3ùë†(a pro-
gram must touch at least once all its input and output variables), those
prefix free representations can be encoded using strings of length
ùëÇ(log ùë†). In particular, every program ùëÉof at most ùë†lines can be rep-
resented by a string of length ùëÇ(ùë†log ùë†). Similarly, every circuit ùê∂of
at most ùë†gates can be represented by a string of length ùëÇ(ùë†log ùë†) (for
example by translating ùê∂to the equivalent program ùëÉ).


--- Page 194 ---

194
introduction to theoretical computer science
5.4 A NAND-CIRC INTERPRETER IN NAND-CIRC
Since we can represent programs as strings, we can also think of a
program as an input to a function. In particular, for every natural
number ùë†, ùëõ, ùëö> 0 we define the function EVALùë†,ùëõ,ùëö‚à∂{0, 1}ùëÜ(ùë†)+ùëõ‚Üí
{0, 1}ùëöas follows:
EVALùë†,ùëõ,ùëö(ùëùùë•) =
‚éß
{
‚é®
{
‚é©
ùëÉ(ùë•)
ùëù‚àà{0, 1}ùëÜ(ùë†) represents a size-ùë†program ùëÉwith ùëõinputs and ùëöoutputs
0ùëö
otherwise
(5.9)
where ùëÜ(ùë†) is defined as in (5.8) and we use the concrete representa-
tion scheme described in Section 5.1.
That is, EVALùë†,ùëõ,ùëötakes as input the concatenation of two strings:
a string ùëù‚àà{0, 1}ùëÜ(ùë†) and a string ùë•‚àà{0, 1}ùëõ. If ùëùis a string that
represents a list of triples ùêøsuch that (ùëõ, ùëö, ùêø) is a list-of-tuples rep-
resentation of a size-ùë†NAND-CIRC program ùëÉ, then EVALùë†,ùëõ,ùëö(ùëùùë•)
is equal to the evaluation ùëÉ(ùë•) of the program ùëÉon the input ùë•. Oth-
erwise, EVALùë†,ùëõ,ùëö(ùëùùë•) equals 0ùëö(this case is not very important: you
can simply think of 0ùëöas some ‚Äújunk value‚Äù that indicates an error).
Take-away points.
The fine details of EVALùë†,ùëõ,ùëö‚Äôs definition are not
very crucial. Rather, what you need to remember about EVALùë†,ùëõ,ùëöis
that:
‚Ä¢ EVALùë†,ùëõ,ùëöis a finite function taking a string of fixed length as input
and outputting a string of fixed length as output.
‚Ä¢ EVALùë†,ùëõ,ùëöis a single function, such that computing EVALùë†,ùëõ,ùëö
allows to evaluate arbitrary NAND-CIRC programs of a certain
length on arbitrary inputs of the appropriate length.
‚Ä¢ EVALùë†,ùëõ,ùëöis a function, not a program (recall the discussion in Sec-
tion 3.6.2). That is, EVALùë†,ùëõ,ùëöis a specification of what output is as-
sociated with what input. The existence of a program that computes
EVALùë†,ùëõ,ùëö(i.e., an implementation for EVALùë†,ùëõ,ùëö) is a separate
fact, which needs to be established (and which we will do in Theo-
rem 5.9, with a more efficient program shown in Theorem 5.10).
One of the first examples of self circularity we will see in this book is
the following theorem, which we can think of as showing a ‚ÄúNAND-
CIRC interpreter in NAND-CIRC‚Äù:
Theorem 5.9 ‚Äî Bounded Universality of NAND-CIRC programs. For every
ùë†, ùëõ, ùëö‚àà‚Ñïwith ùë†‚â•ùëöthere is a NAND-CIRC program ùëàùë†,ùëõ,ùëöthat
computes the function EVALùë†,ùëõ,ùëö.
That is, the NAND-CIRC program ùëàùë†,ùëõ,ùëötakes the description
of any other NAND-CIRC program ùëÉ(of the right length and input-


--- Page 195 ---

code as data, data as code
195
Figure 5.6: A universal circuit ùëàis a circuit that gets as
input the description of an arbitrary (smaller) circuit
ùëÉas a binary string, and an input ùë•, and outputs the
string ùëÉ(ùë•) which is the evaluation of ùëÉon ùë•. We can
also think of ùëàas a straight-line program that gets as
input the code of a straight-line program ùëÉand an
input ùë•, and outputs ùëÉ(ùë•).
s/outputs) and any input ùë•, and computes the result of evaluating the
program ùëÉon the input ùë•. Given the equivalence between NAND-
CIRC programs and Boolean circuits, we can also think of ùëàùë†,ùëõ,ùëöas
a circuit that takes as input the description of other circuits and their
inputs, and returns their evaluation, see Fig. 5.6. We call this NAND-
CIRC program ùëàùë†,ùëõ,ùëöthat computes EVALùë†,ùëõ,ùëöa bounded universal
program (or a universal circuit, see Fig. 5.6). ‚ÄúUniversal‚Äù stands for
the fact that this is a single program that can evaluate arbitrary code,
where ‚Äúbounded‚Äù stands for the fact that ùëàùë†,ùëõ,ùëöonly evaluates pro-
grams of bounded size. Of course this limitation is inherent for the
NAND-CIRC programming language, since a program of ùë†lines (or,
equivalently, a circuit of ùë†gates) can take at most 2ùë†inputs. Later, in
Chapter 7, we will introduce the concept of loops (and the model of
Turing Machines), that allow to escape this limitation.
Proof. Theorem 5.9 is an important result, but it is actually not hard to
prove. Specifically, since EVALùë†,ùëõ,ùëöis a finite function Theorem 5.9 is
an immediate corollary of Theorem 4.12, which states that every finite
function can be computed by some NAND-CIRC program.
‚ñ†
P
Theorem 5.9 is simple but important. Make sure you
understand what this theorem means, and why it is a
corollary of Theorem 4.12.
5.4.1 Efficient universal programs
Theorem 5.9 establishes the existence of a NAND-CIRC program
for computing EVALùë†,ùëõ,ùëö, but it provides no explicit bound on the
size of this program. Theorem 4.12, which we used to prove Theo-
rem 5.9, guarantees the existence of a NAND-CIRC program whose
size can be as large as exponential in the length of its input. This would
mean that even for moderately small values of ùë†, ùëõ, ùëö(for example
ùëõ= 100, ùë†= 300, ùëö= 1), computing EVALùë†,ùëõ,ùëömight require a
NAND program with more lines than there are atoms in the observ-
able universe! Fortunately, we can do much better than that. In fact,
for every ùë†, ùëõ, ùëöthere exists a NAND-CIRC program for comput-
ing EVALùë†,ùëõ,ùëöwith size that is polynomial in its input length. This is
shown in the following theorem.
Theorem 5.10 ‚Äî Efficient bounded universality of NAND-CIRC programs.
For every ùë†, ùëõ, ùëö
‚àà
‚Ñïthere is a NAND-CIRC program of at
most ùëÇ(ùë†2 log ùë†) lines that computes the function EVALùë†,ùëõ,ùëö
‚à∂


--- Page 196 ---

196
introduction to theoretical computer science
{0, 1}ùëÜ+ùëõ‚Üí{0, 1}ùëödefined above (where ùëÜis the number of bits
needed to represent programs of ùë†lines).
P
If you haven‚Äôt done so already, now might be a good
time to review ùëÇnotation in Section 1.4.8. In particu-
lar, an equivalent way to state Theorem 5.10 is that it
says that there exists some number ùëê> 0 such that for
every ùë†, ùëõ, ùëö‚àà‚Ñï, there exists a NAND-CIRC program
ùëÉof at most ùëêùë†2 log ùë†lines that computes the function
EVALùë†,ùëõ,ùëö.
Unlike Theorem 5.9, Theorem 5.10 is not a trivial corollary of the
fact that every finite function can be computed by some circuit. Prov-
ing Theorem 5.9 requires us to present a concrete NAND-CIRC pro-
gram for computing the function EVALùë†,ùëõ,ùëö. We will do so in several
stages.
1. First, we will describe the algorithm to evaluate EVALùë†,ùëõ,ùëöin
‚Äúpseudo code‚Äù.
2. Then, we will show how we can write a program to compute
EVALùë†,ùëõ,ùëöin Python. We will not use much about Python, and
a reader that has familiarity with programming in any language
should be able to follow along.
3. Finally, we will show how we can transform this Python program
into a NAND-CIRC program.
This approach yields much more than just proving Theorem 5.10:
we will see that it is in fact always possible to transform (loop free)
code in high level languages such as Python to NAND-CIRC pro-
grams (and hence to Boolean circuits as well).
5.4.2 A NAND-CIRC interpeter in ‚Äúpseudocode‚Äù
To prove Theorem 5.10 it suffices to give a NAND-CIRC program of
ùëÇ(ùë†2 log ùë†) lines that can evaluate NAND-CIRC programs of ùë†lines.
Let us start by thinking how we would evaluate such programs if we
weren‚Äôt restricted to only performing NAND operations. That is, let us
describe informally an algorithm that on input ùëõ, ùëö, ùë†, a list of triples
ùêø, and a string ùë•‚àà{0, 1}ùëõ, evaluates the program represented by
(ùëõ, ùëö, ùêø) on the string ùë•.
P
It would be highly worthwhile for you to stop here
and try to solve this problem yourself. For example,
you can try thinking how you would write a program


--- Page 197 ---

code as data, data as code
197
NANDEVAL(n,m,s,L,x) that computes this function in
the programming language of your choice.
We will now describe such an algorithm. We assume that we have
access to a bit array data structure that can store for every ùëñ‚àà[ùë°] a
bit ùëáùëñ‚àà{0, 1}. Specifically, if Table is a variable holding this data
structure, then we assume we can perform the operations:
‚Ä¢ GET(Table,i) which retrieves the bit corresponding to i in Table.
The value of i is assumed to be an integer in [ùë°].
‚Ä¢ Table = UPDATE(Table,i,b) which updates Table so the the bit
corresponding to i is now set to b. The value of i is assumed to be
an integer in [ùë°] and b is a bit in {0, 1}.
Algorithm 5.11 ‚Äî Eval NAND-CIRC programs.
Input: Numbers ùëõ, ùëö, ùë†and ùë°
‚â§
3ùë†, as well as a list ùêøof ùë†
triples of numbers in [ùë°], and a string ùë•‚àà{0, 1}ùëõ.
Output: Evaluation of the program represented by
(ùëõ, ùëö, ùêø) on the
Input: ùë•‚àà{0, 1}ùëõ.
1: Let Vartable be table of size ùë°
2: for ùëñin [ùëõ] do
3:
Vartable = UPDATE(Vartable,ùëñ,ùë•ùëñ)
4: end for
5: for (ùëñ, ùëó, ùëò) in ùêødo
6:
ùëé‚ÜêGET(Vartable,ùëó)
7:
ùëè‚ÜêGET(Vartable,ùëò)
8:
Vartable = UPDATE(Vartable,ùëñ,NAND(ùëé,ùëè))
9: end for
10: for ùëóin [ùëö] do
11:
ùë¶ùëó‚ÜêGET(Vartable,ùë°‚àíùëö+ ùëó)
12: end for
13: return ùë¶0, ‚Ä¶ , ùë¶ùëö‚àí1
Algorithm 5.11 evaluates the program given to it as input one line
at a time, updating the Vartable table to contain the value of each
variable. At the end of the execution it outputs the variables at posi-
tions ùë°‚àíùëö, ùë°‚àíùëö+ 1, ‚Ä¶ , ùë°‚àí1 which correspond to the input variables.
5.4.3 A NAND interpreter in Python
To make things more concrete, let us see how we implement Algo-
rithm 5.11 in the Python programming language. (There is nothing
special about Python. We could have easily presented a corresponding


--- Page 198 ---

198
introduction to theoretical computer science
5 Python does not distinguish between lists and
arrays, but allows constant time random access to an
indexed elements to both of them. One could argue
that if we allowed programs of truly unbounded
length (e.g., larger than 264) then the price would
not be constant but logarithmic in the length of the
array/lists, but the difference between ùëÇ(ùë†) and
ùëÇ(ùë†log ùë†) will not be important for our discussions.
function in JavaScript, C, OCaml, or any other programming lan-
guage.) We will construct a function NANDEVAL that on input ùëõ, ùëö, ùêø, ùë•
will output the result of evaluating the program represented by
(ùëõ, ùëö, ùêø) on ùë•. To keep things simple, we will not worry about the case
that ùêødoes not represent a valid program of ùëõinputs and ùëöoutputs.
The code is presented in Fig. 5.7.
Accessing an element of the array Vartable at a given index takes
a constant number of basic operations. Hence (since ùëõ, ùëö‚â§ùë†and
ùë°‚â§3ùë†), the program above will use ùëÇ(ùë†) basic operations.5
5.4.4 Constructing the NAND-CIRC interpreter in NAND-CIRC
We now turn to describing the proof of Theorem 5.10. To prove the
theorem it is not enough to give a Python program. Rather, we need to
show how we compute the function EVALùë†,ùëõ,ùëöusing a NAND-CIRC
program. In other words, our job is to transform, for every ùë†, ùëõ, ùëö, the
Python code of Section 5.4.3 to a NAND-CIRC program ùëàùë†,ùëõ,ùëöthat
computes the function EVALùë†,ùëõ,ùëö.
P
Before reading further, try to think how you could give
a ‚Äúconstructive proof‚Äù of Theorem 5.10. That is, think
of how you would write, in the programming lan-
guage of your choice, a function universal(s,n,m)
that on input ùë†, ùëõ, ùëöoutputs the code for the NAND-
CIRC program ùëàùë†,ùëõ,ùëösuch that ùëàùë†,ùëõ,ùëöcomputes
EVALùë†,ùëõ,ùëö. There is a subtle but crucial difference
between this function and the Python NANDEVAL pro-
gram described above. Rather than actually evaluating
a given program ùëÉon some input ùë§, the function
universal should output the code of a NAND-CIRC
program that computes the map (ùëÉ, ùë•) ‚Ü¶ùëÉ(ùë•).
Our construction will follow very closely the Python implementa-
tion of EVAL above. We will use variables Vartable[0],‚Ä¶,Vartable[2‚Ñì‚àí
1], where ‚Ñì= ‚åàlog 3ùë†‚åâto store our variables. However, NAND doesn‚Äôt
have integer-valued variables, so we cannot write code such as
Vartable[i] for some variable i. However, we can implement the
function GET(Vartable,i) that outputs the i-th bit of the array
Vartable. Indeed, this is nothing but the function LOOKUP‚Ñìthat we
have seen in Theorem 4.10!
P
Please make sure that you understand why GET and
LOOKUP‚Ñìare the same function.
We saw that we can compute LOOKUP‚Ñìin time ùëÇ(2‚Ñì) = ùëÇ(ùë†) for
our choice of ‚Ñì.


--- Page 199 ---

code as data, data as code
199
Figure 5.7: Code for evaluating a NAND-CIRC program given in the list-of-tuples representation
def NANDEVAL(n,m,L,X):
# Evaluate a NAND-CIRC program from list of tuple representation.
s = len(L) # num of lines
t = max(max(a,b,c) for (a,b,c) in L)+1 # max index in L + 1
Vartable = [0] * t # initialize array
# helper functions
def GET(V,i): return V[i]
def UPDATE(V,i,b):
V[i]=b
return V
# load input values to Vartable:
for i in range(n):
Vartable = UPDATE(Vartable,i,X[i])
# Run the program
for (i,j,k) in L:
a = GET(Vartable,j)
b = GET(Vartable,k)
c = NAND(a,b)
Vartable = UPDATE(Vartable,i,c)
# Return outputs Vartable[t-m], Vartable[t-m+1],....,Vartable[t-1]
return [GET(Vartable,t-m+j) for j in range(m)]
# Test on XOR (2 inputs, 1 output)
L = ((2, 0, 1), (3, 0, 2), (4, 1, 2), (5, 3, 4))
print(NANDEVAL(2,1,L,(0,1))) # XOR(0,1)
# [1]
print(NANDEVAL(2,1,L,(1,1))) # XOR(1,1)
# [0]


--- Page 200 ---

200
introduction to theoretical computer science
For every ‚Ñì, let UPDATE‚Ñì‚à∂{0, 1}2‚Ñì+‚Ñì+1 ‚Üí{0, 1}2‚Ñìcorrespond to the
UPDATE function for arrays of length 2‚Ñì. That is, on input ùëâ‚àà{0, 1}2‚Ñì,
ùëñ‚àà{0, 1}‚Ñì, ùëè‚àà{0, 1}, UPDATE‚Ñì(ùëâ, ùëè, ùëñ) is equal to ùëâ‚Ä≤ ‚àà{0, 1}2‚Ñìsuch
that
ùëâ‚Ä≤
ùëó=
‚éß
{
‚é®
{
‚é©
ùëâùëó
ùëó‚â†ùëñ
ùëè
ùëó= 1
(5.10)
where we identify the string ùëñ‚àà{0, 1}‚Ñìwith a number in {0, ‚Ä¶ , 2‚Ñì‚àí1}
using the binary representation. We can compute UPDATE‚Ñìusing an
ùëÇ(2‚Ñì‚Ñì) = (ùë†log ùë†) line NAND-CIRC program as as follows:
1. For every ùëó‚àà[2‚Ñì], there is an ùëÇ(‚Ñì) line NAND-CIRC program to
compute the function EQUALSùëó‚à∂{0, 1}‚Ñì‚Üí{0, 1} that on input ùëñ
outputs 1 if and only if ùëñis equal to (the binary representation of) ùëó.
(We leave verifying this as Exercise 5.2 and Exercise 5.3.)
2. We have seen that we can compute the function IF ‚à∂{0, 1}3 ‚Üí{0, 1}
such that IF(ùëé, ùëè, ùëê) equals ùëèif ùëé= 1 and ùëêif ùëé= 0.
Together, this means that we can compute UPDATE (using some
‚Äúsyntactic sugar‚Äù for bounded length loops) as follows:
def UPDATE_ell(V,i,b):
# Get V[0]...V[2^ell-1], i in {0,1}^ell, b in {0,1}
# Return NewV[0],...,NewV[2^ell-1]
# updated array with NewV[i]=b and all
# else same as V
for j in range(2**ell): # j = 0,1,2,....,2^ell -1
a = EQUALS_j(i)
NewV[j] = IF(a,b,V[j])
return NewV
Since the loop over j in UPDATE is run 2‚Ñìtimes, and computing
EQUALS_j takes ùëÇ(‚Ñì) lines, the total number of lines to compute UP-
DATE is ùëÇ(2‚Ñì‚ãÖ‚Ñì) = ùëÇ(ùë†log ùë†). Once we can compute GET and UPDATE,
the rest of the implementation amounts to ‚Äúbook keeping‚Äù that needs
to be done carefully, but is not too insightful, and hence we omit the
full details. Since we run GET and UPDATE ùë†times, the total number of
lines for computing EVALùë†,ùëõ,ùëöis ùëÇ(ùë†2) + ùëÇ(ùë†2 log ùë†) = ùëÇ(ùë†2 log ùë†).
This completes (up to the omitted details) the proof of Theorem 5.10.
R
Remark 5.12 ‚Äî Improving to quasilinear overhead (ad-
vanced optional note). The NAND-CIRC program
above is less efficient than its Python counterpart,
since NAND does not offer arrays with efficient ran-
dom access. Hence for example the LOOKUP operation


--- Page 201 ---

code as data, data as code
201
6 ARM stands for ‚ÄúAdvanced RISC Machine‚Äù where
RISC in turn stands for ‚ÄúReduced instruction set
computer‚Äù.
on an array of ùë†bits takes Œ©(ùë†) lines in NAND even
though it takes ùëÇ(1) steps (or maybe ùëÇ(log ùë†) steps,
depending on how we count) in Python.
It turns out that it is possible to improve the bound
of Theorem 5.10, and evaluate ùë†line NAND-CIRC
programs using a NAND-CIRC program of ùëÇ(ùë†log ùë†)
lines. The key is to consider the description of NAND-
CIRC programs as circuits, and in particular as di-
rected acyclic graphs (DAGs) of bounded in-degree.
A universal NAND-CIRC program ùëàùë†for ùë†line pro-
grams will correspond to a universal graph ùêªùë†for such
ùë†vertex DAGs. We can think of such a graph ùëàùë†as
fixed ‚Äúwiring‚Äù for a communication network, that
should be able to accommodate any arbitrary pattern
of communication between ùë†vertices (where this pat-
tern corresponds to an ùë†line NAND-CIRC program).
It turns out that such efficient routing networks exist
that allow embedding any ùë†vertex circuit inside a uni-
versal graph of size ùëÇ(ùë†log ùë†), see the bibliographical
notes Section 5.9 for more on this issue.
5.5 A PYTHON INTERPRETER IN NAND-CIRC (DISCUSSION)
To prove Theorem 5.10 we essentially translated every line of the
Python program for EVAL into an equivalent NAND-CIRC snip-
pet. However none of our reasoning was specific to the particu-
lar function EVAL. It is possible to translate every Python program
into an equivalent NAND-CIRC program of comparable efficiency.
(More concretely, if the Python program takes ùëá(ùëõ) operations on
inputs of length at most ùëõthen there exists NAND-CIRC program of
ùëÇ(ùëá(ùëõ) log ùëá(ùëõ)) lines that agrees with the Python program on inputs
of length ùëõ.) Actually doing so requires taking care of many details
and is beyond the scope of this book, but let me try to convince you
why you should believe it is possible in principle.
For starters, one can use CPython (the reference implementation
for Python), to evaluate every Python program using a C program. We
can combine this with a C compiler to transform a Python program
to various flavors of ‚Äúmachine language‚Äù. So, to transform a Python
program into an equivalent NAND-CIRC program, it is enough to
show how to transform a machine language program into an equiva-
lent NAND-CIRC program. One minimalistic (and hence convenient)
family of machine languages is known as the ARM architecture which
powers many mobile devices including essentially all Android de-
vices.6 There are even simpler machine languages, such as the LEG
architecture for which a backend for the LLVM compiler was imple-
mented (and hence can be the target of compiling any of the large
and growing list of languages that this compiler supports). Other ex-


--- Page 202 ---

202
introduction to theoretical computer science
amples include the TinyRAM architecture (motivated by interactive
proof systems that we will discuss in Chapter 22) and the teaching-
oriented Ridiculously Simple Computer architecture. Going one by
one over the instruction sets of such computers and translating them
to NAND snippets is no fun, but it is a feasible thing to do. In fact,
ultimately this is very similar to the transformation that takes place
in converting our high level code to actual silicon gates that are not
so different from the operations of a NAND-CIRC program. Indeed,
tools such as MyHDL that transform ‚ÄúPython to Silicon‚Äù can be used
to convert a Python program to a NAND-CIRC program.
The NAND-CIRC programming language is just a teaching tool,
and by no means do I suggest that writing NAND-CIRC programs, or
compilers to NAND-CIRC, is a practical, useful, or enjoyable activity.
What I do want is to make sure you understand why it can be done,
and to have the confidence that if your life (or at least your grade)
depended on it, then you would be able to do this. Understanding
how programs in high level languages such as Python are eventually
transformed into concrete low-level representation such as NAND is
fundamental to computer science.
The astute reader might notice that the above paragraphs only
outlined why it should be possible to find for every particular Python-
computable function ùëì, a particular comparably efficient NAND-CIRC
program ùëÉthat computes ùëì. But this still seems to fall short of our
goal of writing a ‚ÄúPython interpreter in NAND‚Äù which would mean
that for every parameter ùëõ, we come up with a single NAND-CIRC
program UNIVùë†such that given a description of a Python program
ùëÉ, a particular input ùë•, and a bound ùëáon the number of operations
(where the lengths of ùëÉand ùë•and the value of ùëáare all at most ùë†)
returns the result of executing ùëÉon ùë•for at most ùëásteps. After all,
the transformation above takes every Python program into a different
NAND-CIRC program, and so does not yield ‚Äúone NAND-CIRC pro-
gram to rule them all‚Äù that can evaluate every Python program up to
some given complexity. However, we can in fact obtain one NAND-
CIRC program to evaluate arbitrary Python programs. The reason is
that there exists a Python interpreter in Python: a Python program ùëà
that takes a bit string, interprets it as Python code, and then runs that
code. Hence, we only need to show a NAND-CIRC program ùëà‚àóthat
computes the same function as the particular Python program ùëà, and
this will give us a way to evaluate all Python programs.
What we are seeing time and again is the notion of universality or
self reference of computation, which is the sense that all reasonably rich
models of computation are expressive enough that they can ‚Äúsimulate
themselves‚Äù. The importance of this phenomenon to both the theory
and practice of computing, as well as far beyond it, including the


--- Page 203 ---

code as data, data as code
203
foundations of mathematics and basic questions in science, cannot be
overstated.
5.6 THE PHYSICAL EXTENDED CHURCH-TURING THESIS (DISCUS-
SION)
We‚Äôve seen that NAND gates (and other Boolean operations) can be
implemented using very different systems in the physical world. What
about the reverse direction? Can NAND-CIRC programs simulate any
physical computer?
We can take a leap of faith and stipulate that Boolean circuits (or
equivalently NAND-CIRC programs) do actually encapsulate every
computation that we can think of. Such a statement (in the realm of
infinite functions, which we‚Äôll encounter in Chapter 7) is typically
attributed to Alonzo Church and Alan Turing, and in that context
is known as the Church Turing Thesis. As we will discuss in future
lectures, the Church-Turing Thesis is not a mathematical theorem or
conjecture. Rather, like theories in physics, the Church-Turing Thesis
is about mathematically modeling the real world. In the context of
finite functions, we can make the following informal hypothesis or
prediction:
‚ÄúPhysical Extended Church-Turing Thesis‚Äù (PECTT): If a function
ùêπ‚à∂{0, 1}ùëõ‚Üí{0, 1}ùëöcan be computed in the physical world using ùë†amount
of ‚Äúphysical resources‚Äù then it can be computed by a Boolean circuit program of
roughly ùë†gates.
A priori it might seem rather extreme to hypothesize that our mea-
ger model of NAND-CIRC programs or Boolean circuits captures all
possible physical computation. But yet, in more than a century of
computing technologies, no one has yet built any scalable computing
device that challenges this hypothesis.
We now discuss the ‚Äúfine print‚Äù of the PECTT in more detail, as
well as the (so far unsuccessful) challenges that have been raised
against it. There is no single universally-agreed-upon formalization
of ‚Äúroughly ùë†physical resources‚Äù, but we can approximate this notion
by considering the size of any physical computing device and the
time it takes to compute the output, and ask that any such device can
be simulated by a Boolean circuit with a number of gates that is a
polynomial (with not too large exponent) in the size of the system and
the time it takes it to operate.
In other words, we can phrase the PECTT as stipulating that any
function that can be computed by a device that takes a certain volume
ùëâof space and requires ùë°time to complete the computation, must be
computable by a Boolean circuit with a number of gates ùëù(ùëâ, ùë°) that is
polynomial in ùëâand ùë°.


--- Page 204 ---

204
introduction to theoretical computer science
The exact form of the function ùëù(ùëâ, ùë°) is not universally agreed
upon but it is generally accepted that if ùëì‚à∂{0, 1}ùëõ‚Üí{0, 1} is an
exponentially hard function, in the sense that it has no NAND-CIRC
program of fewer than, say, 2ùëõ/2 lines, then a demonstration of a phys-
ical device that can compute in the real world ùëìfor moderate input
lengths (e.g., ùëõ= 500) would be a violation of the PECTT.
R
Remark 5.13 ‚Äî Advanced note: making PECTT concrete
(advanced, optional). We can attempt a more exact
phrasing of the PECTT as follows. Suppose that ùëçis
a physical system that accepts ùëõbinary stimuli and
has a binary output, and can be enclosed in a sphere
of volume ùëâ. We say that the system ùëçcomputes a
function ùëì‚à∂{0, 1}ùëõ‚Üí{0, 1} within ùë°seconds if when-
ever we set the stimuli to some value ùë•
‚àà
{0, 1}ùëõ, if
we measure the output after ùë°seconds then we obtain
ùëì(ùë•).
One can then phrase the PECTT as stipulating that if
there exists such a system ùëçthat computes ùêπwithin
ùë°seconds, then there exists a NAND-CIRC program
that computes ùêπand has at most ùõº(ùëâùë°)2 lines, where
ùõºis some normalization constant. (We can also con-
sider variants where we use surface area instead
of volume, or take (ùëâùë°) to a different power than 2.
However, none of these choices makes a qualitative
difference to the discussion below.) In particular,
suppose that ùëì
‚à∂
{0, 1}ùëõ
‚Üí
{0, 1} is a function that
requires 2ùëõ/(100ùëõ) > 20.8ùëõlines for any NAND-CIRC
program (such a function exists by Theorem 5.3).
Then the PECTT would imply that either the volume
or the time of a system that computes ùêπwill have to
be at least 20.2ùëõ/‚àöùõº. Since this quantity grows expo-
nentially in ùëõ, it is not hard to set parameters so that
even for moderately large values of ùëõ, such a system
could not fit in our universe.
To fully make the PECTT concrete, we need to decide
on the units for measuring time and volume, and the
normalization constant ùõº. One conservative choice is
to assume that we could squeeze computation to the
absolute physical limits (which are many orders of
magnitude beyond current technology). This corre-
sponds to setting ùõº
=
1 and using the Planck units
for volume and time. The Planck length ‚ÑìùëÉ(which is,
roughly speaking, the shortest distance that can the-
oretically be measured) is roughly 2‚àí120 meters. The
Planck time ùë°ùëÉ(which is the time it takes for light to
travel one Planck length) is about 2‚àí150 seconds. In the
above setting, if a function ùêπtakes, say, 1KB of input
(e.g., roughly 104 bits, which can encode a 100 by 100
bitmap image), and requires at least 20.8ùëõ
=
20.8‚ãÖ104
NAND lines to compute, then any physical system
that computes it would require either volume of


--- Page 205 ---

code as data, data as code
205
20.2‚ãÖ104 Planck length cubed, which is more than 21500
meters cubed or take at least 20.2‚ãÖ104 Planck Time units,
which is larger than 21500 seconds. To get a sense of
how big that number is, note that the universe is only
about 260 seconds old, and its observable radius is
only roughly 290 meters. The above discussion sug-
gests that it is possible to empirically falsify the PECTT
by presenting a smaller-than-universe-size system that
computes such a function.
There are of course several hurdles to refuting the
PECTT in this way, one of which is that we can‚Äôt actu-
ally test the system on all possible inputs. However,
it turns out that we can get around this issue using
notions such as interactive proofs and program checking
that we might encounter later in this book. Another,
perhaps more salient problem, is that while we know
many hard functions exist, at the moment there is no
single explicit function ùêπ
‚à∂{0, 1}ùëõ‚Üí{0, 1} for which
we can prove an ùúî(ùëõ) (let alone Œ©(2ùëõ/ùëõ)) lower bound
on the number of lines that a NAND-CIRC program
needs to compute it.
5.6.1 Attempts at refuting the PECTT
One of the admirable traits of mankind is the refusal to accept limita-
tions. In the best case this is manifested by people achieving long-
standing ‚Äúimpossible‚Äù challenges such as heavier-than-air flight,
putting a person on the moon, circumnavigating the globe, or even
resolving Fermat‚Äôs Last Theorem. In the worst case it is manifested by
people continually following the footsteps of previous failures to try to
do proven-impossible tasks such as build a perpetual motion machine,
trisect an angle with a compass and straightedge, or refute Bell‚Äôs in-
equality. The Physical Extended Church Turing thesis (in its various
forms) has attracted both types of people. Here are some physical
devices that have been speculated to achieve computational tasks that
cannot be done by not-too-large NAND-CIRC programs:
‚Ä¢ Spaghetti sort: One of the first lower bounds that Computer Sci-
ence students encounter is that sorting ùëõnumbers requires making
Œ©(ùëõlog ùëõ) comparisons. The ‚Äúspaghetti sort‚Äù is a description of a
proposed ‚Äúmechanical computer‚Äù that would do this faster. The
idea is that to sort ùëõnumbers ùë•1, ‚Ä¶ , ùë•ùëõ, we could cut ùëõspaghetti
noodles into lengths ùë•1, ‚Ä¶ , ùë•ùëõ, and then if we simply hold them
together in our hand and bring them down to a flat surface, they
will emerge in sorted order. There are a great many reasons why
this is not truly a challenge to the PECTT hypothesis, and I will not
ruin the reader‚Äôs fun in finding them out by her or himself.


--- Page 206 ---

206
introduction to theoretical computer science
Figure 5.8: Scott Aaronson tests a candidate device for
computing Steiner trees using soap bubbles.
7 We were extremely conservative in the suggested
parameters for the PECTT, having assumed that as
many as ‚Ñì‚àí2
ùëÉ10‚àí6 ‚àº1061 bits could potentially be
stored in a millimeter radius region.
‚Ä¢ Soap bubbles: One function ùêπ‚à∂{0, 1}ùëõ‚Üí{0, 1} that is conjectured
to require a large number of NAND lines to solve is the Euclidean
Steiner Tree problem. This is the problem where one is given ùëö
points in the plane (ùë•1, ùë¶1), ‚Ä¶ , (ùë•ùëö, ùë¶ùëö) (say with integer coordi-
nates ranging from 1 till ùëö, and hence the list can be represented
as a string of ùëõ= ùëÇ(ùëölog ùëö) size) and some number ùêæ. The goal
is to figure out whether it is possible to connect all the points by
line segments of total length at most ùêæ. This function is conjec-
tured to be hard because it is NP complete - a concept that we‚Äôll en-
counter later in this course - and it is in fact reasonable to conjecture
that as ùëögrows, the number of NAND lines required to compute
this function grows exponentially in ùëö, meaning that the PECTT
would predict that if ùëöis sufficiently large (such as few hundreds
or so) then no physical device could compute ùêπ. Yet, some people
claimed that there is in fact a very simple physical device that could
solve this problem, that can be constructed using some wooden
pegs and soap. The idea is that if we take two glass plates, and put
ùëöwooden pegs between them in the locations (ùë•1, ùë¶1), ‚Ä¶ , (ùë•ùëö, ùë¶ùëö)
then bubbles will form whose edges touch those pegs in a way that
will minimize the total energy which turns out to be a function of
the total length of the line segments. The problem with this device
is that nature, just like people, often gets stuck in ‚Äúlocal optima‚Äù.
That is, the resulting configuration will not be one that achieves
the absolute minimum of the total energy but rather one that can‚Äôt
be improved with local changes. Aaronson has carried out actual
experiments (see Fig. 5.8), and saw that while this device often
is successful for three or four pegs, it starts yielding suboptimal
results once the number of pegs grows beyond that.
‚Ä¢ DNA computing. People have suggested using the properties of
DNA to do hard computational problems. The main advantage of
DNA is the ability to potentially encode a lot of information in a
relatively small physical space, as well as compute on this infor-
mation in a highly parallel manner. At the time of this writing, it
was demonstrated that one can use DNA to store about 1016 bits
of information in a region of radius about a millimeter, as opposed
to about 1010 bits with the best known hard disk technology. This
does not posit a real challenge to the PECTT but does suggest that
one should be conservative about the choice of constant and not as-
sume that current hard disk + silicon technologies are the absolute
best possible.7
‚Ä¢ Continuous/real computers. The physical world is often described
using continuous quantities such as time and space, and people


--- Page 207 ---

code as data, data as code
207
have suggested that analog devices might have direct access to
computing with real-valued quantities and would be inherently
more powerful than discrete models such as NAND machines.
Whether the ‚Äútrue‚Äù physical world is continuous or discrete is an
open question. In fact, we do not even know how to precisely phrase
this question, let alone answer it. Yet, regardless of the answer, it
seems clear that the effort to measure a continuous quantity grows
with the level of accuracy desired, and so there is no ‚Äúfree lunch‚Äù
or way to bypass the PECTT using such machines (see also this
paper). Related to that are proposals known as ‚Äúhypercomputing‚Äù
or ‚ÄúZeno‚Äôs computers‚Äù which attempt to use the continuity of time
by doing the first operation in one second, the second one in half a
second, the third operation in a quarter second and so on.. These
fail for a similar reason to the one guaranteeing that Achilles will
eventually catch the tortoise despite the original Zeno‚Äôs paradox.
‚Ä¢ Relativity computer and time travel. The formulation above as-
sumed the notion of time, but under the theory of relativity time is
in the eye of the observer. One approach to solve hard problems is
to leave the computer to run for a lot of time from his perspective,
but to ensure that this is actually a short while from our perspective.
One approach to do so is for the user to start the computer and then
go for a quick jog at close to the speed of light before checking on
its status. Depending on how fast one goes, few seconds from the
point of view of the user might correspond to centuries in com-
puter time (it might even finish updating its Windows operating
system!). Of course the catch here is that the energy required from
the user is proportional to how close one needs to get to the speed
of light. A more interesting proposal is to use time travel via closed
timelike curves (CTCs). In this case we could run an arbitrarily long
computation by doing some calculations, remembering the current
state, and then travelling back in time to continue where we left off.
Indeed, if CTCs exist then we‚Äôd probably have to revise the PECTT
(though in this case I will simply travel back in time and edit these
notes, so I can claim I never conjectured it in the first place‚Ä¶)
‚Ä¢ Humans. Another computing system that has been proposed as
a counterexample to the PECTT is a 3 pound computer of about
0.1m radius, namely the human brain. Humans can walk around,
talk, feel, and do other things that are not commonly done by
NAND-CIRC programs, but can they compute partial functions
that NAND-CIRC programs cannot? There are certainly compu-
tational tasks that at the moment humans do better than computers
(e.g., play some video games, at the moment), but based on our
current understanding of the brain, humans (or other animals)


--- Page 208 ---

208
introduction to theoretical computer science
8 This is a very rough approximation that could
be wrong to a few orders of magnitude in either
direction. For one, there are other structures in the
brain apart from neurons that one might need to
simulate, hence requiring higher overhead. On the
other hand, it is by no mean clear that we need to
fully clone the brain in order to achieve the same
computational tasks that it does.
9 There are some well known scientists that have
advocated that humans have inherent computational
advantages over computers. See also this.
have no inherent computational advantage over computers. The
brain has about 1011 neurons, each operating at a speed of about
1000 operations per seconds. Hence a rough first approximation is
that a Boolean circuit of about 1014 gates could simulate one second
of a brain‚Äôs activity.8 Note that the fact that such a circuit (likely)
exists does not mean it is easy to find it. After all, constructing this
circuit took evolution billions of years. Much of the recent efforts
in artificial intelligence research is focused on finding programs
that replicate some of the brain‚Äôs capabilities and they take massive
computational effort to discover, these programs often turn out to
be much smaller than the pessimistic estimates above. For example,
at the time of this writing, Google‚Äôs neural network for machine
translation has about 104 nodes (and can be simulated by a NAND-
CIRC program of comparable size). Philosophers, priests and many
others have since time immemorial argued that there is something
about humans that cannot be captured by mechanical devices such
as computers; whether or not that is the case, the evidence is thin
that humans can perform computational tasks that are inherently
impossible to achieve by computers of similar complexity.9
‚Ä¢ Quantum computation. The most compelling attack on the Physi-
cal Extended Church Turing Thesis comes from the notion of quan-
tum computing. The idea was initiated by the observation that sys-
tems with strong quantum effects are very hard to simulate on a
computer. Turning this observation on its head, people have pro-
posed using such systems to perform computations that we do not
know how to do otherwise. At the time of this writing, scalable
quantum computers have not yet been built, but it is a fascinating
possibility, and one that does not seem to contradict any known law
of nature. We will discuss quantum computing in much more detail
in Chapter 23. Modeling quantum computation involves extending
the model of Boolean circuits into Quantum circuits that have one
more (very special) gate. However, the main takeaway is that while
quantum computing does suggest we need to amend the PECTT,
it does not require a complete revision of our worldview. Indeed,
almost all of the content of this book remains the same regardless of
whether the underlying computational model is Boolean circuits or
quantum circuits.
R
Remark 5.14 ‚Äî Physical Extended Church-Turing Thesis
and Cryptography. While even the precise phrasing of
the PECTT, let alone understanding its correctness, is
still a subject of active research, some variants of it are


--- Page 209 ---

code as data, data as code
209
already implicitly assumed in practice. Governments,
companies, and individuals currently rely on cryptog-
raphy to protect some of their most precious assets,
including state secrets, control of weapon systems
and critical infrastructure, securing commerce, and
protecting the confidentiality of personal information.
In applied cryptography, one often encounters state-
ments such as ‚Äúcryptosystem ùëãprovides 128 bits of
security‚Äù. What such a statement really means is that
(a) it is conjectured that there is no Boolean circuit
(or, equivalently, a NAND-CIRC program) of size
much smaller than 2128 that can break ùëã, and (b) we
assume that no other physical mechanism can do bet-
ter, and hence it would take roughly a 2128 amount of
‚Äúresources‚Äù to break ùëã. We say ‚Äúconjectured‚Äù and not
‚Äúproved‚Äù because, while we can phrase the statement
that breaking the system cannot be done by an ùë†-gate
circuit as a precise mathematical conjecture, at the
moment we are unable to prove such a statement for
any non-trivial cryptosystem. This is related to the P
vs NP question we will discuss in future chapters. We
will explore Cryptography in Chapter 21.
‚úì
Chapter Recap
‚Ä¢ We can think of programs both as describing a pro-
cess, as well as simply a list of symbols that can be
considered as data that can be fed as input to other
programs.
‚Ä¢ We can write a NAND-CIRC program that evalu-
ates arbitrary NAND-CIRC programs (or equiv-
alently a circuit that evaluates other circuits).
Moreover, the efficiency loss in doing so is not too
large.
‚Ä¢ We can even write a NAND-CIRC program that
evaluates programs in other programming lan-
guages such as Python, C, Lisp, Java, Go, etc.
‚Ä¢ By a leap of faith, we could hypothesize that the
number of gates in the smallest circuit that com-
putes a function ùëìcaptures roughly the amount
of physical resources required to compute ùëì. This
statement is known as the Physical Extended Church-
Turing Thesis (PECTT).
‚Ä¢ Boolean circuits (or equivalently AON-CIRC or
NAND-CIRC programs) capture a surprisingly
wide array of computational models. The strongest
currently known challenge to the PECTT comes
from the potential for using quantum mechanical
effects to speed-up computation, a model known as
quantum computers.


--- Page 210 ---

210
introduction to theoretical computer science
Figure 5.9: A finite computational task is specified by
a function ùëì‚à∂{0, 1}ùëõ‚Üí{0, 1}ùëö. We can model
a computational process using Boolean circuits (of
varying gate sets) or straight-line program. Every
function can be computed by many programs. We
say that ùëì‚ààSIZEùëõ,ùëö(ùë†) if there exists a NAND
circuit of at most ùë†gates (equivalently a NAND-CIRC
program of at most ùë†lines) that computes ùëì. Every
function ùëì‚à∂{0, 1}ùëõ‚Üí{0, 1}ùëöcan be computed by
a circuit of ùëÇ(ùëö‚ãÖ2ùëõ/ùëõ) gates. Many functions such
as multiplication, addition, solving linear equations,
computing the shortest path in a graph, and others,
can be computed by circuits of much fewer gates.
In particular there is an ùëÇ(ùë†2 log ùë†)-size circuit
that computes the map ùê∂, ùë•‚Ü¶ùê∂(ùë•) where ùê∂is
a string describing a circuit of ùë†gates. However,
the counting argument shows there do exist some
functions ùëì‚à∂{0, 1}ùëõ‚Üí{0, 1}ùëöthat require
‚Ñ¶(ùëö‚ãÖ2ùëõ/ùëõ) gates to compute.
5.7 RECAP OF PART I: FINITE COMPUTATION
This chapter concludes the first part of this book that deals with finite
computation (computing functions that map a fixed number of Boolean
inputs to a fixed number of Boolean outputs). The main take-aways
from Chapter 3, Chapter 4, and Chapter 5 are as follows (see also
Fig. 5.9):
‚Ä¢ We can formally define the notion of a function ùëì‚à∂{0, 1}ùëõ‚Üí
{0, 1}ùëöbeing computable using ùë†basic operations. Whether these
operations are AND/OR/NOT, NAND, or some other universal
basis does not make much difference. We can describe such a com-
putation either using a circuit or using a straight-line program.
‚Ä¢ We define SIZEùëõ,ùëö(ùë†) to be the set of functions that are computable
by NAND circuits of at most ùë†gates. This set is equal to the set of
functions computable by a NAND-CIRC program of at most ùë†lines
and up to a constant factor in ùë†(which we will not care about);
this is also the same as the set of functions that are computable
by a Boolean circuit of at most ùë†AND/OR/NOT gates. The class
SIZEùëõ,ùëö(ùë†) is a set of functions, not of programs/circuits.
‚Ä¢ Every function ùëì‚à∂{0, 1}ùëõ‚Üí{0, 1}ùëöcan be computed using a
circuit of at most ùëÇ(ùëö‚ãÖ2ùëõ/ùëõ) gates. Some functions require at least
Œ©(ùëö‚ãÖ2ùëõ/ùëõ) gates. We define SIZEùëõ,ùëö(ùë†) to be the set of functions
from {0, 1}ùëõto {0, 1}ùëöthat can be computed using at most ùë†gates.
‚Ä¢ We can describe a circuit/program ùëÉas a string. For every ùë†, there
is a universal circuit/program ùëàùë†that can evaluate programs of
length ùë†given their description as strings. We can use this repre-
sentation also to count the number of circuits of at most ùë†gates and


--- Page 211 ---

code as data, data as code
211
hence prove that some functions cannot be computed by circuits of
smaller-than-exponential size.
‚Ä¢ If there is a circuit of ùë†gates that computes a function ùëì, then we
can build a physical device to compute ùëìusing ùë†basic components
(such as transistors). The ‚ÄúPhysical Extended Church-Turing The-
sis‚Äù postulates that the reverse direction is true as well: if ùëìis a
function for which every circuit requires at least ùë†gates then that
every physical device to compute ùëìwill require about ùë†‚Äúphysical
resources‚Äù. The main challenge to the PECTT is quantum computing,
which we will discuss in Chapter 23.
Sneak preview:
In the next part we will discuss how to model compu-
tational tasks on unbounded inputs, which are specified using functions
ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}‚àó(or ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}) that can take an
unbounded number of Boolean inputs.
5.8 EXERCISES
Exercise 5.1 Which one of the following statements is false:
a. There is an ùëÇ(ùë†3) line NAND-CIRC program that given as input
program ùëÉof ùë†lines in the list-of-tuples representation computes
the output of ùëÉwhen all its input are equal to 1.
b. There is an ùëÇ(ùë†3) line NAND-CIRC program that given as input
program ùëÉof ùë†characters encoded as a string of 7ùë†bits using the
ASCII encoding, computes the output of ùëÉwhen all its input are
equal to 1.
c. There is an ùëÇ(‚àöùë†) line NAND-CIRC program that given as input
program ùëÉof ùë†lines in the list-of-tuples representation computes
the output of ùëÉwhen all its input are equal to 1.
‚ñ†
Exercise 5.2 ‚Äî Equals function. For every ùëò‚àà‚Ñï, show that there is an ùëÇ(ùëò)
line NAND-CIRC program that computes the function EQUALSùëò‚à∂
{0, 1}2ùëò‚Üí{0, 1} where EQUALS(ùë•, ùë•‚Ä≤) = 1 if and only if ùë•= ùë•‚Ä≤.
‚ñ†
Exercise 5.3 ‚Äî Equal to constant function. For every ùëò‚àà‚Ñïand ùë•‚Ä≤ ‚àà{0, 1}ùëò,
show that there is an ùëÇ(ùëò) line NAND-CIRC program that computes
the function EQUALSùë•‚Ä≤ ‚à∂{0, 1}ùëò‚Üí{0, 1} that on input ùë•‚àà{0, 1}ùëò
outputs 1 if and only if ùë•= ùë•‚Ä≤.
‚ñ†
Exercise 5.4 ‚Äî Counting lower bound for multibit functions. Prove that there
exists a number ùõø> 0 such that for every ùëõ, ùëöthere exists a function


--- Page 212 ---

212
introduction to theoretical computer science
10 How many functions from {0, 1}ùëõto {0, 1}ùëöexist?
11 Follow the proof of Theorem 5.5, replacing the use
of the counting argument with Exercise 5.4.
12 Using the adjacency list representation, a graph
with ùëõin-degree zero vertices and ùë†in-degree two
vertices can be represented using roughly 2ùë†log(ùë†+
ùëõ) ‚â§2ùë†(log ùë†+ ùëÇ(1)) bits. The labeling of the ùëõinput
and ùëöoutput vertices can be specified by a list of ùëõ
labels in [ùëõ] and ùëölabels in [ùëö].
13 Hint: Use the results of Exercise 5.6 and the fact that
in this regime ùëö= 1 and ùëõ‚â™ùë†.
14 Hint: An equivalent way to say this is that you
need to prove that the set of functions that can be
computed using at most 2ùëõ/(1000ùëõ) lines has fewer
than 2‚àí10022ùëõelements. Can you see why?
15 Note that if ùëõis big enough, then it is easy to
represent such a pair using ùëõ2 bits, since we can
represent the program using ùëÇ(ùëõ1.1 log ùëõ) bits, and
we can always pad our representation to have exactly
ùëõ2 length.
ùëì‚à∂{0, 1}ùëõ‚Üí{0, 1}ùëöthat requires at least ùõøùëö‚ãÖ2ùëõ/ùëõNAND gates to
compute. See footnote for hint.10
‚ñ†
Exercise 5.5 ‚Äî Size hierarchy theorem for multibit functions. Prove that there
exists a number ùê∂such that for every ùëõ, ùëöand ùëõ+ùëö< ùë†< ùëö‚ãÖ2ùëõ/(ùê∂ùëõ)
there exists a function ùëì‚ààSIZEùëõ,ùëö(ùê∂‚ãÖùë†) ‚ßµSIZEùëõ,ùëö(ùë†). See footnote for
hint.11
‚ñ†
Exercise 5.6 ‚Äî Efficient representation of circuits and a tighter counting upper
bound. Use the ideas of Remark 5.4 to show that for every ùúñ> 0 and
sufficiently large ùë†, ùëõ, ùëö,
|SIZEùëõ,ùëö(ùë†)| < 2(2+ùúñ)ùë†log ùë†+ùëõlog ùëõ+ùëölog ùë†.
(5.11)
Conclude that the implicit constant in Theorem 5.2 can be made arbi-
trarily close to 5. See footnote for hint.12
‚ñ†
Exercise 5.7 ‚Äî Tighter counting lower bound. Prove that for every ùõø< 1/2, if
ùëõis sufficiently large then there exists a function ùëì‚à∂{0, 1}ùëõ‚Üí{0, 1}
such that ùëì‚àâSIZEùëõ,1 ( ùõø2ùëõ
ùëõ). See footnote for hint.13
‚ñ†
Exercise 5.8 ‚Äî Random functions are hard. Suppose ùëõ> 1000 and that we
choose a function ùêπ‚à∂{0, 1}ùëõ‚Üí{0, 1} at random, choosing for every
ùë•‚àà{0, 1}ùëõthe value ùêπ(ùë•) to be the result of tossing an independent
unbiased coin. Prove that the probability that there is a 2ùëõ/(1000ùëõ)
line program that computes ùêπis at most 2‚àí100.14
‚ñ†
Exercise 5.9 The following is a tuple representing a NAND program:
(3, 1, ((3, 2, 2), (4, 1, 1), (5, 3, 4), (6, 2, 1), (7, 6, 6), (8, 0, 0), (9, 7, 8), (10, 5, 0), (11, 9, 10))).
1. Write a table with the eight values ùëÉ(000), ùëÉ(001), ùëÉ(010), ùëÉ(011),
ùëÉ(100), ùëÉ(101), ùëÉ(110), ùëÉ(111) in this order.
2. Describe what the programs does in words.
‚ñ†
Exercise 5.10 ‚Äî EVAL with XOR. For every sufficiently large ùëõ, let ùê∏ùëõ‚à∂
{0, 1}ùëõ2 ‚Üí{0, 1} be the function that takes an ùëõ2-length string that
encodes a pair (ùëÉ, ùë•) where ùë•‚àà{0, 1}ùëõand ùëÉis a NAND program
of ùëõinputs, a single output, and at most ùëõ1.1 lines, and returns the
output of ùëÉon ùë•.15 That is, ùê∏ùëõ(ùëÉ, ùë•) = ùëÉ(ùë•).
Prove that for every sufficiently large ùëõ, there does not exist an XOR
circuit ùê∂that computes the function ùê∏ùëõ, where a XOR circuit has the
XOR gate as well as the constants 0 and 1 (see Exercise 3.5). That is,


--- Page 213 ---

code as data, data as code
213
16 Hint: Use our bound on the number of program-
s/circuits of size ùë†(Theorem 5.2), as well as the
Chernoff Bound ( Theorem 18.12) and the union
bound.
prove that there is some constant ùëõ0 such that for every ùëõ> ùëõ0 and
XOR circuit ùê∂of ùëõ2 inputs and a single output, there exists a pair
(ùëÉ, ùë•) such that ùê∂(ùëÉ, ùë•) ‚â†ùê∏ùëõ(ùëÉ, ùë•).
‚ñ†
Exercise 5.11 ‚Äî Learning circuits (challenge, optional, assumes more background).
(This exercise assumes background in probability theory and/or
machine learning that you might not have at this point. Feel free
to come back to it at a later point and in particular after going over
Chapter 18.) In this exercise we will use our bound on the number of
circuits of size ùë†to show that (if we ignore the cost of computation)
every such circuit can be learned from not too many training samples.
Specifically, if we find a size-ùë†circuit that classifies correctly a training
set of ùëÇ(ùë†log ùë†) samples from some distribution ùê∑, then it is guaran-
teed to do well on the whole distribution ùê∑. Since Boolean circuits
model very many physical processes (maybe even all of them, if the
(controversial) physical extended Church-Turing thesis is true), this
shows that all such processes could be learned as well (again, ignor-
ing the computation cost of finding a classifier that does well on the
training data).
Let ùê∑be any probability distribution over {0, 1}ùëõand let ùê∂be a
NAND circuit with ùëõinputs, one output, and size ùë†‚â•ùëõ. Prove that
there is some constant ùëêsuch that with probability at least 0.999 the
following holds: if ùëö= ùëêùë†log ùë†and ùë•0, ‚Ä¶ , ùë•ùëö‚àí1 are chosen indepen-
dently from ùê∑, then for every circuit ùê∂‚Ä≤ such that ùê∂‚Ä≤(ùë•ùëñ) = ùê∂(ùë•ùëñ) on
every ùëñ‚àà[ùëö], Prùë•‚àºùê∑[ùê∂‚Ä≤(ùë•) ‚â§ùê∂(ùë•)] ‚â§0.99.
In other words, if ùê∂‚Ä≤ is a so called ‚Äúempirical risk minimizer‚Äù that
agrees with ùê∂on all the training examples ùë•0, ‚Ä¶ , ùë•ùëõ‚àí1, then it will
also agree with ùê∂with high probability for samples drawn from the
distribution ùê∑(i.e., it ‚Äúgeneralizes‚Äù, to use Machine-Learning lingo).
See footnote for hint.16
‚ñ†
5.9 BIBLIOGRAPHICAL NOTES
The EVAL function is usually known as a universal circuit. The imple-
mentation we describe is not the most efficient known. Valiant [Val76]
first showed a universal circuit of ùëÇ(ùëõlog ùëõ) size where ùëõis the size of
the input. Universal circuits have seen in recent years new motivations
due to their applications for cryptography, see [LMS16; GKS17] .
While we‚Äôve seen that ‚Äúmost‚Äù functions mapping ùëõbits to one bit
require circuits of exponential size Œ©(2ùëõ/ùëõ), we actually do not know
of any explicit function for which we can prove that it requires, say, at
least ùëõ100 or even 100ùëõsize. At the moment, the strongest such lower
bound we know is that there are quite simple and explicit ùëõ-variable


--- Page 214 ---

214
introduction to theoretical computer science
functions that require at least (5 ‚àíùëú(1))ùëõlines to compute, see this
paper of Iwama et al as well as this more recent work of Kulikov et al.
Proving lower bounds for restricted models of circuits is an extremely
interesting research area, for which Jukna‚Äôs book [Juk12] (see also
Wegener [Weg87]) provides a very good introduction and overview. I
learned of the proof of the size hierarchy theorem (Theorem 5.5) from
Sasha Golovnev.
Scott Aaronson‚Äôs blog post on how information is physical is a good
discussion on issues related to the physical extended Church-Turing
Physics. Aaronson‚Äôs survey on NP complete problems and physical
reality [Aar05] discusses these issues as well, though it might be
easier to read after we reach Chapter 15 on NP and NP-completeness.


--- Page 215 ---

II
UNIFORM COMPUTATION


--- Page 216 ---



--- Page 217 ---

Figure 6.1: Once you know how to multiply multi-
digit numbers, you can do so for every number ùëõ
of digits, but if you had to describe multiplication
using Boolean circuits or NAND-CIRC programs,
you would need a different program/circuit for every
length ùëõof the input.
6
Functions with Infinite domains, Automata, and Regular ex-
pressions
‚ÄúAn algorithm is a finite answer to an infinite number of questions.‚Äù, At-
tributed to Stephen Kleene.
The model of Boolean circuits (or equivalently, the NAND-CIRC
programming language) has one very significant drawback: a Boolean
circuit can only compute a finite function ùëì, and in particular since
every gate has two inputs, a size ùë†circuit can compute on an input
of length at most 2ùë†. This does not capture our intuitive notion of an
algorithm as a single recipe to compute a potentially infinite function.
For example, the standard elementary school multiplication algorithm
is a single algorithm that multiplies numbers of all lengths, but yet
we cannot express this algorithm as a single circuit, but rather need a
different circuit (or equivalently, a NAND-CIRC program) for every
input length (see Fig. 6.1).
In this chapter we extend our definition of computational tasks to
consider functions with the unbounded domain of {0, 1}‚àó. We focus
on the question of defining what tasks to compute, mostly leaving
the question of how to do so to later chapters, where we will see
Turing Machines and other computational models for computing on
unbounded inputs. In this chapter we will see however one example
for a simple and restricted model of computation - deterministic finite
automata (DFAs).
This chapter: A non-mathy overview
In this chapter we discuss functions that as input strings of
arbitary length. Such functions could have outputs that are
long strings as well, but the case of Boolean functions, where
the output is a single bit, is of particular importance. The task
of computing a Boolean function is equivalent to the task
Compiled on 8.26.2020 18:10
Learning Objectives:
‚Ä¢ Define functions on unbounded length inputs,
that cannot be described by a finite size table
of inputs and outputs.
‚Ä¢ Equivalence with the task of deciding
membership in a language.
‚Ä¢ Deterministic finite automatons (optional): A
simple example for a model for unbounded
computation
‚Ä¢ Equivalence with regular expressions.


--- Page 218 ---

218
introduction to theoretical computer science
Figure 6.2: The NAND circuit and NAND-CIRC
program for computing the XOR of 5 bits. Note how
the circuit for XOR5 merely repeats four times the
circuit to compute the XOR of 2 bits.
of deciding a language. We will also define the restriction of
a function over unbounded length strings to a finite length,
and how this restriction can be computed by Boolean circuits.
In the second half of this chapter we discuss finite automata,
which is a model for computing functions of unbounded
length. This model is not as powerful as Python or other
general-purpose programming languages, but can serve
as an introduction to these more general models. We also
show a beautiful result - the functions computable by finite
automata are exactly the ones that correspond to regular
expressions. However, the reader can also feel free to skip
automata and go straight to our discussion of Turing Machines
in Chapter 7.
6.1 FUNCTIONS WITH INPUTS OF UNBOUNDED LENGTH
Up until now, we considered the computational task of mapping
some string of length ùëõinto a string of length ùëö. However, in gen-
eral computational tasks can involve inputs of unbounded length.
For example, the following Python function computes the function
XOR ‚à∂{0, 1}‚àó‚Üí{0, 1}, where XOR(ùë•) equals 1 iff the number of 1‚Äôs
in ùë•is odd. (In other words, XOR(ùë•) = ‚àë
|ùë•|‚àí1
ùëñ=0 ùë•ùëñmod 2 for every
ùë•‚àà{0, 1}‚àó.) As simple as it is, the XOR function cannot be com-
puted by a Boolean circuit. Rather, for every ùëõ, we can compute XORùëõ
(the restriction of XOR to {0, 1}ùëõ) using a different circuit (e.g., see
Fig. 6.2).
def XOR(X):
'''Takes list X of 0's and 1's
Outputs 1 if the number of 1's is odd and outputs 0
otherwise'''
‚Ü™
result = 0
for i in range(len(X)):
result += X[i] % 2
return result
Previously in this book we studied the computation of finite func-
tions ùëì‚à∂{0, 1}ùëõ‚Üí{0, 1}ùëö. Such a function ùëìcan always be described
by listing all the 2ùëõvalues it takes on inputs ùë•‚àà{0, 1}ùëõ. In this chap-
ter we consider functions such as XOR that take inputs of unbounded
size. While we can describe XOR using a finite number of symbols
(in fact we just did so above), it takes infinitely many possible inputs
and so we cannot just write down all of its values. The same is true
for many other functions capturing important computational tasks


--- Page 219 ---

functions with infinite domains, automata, and regular expressions
219
including addition, multiplication, sorting, finding paths in graphs,
fitting curves to points, and so on and so forth. To contrast with the
finite case, we will sometimes call a function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1} (or
ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}‚àó) infinite. However, this does not mean that ùêπ
taks as input strings of infinite length! It just means that ùêπcan take as
input a string of that can be arbitrarily long, and so we cannot simply
write down a table of all the outputs of ùêπon different inputs.
ÔÉ´Big Idea 8 A function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}‚àóspecifies the computa-
tional task mapping an input ùë•‚àà{0, 1}‚àóinto the output ùêπ(ùë•).
As we‚Äôve seen before, restricting attention to functions that use
binary strings as inputs and outputs does not detract from our gener-
ality, since other objects, including numbers, lists, matrices, images,
videos, and more, can be encoded as binary strings.
As before, it is important to differentiate between specification and
implementation. For example, consider the following function:
TWINP(ùë•) =
‚éß
{
‚é®
{
‚é©
1
‚àÉùëù‚àà‚Ñïs.t.ùëù, ùëù+ 2 are primes and ùëù> |ùë•|
0
otherwise
(6.1)
This is a mathematically well-defined function. For every ùë•,
TWINP(ùë•) has a unique value which is either 0 or 1. However, at
the moment no one knows of a Python program that computes this
function. The Twin prime conjecture posits that for every ùëõthere
exists ùëù> ùëõsuch that both ùëùand ùëù+ 2 are primes. If this conjecture
is true, then ùëáis easy to compute indeed - the program def T(x):
return 1 will do the trick. However, mathematicians have tried
unsuccesfully to prove this conjecture since 1849. That said, whether
or not we know how to implement the function TWINP, the definition
above provides its specification.
6.1.1 Varying inputs and outputs
Many of the functions we are interested in take more than one input.
For example the function
MULT(ùë•, ùë¶) = ùë•‚ãÖùë¶
(6.2)
that takes the binary representation of a pair of integers ùë•, ùë¶‚àà‚Ñï
and outputs the binary representation of their product ùë•‚ãÖùë¶. How-
ever, since we can represent a pair of strings as a single string, we will
consider functions such as MULT as mapping {0, 1}‚àóto {0, 1}‚àó. We
will typically not be concerned with low-level details such as the pre-
cise way to represent a pair of integers as a string, since essentially all
choices will be equivalent for our purposes.


--- Page 220 ---

220
introduction to theoretical computer science
Another example for a function we want to compute is
PALINDROME(ùë•) =
‚éß
{
‚é®
{
‚é©
1
‚àÄùëñ‚àà[|ùë•|]ùë•ùëñ= ùë•|ùë•|‚àíùëñ
0
otherwise
(6.3)
PALINDROME has a single bit as output. Functions with a single
bit of output are known as Boolean functions. Boolean functions are
central to the theory of computation, and we will come discuss them
often in this book. Note that even though Boolean functions have a
single bit of output, their input can be of arbitrary length. Thus they
are still infinite functions, that cannot be described via a finite table of
values.
‚ÄúBooleanizing‚Äù functions. Sometimes it might be convenient to ob-
tain a Boolean variant for a non-Boolean function. For example, the
following is a Boolean variant of MULT.
BMULT(ùë•, ùë¶, ùëñ) =
‚éß
{
‚é®
{
‚é©
ùëñùë°‚Ñébit of ùë•‚ãÖùë¶
ùëñ< |ùë•‚ãÖùë¶|
0
otherwise
(6.4)
If we can compute BMULT via any programming language such
as Python, C, Java, etc. then we can compute MULT as well, and vice
versa.
Solved Exercise 6.1 ‚Äî Booleanizing general functions. Show that for every
function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}‚àóthere exists a Boolean function BF ‚à∂
{0, 1}‚àó‚Üí{0, 1} such that a Python program to compute BF can be
transformed into a program to compute ùêπand vice versa.
‚ñ†
Solution:
For every ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}‚àó, we can define
BF(ùë•, ùëñ, ùëè) =
‚éß
{
{
‚é®
{
{
‚é©
ùêπ(ùë•)ùëñ
ùëñ< |ùêπ(ùë•)|, ùëè= 0
1
ùëñ< |ùêπ(ùë•)|, ùëè= 1
0
ùëñ‚â•|ùë•|
(6.5)
to be the function that on input ùë•‚àà{0, 1}‚àó, ùëñ‚àà‚Ñï, ùëè‚àà{0, 1} out-
puts the ùëñùë°‚Ñébit of ùêπ(ùë•) if ùëè= 0 and ùëñ< |ùë•|. If ùëè= 1 then BF(ùë•, ùëñ, ùëè)
outputs 1 iff ùëñ< |ùêπ(ùë•)| and hence this allows to compute the length
of ùêπ(ùë•).
Computing BF from ùêπis straightforward. For the other direc-
tion, given a Python function BF that computes BF, we can compute
ùêπas follows:
def F(x):
res = []


--- Page 221 ---

functions with infinite domains, automata, and regular expressions
221
i = 0
while BF(x,i,1):
res.apppend(BF(x,i,0))
i += 1
return res
‚ñ†
6.1.2 Formal Languages
For every Boolean function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}, we can define the
set ùêøùêπ= {ùë•|ùêπ(ùë•) = 1} of strings on which ùêπoutputs 1. Such sets
are known as languages. This name is rooted in formal language theory
as pursued by linguists such as Noam Chomsky. A formal language
is a subset ùêø‚äÜ{0, 1}‚àó(or more generally ùêø‚äÜŒ£‚àófor some finite
alphabet Œ£). The membership or decision problem for a language ùêø, is
the task of determining, given ùë•‚àà{0, 1}‚àó, whether or not ùë•‚ààùêø. If
we can compute the function ùêπthen we can decide membership in the
language ùêøùêπand vice versa. Hence, many texts such as [Sip97] refer
to the task of computing a Boolean function as ‚Äúdeciding a language‚Äù.
In this book we mostly describe computational task using the function
notation, which is easier to generalize to computation with more than
one bit of output. However, since the language terminology is so
popular in the literature, we will sometimes mention it.
6.1.3 Restrictions of functions
If ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1} is a Boolean function and ùëõ‚àà‚Ñïthen the re-
striction of ùêπto inputs of length ùëõ, denoted as ùêπùëõ, is the finite function
ùëì‚à∂{0, 1}ùëõ‚Üí{0, 1} such that ùëì(ùë•) = ùêπ(ùë•) for every ùë•‚àà{0, 1}ùëõ. That
is, ùêπùëõis the finite function that is only defined on inputs in {0, 1}ùëõ, but
agrees with ùêπon those inputs. Since ùêπùëõis a finite function, it can be
computed by a Boolean circuit, implying the following theorem:
Theorem 6.1 ‚Äî Circuit collection for every infinite function. Let ùêπ‚à∂{0, 1}‚àó‚Üí
{0, 1}. Then there is a collection {ùê∂ùëõ}ùëõ‚àà{1,2,‚Ä¶} of circuits such that
for every ùëõ
>
0, ùê∂ùëõcomputes the restriction ùêπùëõof ùêπto inputs of
length ùëõ.
Proof. This is an immediate corollary of the universality of Boolean
circuits. Indeed, since ùêπùëõmaps {0, 1}ùëõto {0, 1}, Theorem 4.15 implies
that there exists a Boolean circuit ùê∂ùëõto compute it. In fact. the size of
this circuit is at most ùëê‚ãÖ2ùëõ/ùëõgates for some constant ùëê‚â§10.
‚ñ†
In particular, Theorem 6.1 implies that there exists such as circuit
collection {ùê∂ùëõ} even for the TWINP function we described before,


--- Page 222 ---

222
introduction to theoretical computer science
despite the fact that we don‚Äôt know of any program to compute it. In-
deed, this is not that surprising: for every particular ùëõ‚àà‚Ñï, TWINPùëõ
is either the constant zero function or the constant one function, both
of which can be computed by very simple Boolean circuits. Hence
a collection of circuits {ùê∂ùëõ} that computes TWINP certainly exists.
The difficulty in computing TWINP using Python or any other pro-
gramming language arises from the fact that we don‚Äôt know for each
particular ùëõwhat is the circuit ùê∂ùëõin this collection.
6.2 DETERMINISTIC FINITE AUTOMATA (OPTIONAL)
All our computational models so far - Boolean circuits and straight-
line programs - were only applicable for finite functions. In Chapter 7
we will present Turing Machines, which are the central models of com-
putation for functions of unbounded input length. However, in this
section we will present the more basic model of deterministic finite
automata (DFA). Automata can serve as a good stepping-stone for
Turing machines, though they will not be used much in later parts of
this book, and so the reader can feel free to skip ahead fo Chapter 7.
DFAs turn out to be equivalent in power to regular expressions, which
are a powerful mechanism to specify patterns that is widely used in
practice. Our treatment of automata is quite brief. There are plenty of
resources that help you get more comfortable with DFA‚Äôs. In particu-
lar, Chapter 1 of Sipser‚Äôs book [Sip97] contains an excellent exposition
of this material. There are also many websites with online simula-
tors for automata, as well as translators from regular expressions to
automata and vice versa (see for example here and here).
At a high level, an algorithm is a recipe for computing an output
from an input via a combination of the following steps:
1. Read a bit from the input
2. Update the state (working memory)
3. Stop and produce an output
For example, recall the Python program that computes the XOR
function:
def XOR(X):
'''Takes list X of 0's and 1's
Outputs 1 if the number of 1's is odd and outputs 0
otherwise'''
‚Ü™
result = 0
for i in range(len(X)):
result += X[i] % 2
return result


--- Page 223 ---

functions with infinite domains, automata, and regular expressions
223
Figure 6.3: A deterministic finite automaton that
computes the XOR function. It has two states 0 and 1,
and when it observes ùúéit transitions from ùë£to ùë£‚äïùúé.
In each step, this program reads a single bit X[i] and updates
its state result based on that bit (flipping result if X[i] is 1 and
keeping it the same otherwise). When its done transversing the input,
the program outputs result. In computer science, such a program is
called a single-pass constant-memory algorithm since it makes a single
pass over the input and its working memory is of finite size. (Indeed
in this case result can either be 0 or 1.) Such an algorithm is also
known as a Deterministic Finite Automaton or DFA (another name for
DFA‚Äôs is a finite state machine). We can think of such an algorithm as
a ‚Äúmachine‚Äù that can be in one of ùê∂states, for some constant ùê∂. The
machine starts in some initial state, and then reads its input ùë•‚àà{0, 1}‚àó
one bit at a time. Whenever the machine reads a bit ùúé‚àà{0, 1}, it
transitions into a new state based on ùúéand its prior state. The output
of the machine is based on the final state. Every single-pass constant-
memory algorithm corresponds to such a machine. If an algorithm
uses ùëêbits of memory, then the contents of its memory are a string of
length ùëê. Since there are 2ùëêsuch strings, at any point in the execution,
such an algorithm can be in one of 2ùëêstates.
We can specify a DFA of ùê∂states by a list of ùê∂‚ãÖ2 rules. Each rule
will be of the form ‚ÄúIf the DFA is in state ùë£and the bit read from the
input is ùúéthen the new state is ùë£‚Ä≤‚Äù. At the end of the computation
we will also have a rule of the form ‚ÄúIf the final state is one of the
following ‚Ä¶ then output 1, otherwise output 0‚Äù. For example, the
Python program above can be represented by a two state automata for
computing XOR of the following form:
‚Ä¢ Initialize in state 0
‚Ä¢ For every state ùë†‚àà{0, 1} and input bit ùúéread, if ùúé= 1 then change
to state 1 ‚àíùë†, otherwise stay in state ùë†.
‚Ä¢ At the end output 1 iff ùë†= 1.
We can also describe a ùê∂-state DFA as a labeled graph of ùê∂vertices.
For every state ùë†and bit ùúé, we add a directed edge labeled with ùúé
between ùë†and the state ùë†‚Ä≤ such that if the DFA is at state ùë†and reads ùúé
then it transitions to ùë†‚Ä≤. (If the state stays the same then this edge will
be a self loop; similarly, if ùë†transitions to ùë†‚Ä≤ in both the case ùúé= 0 and
ùúé= 1 then the graph will contain two parallel edges.) We also label
the set ùíÆof states on which the automate will output 1 at the end of
the computation. This set is known as the set of accepting states. See
Fig. 6.3 for the graphical representation of the XOR automaton.
Formally, a DFA is specified by (1) the table of the ùê∂‚ãÖ2 rules, which
can be represented as a transition function ùëáthat maps a state ùë†‚àà[ùê∂]
and bit ùúé‚àà{0, 1} to the state ùë†‚Ä≤ ‚àà[ùê∂] which the DFA will transition to
from state ùëêon input ùúéand (2) the set ùíÆof accepting states. This leads
to the following definition.


--- Page 224 ---

224
introduction to theoretical computer science
Definition 6.2 ‚Äî Deterministic Finite Automaton. A deterministic finite
automaton (DFA) with ùê∂states over {0, 1} is a pair (ùëá, ùíÆ) with
ùëá‚à∂[ùê∂] √ó {0, 1} ‚Üí[ùê∂] and ùíÆ‚äÜ[ùê∂]. The finite function ùëáis known
as the transition function of the DFA and the set ùíÆis known as the
set of accepting states.
Let ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1} be a Boolean function with the infinite
domain {0, 1}‚àó. We say that (ùëá, ùíÆ) computes a function ùêπ‚à∂{0, 1}‚àó‚Üí
{0, 1} if for every ùëõ‚àà‚Ñïand ùë•‚àà{0, 1}ùëõ, if we define ùë†0 = 0 and
ùë†ùëñ+1 = ùëá(ùë†ùëñ, ùë•ùëñ) for every ùëñ‚àà[ùëõ], then
ùë†ùëõ‚ààùíÆ‚áîùêπ(ùë•) = 1
(6.6)
P
Make sure not to confuse the transition function of
an automaton (ùëáin Definition 6.2), which is a finite
function specifying the table of ‚Äúrules‚Äù which it fol-
lows, with the function the automaton computes (ùêπin
Definition 6.2) which is an infinite function.
R
Remark 6.3 ‚Äî Definitions in other texts. Deterministic
finite automata can be defined in several equivalent
ways. In particular Sipser [Sip97] defines a DFAs as a
five-tuple (ùëÑ, Œ£, ùõø, ùëû0, ùêπ) where ùëÑis the set of states,
Œ£ is the alphabet, ùõøis the transition function, ùëû0 is
the initial state, and ùêπis the set of accepting states.
In this book the set of states is always of the form
ùëÑ= {0, ‚Ä¶ , ùê∂‚àí1} and the initial state is always ùëû0 = 0,
but this makes no difference to the computational
power of these models. Also, we restrict our attention
to the case that the alphabet Œ£ is equal to {0, 1}.
Solved Exercise 6.2 ‚Äî DFA for (010)‚àó. Prove that there is a DFA that com-
putes the following function ùêπ:
ùêπ(ùë•) =
‚éß
{
‚é®
{
‚é©
1
3 divides |ùë•| and ‚àÄùëñ‚àà[|ùë•|/3]ùë•ùëñùë•ùëñ+1ùë•ùëñ+2 = 010
0
otherwise
(6.7)
‚ñ†
Solution:
When asked to construct a deterministic finite automaton, it
helps to start by thinking of it a single-pass constant-memory al-


--- Page 225 ---

functions with infinite domains, automata, and regular expressions
225
Figure 6.4: A DFA that outputs 1 only on inputs
ùë•‚àà{0, 1}‚àóthat are a concatenation of zero or more
copies of 010. The state 0 is both the starting state
and the only accepting state. The table denotes the
transition function of ùëá, which maps the current state
and symbol read to the new symbol.
gorithm (for example, a Python program) and then translate this
program into a DFA. Here is a simple Python program for comput-
ing ùêπ:
def F(X):
'''Return 1 iff X is a concatenation of zero/more
copies of [0,1,0]'''
‚Ü™
if len(X) % 3 != 0:
return False
ultimate = 0
penultimate = 1
antepenultimate = 0
for idx, b in enumerate(X):
antepenultimate = penultimate
penultimate = ultimate
ultimate = b
if idx % 3 == 2 and ((antepenultimate,
penultimate, ultimate) != (0,1,0)):
‚Ü™
return False
return True
Since we keep three Boolean variables, the working memory can
be in one of 23
=
8 configurations, and so the program above can
be directly translated into an 8 state DFA. While this is not needed
to solve the question, by examining the resulting DFA, we can see
that we can merge some states together and obtain a 4 state au-
tomaton, described in Fig. 6.4. See also Fig. 6.5, which depicts the
execution of this DFA on a particular input.
‚ñ†
6.2.1 Anatomy of an automaton (finite vs. unbounded)
Now that we consider computational tasks with unbounded input
sizes, it is crucially important to distinguish between the components
of our algorithm that are fixed in size, and the components that grow
with the size of the input. For the case of DFAs these are the follow-
ing:
Constant size components:
Given a DFA ùê¥, the following quantities are
fixed independent of the input size:
‚Ä¢ The number of states ùê∂in ùê¥.
‚Ä¢ The transition function ùëá(which has 2ùê∂inputs, and so can be speci-
fied by a table of 2ùê∂rows, each entry in which is a number in [ùê∂]).


--- Page 226 ---

226
introduction to theoretical computer science
‚Ä¢ The set ùíÆ‚äÜ[ùê∂] of accepting states. There are at most 2ùê∂such states,
each of which can be described by a string in {0, 1}ùê∂specifiying
which state is in ùíÆand which isn‚Äôt
Together the above means that we can fully describe an automaton
using finitely many symbols. This is a property we require out of any
notion of ‚Äúalgorithm‚Äù: we should be able to write down a completely
specification of how it produces an output from an input.
Components of unbounded size:
The following quantities relating to a
DFA are not bounded by any constant. We stress that these are still
finite for any given input.
‚Ä¢ The length of the input ùë•‚àà{0, 1}‚àóthat the DFA is provided with. It
is always finite, but not bounded.
‚Ä¢ The number of steps that the DFA takes can grow with the length of
the input. Indeed, a DFA makes a single pass on the input and so it
takes exactly |ùë•| steps on an input ùë•‚àà{0, 1}‚àó.
Figure 6.5: Execution of the DFA of Fig. 6.4. The num-
ber of states and the size of the transition function
are bounded, but the input can be arbitrarily long. If
the DFA is at state ùë†and observes the value ùúéthen it
moves to the state ùëá(ùë†, ùúé). At the end of the execution
the DFA accepts iff the final state is in ùíÆ.
6.2.2 DFA-computable functions
We say that a function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1} is DFA computable if there
exists some DFA that computes ùêπ. In Chapter 4 we saw that every
finite function is computable by some Boolean circuit. Thus, at this
point you might expect that every infinite function is computable by
some DFA. However, this is very much not the case. We will soon see
some simple examples of infinite functions that are not computable by
DFAs, but for starters, let‚Äôs prove that such functions exist.


--- Page 227 ---

functions with infinite domains, automata, and regular expressions
227
Theorem 6.4 ‚Äî DFA-computable functions are countable. Let DFACOMP be
the set of all Boolean functions ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1} such that there
exists a DFA computing ùêπ. Then DFACOMP is countable.
Proof Idea:
Every DFA can be described by a finite length string, which yields
an onto map from {0, 1}‚àóto DFACOMP: namely the function that
maps a string describing an automaton ùê¥to the function that it com-
putes.
‚ãÜ
Proof of Theorem 6.4. Every DFA can be described by a finite string,
representing the transition function ùëáand the set of accepting states,
and every DFA ùê¥computes some function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}. Thus
we can define the following function ùëÜùë°ùê∑ùê∂‚à∂{0, 1}‚àó‚ÜíDFACOMP:
ùëÜùë°ùê∑ùê∂(ùëé) =
‚éß
{
‚é®
{
‚é©
ùêπ
ùëérepresents automaton ùê¥and ùêπis the function ùê¥computes
ONE
otherwise
(6.8)
where ONE ‚à∂{0, 1}‚àó‚Üí{0, 1} is the constant function that outputs
1 on all inputs (and is a member of DFACOMP). Since by definition,
every function ùêπin DFACOMP is computable by some automaton,
ùëÜùë°ùê∑ùê∂is an onto function from {0, 1}‚àóto DFACOMP, which means
that DFACOMP is countable (see Section 2.4.2).
‚ñ†
Since the set of all Boolean functions is uncountable, we get the
following corollary:
Theorem 6.5 ‚Äî Existence of DFA-uncomputable functions. There exists a
Boolean function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1} that is not computable by any
DFA.
Proof. If every Boolean function ùêπis computable by some DFA then
DFACOMP equals the set ALL of all Boolean functions, but by Theo-
rem 2.12, the latter set is uncountable, contradicting Theorem 6.4.
‚ñ†
6.3 REGULAR EXPRESSIONS
Searching for a piece of text is a common task in computing. At its
heart, the search problem is quite simple. We have a collection ùëã=
{ùë•0, ‚Ä¶ , ùë•ùëò} of strings (e.g., files on a hard-drive, or student records in
a database), and the user wants to find out the subset of all the ùë•‚ààùëã


--- Page 228 ---

228
introduction to theoretical computer science
that are matched by some pattern (e.g., all files whose names end with
the string .txt). In full generality, we can allow the user to specify the
pattern by specifying a (computable) function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1},
where ùêπ(ùë•) = 1 corresponds to the pattern matching ùë•. That is, the
user provides a program ùëÉin some Turing-complete programming
language such as Python, and the system will return all the ùë•‚ààùëã
such that ùëÉ(ùë•) = 1. For example, one could search for all text files
that contain the string important document or perhaps (letting ùëÉ
correspond to a neural-network based classifier) all images that con-
tain a cat. However, we don‚Äôt want our system to get into an infinite
loop just trying to evaluate the program ùëÉ! For this reason, typical
systems for searching files or databases do not allow users to specify
the patterns using full-fledged programming languages. Rather, such
systems use restricted computational models that on the one hand are
rich enough to capture many of the queries needed in practice (e.g., all
filenames ending with .txt, or all phone numbers of the form (617)
xxx-xxxx), but on the other hand are restricted enough so the queries
can be evaluated very efficiently on huge files and in particular cannot
result in an infinite loop.
One of the most popular such computational models is regular
expressions. If you ever used an advanced text editor, a command line
shell, or have done any kind of manipulation of text files, then you
have probably come across regular expressions.
A regular expression over some alphabet Œ£ is obtained by combin-
ing elements of Œ£ with the operation of concatenation, as well as |
(corresponding to or) and ‚àó(corresponding to repetition zero or
more times). (Common implementations of regular expressions in
programming languages and shells typically include some extra oper-
ations on top of | and ‚àó, but these operations can be implemented as
‚Äúsyntactic sugar‚Äù using the operators | and ‚àó.) For example, the fol-
lowing regular expression over the alphabet {0, 1} corresponds to the
set of all strings ùë•‚àà{0, 1}‚àówhere every digit is repeated at least twice:
(00(0‚àó)|11(1‚àó))‚àó.
(6.9)
The following regular expression over the alphabet {ùëé, ‚Ä¶ , ùëß, 0, ‚Ä¶ , 9}
corresponds to the set of all strings that consist of a sequence of one
or more of the letters ùëé-ùëëfollowed by a sequence of one or more digits
(without a leading zero):
(ùëé|ùëè|ùëê|ùëë)(ùëé|ùëè|ùëê|ùëë)‚àó(1|2|3|4|5|6|7|8|9)(0|1|2|3|4|5|6|7|8|9)‚àó.
(6.10)
Formally, regular expressions are defined by the following recursive
definition:


--- Page 229 ---

functions with infinite domains, automata, and regular expressions
229
Definition 6.6 ‚Äî Regular expression. A regular expression ùëíover an al-
phabet Œ£ is a string over Œ£
‚à™
{(, ), |, ‚àó, ‚àÖ, ""} that has one of the
following forms:
1. ùëí= ùúéwhere ùúé‚ààŒ£
2. ùëí= (ùëí‚Ä≤|ùëí‚Ä≥) where ùëí‚Ä≤, ùëí‚Ä≥ are regular expressions.
3. ùëí
=
(ùëí‚Ä≤)(ùëí‚Ä≥) where ùëí‚Ä≤, ùëí‚Ä≥ are regular expressions. (We often
drop the parentheses when there is no danger of confusion and
so write this as ùëí‚Ä≤ ùëí‚Ä≥.)
4. ùëí= (ùëí‚Ä≤)‚àówhere ùëí‚Ä≤ is a regular expression.
Finally we also allow the following ‚Äúedge cases‚Äù: ùëí
=
‚àÖand
ùëí= "". These are the regular expressions corresponding to accept-
ing no strings, and accepting only the empty string respectively.
We will drop parenthesis when they can be inferred from the
context. We also use the convention that OR and concatenation are
left-associative, and we give highest precedence to ‚àó, then concate-
nation, and then OR. Thus for example we write 00‚àó|11 instead of
((0)(0‚àó))|((1)(1)).
Every regular expression ùëícorresponds to a function Œ¶ùëí‚à∂Œ£‚àó‚Üí
{0, 1} where Œ¶ùëí(ùë•) = 1 if ùë•matches the regular expression. For exam-
ple, if ùëí= (00|11)‚àóthen Œ¶ùëí(110011) = 1 but Œ¶ùëí(101) = 0 (can you see
why?).
P
The formal definition of Œ¶ùëíis one of those definitions
that is more cumbersome to write than to grasp. Thus
it might be easier for you to first work it out on your
own and then check that your definition matches what
is written below.
Definition 6.7 ‚Äî Matching a regular expression. Let ùëíbe a regular expres-
sion over the alphabet Œ£. The function Œ¶ùëí‚à∂Œ£‚àó‚Üí{0, 1} is defined
as follows:
1. If ùëí= ùúéthen Œ¶ùëí(ùë•) = 1 iff ùë•= ùúé.
2. If ùëí= (ùëí‚Ä≤|ùëí‚Ä≥) then Œ¶ùëí(ùë•) = Œ¶ùëí‚Ä≤(ùë•)‚à®Œ¶ùëí‚Ä≥(ùë•) where ‚à®is the OR op-
erator.


--- Page 230 ---

230
introduction to theoretical computer science
3. If ùëí= (ùëí‚Ä≤)(ùëí‚Ä≥) then Œ¶ùëí(ùë•) = 1 iff there is some ùë•‚Ä≤, ùë•‚Ä≥ ‚ààŒ£‚àósuch
that ùë•is the concatenation of ùë•‚Ä≤ and ùë•‚Ä≥ and Œ¶ùëí‚Ä≤(ùë•‚Ä≤) = Œ¶ùëí‚Ä≥(ùë•‚Ä≥) =
1.
4. If ùëí= (ùëí‚Ä≤)‚àóthen Œ¶ùëí(ùë•) = 1 iff there is some ùëò‚àà‚Ñïand some
ùë•0, ‚Ä¶ , ùë•ùëò‚àí1 ‚ààŒ£‚àósuch that ùë•is the concatenation ùë•0 ‚ãØùë•ùëò‚àí1 and
Œ¶ùëí‚Ä≤(ùë•ùëñ) = 1 for every ùëñ‚àà[ùëò].
5. Finally, for the edge cases Œ¶‚àÖis the constant zero function, and
Œ¶"" is the function that only outputs 1 on the empty string "".
We say that a regular expression ùëíover Œ£ matches a string ùë•‚ààŒ£‚àó
if Œ¶ùëí(ùë•) = 1.
P
The definitions above are not inherently difficult, but
are a bit cumbersome. So you should pause here and
go over it again until you understand why it corre-
sponds to our intuitive notion of regular expressions.
This is important not just for understanding regular
expressions themselves (which are used time and
again in a great many applications) but also for get-
ting better at understanding recursive definitions in
general.
A Boolean function is called ‚Äúregular‚Äù if it outputs 1 on precisely
the set of strings that are matched by some regular expression. That is,
Definition 6.8 ‚Äî Regular functions / languages. Let Œ£ be a finite set and
ùêπ
‚à∂Œ£‚àó‚Üí{0, 1} be a Boolean function. We say that ùêπis regular if
ùêπ= Œ¶ùëífor some regular expression ùëí.
Similarly, for every formal language ùêø‚äÜŒ£‚àó, we say that ùêøis reg-
ular if and only if there is a regular expression ùëísuch that ùë•‚ààùêøiff
ùëímatches ùë•.
‚ñ†Example 6.9 ‚Äî A regular function. Let Œ£ = {ùëé, ùëè, ùëê, ùëë, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9}
and ùêπ
‚à∂Œ£‚àó
‚Üí{0, 1} be the function such that ùêπ(ùë•) outputs 1 iff
ùë•consists of one or more of the letters ùëé-ùëëfollowed by a sequence
of one or more digits (without a leading zero). Then ùêπis a regular
function, since ùêπ= Œ¶ùëíwhere
ùëí= (ùëé|ùëè|ùëê|ùëë)(ùëé|ùëè|ùëê|ùëë)‚àó(0|1|2|3|4|5|6|7|8|9)(0|1|2|3|4|5|6|7|8|9)‚àó
(6.11)
is the expression we saw in (6.10).


--- Page 231 ---

functions with infinite domains, automata, and regular expressions
231
If we wanted to verify, for example, that Œ¶ùëí(ùëéùëèùëê12078)
=
1,
we can do so by noticing that the expression (ùëé|ùëè|ùëê|ùëë) matches
the string ùëé, (ùëé|ùëè|ùëê|ùëë)‚àómatches ùëèùëê, (0|1|2|3|4|5|6|7|8|9) matches the
string 1, and the expression (0|1|2|3|4|5|6|7|8|9)‚àómatches the string
2078. Each one of those boils down to a simpler expression. For ex-
ample, the expression (ùëé|ùëè|ùëê|ùëë)‚àómatches the string ùëèùëêbecause both
of the one-character strings ùëèand ùëêare matched by the expression
ùëé|ùëè|ùëê|ùëë.
Regular expression can be defined over any finite alphabet Œ£, but
as usual, we will mostly focus our attention on the binary case, where
Œ£ = {0, 1}. Most (if not all) of the theoretical and practical general
insights about regular expressions can be gleaned from studying the
binary case.
6.3.1 Algorithms for matching regular expressions
Regular expressions would not be very useful for search if we could
not actually evaulate, given a regular expression ùëí, whether a string ùë•
is matched by ùëí. Luckily, there is an algorithm to do so. Specifically,
there is an algorithm (think ‚ÄúPython program‚Äù though later we will
formalize the notion of algorithms using Turing machines) that on
input a regular expression ùëíover the alphabet {0, 1} and a string ùë•‚àà
{0, 1}‚àó, outputs 1 iff ùëímatches ùë•(i.e., outputs Œ¶ùëí(ùë•)).
Indeed, Definition 6.7 actually specifies a recursive algorithm for
computing Œ¶ùëí. Specifically, each one of our operations -concatenation,
OR, and star- can be thought of as reducing the task of testing whether
an expression ùëímatches a string ùë•to testing whether some sub-
expressions of ùëímatch substrings of ùë•. Since these sub-expressions
are always shorter than the original expression, this yields a recursive
algorithm for checking if ùëímatches ùë•which will eventually terminate
at the base cases of the expressions that correspond to a single symbol
or the empty string.


--- Page 232 ---

232
introduction to theoretical computer science
Algorithm 6.10 ‚Äî Regular expression matching.
Input: Regular expression ùëíover Œ£‚àó, ùë•‚ààŒ£‚àó
Output: Œ¶ùëí(ùë•)
1: procedure Match(ùëí,ùë•)
2:
if ùëí= ‚àÖthen return 0 ;
3:
if ùë•= "" then return MatchEmpty(()ùëí) ;
4:
if ùëí‚ààŒ£ then return 1 iff ùë•= ùëí;
5:
if ùëí= (ùëí‚Ä≤|ùëí‚Ä≥) then return Match(ùëí‚Ä≤, ùë•) or Match(ùëí‚Ä≥, ùë•)
;
6:
if ùëí= (ùëí‚Ä≤)(ùëí‚Ä≥) then
7:
for ùëñ‚àà[|ùë•| + 1] do
8:
if Match(ùëí‚Ä≤, ùë•0 ‚ãØùë•ùëñ‚àí1) and Match(ùëí‚Ä≥, ùë•ùëñ‚ãØùë•|ùë•|‚àí1)
then return 1 ;
9:
end for
10:
end if
11:
if ùëí= (ùëí‚Ä≤)‚àóthen
12:
if ùëí‚Ä≤ = "" then return Match("", ùë•) ;
13:
# ("")‚àóis the same as ""
14:
for ùëñ‚àà[|ùë•|] do
15:
# ùë•0 ‚ãØùë•ùëñ‚àí1 is shorter than ùë•
16:
if Match(ùëí, ùë•0 ‚ãØùë•ùëñ‚àí1) and Match(ùëí‚Ä≤, ùë•ùëñ‚ãØùë•|ùë•|‚àí1)
then return 1 ;
17:
end for
18:
end if
19:
return 0
20: end procedure
We assume above that we have a procedure MatchEmpty that
on input a regular expression ùëíoutputs 1 if and only if ùëímatches the
empty string "".
The key observation is that in our recursive definition of regular ex-
pressions, whenever ùëíis made up of one or two expressions ùëí‚Ä≤, ùëí‚Ä≥ then
these two regular expressions are smaller than ùëí. Eventually (when
they have size 1) then they must correspond to the non-recursive
case of a single alphabet symbol. Correspondingly, the recursive calls
made in Algorithm 6.10 always correspond to a shorter expression or
(in the case of an expression of the form (ùëí‚Ä≤)‚àó) a shorter input string.
Thus, we can prove the correctness of Algorithm 6.10 on inputs of
the form (ùëí, ùë•) by induction over min{|ùëí|, |ùë•|}. The base case is when
either ùë•= "" or ùëíis a single alphabet symbol, "" or ‚àÖ. In the case
the expression is of the forrm ùëí= (ùëí‚Ä≤|ùëí‚Ä≥) or ùëí= (ùëí‚Ä≤)(ùëí‚Ä≥) then we
make recursive calls with the shorter expressions ùëí‚Ä≤, ùëí‚Ä≥. In the case
the expression is of the form ùëí= (ùëí‚Ä≤)‚àówe make recursive calls with


--- Page 233 ---

functions with infinite domains, automata, and regular expressions
233
either a shorter string ùë•and the same expression, or with the shorter
expression ùëí‚Ä≤ and a string ùë•‚Ä≤ that is equal in length or shorter than ùë•.
Solved Exercise 6.3 ‚Äî Match the empty string. Give an algorithm that on
input a regular expression ùëí, outputs 1 if and only if Œ¶ùëí("") = 1.
‚ñ†
Solution:
We can obtain such a recursive algorithm by using the following
observations:
1. An expression of the form "" or (ùëí‚Ä≤)‚àóalways matches the empty
string.
2. An expression of the form ùúé, where ùúé
‚àà
Œ£ is an alphabet sym-
bol, never matches the empty string.
3. The regular expression ‚àÖdoes not match the empty string.
4. An expression of the form ùëí‚Ä≤|ùëí‚Ä≥ matches the empty string if and
only if one of ùëí‚Ä≤ or ùëí‚Ä≥ matches it.
5. An expression of the form (ùëí‚Ä≤)(ùëí‚Ä≥) matches the empty string if
and only if both ùëí‚Ä≤ and ùëí‚Ä≥ match it.
Given the above observations, we see that the following algo-
rithm will check if ùëímatches the empty string:
procedure{MatchEmpty}{ùëí} lIf {ùëí
=
‚àÖ} return 0 lendif lIf
{ùëí=
""} return 1 lendif lIf {ùëí= ‚àÖor ùëí‚ààŒ£} return 0 lendif lIf
{ùëí
=
(ùëí‚Ä≤|ùëí‚Ä≥)} return ùëÄùëéùë°ùëê‚Ñéùê∏ùëöùëùùë°ùë¶(ùëí‚Ä≤) or ùëÄùëéùë°ùëê‚Ñéùê∏ùëöùëùùë°ùë¶(ùëí‚Ä≥) lendif
LIf {ùëí
=
(ùëí‚Ä≤)(ùëü‚Ä≤)} return ùëÄùëéùë°ùëê‚Ñéùê∏ùëöùëùùë°ùë¶(ùëí‚Ä≤) or ùëÄùëéùë°ùëê‚Ñéùê∏ùëöùëùùë°ùë¶(ùëí‚Ä≥)
lendif lIf {ùëí= (ùëí‚Ä≤)‚àó} return 1 lendif endprocedure
‚ñ†
6.4 EFFICIENT MATCHING OF REGULAR EXPRESSIONS (OP-
TIONAL)
Algorithm 6.10 is not very efficient. For example, given an expression
involving concatenation or the ‚Äústar‚Äù operation and a string of length
ùëõ, it can make ùëõrecursive calls, and hence it can be shown that in the
worst case Algorithm 6.10 can take time exponential in the length of
the input string ùë•. Fortunately, it turns out that there is a much more
efficient algorithm that can match regular expressions in linear (i.e.,
ùëÇ(ùëõ)) time. Since we have not yet covered the topics of time and space
complexity, we describe this algorithm in high level terms, without
making the computational model precise, using the colloquial no-
tion of ùëÇ(ùëõ) running time as is used in introduction to programming


--- Page 234 ---

234
introduction to theoretical computer science
courses and whiteboard coding interviews. We will see a formal defi-
nition of time complexity in Chapter 13.
Theorem 6.11 ‚Äî Matching regular expressions in linear time. Let ùëíbe a
regular expression. Then there is an ùëÇ(ùëõ) time algorithm that
computes Œ¶ùëí.
The implicit constant in the ùëÇ(ùëõ) term of Theorem 6.11 depends
on the expression ùëí. Thus, another way to state Theorem 6.11 is that
for every expression ùëí, there is some constant ùëêand an algorithm ùê¥
that computes Œ¶ùëíon ùëõ-bit inputs using at most ùëê‚ãÖùëõsteps. This makes
sense, since in practice we often want to compute Œ¶ùëí(ùë•) for a small
regular expression ùëíand a large document ùë•. Theorem 6.11 tells us
that we can do so with running time that scales linearly with the size
of the document, even if it has (potentially) worse dependence on the
size of the regular expression.
We prove Theorem 6.11 by obtaining more efficient recursive al-
gorithm, that determines whether ùëímatches a string ùë•‚àà{0, 1}ùëõby
reducing this task to determining whether a related expression ùëí‚Ä≤
matches ùë•0, ‚Ä¶ , ùë•ùëõ‚àí2. This will result in an expression for the running
time of the form ùëá(ùëõ) = ùëá(ùëõ‚àí1) + ùëÇ(1) which solves to ùëá(ùëõ) = ùëÇ(ùëõ).
Restrictions of regular expressions.
The central definition for the algo-
rithm behind Theorem 6.11 is the notion of a restriction of a regular
expression. The idea is that for every regular expression ùëíand symbol
ùúéin its alphabet, it is possible to define a regular expression ùëí[ùúé] such
that ùëí[ùúé] matches a string ùë•if and only if ùëímatches the string ùë•ùúé. For
example, if ùëíis the regular expression 01|(01) ‚àó(01) (i.e., one or more
occurrences of 01) then ùëí[1] is equal to 0|(01) ‚àó0 and ùëí[0] will be ‚àÖ.
(Can you see why?)
Algorithm 6.12 computes the resriction ùëí[ùúé] given a regular ex-
pression ùëíand an alphabet symbol ùúé. It always terminates, since the
recursive calls it makes are always on expressions smaller than the
input expression. Its correctness can be proven by induction on the
length of the regular expression ùëí, with the base cases being when ùëíis
"", ‚àÖ, or a single alphabet symbol ùúè.


--- Page 235 ---

functions with infinite domains, automata, and regular expressions
235
Algorithm 6.12 ‚Äî Restricting regular expression.
Input: Regular expression ùëíover Œ£, symbol ùúé‚ààŒ£
Output: Regular expression ùëí‚Ä≤
= ùëí[ùúé] such that Œ¶ùëí‚Ä≤(ùë•) =
Œ¶ùëí(ùë•ùúé) for every ùë•‚ààŒ£‚àó
1: procedure Restrict(ùëí,ùúé)
2:
if ùëí= "" or ùëí= ‚àÖthen return ‚àÖ;
3:
if ùëí= ùúèfor ùúè‚ààŒ£ then return "" if ùúè= ùúéand return
‚àÖotherwise ;
4:
if ùëí= (ùëí‚Ä≤|ùëí‚Ä≥) then return (Restrict(ùëí‚Ä≤, ùúé)|Restrict(ùëí‚Ä≥, ùúé))
;
5:
if ùëí= (ùëí‚Ä≤)‚àóthen return (ùëí‚Ä≤)‚àó(Restrict(ùëí‚Ä≤, ùúé)) ;
6:
if ùëí
=
(ùëí‚Ä≤)(ùëí‚Ä≥) and Œ¶ùëí‚Ä≥("")
=
0 then return
(ùëí‚Ä≤)(Restrict(ùëí‚Ä≥, ùúé)) ;
7:
if ùëí
=
(ùëí‚Ä≤)(ùëí‚Ä≥) and Œ¶ùëí‚Ä≥("")
=
1 then return
(ùëí‚Ä≤)(Restrict(ùëí‚Ä≥, ùúé) | Restrict(ùëí‚Ä≤, ùúé)) ;
8: end procedure
Using this notion of restriction, we can define the following recur-
sive algorithm for regular expression matching:
Algorithm 6.13 ‚Äî Regular expression matching in linear time.
Input: Regular expression ùëíover Œ£‚àó, ùë•‚ààŒ£ùëõwhere ùëõ‚àà‚Ñï
Output: Œ¶ùëí(ùë•)
1: procedure FMatch(ùëí,ùë•)
2:
if ùë•= "" then return MatchEmpty(()ùëí) ;
3:
Let ùëí‚Ä≤ ‚ÜêRestrict(ùëí, ùë•ùëõ‚àí2)
4:
return FMatch(ùëí‚Ä≤, ùë•0 ‚ãØùë•ùëõ‚àí1)
5: end procedure
By the definition of a restriction, for every ùúé‚ààŒ£ and ùë•‚Ä≤ ‚ààŒ£‚àó,
the expression ùëímatches ùë•‚Ä≤ùúéif and only if ùëí[ùúé] matches ùë•‚Ä≤. Hence for
every ùëíand ùë•‚ààŒ£ùëõ, Œ¶ùëí[ùë•ùëõ‚àí1](ùë•0 ‚ãØùë•ùëõ‚àí2) = Œ¶ùëí(ùë•) and Algorithm 6.13
does return the correct answer. The only remaining task is to analyze
its running time. Note that Algorithm 6.13 uses the MatchEmpty
procedure of Solved Exercise 6.3 in the base case that ùë•= "". However,
this is OK since the running time of this procedure depends only on ùëí
and is independent of the length of the original input.
For simplicity, let us restrict our attention to the case that the al-
phabet Œ£ is equal to {0, 1}. Define ùê∂(‚Ñì) to be the maximum number
of operations that Algorithm 6.12 takes when given as input a regular
expression ùëíover {0, 1} of at most ‚Ñìsymbols. The value ùê∂(‚Ñì) can be
shown to be polynomial in ‚Ñì, though this is not important for this the-
orem, since we only care about the dependence of the time to compute


--- Page 236 ---

236
introduction to theoretical computer science
Œ¶ùëí(ùë•) on the length of ùë•and not about the dependence of this time on
the length of ùëí.
Algorithm 6.13 is a recursive algorithm that input an expression
ùëíand a string ùë•‚àà{0, 1}ùëõ, does computation of at most ùê∂(|ùëí|) steps
and then calls itself with input some expression ùëí‚Ä≤ and a string ùë•‚Ä≤ of
length ùëõ‚àí1. It will terminate after ùëõsteps when it reaches a string of
length 0. So, the running time ùëá(ùëí, ùëõ) that it takes for Algorithm 6.13
to compute Œ¶ùëífor inputs of length ùëõsatisfies the recursive equation:
ùëá(ùëí, ùëõ) = max{ùëá(ùëí[0], ùëõ‚àí1), ùëá(ùëí[1], ùëõ‚àí1)} + ùê∂(|ùëí|)
(6.12)
(In the base case ùëõ= 0, ùëá(ùëí, 0) is equal to some constant depending
only on ùëí.) To get some intuition for the expression Eq. (6.12), let us
open up the recursion for one level, writing ùëá(ùëí, ùëõ) as
ùëá(ùëí, ùëõ) = max{ùëá(ùëí[0][0], ùëõ‚àí2) + ùê∂(|ùëí[0]|),
ùëá(ùëí[0][1], ùëõ‚àí2) + ùê∂(|ùëí[0]|),
ùëá(ùëí[1][0], ùëõ‚àí2) + ùê∂(|ùëí[1]|),
ùëá(ùëí[1][1], ùëõ‚àí2) + ùê∂(|ùëí[1]|)} + ùê∂(|ùëí|) .
(6.13)
Continuing this way, we can see that ùëá(ùëí, ùëõ) ‚â§ùëõ‚ãÖùê∂(ùêø) + ùëÇ(1)
where ùêøis the largest length of any expression ùëí‚Ä≤ that we encounter
along the way. Therefore, the following claim suffices to show that
Algorithm 6.13 runs in ùëÇ(ùëõ) time:
Claim:
Let ùëíbe a regular expression over {0, 1}, then there is a num-
ber ùêø(ùëí) ‚àà‚Ñï, such that for every sequence of symbols ùõº0, ‚Ä¶ , ùõºùëõ‚àí1, if
we define ùëí‚Ä≤ = ùëí[ùõº0][ùõº1] ‚ãØ[ùõºùëõ‚àí1] (i.e., restricting ùëíto ùõº0, and then ùõº1
and so on and so forth), then |ùëí‚Ä≤| ‚â§ùêø(ùëí).
Proof of claim: For a regular expression ùëíover {0, 1} and ùõº‚àà{0, 1}ùëö,
we denote by ùëí[ùõº] the expression ùëí[ùõº0][ùõº1] ‚ãØ[ùõºùëö‚àí1] obtained by restrict-
ing ùëíto ùõº0 and then to ùõº1 and so on. We let ùëÜ(ùëí) = {ùëí[ùõº]|ùõº‚àà{0, 1}‚àó}.
We will prove the claim by showing that for every ùëí, the set ùëÜ(ùëí) is fi-
nite, and hence so is the number ùêø(ùëí) which is the maximum length of
ùëí‚Ä≤ for ùëí‚Ä≤ ‚ààùëÜ(ùëí).
We prove this by induction on the structure of ùëí. If ùëíis a symbol, the
empty string, or the empty set, then this is straightforward to show
as the most expressions ùëÜ(ùëí) can contain are the expression itself, "",
and ‚àÖ. Otherwise we split to the two cases (i) ùëí= ùëí‚Ä≤‚àóand (ii) ùëí=
ùëí‚Ä≤ùëí‚Ä≥, where ùëí‚Ä≤, ùëí‚Ä≥ are smaller expressions (and hence by the induction
hypothesis ùëÜ(ùëí‚Ä≤) and ùëÜ(ùëí‚Ä≥) are finite). In the case (i), if ùëí= (ùëí‚Ä≤)‚àóthen
ùëí[ùõº] is either equal to (ùëí‚Ä≤)‚àóùëí‚Ä≤[ùõº] or it is simply the empty set if ùëí‚Ä≤[ùõº] = ‚àÖ.
Since ùëí‚Ä≤[ùõº] is in the set ùëÜ(ùëí‚Ä≤), the number of distinct expressions in
ùëÜ(ùëí) is at most |ùëÜ(ùëí‚Ä≤)| + 1. In the case (ii), if ùëí= ùëí‚Ä≤ùëí‚Ä≥ then all the
restrictions of ùëíto strings ùõºwill either have the form ùëí‚Ä≤ùëí‚Ä≥[ùõº] or the form
ùëí‚Ä≤ùëí‚Ä≥[ùõº]|ùëí‚Ä≤[ùõº‚Ä≤] where ùõº‚Ä≤ is some string such that ùõº= ùõº‚Ä≤ùõº‚Ä≥ and ùëí‚Ä≥[ùõº‚Ä≥]


--- Page 237 ---

functions with infinite domains, automata, and regular expressions
237
matches the empty string. Since ùëí‚Ä≥[ùõº] ‚ààùëÜ(ùëí‚Ä≥) and ùëí‚Ä≤[ùõº‚Ä≤] ‚ààùëÜ(ùëí‚Ä≤), the
number of the possible distinct expressions of the form ùëí[ùõº] is at most
|ùëÜ(ùëí‚Ä≥)| + |ùëÜ(ùëí‚Ä≥)| ‚ãÖ|ùëÜ(ùëí‚Ä≤)|. This completes the proof of the claim.
The bottom line is that while running Algorithm 6.13 on a regular
expression ùëí, all the expressions we ever encounter are in the finite set
ùëÜ(ùëí), no matter how large the input ùë•is, and so the running time of
Algorithm 6.13 satisfies the equation ùëá(ùëõ) = ùëá(ùëõ‚àí1) + ùê∂‚Ä≤ for some
constant ùê∂‚Ä≤ depending on ùëí. This solves to ùëÇ(ùëõ) where the implicit
constant in the O notation can (and will) depend on ùëíbut crucially,
not on the length of the input ùë•.
6.4.1 Matching regular expressions using DFAs
Theorem 6.11 is already quite impressive, but we can do even better.
Specifically, no matter how long the string ùë•is, we can compute Œ¶ùëí(ùë•)
by maintaining only a constant amount of memory and moreover
making a single pass over ùë•. That is, the algorithm will scan the input
ùë•once from start to finish, and then determine whether or not ùë•is
matched by the expression ùëí. This is important in the common case
of trying to match a short regular expression over a huge file or docu-
ment that might not even fit in our computer‚Äôs memory. Of course, as
we have seen before, a single-pass constant-memory algorithm is sim-
ply a deterministic finite automaton. As we will see in Theorem 6.16, a
function is can be computed by regular expression if and only if it can
be computed by a DFA. We start with showing the ‚Äúonly if‚Äù direction:
Theorem 6.14 ‚Äî DFA for regular expression matching. Let ùëíbe a regular
expression. Then there is an algorithm that on input ùë•
‚àà
{0, 1}‚àó
computes Œ¶ùëí(ùë•) while making a single pass over ùë•and maintaining
a constant amount of memory.
Proof Idea:
The single-pass constant-memory for checking if a string matches
a regular expression is presented in Algorithm 6.15. The idea is to
replace the recursive algorithm of Algorithm 6.13 with a dynamic pro-
gram, using the technique of memoization. If you haven‚Äôt taken yet an
algorithms course, you might not know these techniques. This is OK;
while this more efficient algorithm is crucial for the many practical
applications of regular expressions, it is not of great importance for
this book.
‚ãÜ


--- Page 238 ---

238
introduction to theoretical computer science
Algorithm 6.15 ‚Äî Regular expression matching by a DFA.
Input: Regular expression ùëíover Œ£‚àó, ùë•‚ààŒ£ùëõwhere ùëõ‚àà‚Ñï
Output: Œ¶ùëí(ùë•)
1: procedure DFAMatch(ùëí,ùë•)
2:
Let ùëÜ‚ÜêùëÜ(ùëí) be the set {ùëí[ùõº]|ùõº‚àà{0, 1}‚àó} as defined
in the proof of [reglintimethm]().ref.
3:
for ùëí‚Ä≤ ‚ààùëÜdo
4:
Let ùë£ùëí‚Ä≤ ‚Üê1 if Œ¶ùëí‚Ä≤("") = 1 and ùë£ùëí‚Ä≤ ‚Üê0 otherwise
5:
end for
6:
for ùëñ‚àà[ùëõ] do
7:
Let ùëôùëéùë†ùë°ùëí‚Ä≤ ‚Üêùë£ùëí‚Ä≤ for all ùëí‚Ä≤ ‚ààùëÜ
8:
Let ùë£ùëí‚Ä≤ ‚Üêùëôùëéùë†ùë°ùëí‚Ä≤[ùë•ùëñ] for all ùëí‚Ä≤ ‚ààùëÜ
9:
end for
10:
return ùë£ùëí
11: end procedure
Proof of Theorem 6.14. Algorithm 6.15 checks if a given string ùë•‚ààŒ£‚àó
is matched by the regular expression ùëí. For every regular expres-
sion ùëí, this algorithm has a constant number 2|ùëÜ(ùëí)| Boolean vari-
ables (ùë£ùëí‚Ä≤, ùëôùëéùë†ùë°ùëí‚Ä≤ for ùëí‚Ä≤ ‚ààùëÜ(ùëí)), and it makes a single pass over
the input string. Hence it corresponds to a DFA. We prove its cor-
rectness by induction on the length ùëõof the input. Specifically, we
will argue that before reading the ùëñ-th bit of ùë•, the variable ùë£ùëí‚Ä≤ is
equal to Œ¶ùëí‚Ä≤(ùë•0 ‚ãØùë•ùëñ‚àí1) for every ùëí‚Ä≤ ‚ààùëÜ(ùëí). In the case ùëñ= 0 this
holds since we initialize ùë£ùëí‚Ä≤ = Œ¶ùëí‚Ä≤("") for all ùëí‚Ä≤ ‚ààùëÜ(ùëí). For ùëñ> 0
this holds by induction since the inductive hypothesis implies that
ùëôùëéùë†ùë°‚Ä≤
ùëí= Œ¶ùëí‚Ä≤(ùë•0 ‚ãØùë•ùëñ‚àí2) for all ùëí‚Ä≤ ‚ààùëÜ(ùëí) and by the definition of the set
ùëÜ(ùëí‚Ä≤), for every ùëí‚Ä≤ ‚ààùëÜ(ùëí) and ùë•ùëñ‚àí1 ‚ààŒ£, ùëí‚Ä≥ = ùëí‚Ä≤[ùë•ùëñ‚àí1] is in ùëÜ(ùëí) and
Œ¶ùëí‚Ä≤(ùë•0 ‚ãØùë•ùëñ‚àí1) = Œ¶ùëí‚Ä≥(ùë•0 ‚ãØùë•ùëñ).
‚ñ†
6.4.2 Equivalence of regular expressions and automata
Recall that a Boolean function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1} is defined to be
regular if it is equal to Œ¶ùëífor some regular expression ùëí. (Equivalently,
a language ùêø‚äÜ{0, 1}‚àóis defined to be regular if there is a regular
expression ùëísuch that ùëímatches ùë•iff ùë•‚ààùêø.) The following theorem is
the central result of automata theory:
Theorem 6.16 ‚Äî DFA and regular expression equivalency. Let ùêπ‚à∂{0, 1}‚àó‚Üí
{0, 1}. Then ùêπis regular if and only if there exists a DFA (ùëá, ùíÆ) that
computes ùêπ.
Proof Idea:


--- Page 239 ---

functions with infinite domains, automata, and regular expressions
239
Figure 6.6: A deterministic finite automaton that
computes the function Œ¶(01)‚àó.
Figure 6.7: Given a DFA of ùê∂states, for every ùë£, ùë§‚àà
[ùê∂] and number ùë°‚àà{0, ‚Ä¶ , ùê∂} we define the function
ùêπùë°
ùë£,ùë§‚à∂{0, 1}‚àó‚Üí{0, 1} to output one on input
ùë•‚àà{0, 1}‚àóif and only if when the DFA is initialized
in the state ùë£and is given the input ùë•, it will reach the
state ùë§while going only through the intermediate
states {0, ‚Ä¶ , ùë°‚àí1}.
One direction follows from Theorem 6.14, which shows that for
every regular expression ùëí, the function Œ¶ùëícan be computed by a DFA
(see for example Fig. 6.6). For the other direction, we show that given
a DFA (ùëá, ùíÆ) for every ùë£, ùë§‚àà[ùê∂] we can find a regular expression that
would match ùë•‚àà{0, 1}‚àóif and only if the DFA starting in state ùë£, will
end up in state ùë§after reading ùë•.
‚ãÜ
Proof of Theorem 6.16. Since Theorem 6.14 proves the ‚Äúonly if‚Äù direc-
tion, we only need to show the ‚Äúif‚Äù direction. Let ùê¥= (ùëá, ùíÆ) be a DFA
with ùê∂states that computes the function ùêπ. We need to show that ùêπis
regular.
For every ùë£, ùë§‚àà[ùê∂], we let ùêπùë£,ùë§‚à∂{0, 1}‚àó‚Üí{0, 1} be the function
that maps ùë•‚àà{0, 1}‚àóto 1 if and only if the DFA ùê¥, starting at the
state ùë£, will reach the state ùë§if it reads the input ùë•. We will prove that
ùêπùë£,ùë§is regular for every ùë£, ùë§. This will prove the theorem, since by
Definition 6.2, ùêπ(ùë•) is equal to the OR of ùêπ0,ùë§(ùë•) for every ùë§‚ààùíÆ.
Hence if we have a regular expression for every function of the form
ùêπùë£,ùë§then (using the | operation) we can obtain a regular expression
for ùêπas well.
To give regular expressions for the functions ùêπùë£,ùë§, we start by
defining the following functions ùêπùë°
ùë£,ùë§: for every ùë£, ùë§‚àà[ùê∂] and
0 ‚â§ùë°‚â§ùê∂, ùêπùë°
ùë£,ùë§(ùë•) = 1 if and only if starting from ùë£and observ-
ing ùë•, the automata reaches ùë§with all intermediate states being in the set
[ùë°] = {0, ‚Ä¶ , ùë°‚àí1} (see Fig. 6.7). That is, while ùë£, ùë§themselves might
be outside [ùë°], ùêπùë°
ùë£,ùë§(ùë•) = 1 if and only if throughout the execution of
the automaton on the input ùë•(when initiated at ùë£) it never enters any
of the states outside [ùë°] and still ends up at ùë§. If ùë°= 0 then [ùë°] is the
empty set, and hence ùêπ0
ùë£,ùë§(ùë•) = 1 if and only if the automaton reaches
ùë§from ùë£directly on ùë•, without any intermediate state. If ùë°= ùê∂then
all states are in [ùë°], and hence ùêπùë°
ùë£,ùë§= ùêπùë£,ùë§.
We will prove the theorem by induction on ùë°, showing that ùêπùë°
ùë£,ùë§is
regular for every ùë£, ùë§and ùë°. For the base case of ùë°= 0, ùêπ0
ùë£,ùë§is regular
for every ùë£, ùë§since it can be described as one of the expressions "", ‚àÖ,
0, 1 or 0|1. Specifically, if ùë£= ùë§then ùêπ0
ùë£,ùë§(ùë•) = 1 if and only if ùë•is
the empty string. If ùë£‚â†ùë§then ùêπ0
ùë£,ùë§(ùë•) = 1 if and only if ùë•consists
of a single symbol ùúé‚àà{0, 1} and ùëá(ùë£, ùúé) = ùë§. Therefore in this case
ùêπ0
ùë£,ùë§corresponds to one of the four regular expressions 0|1, 0, 1 or ‚àÖ,
depending on whether ùê¥transitions to ùë§from ùë£when it reads either 0
or 1, only one of these symbols, or neither.
Inductive step: Now that we‚Äôve seen the base case, let‚Äôs prove the
general case by induction. Assume, via the induction hypothesis, that
for every ùë£‚Ä≤, ùë§‚Ä≤ ‚àà[ùê∂], we have a regular expression ùëÖùë°
ùë£,ùë§that computes
ùêπùë°
ùë£‚Ä≤,ùë§‚Ä≤. We need to prove that ùêπùë°+1
ùë£,ùë§is regular for every ùë£, ùë§. If the


--- Page 240 ---

240
introduction to theoretical computer science
automaton arrives from ùë£to ùë§using the intermediate states [ùë°+ 1],
then it visits the ùë°-th state zero or more times. If the path labeled by ùë•
causes the automaton to get from ùë£to ùë§without visiting the ùë°-th state
at all, then ùë•is matched by the regular expression ùëÖùë°
ùë£,ùë§. If the path
labeled by ùë•causes the automaton to get from ùë£to ùë§while visiting the
ùë°-th state ùëò> 0 times then we can think of this path as:
‚Ä¢ First travel from ùë£to ùë°using only intermediate states in [ùë°‚àí1].
‚Ä¢ Then go from ùë°back to itself ùëò‚àí1 times using only intermediate
states in [ùë°‚àí1]
‚Ä¢ Then go from ùë°to ùë§using only intermediate states in [ùë°‚àí1].
Therefore in this case the string ùë•is matched by the regular expres-
sion ùëÖùë°
ùë£,ùë°(ùëÖùë°
ùë°,ùë°)‚àóùëÖùë°
ùë°,ùë§. (See also Fig. 6.8.)
Therefore we can compute ùêπùë°+1
ùë£,ùë§using the regular expression
ùëÖùë°
ùë£,ùë§| ùëÖùë°
ùë£,ùë°(ùëÖùë°
ùë°,ùë°)‚àóùëÖùë°
ùë°,ùë§.
(6.14)
This completes the proof of the inductive step and hence of the theo-
rem.
‚ñ†
Figure 6.8: If we have regular expressions ùëÖùë°
ùë£‚Ä≤,ùë§‚Ä≤
corresponding to ùêπùë°
ùë£‚Ä≤,ùë§‚Ä≤ for every ùë£‚Ä≤, ùë§‚Ä≤ ‚àà[ùê∂], we can
obtain a regular expression ùëÖùë°+1
ùë£,ùë§corresponding to
ùêπùë°+1
ùë£,ùë§. The key observation is that a path from ùë£to ùë§
using {0, ‚Ä¶ , ùë°} either does not touch ùë°at all, in which
case it is captured by the expression ùëÖùë°
ùë£,ùë§, or it goes
from ùë£to ùë°, comes back to ùë°zero or more times, and
then goes from ùë°to ùë§, in which case it is captured by
the expression ùëÖùë°
ùë£,ùë°(ùëÖùë°
ùë°,ùë°)‚àóùëÖùë°
ùë°,ùë§.
6.4.3 Closure properties of regular expressions
If ùêπand ùê∫are regular functions computed by the expressions ùëíand ùëì
respectively, then the expression ùëí|ùëìcomputes the function ùêª= ùêπ‚à®ùê∫
defined as ùêª(ùë•) = ùêπ(ùë•) ‚à®ùê∫(ùë•). Another way to say this is that the set
of regular functions is closed under the OR operation. That is, if ùêπand ùê∫
are regular then so is ùêπ‚à®ùê∫. An important corollary of Theorem 6.16
is that this set is also closed under the NOT operation:


--- Page 241 ---

functions with infinite domains, automata, and regular expressions
241
Lemma 6.17 ‚Äî Regular expressions closed under complement. If ùêπ‚à∂{0, 1}‚àó‚Üí
{0, 1} is regular then so is the function ùêπ, where ùêπ(ùë•) = 1 ‚àíùêπ(ùë•) for
every ùë•‚àà{0, 1}‚àó.
Proof. If ùêπis regular then by Theorem 6.11 it can be computed by a
DFA ùê¥= (ùëá, ùíú) with some ùê∂states. But then the DFA ùê¥= (ùëá, [ùê∂]‚ßµùíú)
which does the same computation but where flips the set of accepted
states will compute ùêπ. By Theorem 6.16 this implies that ùêπis regular
as well.
‚ñ†
Since ùëé‚àßùëè= ùëé‚à®ùëè, Lemma 6.17 implies that the set of regular
functions is closed under the AND operation as well. Moreover, since
OR, NOT and AND are a universal basis, this set is also closed un-
der NAND, XOR, and any other finite function. That is, we have the
following corollary:
Theorem 6.18 ‚Äî Closure of regular expressions. Let ùëì‚à∂{0, 1}ùëò‚Üí{0, 1} be
any finite Boolean function, and let ùêπ0, ‚Ä¶ , ùêπùëò‚àí1 ‚à∂{0, 1}‚àó‚Üí{0, 1} be
regular functions. Then the function ùê∫(ùë•) = ùëì(ùêπ0(ùë•), ùêπ1(ùë•), ‚Ä¶ , ùêπùëò‚àí1(ùë•))
is regular.
Proof. This is a direct consequence of the closure of regular functions
under OR and NOT (and hence AND) and Theorem 4.13, that states
that every ùëìcan be computed by a Boolean circuit (which is simply a
combination of the AND, OR, and NOT operations).
‚ñ†
6.5 LIMITATIONS OF REGULAR EXPRESSIONS AND THE PUMPING
LEMMA
The efficiency of regular expression matching makes them very useful.
This is the reason why operating systems and text editors often restrict
their search interface to regular expressions and don‚Äôt allow searching
by specifying an arbitrary function. But this efficiency comes at a cost.
As we have seen, regular expressions cannot compute every function.
In fact there are some very simple (and useful!) functions that they
cannot compute. Here is one example:
Lemma 6.19 ‚Äî Matching parenthesis. Let Œ£ = {‚ü®, ‚ü©} and MATCHPAREN ‚à∂
Œ£‚àó‚Üí{0, 1} be the function that given a string of parentheses, out-
puts 1 if and only if every opening parenthesis is matched by a corre-
sponding closed one. Then there is no regular expression over Œ£ that
computes MATCHPAREN.
Lemma 6.19 is a consequence of the following result, which is
known as the pumping lemma:


--- Page 242 ---

242
introduction to theoretical computer science
Theorem 6.20 ‚Äî Pumping Lemma. Let ùëíbe a regular expression over
some alphabet Œ£. Then there is some number ùëõ0 such that for ev-
ery ùë§‚ààŒ£‚àówith |ùë§| > ùëõ0 and Œ¶ùëí(ùë§) = 1, we can write ùë§= ùë•ùë¶ùëßfor
strings ùë•, ùë¶, ùëß‚ààŒ£‚àósatisfying the following conditions:
1. |ùë¶| ‚â•1.
2. |ùë•ùë¶| ‚â§ùëõ0.
3. Œ¶ùëí(ùë•ùë¶ùëòùëß) = 1 for every ùëò‚àà‚Ñï.
Figure 6.9: To prove the ‚Äúpumping lemma‚Äù we look
at a word ùë§that is much larger than the regular
expression ùëíthat matches it. In such a case, part of
ùë§must be matched by some sub-expression of the
form (ùëí‚Ä≤)‚àó, since this is the only operator that allows
matching words longer than the expression. If we
look at the ‚Äúleftmost‚Äù such sub-expression and define
ùë¶ùëòto be the string that is matched by it, we obtain the
partition needed for the pumping lemma.
Proof Idea:
The idea behind the proof the following. Let ùëõ0 be twice the num-
ber of symbols that are used in the expression ùëí, then the only way
that there is some ùë§with |ùë§| > ùëõ0 and Œ¶ùëí(ùë§) = 1 is that ùëícontains
the ‚àó(i.e. star) operator and that there is a nonempty substring ùë¶of
ùë§that was matched by (ùëí‚Ä≤)‚àófor some sub-expression ùëí‚Ä≤ of ùëí. We can
now repeat ùë¶any number of times and still get a matching string. See
also Fig. 6.9.
‚ãÜ
P
The pumping lemma is a bit cumbersome to state,
but one way to remember it is that it simply says the
following: ‚Äúif a string matching a regular expression is
long enough, one of its substrings must be matched using
the ‚àóoperator‚Äù.


--- Page 243 ---

functions with infinite domains, automata, and regular expressions
243
Proof of Theorem 6.20. To prove the lemma formally, we use induction
on the length of the expression. Like all induction proofs, this is going
to be somewhat lengthy, but at the end of the day it directly follows
the intuition above that somewhere we must have used the star oper-
ation. Reading this proof, and in particular understanding how the
formal proof below corresponds to the intuitive idea above, is a very
good way to get more comfortable with inductive proofs of this form.
Our inductive hypothesis is that for an ùëõlength expression, ùëõ0 =
2ùëõsatisfies the conditions of the lemma. The base case is when the
expression is a single symbol ùúé‚ààŒ£ or that the expression is ‚àÖor
"". In all these cases the conditions of the lemma are satisfied simply
because there ùëõ0 = 2 and there is no string ùë•of length larger than ùëõ0
that is matched by the expression.
We now prove the inductive step. Let ùëíbe a regular expression
with ùëõ> 1 symbols. We set ùëõ0 = 2ùëõand let ùë§‚ààŒ£‚àóbe a string
satisfying |ùë§| > ùëõ0. Since ùëíhas more than one symbol, it has one of
the the forms (a) ùëí‚Ä≤|ùëí‚Ä≥, (b), (ùëí‚Ä≤)(ùëí‚Ä≥), or (c) (ùëí‚Ä≤)‚àówhere in all these
cases the subexpressions ùëí‚Ä≤ and ùëí‚Ä≥ have fewer symbols than ùëíand
hence satisfy the induction hypothesis.
In the case (a), every string ùë§matched by ùëímust be matched by
either ùëí‚Ä≤ or ùëí‚Ä≥. If ùëí‚Ä≤ matches ùë§then, since |ùë§| > 2|ùëí‚Ä≤|, by the induction
hypothesis there exist ùë•, ùë¶, ùëßwith |ùë¶| ‚â•1 and |ùë•ùë¶| ‚â§2|ùëí‚Ä≤| < ùëõ0 such
that ùëí‚Ä≤ (and therefore also ùëí= ùëí‚Ä≤|ùëí‚Ä≥) matches ùë•ùë¶ùëòùëßfor every ùëò. The
same arguments works in the case that ùëí‚Ä≥ matches ùë§.
In the case (b), if ùë§is matched by (ùëí‚Ä≤)(ùëí‚Ä≥) then we can write ùë§=
ùë§‚Ä≤ùë§‚Ä≥ where ùëí‚Ä≤ matches ùë§‚Ä≤ and ùëí‚Ä≥ matches ùë§‚Ä≥. We split to subcases. If
|ùë§‚Ä≤| > 2|ùëí‚Ä≤| then by the induction hypothesis there exist ùë•, ùë¶, ùëß‚Ä≤ with
|ùë¶| ‚â§1, |ùë•ùë¶| ‚â§2|ùëí‚Ä≤| < ùëõ0 such that ùë§‚Ä≤ = ùë•ùë¶ùëß‚Ä≤ and ùëí‚Ä≤ matches ùë•ùë¶ùëòùëß‚Ä≤
for every ùëò‚àà‚Ñï. This completes the proof since if we set ùëß= ùëß‚Ä≤ùë§‚Ä≥
then we see that ùë§= ùë§‚Ä≤ùë§‚Ä≥ = ùë•ùë¶ùëßand ùëí= (ùëí‚Ä≤)(ùëí‚Ä≥) matches ùë•ùë¶ùëòùëßfor
every ùëò‚àà‚Ñï. Otherwise, if |ùë§‚Ä≤| ‚â§2|ùëí‚Ä≤| then since |ùë§| = |ùë§‚Ä≤| + |ùë§‚Ä≥| >
ùëõ0 = 2(|ùëí‚Ä≤| + |ùëí‚Ä≥|), it must be that |ùë§‚Ä≥| > 2|ùëí‚Ä≥|. Hence by the induction
hypothesis there exist ùë•‚Ä≤, ùë¶, ùëßsuch that |ùë¶| ‚â•1, |ùë•‚Ä≤ùë¶| ‚â§2|ùëí‚Ä≥| and ùëí‚Ä≥
matches ùë•‚Ä≤ùë¶ùëòùëßfor every ùëò‚àà‚Ñï. But now if we set ùë•= ùë§‚Ä≤ùë•‚Ä≤ we see that
|ùë•ùë¶| ‚â§|ùë§‚Ä≤| + |ùë•‚Ä≤ùë¶| ‚â§2|ùëí‚Ä≤| + 2|ùëí‚Ä≥| = ùëõ0 and on the other hand the
expression ùëí= (ùëí‚Ä≤)(ùëí‚Ä≥) matches ùë•ùë¶ùëòùëß= ùë§‚Ä≤ùë•‚Ä≤ùë¶ùëòùëßfor every ùëò‚àà‚Ñï.
In case (c), if ùë§is matched by (ùëí‚Ä≤)‚àóthen ùë§= ùë§0 ‚ãØùë§ùë°where for
every ùëñ‚àà[ùë°], ùë§ùëñis a nonempty string matched by ùëí‚Ä≤. If |ùë§0| > 2|ùëí‚Ä≤|
then we can use the same approach as in the concatenation case above.
Otherwise, we simply note that if ùë•is the empty string, ùë¶= ùë§0, and
ùëß= ùë§1 ‚ãØùë§ùë°then |ùë•ùë¶| ‚â§ùëõ0 and ùë•ùë¶ùëòùëßis matched by (ùëí‚Ä≤)‚àófor every
ùëò‚àà‚Ñï.
‚ñ†


--- Page 244 ---

244
introduction to theoretical computer science
R
Remark 6.21 ‚Äî Recursive definitions and inductive
proofs. When an object is recursively defined (as in the
case of regular expressions) then it is natural to prove
properties of such objects by induction. That is, if we
want to prove that all objects of this type have prop-
erty ùëÉ, then it is natural to use an inductive steps that
says that if ùëú‚Ä≤, ùëú‚Ä≥, ùëú‚Ä¥ etc have property ùëÉthen so is an
object ùëúthat is obtained by composing them.
Using the pumping lemma, we can easily prove Lemma 6.19 (i.e.,
the non-regularity of the ‚Äúmatching parenthesis‚Äù function):
Proof of Lemma 6.19. Suppose, towards the sake of contradiction, that
there is an expression ùëísuch that Œ¶ùëí= MATCHPAREN. Let ùëõ0 be
the number obtained from Theorem 6.20 and let ùë§= ‚ü®ùëõ0‚ü©ùëõ0 (i.e.,
ùëõ0 left parenthesis followed by ùëõ0 right parenthesis). Then we see
that if we write ùë§= ùë•ùë¶ùëßas in Lemma 6.19, the condition |ùë•ùë¶| ‚â§ùëõ0
implies that ùë¶consists solely of left parenthesis. Hence the string
ùë•ùë¶2ùëßwill contain more left parenthesis than right parenthesis. Hence
MATCHPAREN(ùë•ùë¶2ùëß) = 0 but by the pumping lemma Œ¶ùëí(ùë•ùë¶2ùëß) = 1,
contradicting our assumption that Œ¶ùëí= MATCHPAREN.
‚ñ†
The pumping lemma is a very useful tool to show that certain func-
tions are not computable by a regular expression. However, it is not
an ‚Äúif and only if‚Äù condition for regularity: there are non regular
functions that still satisfy the conditions of the pumping lemma. To
understand the pumping lemma, it is important to follow the order of
quantifiers in Theorem 6.20. In particular, the number ùëõ0 in the state-
ment of Theorem 6.20 depends on the regular expression (in the proof
we chose ùëõ0 to be twice the number of symbols in the expression). So,
if we want to use the pumping lemma to rule out the existence of a
regular expression ùëícomputing some function ùêπ, we need to be able
to choose an appropriate input ùë§‚àà{0, 1}‚àóthat can be arbitrarily large
and satisfies ùêπ(ùë§) = 1. This makes sense if you think about the intu-
ition behind the pumping lemma: we need ùë§to be large enough as to
force the use of the star operator.
Solved Exercise 6.4 ‚Äî Palindromes is not regular. Prove that the following
function over the alphabet {0, 1, ; } is not regular: PAL(ùë§) = 1 if and
only if ùë§= ùë¢; ùë¢ùëÖwhere ùë¢‚àà{0, 1}‚àóand ùë¢ùëÖdenotes ùë¢‚Äúreversed‚Äù:
the string ùë¢|ùë¢|‚àí1 ‚ãØùë¢0. (The Palindrome function is most often defined
without an explicit separator character ;, but the version with such a
separator is a bit cleaner and so we use it here. This does not make


--- Page 245 ---

functions with infinite domains, automata, and regular expressions
245
Figure 6.10: A cartoon of a proof using the pumping lemma that a function ùêπis not regular. The pumping lemma states that if ùêπis regular then there
exists a number ùëõ0 such that for every large enough ùë§with ùêπ(ùë§)
=
1, there exists a partition of ùë§to ùë§
=
ùë•ùë¶ùëßsatisfying certain conditions such
that for every ùëò
‚àà
‚Ñï, ùêπ(ùë•ùë¶ùëòùëß)
=
1. You can imagine a pumping-lemma based proof as a game between you and the adversary. Every there exists
quantifier corresponds to an object you are free to choose on your own (and base your choice on previously chosen objects). Every for every quantifier
corresponds to an object the adversary can choose arbitrarily (and again based on prior choices) as long as it satisfies the conditions. A valid proof
corresponds to a strategy by which no matter what the adversary does, you can win the game by obtaining a contradiction which would be a choice
of ùëòthat would result in ùêπ(ùë•ùë¶ùëòùëß) = 0, hence violating the conclusion of the pumping lemma.


--- Page 246 ---

246
introduction to theoretical computer science
much difference, as one can easily encode the separator as a special
binary string instead.)
‚ñ†
Solution:
We use the pumping lemma. Suppose towards the sake of con-
tradiction that there is a regular expression ùëícomputing PAL,
and let ùëõ0 be the number obtained by the pumping lemma (The-
orem 6.20). Consider the string ùë§
=
0ùëõ0; 0ùëõ0. Since the reverse
of the all zero string is the all zero string, PAL(ùë§)
=
1. Now, by
the pumping lemma, if PAL is computed by ùëí, then we can write
ùë§= ùë•ùë¶ùëßsuch that |ùë•ùë¶| ‚â§ùëõ0, |ùë¶| ‚â•1 and PAL(ùë•ùë¶ùëòùëß) = 1 for
every ùëò‚àà‚Ñï. In particular, it must hold that PAL(ùë•ùëß) = 1, but this
is a contradiction, since ùë•ùëß
=
0ùëõ0‚àí|ùë¶|; 0ùëõ0 and so its two parts are
not of the same length and in particular are not the reverse of one
another.
‚ñ†
For yet another example of a pumping-lemma based proof, see
Fig. 6.10 which illustrates a cartoon of the proof of the non-regularity
of the function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1} which is defined as ùêπ(ùë•) = 1 iff
ùë•= 0ùëõ1ùëõfor some ùëõ‚àà‚Ñï(i.e., ùë•consists of a string of consecutive
zeroes, followed by a string of consecutive ones of the same length).
6.6 ANSWERING SEMANTIC QUESTIONS ABOUT REGULAR EX-
PRESSIONS
Regular expressions have applications beyond search. For example,
regular expressions are often used to define tokens (such as what is a
valid variable identifier, or keyword) in the design of parsers, compilers
and interpreters for programming languages. There other applications
too: for example, in recent years, the world of networking moved from
fixed topologies to ‚Äúsoftware defined networks‚Äù. Such networks are
routed by programmable switches that can implement policies such
as ‚Äúif packet is secured by SSL then forward it to A, otherwise for-
ward it to B‚Äù. To represent such policies we need a language that is
on one hand sufficiently expressive to capture the policies we want
to implement, but on the other hand sufficiently restrictive so that we
can quickly execute them at network speed and also be able to answer
questions such as ‚Äúcan C see the packets moved from A to B?‚Äù. The
NetKAT network programming language uses a variant of regular
expressions to achieve precisely that. For this application, it is im-
portant that we are not able to merely answer whether an expression
ùëímatches a string ùë•but also answer semantic questions about regu-
lar expressions such as ‚Äúdo expressions ùëíand ùëí‚Ä≤ compute the same


--- Page 247 ---

functions with infinite domains, automata, and regular expressions
247
function?‚Äù and ‚Äúdoes there exist a string ùë•that is matched by the ex-
pression ùëí?‚Äù. The following theorem shows that we can answer the
latter question:
Theorem 6.22 ‚Äî Emptiness of regular languages is computable. There is an
algorithm that given a regular expression ùëí, outputs 1 if and only if
Œ¶ùëíis the constant zero function.
Proof Idea:
The idea is that we can directly observe this from the structure
of the expression. The only way a regular expression ùëícomputes
the constant zero function is if ùëíhas the form ‚àÖor is obtained by
concatenating ‚àÖwith other expressions.
‚ãÜ
Proof of Theorem 6.22. Define a regular expression to be ‚Äúempty‚Äù if it
computes the constant zero function. Given a regular expression ùëí, we
can determine if ùëíis empty using the following rules:
‚Ä¢ If ùëíhas the form ùúéor "" then it is not empty.
‚Ä¢ If ùëíis not empty then ùëí|ùëí‚Ä≤ is not empty for every ùëí‚Ä≤.
‚Ä¢ If ùëíis not empty then ùëí‚àóis not empty.
‚Ä¢ If ùëíand ùëí‚Ä≤ are both not empty then ùëíùëí‚Ä≤ is not empty.
‚Ä¢ ‚àÖis empty.
Using these rules it is straightforward to come up with a recursive
algorithm to determine emptiness.
‚ñ†
Using Theorem 6.22, we can obtain an algorithm that determines
whether or not two regular expressions ùëíand ùëí‚Ä≤ are equivalent, in the
sense that they compute the same function.
Theorem 6.23 ‚Äî Equivalence of regular expressions is computable. Let
REGEQ ‚à∂{0, 1}‚àó‚Üí{0, 1} be the function that on input (a string
representing) a pair of regular expressions ùëí, ùëí‚Ä≤, REGEQ(ùëí, ùëí‚Ä≤) = 1
if and only if Œ¶ùëí
=
Œ¶ùëí‚Ä≤. Then there is an algorithm that computes
REGEQ.
Proof Idea:
The idea is to show that given a pair of regular expressions ùëíand
ùëí‚Ä≤ we can find an expression ùëí‚Ä≥ such that Œ¶ùëí‚Ä≥(ùë•) = 1 if and only if
Œ¶ùëí(ùë•) ‚â†Œ¶ùëí‚Ä≤(ùë•). Therefore Œ¶ùëí‚Ä≥ is the constant zero function if and only


--- Page 248 ---

248
introduction to theoretical computer science
if ùëíand ùëí‚Ä≤ are equivalent, and thus we can test for emptiness of ùëí‚Ä≥ to
determine equivalence of ùëíand ùëí‚Ä≤.
‚ãÜ
Proof of Theorem 6.23. We will prove Theorem 6.23 from Theorem 6.22.
(The two theorems are in fact equivalent: it is easy to prove Theo-
rem 6.22 from Theorem 6.23, since checking for emptiness is the same
as checking equivalence with the expression ‚àÖ.) Given two regu-
lar expressions ùëíand ùëí‚Ä≤, we will compute an expression ùëí‚Ä≥ such that
Œ¶ùëí‚Ä≥(ùë•) = 1 if and only if Œ¶ùëí(ùë•) ‚â†Œ¶ùëí‚Ä≤(ùë•). One can see that ùëíis equiva-
lent to ùëí‚Ä≤ if and only if ùëí‚Ä≥ is empty.
We start with the observation that for every bit ùëé, ùëè‚àà{0, 1}, ùëé‚â†ùëèif
and only if
(ùëé‚àßùëè) ‚à®(ùëé‚àßùëè) .
(6.15)
Hence we need to construct ùëí‚Ä≥ such that for every ùë•,
Œ¶ùëí‚Ä≥(ùë•) = (Œ¶ùëí(ùë•) ‚àßŒ¶ùëí‚Ä≤(ùë•)) ‚à®(Œ¶ùëí(ùë•) ‚àßŒ¶ùëí‚Ä≤(ùë•)) .
(6.16)
To construct the expression ùëí‚Ä≥, we will show how given any pair of
expressions ùëíand ùëí‚Ä≤, we can construct expressions ùëí‚àßùëí‚Ä≤ and ùëíthat
compute the functions Œ¶ùëí‚àßŒ¶ùëí‚Ä≤ and Œ¶ùëírespectively. (Computing the
expression for ùëí‚à®ùëí‚Ä≤ is straightforward using the | operation of regular
expressions.)
Specifically, by Lemma 6.17, regular functions are closed under
negation, which means that for every regular expression ùëí, there is an
expression ùëísuch that Œ¶ùëí(ùë•) = 1 ‚àíŒ¶ùëí(ùë•) for every ùë•‚àà{0, 1}‚àó. Now,
for every two expression ùëíand ùëí‚Ä≤, the expression
ùëí‚àßùëí‚Ä≤ = (ùëí|ùëí‚Ä≤)
(6.17)
computes the AND of the two expressions. Given these two transfor-
mations, we see that for every regular expressions ùëíand ùëí‚Ä≤ we can find
a regular expression ùëí‚Ä≥ satisfying (6.16) such that ùëí‚Ä≥ is empty if and
only if ùëíand ùëí‚Ä≤ are equivalent.
‚ñ†
‚úì
Chapter Recap
‚Ä¢ We model computational tasks on arbitrarily large
inputs using infinite functions ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}‚àó.
‚Ä¢ Such functions take an arbitrarily long (but still
finite!) string as input, and cannot be described by
a finite table of inputs and ouptuts.
‚Ä¢ A function with a single bit of output is known as
a Boolean function, and the tasks of computing it is
equivalent to deciding a language ùêø‚äÜ{0, 1}‚àó.


--- Page 249 ---

functions with infinite domains, automata, and regular expressions
249
‚Ä¢ Deterministic finite automata (DFAs) are one simple
model for computing (infinite) Boolean functions.
‚Ä¢ There are some functions that cannot be computed
by DFAs.
‚Ä¢ The set of functions computable by DFAs is the
same as the set of languages that can be recognized
by regular expressions.
6.7 EXERCISES
Exercise 6.1 ‚Äî Closure properties of regular functions. Suppose that ùêπ, ùê∫‚à∂
{0, 1}‚àó‚Üí{0, 1} are regular. For each one of the following defini-
tions of the function ùêª, either prove that ùêªis always regular or give a
counterexample for regular ùêπ, ùê∫that would make ùêªnot regular.
1. ùêª(ùë•) = ùêπ(ùë•) ‚à®ùê∫(ùë•).
2. ùêª(ùë•) = ùêπ(ùë•) ‚àßùê∫(ùë•)
3. ùêª(ùë•) = NAND(ùêπ(ùë•), ùê∫(ùë•)).
4. ùêª(ùë•) = ùêπ(ùë•ùëÖ) where ùë•ùëÖis the reverse of ùë•: ùë•ùëÖ= ùë•ùëõ‚àí1ùë•ùëõ‚àí2 ‚ãØùë•ùëúfor
ùëõ= |ùë•|.
5. ùêª(ùë•) =
‚éß
{
‚é®
{
‚é©
1
ùë•= ùë¢ùë£s.t. ùêπ(ùë¢) = ùê∫(ùë£) = 1
0
otherwise
6. ùêª(ùë•) =
‚éß
{
‚é®
{
‚é©
1
ùë•= ùë¢ùë¢s.t. ùêπ(ùë¢) = ùê∫(ùë¢) = 1
0
otherwise
7. ùêª(ùë•) =
‚éß
{
‚é®
{
‚é©
1
ùë•= ùë¢ùë¢ùëÖs.t. ùêπ(ùë¢) = ùê∫(ùë¢) = 1
0
otherwise
‚ñ†
Exercise 6.2 One among the following two functions that map {0, 1}‚àó
to {0, 1} can be computed by a regular expression, and the other one
cannot. For the one that can be computed by a regular expression,
write the expression that does it. For the one that cannot, prove that
this cannot be done using the pumping lemma.
‚Ä¢ ùêπ(ùë•) = 1 if 4 divides ‚àë
|ùë•|‚àí1
ùëñ=0 ùë•ùëñand ùêπ(ùë•) = 0 otherwise.
‚Ä¢ ùê∫(ùë•) = 1 if and only if ‚àë
|ùë•|‚àí1
ùëñ=0 ùë•ùëñ‚â•|ùë•|/4 and ùê∫(ùë•) = 0 otherwise.
‚ñ†
Exercise 6.3 ‚Äî Non regularity. 1. Prove that the following function ùêπ‚à∂
{0, 1}‚àó‚Üí{0, 1} is not regular. For every ùë•‚àà{0, 1}‚àó, ùêπ(ùë•) = 1 iff ùë•is
of the form ùë•= 13ùëñfor some ùëñ> 0.


--- Page 250 ---

250
introduction to theoretical computer science
2. Prove that the following function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1} is not regular.
For every ùë•‚àà{0, 1}‚àó, ùêπ(ùë•) = 1 iff ‚àëùëóùë•ùëó= 3ùëñfor some ùëñ> 0.
‚ñ†
6.8 BIBLIOGRAPHICAL NOTES
The relation of regular expressions with finite automata is a beautiful
topic, on which we only touch upon in this text. It is covered more
extensively in [Sip97; HMU14; Koz97]. These texts also discuss top-
ics such as non deterministic finite automata (NFA) and the relation
between context-free grammars and pushdown automata.
The automaton of Fig. 6.4 was generated using the FSM simulator
of Ivan Zuzak and Vedrana Jankovic. Our proof of Theorem 6.11 is
closely related to the Myhill-Nerode Theorem. One direction of the
Myhill-Nerode theorem can be stated as saying that if ùëíis a regular
expression then there is at most a finite number of strings ùëß0, ‚Ä¶ , ùëßùëò‚àí1
such that Œ¶ùëí[ùëßùëñ] ‚â†Œ¶ùëí[ùëßùëó] for every 0 ‚â§ùëñ‚â†ùëó< ùëò.


--- Page 251 ---

Figure 7.1: An algorithm is a finite recipe to compute
on arbitrarily long inputs. The components of an
algorithm include the instructions to be performed,
finite state or ‚Äúlocal variables‚Äù, the memory to store
the input and intermediate computations, as well as
mechanisms to decide which part of the memory to
access, and when to repeat instructions and when to
halt.
7
Loops and infinity
‚ÄúThe bounds of arithmetic were however outstepped the moment the idea of
applying the [punched] cards had occurred; and the Analytical Engine does
not occupy common ground with mere ‚Äòcalculating machines.‚Äô ‚Ä¶ In enabling
mechanism to combine together general symbols, in successions of unlim-
ited variety and extent, a uniting link is established between the operations of
matter and the abstract mental processes of the most abstract branch of mathe-
matical science.‚Äù, Ada Augusta, countess of Lovelace, 1843
As the quote of Chapter 6 says, an algorithm is ‚Äúa finite answer to
an infinite number of questions‚Äù. To express an algorithm we need to
write down a finite set of instructions that will enable us to compute
on arbitrarily long inputs. To describe and execute an algorithm we
need the following components (see Fig. 7.1):
‚Ä¢ The finite set of instructions to be performed.
‚Ä¢ Some ‚Äúlocal variables‚Äù or finite state used in the execution.
‚Ä¢ A potentially unbounded working memory to store the input as
well as any other values we may require later.
‚Ä¢ While the memory is unbounded, at every single step we can only
read and write to a finite part of it, and we need a way to address
which are the parts we want to read from and write to.
‚Ä¢ If we only have a finite set of instructions but our input can be
arbitrarily long, we will need to repeat instructions (i.e., loop back).
We need a mechanism to decide when we will loop and when we
will halt.
This chapter: A non-mathy overview
In this chapter we give a general model of an algorithm,
which (unlike Boolean circuits) is not restricted to a fixed
input lengths, and (unlike finite automata) is not restricted to
Compiled on 8.26.2020 18:10
Learning Objectives:
‚Ä¢ Learn the model of Turing machines, which
can compute functions of arbitrary input
lengths.
‚Ä¢ See a programming-language description of
Turing machines, using NAND-TM programs,
which add loops and arrays to NAND-CIRC.
‚Ä¢ See some basic syntactic sugar and
equivalence of variants of Turing machines
and NAND-TM programs.


--- Page 252 ---

252
introduction to theoretical computer science
a finite amount of working memory. We will see two ways to
model algorithms:
‚Ä¢ Turing machines, invented by Alan Turing in 1936, are an
hypothetical abstract device that yields a finite description
of an algorithm that can handle arbitrarily long inputs.
‚Ä¢ The NAND-TM Programming language extends NAND-
CIRC with the notion of loops and arrays to obtain finite
programs that can compute a function with arbitrarily
long inputs.
It turns out that these two models are equivalent, and in fact
they are equivalent to many other computational models
including programming languages such as C, Lisp, Python,
JavaScript, etc. This notion, known as Turing equivalence
or Turing completeness, will be discussed in Chapter 8. See
Fig. 7.2 for an overview of the models presented in this chap-
ter and Chapter 8.
Figure 7.2: Overview of our models for finite and
unbounded computation. In the previous chapters
we study the computation of finite functions, which
are functions ùëì‚à∂{0, 1}ùëõ‚Üí{0, 1}ùëöfor some fixed
ùëõ, ùëö, and modeled computing these functions using
circuits or straightline programs. In this chapter we
study computing unbounded functions of the form
ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}ùëöor ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}‚àó.
We model computing these functions using Turing
Machines or (equivalently) NAND-TM programs
which add the notion of loops to the NAND-CIRC
programming language. In Chapter 8 we will show
that these models are equivalent to many other
models, including RAM machines, the ùúÜcalculus, and
all the common programming languages including C,
Python, Java, JavaScript, etc.
7.1 TURING MACHINES
‚ÄúComputing is normally done by writing certain symbols on paper. We may
suppose that this paper is divided into squares like a child‚Äôs arithmetic book..
The behavior of the [human] computer at any moment is determined by the
symbols which he is observing, and of his ‚Äòstate of mind‚Äô at that moment‚Ä¶ We
may suppose that in a simple operation not more than one symbol is altered.‚Äù,
‚ÄúWe compare a man in the process of computing ‚Ä¶ to a machine which is only
capable of a finite number of configurations‚Ä¶ The machine is supplied with a
‚Äòtape‚Äô (the analogue of paper) ‚Ä¶ divided into sections (called ‚Äòsquares‚Äô) each
capable of bearing a ‚Äòsymbol‚Äô ‚Äù, Alan Turing, 1936


--- Page 253 ---

loops and infinity
253
Figure 7.3: Aside from his many other achievements,
Alan Turing was an excellent long distance runner
who just fell shy of making England‚Äôs olympic team.
A fellow runner once asked him why he punished
himself so much in training. Alan said ‚ÄúI have such
a stressful job that the only way I can get it out of my
mind is by running hard; it‚Äôs the only way I can get
some release.‚Äù
Figure 7.4: Until the advent of electronic computers,
the word ‚Äúcomputer‚Äù was used to describe a person
that performed calculations. Most of these ‚Äúhuman
computers‚Äù were women, and they were absolutely
essential to many achievements including mapping
the stars, breaking the Enigma cipher, and the NASA
space mission; see also the bibliographical notes.
Photo from National Photo Company Collection; see
also [Sob17].
Figure 7.5: Steam-powered Turing Machine mural,
painted by CSE grad students at the University of
Washington on the night before spring qualifying
examinations, 1987. Image from https://www.cs.
washington.edu/building/art/SPTM.
‚ÄúWhat is the difference between a Turing machine and the modern computer?
It‚Äôs the same as that between Hillary‚Äôs ascent of Everest and the establishment
of a Hilton hotel on its peak.‚Äù , Alan Perlis, 1982.
The ‚Äúgranddaddy‚Äù of all models of computation is the Turing Ma-
chine. Turing machines were defined in 1936 by Alan Turing in an
attempt to formally capture all the functions that can be computed
by human ‚Äúcomputers‚Äù (see Fig. 7.4) that follow a well-defined set of
rules, such as the standard algorithms for addition or multiplication.
Turing thought of such a person as having access to as much
‚Äúscratch paper‚Äù as they need. For simplicity we can think of this
scratch paper as a one dimensional piece of graph paper (or tape, as
it is commonly referred to), which is divided to ‚Äúcells‚Äù, where each
‚Äúcell‚Äù can hold a single symbol (e.g., one digit or letter, and more
generally some element of a finite alphabet). At any point in time, the
person can read from and write to a single cell of the paper, and based
on the contents can update his/her finite mental state, and/or move to
the cell immediately to the left or right of the current one.
Turing modeled such a computation by a ‚Äúmachine‚Äù that maintains
one of ùëòstates. At each point in time the machine reads from its ‚Äúwork
tape‚Äù a single symbol from a finite alphabet Œ£ and uses that to up-
date its state, write to tape, and possibly move to an adjacent cell (see
Fig. 7.7). To compute a function ùêπusing this machine, we initialize the
tape with the input ùë•‚àà{0, 1}‚àóand our goal is to ensure that the tape
will contain the value ùêπ(ùë•) at the end of the computation. Specifically,
a computation of a Turing Machine ùëÄwith ùëòstates and alphabet Œ£ on
input ùë•‚àà{0, 1}‚àóproceeds as follows:
‚Ä¢ Initially the machine is at state 0 (known as the ‚Äústarting state‚Äù)
and the tape is initialized to ‚ñ∑, ùë•0, ‚Ä¶ , ùë•ùëõ‚àí1, ‚àÖ, ‚àÖ, ‚Ä¶. We use the
symbol ‚ñ∑to denote the beginning of the tape, and the symbol ‚àÖto
denote an empty cell. We will always assume that the alphabet Œ£ is
a (potentially strict) superset of {‚ñ∑, ‚àÖ, 0, 1}.
‚Ä¢ The location ùëñto which the machine points to is set to 0.
‚Ä¢ At each step, the machine reads the symbol ùúé= ùëá[ùëñ] that is in the
ùëñùë°‚Ñélocation of the tape, and based on this symbol and its state ùë†
decides on:
‚Äì What symbol ùúé‚Ä≤ to write on the tape
‚Äì Whether to move Left (i.e., ùëñ‚Üêùëñ‚àí1), Right (i.e., ùëñ‚Üêùëñ+ 1), Stay
in place, or Halt the computation.
‚Äì What is going to be the new state ùë†‚àà[ùëò]
‚Ä¢ The set of rules the Turing machine follows is known as its transi-
tion function.


--- Page 254 ---

254
introduction to theoretical computer science
Figure 7.6: The components of a Turing Machine. Note
how they correspond to the general components of
algorithms as described in Fig. 7.1.
‚Ä¢ When the machine halts then its output is the binary string ob-
tained by reading the tape from the beginning until the head posi-
tion, dropping all symbols such as ‚ñ∑, ‚àÖ, etc. that are not either 0 or
1.
7.1.1 Extended example: A Turing machine for palindromes
Let PAL (for palindromes) be the function that on input ùë•‚àà{0, 1}‚àó,
outputs 1 if and only if ùë•is an (even length) palindrome, in the sense
that ùë•= ùë§0 ‚ãØùë§ùëõ‚àí1ùë§ùëõ‚àí1ùë§ùëõ‚àí2 ‚ãØùë§0 for some ùëõ‚àà‚Ñïand ùë§‚àà{0, 1}ùëõ.
We now show a Turing Machine ùëÄthat computes PAL. To specify
ùëÄwe need to specify (i) ùëÄ‚Äôs tape alphabet Œ£ which should contain at
least the symbols 0,1, ‚ñ∑and ‚àÖ, and (ii) ùëÄ‚Äôs transition function which
determines what action ùëÄtakes when it reads a given symbol while it
is in a particular state.
In our case, ùëÄwill use the alphabet {0, 1, ‚ñ∑, ‚àÖ, √ó} and will have
ùëò= 14 states. Though the states are simply numbers between 0 and
ùëò‚àí1, for convenience we will give them the following labels:
State
Label
0
START
1
RIGHT_0
2
RIGHT_1
3
LOOK_FOR_0
4
LOOK_FOR_1
5
RETURN
6
REJECT
7
ACCEPT
8
OUTPUT_0
9
OUTPUT_1
10
0_AND_BLANK
11
1_AND_BLANK
12
BLANK_AND_STOP
We describe the operation of our Turing Machine ùëÄin words:
‚Ä¢ ùëÄstarts in state START and will go right, looking for the first sym-
bol that is 0 or 1. If we find ‚àÖbefore we hit such a symbol then we
will move to the OUTPUT_1 state that we describe below.
‚Ä¢ Once ùëÄfinds such a symbol ùëè‚àà{0, 1}, ùëÄdeletes ùëèfrom the tape
by writing the √ó symbol, it enters either the RIGHT_0 or RIGHT_1
mode according to the value of ùëèand starts moving rightwards
until it hits the first ‚àÖor √ó symbol.


--- Page 255 ---

loops and infinity
255
‚Ä¢ Once we find this symbol we go into the state LOOK_FOR_0 or
LOOK_FOR_1 depending on whether we were in the state RIGHT_0
or RIGHT_1 and make one left move.
‚Ä¢ In the state LOOK_FOR_ùëè, we check whether the value on the tape is
ùëè. If it is, then we delete it by changing its value to √ó, and move to
the state RETURN. Otherwise, we change to the OUTPUT_0 state.
‚Ä¢ The RETURN state means we go back to the beginning. Specifically,
we move leftward until we hit the first symbol that is not 0 or 1, in
which case we change our state to START.
‚Ä¢ The OUTPUT_ùëèstates mean that we are going to output the value ùëè.
In both these states we go left until we hit ‚ñ∑. Once we do so, we
make a right step, and change to the 1_AND_BLANK or 0_AND_BLANK
states respectively. In the latter states, we write the corresponding
value, and then move right and change to the BLANK_AND_STOP
state, in which we write ‚àÖto the tape and halt.
The above description can be turned into a table describing for each
one of the 13 ‚ãÖ5 combination of state and symbol, what the Turing
machine will do when it is in that state and it reads that symbol. This
table is known as the transition function of the Turing machine.
7.1.2 Turing machines: a formal definition
Figure 7.7: A Turing machine has access to a tape of
unbounded length. At each point in the execution,
the machine can read a single symbol of the tape,
and based on that and its current state, write a new
symbol, update the tape, decide whether to move left,
right, stay, or halt.
The formal definition of Turing machines is as follows:
Definition 7.1 ‚Äî Turing Machine. A (one tape) Turing machine with ùëò
states and alphabet Œ£
‚äá
{0, 1, ‚ñ∑, ‚àÖ} is represented by a transition
function ùõøùëÄ‚à∂[ùëò] √ó Œ£ ‚Üí[ùëò] √ó Œ£ √ó {L, R, S, H}.


--- Page 256 ---

256
introduction to theoretical computer science
For every ùë•
‚àà
{0, 1}‚àó, the output of ùëÄon input ùë•, denoted by
ùëÄ(ùë•), is the result of the following process:
‚Ä¢ We initialize ùëáto be the sequence ‚ñ∑, ùë•0, ùë•1, ‚Ä¶ , ùë•ùëõ‚àí1, ‚àÖ, ‚àÖ, ‚Ä¶,
where ùëõ= |ùë•|. (That is, ùëá[0] = ‚ñ∑, ùëá[ùëñ+ 1] = ùë•ùëñfor ùëñ‚àà[ùëõ], and
ùëá[ùëñ] = ‚àÖfor ùëñ> ùëõ.)
‚Ä¢ We also initialize ùëñ= 0 and ùë†= 0.
‚Ä¢ We then repeat the following process:
1. Let (ùë†‚Ä≤, ùúé‚Ä≤, ùê∑) = ùõøùëÄ(ùë†, ùëá[ùëñ]).
2. Set ùë†‚Üíùë†‚Ä≤, ùëá[ùëñ] ‚Üíùúé‚Ä≤.
3. If ùê∑= R then set ùëñ‚Üíùëñ+1, if ùê∑= L then set ùëñ‚Üímax{ùëñ‚àí1, 0}.
(If ùê∑= S then we keep ùëñthe same.)
4. If ùê∑= H then halt.
‚Ä¢ If the process above halts, then ùëÄ‚Äôs output, denoted by ùëÄ(ùë•),
is the string ùë¶
‚àà
{0, 1}‚àóobtained by concatenating all the sym-
bols in {0, 1} in positions ùëá[0], ‚Ä¶ , ùëá[ùëñ] where ùëñis the final head
position.
‚Ä¢ If The Turing machine does not halt then we denote ùëÄ(ùë•) = ‚ä•.
P
You should make sure you see why this formal def-
inition corresponds to our informal description of
a Turing Machine. To get more intuition on Turing
Machines, you can explore some of the online avail-
able simulators such as Martin Ugarte‚Äôs, Anthony
Morphett‚Äôs, or Paul Rendell‚Äôs.
One should not confuse the transition function ùõøùëÄof a Turing ma-
chine ùëÄwith the function that the machine computes. The transition
function ùõøùëÄis a finite function, with ùëò|Œ£| inputs and 4ùëò|Œ£| outputs.
(Can you see why?) The machine can compute an infinite function ùêπ
that takes as input a string ùë•‚àà{0, 1}‚àóof arbitrary length and might
also produce an arbitrary length string as output.
In our formal definition, we identified the machine ùëÄwith its tran-
sition function ùõøùëÄsince the transition function tells us everything
we need to know about the Turing machine, and hence serves as a
good mathematical representation of it. This choice of representa-
tion is somewhat arbitrary, and is based on our convention that the
state space is always the numbers {0, ‚Ä¶ , ùëò‚àí1} with 0 as the starting
state. Other texts use different conventions and so their mathematical


--- Page 257 ---

loops and infinity
257
definition of a Turing machine might look superficially different, but
these definitions describe the same computational process and has
the same computational powers. See Section 7.7 for a comparison be-
tween Definition 7.1 and the way Turing Machines are defined in texts
such as Sipser [Sip97]. These definitions are equivalent despite their
superficial differences.
7.1.3 Computable functions
We now turn to making one of the most important definitions in this
book, that of computable functions.
Definition 7.2 ‚Äî Computable functions. Let ùêπ
‚à∂
{0, 1}‚àó
‚Üí
{0, 1}‚àóbe
a (total) function and let ùëÄbe a Turing machine. We say that ùëÄ
computes ùêπif for every ùë•‚àà{0, 1}‚àó, ùëÄ(ùë•) = ùêπ(ùë•).
We say that a function ùêπis computable if there exists a Turing
machine ùëÄthat computes it.
Defining a function ‚Äúcomputable‚Äù if and only if it can be computed
by a Turing machine might seem ‚Äúreckless‚Äù but, as we‚Äôll see in Chap-
ter 8, it turns out that being computable in the sense of Definition 7.2
is equivalent to being computable in essentially any reasonable model
of computation. This is known as the Church-Turing Thesis. (Unlike
the extended Church-Turing Thesis which we discussed in Section 5.6,
the Church-Turing thesis itself is widely believed and there are no
candidate devices that attack it.)
ÔÉ´Big Idea 9 We can precisely define what it means for a function to
be computable by any possible algorithm.
This is a good point to remind the reader that functions are not the
same as programs:
Functions ‚â†Programs .
(7.1)
A Turing machine (or program) ùëÄcan compute some function
ùêπ, but it is not the same as ùêπ. In particular there can be more than
one program to compute the same function. Being computable is a
property of functions, not of machines.
We will often pay special attention to functions ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}
that have a single bit of output. Hence we give a special name for the
set of functions of this form that are computable.
Definition 7.3 ‚Äî The class R. We define R be the set of all computable
functions ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}.


--- Page 258 ---

258
introduction to theoretical computer science
1 A partial function ùêπfrom a set ùê¥to a set ùêµis a
function that is only defined on a subset of ùê¥, (see
Section 1.4.3). We can also think of such a function as
mapping ùê¥to ùêµ‚à™{‚ä•} where ‚ä•is a special ‚Äúfailure‚Äù
symbol such that ùêπ(ùëé) = ‚ä•indicates the function ùêπ
is not defined on ùëé.
R
Remark 7.4 ‚Äî Functions vs. languages. As discussed
in Section 6.1.2, many texts use the terminology of
‚Äúlanguages‚Äù rather than functions to refer to compu-
tational tasks. A Turing machine ùëÄdecides a language
ùêøif for every input ùë•
‚àà
{0, 1}‚àó, ùëÄ(ùë•) outputs 1 if
and only if ùë•
‚àà
ùêø. This is equivalent to computing
the Boolean function ùêπ
‚à∂{0, 1}‚àó‚Üí{0, 1} defined as
ùêπ(ùë•) = 1 iff ùë•‚ààùêø. A language ùêøis decidable if there
is a Turing machine ùëÄthat decides it. For historical
reasons, some texts also call such a language recursive
(which is the reason that the letter R is often used
to denote the set of computable Boolean functions /
decidable languages defined in Definition 7.3).
In this book we stick to the terminology of functions
rather than languages, but all definitions and results
can be easily translated back and forth by using the
equivalence between the function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}
and the language ùêø= {ùë•‚àà{0, 1}‚àó| ùêπ(ùë•) = 1}.
7.1.4 Infinite loops and partial functions
One crucial difference between circuits/straight-line programs and
Turing machines is the following. Looking at a NAND-CIRC program
ùëÉ, we can always tell how many inputs and how many outputs it has
(by simply looking at the X and Y variables). Furthermore, we are
guaranteed that if we invoke ùëÉon any input then some output will be
produced.
In contrast, given any Turing machine ùëÄ, we cannot determine
a priori the length of the output. In fact, we don‚Äôt even know if an
output would be produced at all! For example, it is very easy to come
up with a Turing machine whose transition function never outputs H
and hence never halts.
If a machine ùëÄfails to stop and produce an output on some an
input ùë•, then it cannot compute any total function ùêπ, since clearly on
input ùë•, ùëÄwill fail to output ùêπ(ùë•). However, ùëÄcan still compute a
partial function.1
For example, consider the partial function DIV that on input a pair
(ùëé, ùëè) of natural numbers, outputs ‚åàùëé/ùëè‚åâif ùëè> 0, and is undefined
otherwise. We can define a Turing machine ùëÄthat computes DIV on
input ùëé, ùëèby outputting the first ùëê= 0, 1, 2, ‚Ä¶ such that ùëêùëè‚â•ùëé. If ùëé> 0
and ùëè= 0 then the machine ùëÄwill never halt, but this is OK, since
DIV is undefined on such inputs. If ùëé= 0 and ùëè= 0, the machine ùëÄ
will output 0, which is also OK, since we don‚Äôt care about what the
program outputs on inputs on which DIV is undefined. Formally, we
define computability of partial functions as follows:


--- Page 259 ---

loops and infinity
259
Definition 7.5 ‚Äî Computable (partial or total) functions. Let ùêπbe either a
total or partial function mapping {0, 1}‚àóto {0, 1}‚àóand let ùëÄbe a
Turing machine. We say that ùëÄcomputes ùêπif for every ùë•‚àà{0, 1}‚àó
on which ùêπis defined, ùëÄ(ùë•)
=
ùêπ(ùë•). We say that a (partial or
total) function ùêπis computable if there is a Turing machine that
computes it.
Note that if ùêπis a total function, then it is defined on every ùë•‚àà
{0, 1}‚àóand hence in this case, Definition 7.5 is identical to Defini-
tion 7.2.
R
Remark 7.6 ‚Äî Bot symbol. We often use ‚ä•as our spe-
cial ‚Äúfailure symbol‚Äù. If a Turing machine ùëÄfails to
halt on some input ùë•‚àà{0, 1}‚àóthen we denote this by
ùëÄ(ùë•)
=
‚ä•. This does not mean that ùëÄoutputs some
encoding of the symbol ‚ä•but rather that ùëÄenters
into an infinite loop when given ùë•as input.
If a partial function ùêπis undefined on ùë•then we can
also write ùêπ(ùë•)
=
‚ä•. Therefore one might think
that Definition 7.5 can be simplified to requiring that
ùëÄ(ùë•) = ùêπ(ùë•) for every ùë•‚àà{0, 1}‚àó, which would imply
that for every ùë•, ùëÄhalts on ùë•if and only if ùêπis de-
fined on ùë•. However this is not the case: for a Turing
Machine ùëÄto compute a partial function ùêπit is not
necessary for ùëÄto enter an infinite loop on inputs ùë•
on which ùêπis not defined. All that is needed is for ùëÄ
to output ùêπ(ùë•) on ùë•‚Äôs on which ùêπis defined: on other
inputs it is OK for ùëÄto output an arbitrary value such
as 0, 1, or anything else, or not to halt at all. To borrow
a term from the C programming language, on inputs ùë•
on which ùêπis not defined, what ùëÄdoes is ‚Äúundefined
behavior‚Äù.
7.2 TURING MACHINES AS PROGRAMMING LANGUAGES
The name ‚ÄúTuring machine‚Äù, with its ‚Äútape‚Äù and ‚Äúhead‚Äù evokes a
physical object, while in contrast we think of a program as a piece
of text. But we can think of a Turing machine as a program as well.
For example, consider the Turing Machine ùëÄof Section 7.1.1 that
computes the function PAL such that PAL(ùë•) = 1 iff ùë•is a palindrome.
We can also describe this machine as a program using the Python-like
pseudocode of the form below
# Gets an array Tape initialized to
# [">", x_0 , x_1 , .... , x_(n-1), "‡∏Ä", "‡∏Ä", ...]
# At the end of the execution, Tape[1] is equal to 1
# if x is a palindrome and is equal to 0 otherwise


--- Page 260 ---

260
introduction to theoretical computer science
2 Most programming languages use arrays of fixed
size, while a Turing machine‚Äôs tape is unbounded. But
of course there is no need to store an infinite number
of ‚àÖsymbols. If you want, you can think of the tape
as a list that starts off just long enough to store the
input, but is dynamically grown in size as the Turing
machine‚Äôs head explores new positions.
def PAL(Tape):
head = 0
state = 0 # START
while (state != 12):
if (state == 0 && Tape[head]=='0'):
state = 3 # LOOK_FOR_0
Tape[head] = 'x'
head += 1 # move right
if (state==0 && Tape[head]=='1')
state = 4 # LOOK_FOR_1
Tape[head] = 'x'
head += 1 # move right
... # more if statements here
The particular details of this program are not important. What mat-
ters is that we can describe Turing machines as programs. Moreover,
note that when translating a Turing machine into a program, the tape
becomes a list or array that can hold values from the finite set Œ£.2 The
head position can be thought of as an integer valued variable that can
hold integers of unbounded size. The state is a local register that can
hold one of a fixed number of values in [ùëò].
More generally we can think of every Turing Machine ùëÄas equiva-
lent to a program similar to the following:
# Gets an array Tape initialized to
# [">", x_0 , x_1 , .... , x_(n-1), "‡∏Ä", "‡∏Ä", ...]
def M(Tape):
state = 0
i
= 0 # holds head location
while (True):
# Move head, modify state, write to tape
# based on current state and cell at head
# below are just examples for how program looks
for a particular transition function
‚Ü™
if Tape[i]=="0" and state==7: #
Œ¥_M(7,"0")=(19,"1","R")
‚Ü™
i += 1
Tape[i]="1"
state = 19
elif Tape[i]==">" and state == 13: #
Œ¥_M(13,">")=(15,"0","S")
‚Ü™
Tape[i]="0"
state = 15
elif ...
...


--- Page 261 ---

loops and infinity
261
elif Tape[i]==">" and state == 29: #
Œ¥_M(29,">")=(.,.,"H")
‚Ü™
break # Halt
If we wanted to use only Boolean (i.e., 0/1-valued) variables then
we can encode the state variables using ‚åàlog ùëò‚åâbits. Similarly, we
can represent each element of the alphabet Œ£ using ‚Ñì= ‚åàlog |Œ£|‚åâbits
and hence we can replace the Œ£-valued array Tape[] with ‚ÑìBoolean-
valued arrays Tape0[],‚Ä¶, Tape(‚Ñì‚àí1)[].
7.2.1 The NAND-TM Programming language
We now introduce the NAND-TM programming language, which aims
to capture the power of a Turing machine in a programming language
formalism. Just like the difference between Boolean circuits and Tur-
ing Machines, the main difference between NAND-TM and NAND-
CIRC is that NAND-TM models a single uniform algorithm that can
compute a function that takes inputs of arbitrary lengths. To do so, we
extend the NAND-CIRC programming language with two constructs:
‚Ä¢ Loops: NAND-CIRC is a straight-line programming language- a
NAND-CIRC program of ùë†lines takes exactly ùë†steps of computa-
tion and hence in particular cannot even touch more than 3ùë†vari-
ables. Loops allow us to capture in a short program the instructions
for a computation that can take an arbitrary amount of time.
‚Ä¢ Arrays: A NAND-CIRC program of ùë†lines touches at most 3ùë†vari-
ables. While we can use variables with names such as Foo_17 or
Bar[22], they are not true arrays, since the number in the identifier
is a constant that is ‚Äúhardwired‚Äù into the program.
Figure 7.8: A NAND-TM program has scalar variables
that can take a Boolean value, array variables that
hold a sequence of Boolean values, and a special
index variable i that can be used to index the array
variables. We refer to the i-th value of the array
variable Spam using Spam[i]. At each iteration of
the program the index variable can be incremented
or decremented by one step using the MODANDJMP
operation.
Thus a good way to remember NAND-TM is using the following
informal equation:
NAND-TM = NAND-CIRC + loops + arrays
(7.2)


--- Page 262 ---

262
introduction to theoretical computer science
R
Remark 7.7 ‚Äî NAND-CIRC + loops + arrays = every-
thing.. As we will see, adding loops and arrays to
NAND-CIRC is enough to capture the full power of
all programming languages! Hence we could replace
‚ÄúNAND-TM‚Äù with any of Python, C, Javascript, OCaml,
etc. in the lefthand side of (7.2). But we‚Äôre getting
ahead of ourselves: this issue will be discussed in
Chapter 8.
Concretely, the NAND-TM programming language adds the fol-
lowing features on top of NAND-CIRC (see Fig. 7.8):
‚Ä¢ We add a special integer valued variable i. All other variables in
NAND-TM are Boolean valued (as in NAND-CIRC).
‚Ä¢ Apart from i NAND-TM has two kinds of variables: scalars and
arrays. Scalar variables hold one bit (just as in NAND-CIRC). Array
variables hold an unbounded number of bits. At any point in the
computation we can access the array variables at the location in-
dexed by i using Foo[i]. We cannot access the arrays at locations
other than the one pointed to by i.
‚Ä¢ We use the convention that arrays always start with a capital letter,
and scalar variables (which are never indexed with i) start with
lowercase letters. Hence Foo is an array and bar is a scalar variable.
‚Ä¢ The input and output X and Y are now considered arrays with val-
ues of zeroes and ones. (There are also two other special arrays
X_nonblank and Y_nonblank, see below.)
‚Ä¢ We add a special MODANDJUMP instruction that takes two boolean
variables ùëé, ùëèas input and does the following:
‚Äì If ùëé= 1 and ùëè= 1 then MODANDJUMP(ùëé, ùëè) increments i by one
and jumps to the first line of the program.
‚Äì If ùëé= 0 and ùëè= 1 then MODANDJUMP(ùëé, ùëè) decrements i by one
and jumps to the first line of the program. (If i is already equal
to 0 then it stays at 0.)
‚Äì If ùëé= 1 and ùëè= 0 then MODANDJUMP(ùëé, ùëè) jumps to the first line of
the program without modifying i.
‚Äì If ùëé= ùëè= 0 then MODANDJUMP(ùëé, ùëè) halts execution of the
program.
‚Ä¢ The MODANDJUMP instruction always appears in the last line of a
NAND-TM program and nowhere else.


--- Page 263 ---

loops and infinity
263
Default values.
We need one more convention to handle ‚Äúdefault val-
ues‚Äù. Turing machines have the special symbol ‚àÖto indicate that tape
location is ‚Äúblank‚Äù or ‚Äúuninitialized‚Äù. In NAND-TM there is no such
symbol, and all variables are Boolean, containing either 0 or 1. All
variables and locations of arrays are default to 0 if they have not been
initialized to another value. To keep track of whether a 0 in an array
corresponds to a true zero or to an uninitialized cell, a programmer
can always add to an array Foo a ‚Äúcompanion array‚Äù Foo_nonblank
and set Foo_nonblank[i] to 1 whenever the i‚Äôth location is initial-
ized. In particular we will use this convention for the input and out-
put arrays X and Y. A NAND-TM program has four special arrays X,
X_nonblank, Y, and Y_nonblank. When a NAND-TM program is exe-
cuted on input ùë•‚àà{0, 1}‚àóof length ùëõ, the first ùëõcells of the array X are
initialized to ùë•0, ‚Ä¶ , ùë•ùëõ‚àí1 and the first ùëõcells of the array X_nonblank
are initialized to 1. (All uninitialized cells default to 0.) The output of
a NAND-TM program is the string Y[0], ‚Ä¶, Y[ùëö‚àí1] where ùëöis the
smallest integer such that Y_nonblank[ùëö]= 0. A NAND-TM program
gets called with X and X_nonblank initialized to contain the input, and
writes to Y and Y_nonblank to produce the output.
Formally, NAND-TM programs are defined as follows:
Definition 7.8 ‚Äî NAND-TM programs. A NAND-TM program consists of
a sequence of lines of the form foo = NAND(bar,blah) ending
with a line of the form MODANDJMP(foo,bar), where foo,bar,blah
are either scalar variables (sequences of letters, digits, and under-
scores) or array variables of the form Foo[i] (starting with capital
letter and indexed by i). The program has the array variables X,
X_nonblank, Y, Y_nonblank and the index variable i built in, and
can use additional array and scalar variables.
If ùëÉis a NAND-TM program and ùë•‚àà{0, 1}‚àóis an input then an
execution of ùëÉon ùë•is the following process:
1. The arrays X and X_nonblank are initialized by X[ùëñ]=
ùë•ùëñand
X_nonblank[ùëñ]= 1 for all ùëñ‚àà[|ùë•|]. All other variables and cells
are initialized to 0. The index variable i is also initialized to 0.
2. The program is executed line by line, when the last line MODAND-
JMP(foo,bar) is executed then we do as follows:
a. If foo= 1 and bar= 0 then jump to the first line without mod-
ifying the value of i.
b. If foo=
1 and bar=
1 then increment i by one and jump to
the first line.


--- Page 264 ---

264
introduction to theoretical computer science
c. If foo= 0 and bar= 1 then decrement i by one (unless it is al-
ready zero) and jump to the first line.
d. If foo= 0 and bar= 0 then halt and output Y[0], ‚Ä¶, Y[ùëö‚àí1]
where ùëöis the smallest integer such that Y_nonblank[ùëö]= 0.
7.2.2 Sneak peak: NAND-TM vs Turing machines
As the name implies, NAND-TM programs are a direct implemen-
tation of Turing machines in programming language form. We will
show the equivalence below but you can already see how the compo-
nents of Turing machines and NAND-TM programs correspond to one
another:
Table 7.2: Turing Machine and NAND-TM analogs
Turing Machine
NAND-TM program
State: single register that
takes values in [ùëò]
Scalar variables: Several variables
such as foo, bar etc.. each taking
values in {0, 1}.
Tape: One tape containing
values in a finite set Œ£.
Potentially infinite but ùëá[ùë°]
defaults to ‚àÖfor all locations
ùë°that have not been
accessed.
Arrays: Several arrays such as Foo,
Bar etc.. for each such array Arr and
index ùëó, the value of Arr at position ùëó
is either 0 or 1. The value defaults to
0 for position that have not been
written to.
Head location: A number
ùëñ‚àà‚Ñïthat encodes the
position of the head.
Index variable: The variable i that can
be used to access the arrays.
Accessing memory: At every
step the Turing machine has
access to its local state, but
can only access the tape at
the position of the current
head location.
Accessing memory: At every step a
NAND-TM program has access to all
the scalar variables, but can only
access the arrays at the location i of
the index variable
Control of location: In each
step the machine can move
the head location by at most
one position.
Control of index variable: In each
iteration of its main loop the
program can modify the index i by
at most one.
7.2.3 Examples
We now present some examples of NAND-TM programs.


--- Page 265 ---

loops and infinity
265
‚ñ†Example 7.9 ‚Äî Increment in NAND-TM. The following is a NAND-TM
program to compute the increment function. That is, INC ‚à∂{0, 1}‚àó‚Üí
{0, 1}‚àósuch that for every ùë•‚àà{0, 1}ùëõ, INC(ùë•) is the ùëõ+ 1 bit long
string ùë¶such that if ùëã= ‚àë
ùëõ‚àí1
ùëñ=0 ùë•ùëñ‚ãÖ2ùëñis the number represented by
ùë•, then ùë¶is the (least-significant digit first) binary representation of
the number ùëã+ 1.
We start by showing the program using the ‚Äúsyntactic sugar‚Äù
we‚Äôve seen before of using shorthand for some NAND-CIRC pro-
grams we have seen before to compute simple functions such as
IF, XOR and AND (as well as the constant one function as well as the
function COPY that just maps a bit to itself).
carry = IF(started,carry,one(started))
started = one(started)
Y[i] = XOR(X[i],carry)
carry = AND(X[i],carry)
Y_nonblank[i] = one(started)
MODANDJUMP(X_nonblank[i],X_nonblank[i])
The above is not, strictly speaking, a valid NAND-TM program.
If we ‚Äúopen up‚Äù all of the syntactic sugar, we get the following
‚Äúsugar free‚Äù valid program to compute the same function.
temp_0 = NAND(started,started)
temp_1 = NAND(started,temp_0)
temp_2 = NAND(started,started)
temp_3 = NAND(temp_1,temp_2)
temp_4 = NAND(carry,started)
carry = NAND(temp_3,temp_4)
temp_6 = NAND(started,started)
started = NAND(started,temp_6)
temp_8 = NAND(X[i],carry)
temp_9 = NAND(X[i],temp_8)
temp_10 = NAND(carry,temp_8)
Y[i] = NAND(temp_9,temp_10)
temp_12 = NAND(X[i],carry)
carry = NAND(temp_12,temp_12)
temp_14 = NAND(started,started)
Y_nonblank[i] = NAND(started,temp_14)
MODANDJUMP(X_nonblank[i],X_nonblank[i])


--- Page 266 ---

266
introduction to theoretical computer science
‚ñ†Example 7.10 ‚Äî XOR in NAND-TM. The following is a NAND-TM pro-
gram to compute the XOR function on inputs of arbitrary length.
That is XOR
‚à∂
{0, 1}‚àó
‚Üí
{0, 1} such that XOR(ùë•)
=
‚àë
|ùë•|‚àí1
ùëñ=0 ùë•ùëñ
mod 2 for every ùë•
‚àà
{0, 1}‚àó. Once again, we use a certain ‚Äúsyn-
tactic sugar‚Äù. Specifically, we access the arrays X and Y at their
zero-th entry, while NAND-TM only allows access to arrays in the
coordinate of the variable i.
temp_0 = NAND(X[0],X[0])
Y_nonblank[0] = NAND(X[0],temp_0)
temp_2 = NAND(X[i],Y[0])
temp_3 = NAND(X[i],temp_2)
temp_4 = NAND(Y[0],temp_2)
Y[0] = NAND(temp_3,temp_4)
MODANDJUMP(X_nonblank[i],X_nonblank[i])
To transform the program above to a valid NAND-TM program,
we can transform references such as X[0] and Y[0] to scalar vari-
ables x_0 and y_0 (similarly we can transform any reference of the
form Foo[17] or Bar[15] to scalars such as foo_17 and bar_15).
We then need to add code to load the value of X[0] to x_0 and
similarly to write to Y[0] the value of y_0, but this is not hard to
do. Using the fact that variables are initialized to zero by default,
we can create a variable init which will be set to 1 at the end of
the first iteration and not changed since then. We can then add an
array Atzero and code that will modify Atzero[i] to 1 if init is
0 and otherwise leave it as it is. This will ensure that Atzero[i] is
equal to 1 if and only if i is set to zero, and allow the program to
know when we are at the zero-th location. Thus we can add code
to read and write to the corresponding scalars x_0, y_0 when we
are at the zero-th location, and also code to move i to zero and
then halt at the end. Working this out fully is somewhat tedious,
but can be a good exercise.
P
Working out the above two examples can go a long
way towards understanding the NAND-TM language.
See our GitHub repository for a full specification of
the NAND-TM language.


--- Page 267 ---

loops and infinity
267
7.3 EQUIVALENCE OF TURING MACHINES AND NAND-TM PRO-
GRAMS
Given the above discussion, it might not be surprising that Turing
machines turn out to be equivalent to NAND-TM programs. Indeed,
we designed the NAND-TM language to have this property. Never-
theless, this is an important result, and the first of many other such
equivalence results we will see in this book.
Theorem 7.11 ‚Äî Turing machines and NAND-TM programs are equivalent. For
every ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}‚àó, ùêπis computable by a NAND-TM pro-
gram ùëÉif and only if there is a Turing Machine ùëÄthat computes
ùêπ.
Proof Idea:
To prove such an equivalence theorem, we need to show two di-
rections. We need to be able to (1) transform a Turing machine ùëÄto
a NAND-TM program ùëÉthat computes the same function as ùëÄand
(2) transform a NAND-TM program ùëÉinto a Turing machine ùëÄthat
computes the same function as ùëÉ.
The idea of the proof is illustrated in Fig. 7.9. To show (1), given
a Turing machine ùëÄ, we will create a NAND-TM program ùëÉthat
will have an array Tape for the tape of ùëÄand scalar (i.e., non array)
variable(s) state for the state of ùëÄ. Specifically, since the state of a
Turing machine is not in {0, 1} but rather in a larger set [ùëò], we will use
‚åàlog ùëò‚åâvariables state_0 , ‚Ä¶, state_‚åàlog ùëò‚åâ‚àí1 variables to store the
representation of the state. Similarly, to encode the larger alphabet Œ£
of the tape, we will use ‚åàlog |Œ£|‚åâarrays Tape_0 , ‚Ä¶, Tape_‚åàlog |Œ£|‚åâ‚àí1,
such that the ùëñùë°‚Ñélocation of these arrays encodes the ùëñùë°‚Ñésymbol in the
tape for every tape. Using the fact that every function can be computed
by a NAND-CIRC program, we will be able to compute the transition
function of ùëÄ, replacing moving left and right by decrementing and
incrementing i respectively.
We show (2) using very similar ideas. Given a program ùëÉthat
uses ùëéarray variables and ùëèscalar variables, we will create a Turing
machine with about 2ùëèstates to encode the values of scalar variables,
and an alphabet of about 2ùëéso we can encode the arrays using our
tape. (The reason the sizes are only ‚Äúabout‚Äù 2ùëéand 2ùëèis that we will
need to add some symbols and steps for bookkeeping purposes.) The
Turing Machine ùëÄwill simulate each iteration of the program ùëÉby
updating its state and tape accordingly.
‚ãÜ
Proof of Theorem 7.11. We start by proving the ‚Äúif‚Äù direction of The-
orem 7.11. Namely we show that given a Turing machine ùëÄ, we can


--- Page 268 ---

268
introduction to theoretical computer science
Figure 7.9: Comparing a Turing Machine to a NAND-
TM program. Both have an unbounded memory
component (the tape for a Turing machine, and the ar-
rays for a NAND-TM program), as well as a constant
local memory (state for a Turing machine, and scalar
variables for a NAND-TM program). Both can only
access at each step one location of the unbounded
memory, this is the ‚Äúhead‚Äù location for a Turing
machine, and the value of the index variable i for a
NAND-TM program.
find a NAND-TM program ùëÉùëÄsuch that for every input ùë•, if ùëÄhalts
on input ùë•with output ùë¶then ùëÉùëÄ(ùë•) = ùë¶. Since our goal is just to
show such a program ùëÉùëÄexists, we don‚Äôt need to write out the full
code of ùëÉùëÄline by line, and can take advantage of our various ‚Äúsyn-
tactic sugar‚Äù in describing it.
The key observation is that by Theorem 4.12 we can compute every
finite function using a NAND-CIRC program. In particular, consider
the transition function ùõøùëÄ‚à∂[ùëò] √ó Œ£ ‚Üí[ùëò] √ó Œ£ √ó {L, R} of our Turing
Machine. We can encode the its components as follows:
‚Ä¢ We encode [ùëò] using {0, 1}‚Ñìand Œ£ using {0, 1}‚Ñì‚Ä≤, where ‚Ñì= ‚åàlog ùëò‚åâ
and ‚Ñì‚Ä≤ = ‚åàlog |Œ£|‚åâ.
‚Ä¢ We encode the set {L, R, S, H} using {0, 1}2. We will choose the
encode L ‚Ü¶01, R ‚Ü¶11, S ‚Ü¶10, H ‚Ü¶00. (This conveniently
corresponds to the semantics of the MODANDJUMP operation.)
Hence we can identify ùõøùëÄwith a function ùëÄ‚à∂{0, 1}‚Ñì+‚Ñì‚Ä≤ ‚Üí
{0, 1}‚Ñì+‚Ñì‚Ä≤+2, mapping strings of length ‚Ñì+ ‚Ñì‚Ä≤ to strings of length
‚Ñì+ ‚Ñì‚Ä≤ + 2. By Theorem 4.12 there exists a finite length NAND-CIRC
program ComputeM that computes this function ùëÄ. The NAND-TM
program to simulate ùëÄwill essentially be the following:


--- Page 269 ---

loops and infinity
269
Algorithm 7.12 ‚Äî NAND-TM program to simulate TM ùëÄ.
Input: ùë•‚àà{0, 1}‚àó
Output: ùëÄ(ùë•) if ùëÄhalts on ùë•. Otherwise go into infinite
loop
1:
# We use variables state_0 ‚Ä¶ state_‚Ñì‚àí1 to encode ùëÄ‚Äôs
state
2: # We use arrays Tape_0[] ‚Ä¶ Tape_‚Ñì‚Ä≤ ‚àí1[] to encode ùëÄ‚Äôs
tape
3:
# We omit the initial and final ‚Äùbook keeping‚Äù to copy
Input: to Tape and copy
Output: from Tape
4:
# Use the fact that transition is finite and computable by
NAND-CIRC program:
5: state_0 ‚Ä¶ state_‚Ñì‚àí1, Tape_0[i]‚Ä¶ Tape_‚Ñì‚Ä≤ ‚àí1[i],
dir0,dir1 ‚ÜêTRANSITION( state_0 ‚Ä¶ state_‚Ñì
‚àí
1,
Tape_0[i]‚Ä¶ Tape_‚Ñì‚Ä≤ ‚àí1[i], dir0,dir1 )
6: MODANDJMP(dir0,dir1)
Every step of the main loop of the above program perfectly mimics
the computation of the Turing Machine ùëÄand so the program carries
out exactly the definition of computation by a Turing Machine as per
Definition 7.1.
For the other direction, suppose that ùëÉis a NAND-TM program
with ùë†lines, ‚Ñìscalar variables, and ‚Ñì‚Ä≤ array variables. We will show
that there exists a Turing machine ùëÄùëÉwith 2‚Ñì+ ùê∂states and alphabet
Œ£ of size ùê∂‚Ä≤ + 2‚Ñì‚Ä≤ that computes the same functions as ùëÉ(where ùê∂, ùê∂‚Ä≤
are some constants to be determined later).
Specifically, consider the function ùëÉ‚à∂{0, 1}‚Ñì√ó {0, 1}‚Ñì‚Ä≤ ‚Üí{0, 1}‚Ñì√ó
{0, 1}‚Ñì‚Ä≤ that on input the contents of ùëÉ‚Äôs scalar variables and the con-
tents of the array variables at location i in the beginning of an itera-
tion, outputs all the new values of these variables at the last line of the
iteration, right before the MODANDJUMP instruction is executed.
If foo and bar are the two variables that are used as input to the
MODANDJUMP instruction, then this means that based on the values of
these variables we can compute whether i will increase, decrease or
stay the same, and whether the program will halt or jump back to the
beginning. Hence a Turing machine can simulate an execution of ùëÉin
one iteration using a finite function applied to its alphabet. The overall
operation of the Turing machine will be as follows:
1. The machine ùëÄùëÉencodes the contents of the array variables of ùëÉ
in its tape, and the contents of the scalar variables in (part of) its
state. Specifically, if ùëÉhas ‚Ñìlocal variables and ùë°arrays, then the
state space of ùëÄwill be large enough to encode all 2‚Ñìassignments


--- Page 270 ---

270
introduction to theoretical computer science
to the local variables and the alphabet Œ£ of ùëÄwill be large enough
to encode all 2ùë°assignments for the array variables at each location.
The head location corresponds to the index variable i.
2. Recall that every line of the program ùëÉcorresponds to reading and
writing either a scalar variable, or an array variable at the location
i. In one iteration of ùëÉthe value of i remains fixed, and so the
machine ùëÄcan simulate this iteration by reading the values of
all array variables at i (which are encoded by the single symbol
in the alphabet Œ£ located at the i-th cell of the tape) , reading the
values of all scalar variables (which are encoded by the state), and
updating both. The transition function of ùëÄcan output L, S, R
depending on whether the values given to the MODANDJMP operation
are 01, 10 or 11 respectively.
3. When the program halts (i.e., MODANDJMP gets 00) then the Turing
machine will enter into a special loop to copy the results of the Y
array into the output and then halt. We can achieve this by adding a
few more states.
The above is not a full formal description of a Turing Machine, but
our goal is just to show that such a machine exists. One can see that
ùëÄùëÉsimulates every step of ùëÉ, and hence computes the same function
as ùëÉ.
‚ñ†
R
Remark 7.13 ‚Äî Running time equivalence (optional). If
we examine the proof of Theorem 7.11 then we can see
that every iteration of the loop of a NAND-TM pro-
gram corresponds to one step in the execution of the
Turing machine. We will come back to this question
of measuring number of computation steps later in
this course. For now the main take away point is that
NAND-TM programs and Turing Machines are essen-
tially equivalent in power even when taking running
time into account.
7.3.1 Specification vs implementation (again)
Once you understand the definitions of both NAND-TM programs
and Turing Machines, Theorem 7.11 is fairly straightforward. Indeed,
NAND-TM programs are not as much a different model from Turing
Machines as they are simply a reformulation of the same model using
programming language notation. You can think of the difference be-
tween a Turing machine and a NAND-TM program as the difference
between representing a number using decimal or binary notation. In


--- Page 271 ---

loops and infinity
271
contrast, the difference between a function ùêπand a Turing machine
that computes ùêπis much more profound: it is like the difference be-
tween the equation ùë•2 + ùë•= 12 and the number 3 that is a solution
for this equation. For this reason, while we take special care in distin-
guishing functions from programs or machines, we will often identify
the two latter concepts. We will move freely between describing an
algorithm as a Turing machine or as a NAND-TM program (as well
as some of the other equivalent computational models we will see in
Chapter 8 and beyond).
Table 7.3: Specification vs Implementation formalisms
Setting
Specification
Implementation
Finite com-
putation
Functions mapping {0, 1}ùëõto
{0, 1}ùëö
Circuits, Straightline
programs
Infinite
computa-
tion
Functions mapping {0, 1}‚àóto
{0, 1} or to {0, 1}‚àó.
Algorithms, Turing
Machines, Programs
7.4 NAND-TM SYNTACTIC SUGAR
Just like we did with NAND-CIRC in Chapter 4, we can use ‚Äúsyntactic
sugar‚Äù to make NAND-TM programs easier to write. For starters, we
can use all of the syntactic sugar of NAND-CIRC, and so have access
to macro definitions and conditionals (i.e., if/then). But we can go
beyond this and achieve for example:
‚Ä¢ Inner loops such as the while and for operations common to many
programming language.
‚Ä¢ Multiple index variables (e.g., not just i but we can add j, k, etc.).
‚Ä¢ Arrays with more than one dimension (e.g., Foo[i][j],
Bar[i][j][k] etc.)
In all of these cases (and many others) we can implement the new
feature as mere ‚Äúsyntactic sugar‚Äù on top of standard NAND-TM,
which means that the set of functions computable by NAND-TM
with this feature is the same as the set of functions computable by
standard NAND-TM. Similarly, we can show that the set of functions
computable by Turing Machines that have more than one tape, or
tapes of more dimensions than one, is the same as the set of functions
computable by standard Turing machines.


--- Page 272 ---

272
introduction to theoretical computer science
7.4.1 ‚ÄúGOTO‚Äù and inner loops
We can implement more advanced looping constructs than the simple
MODANDJUMP. For example, we can implement GOTO. A GOTO statement
corresponds to jumping to a certain line in the execution. For example,
if we have code of the form
"start":
do foo
GOTO("end")
"skip": do bar
"end": do blah
then the program will only do foo and blah as when it reaches the
line GOTO("end") it will jump to the line labeled with "end". We can
achieve the effect of GOTO in NAND-TM using conditionals. In the
code below, we assume that we have a variable pc that can take strings
of some constant length. This can be encoded using a finite number
of Boolean variables pc_0, pc_1, ‚Ä¶, pc_ùëò‚àí1, and so when we write
below pc = "label" what we mean is something like pc_0 = 0,pc_1
= 1, ‚Ä¶ (where the bits 0, 1, ‚Ä¶ correspond to the encoding of the finite
string "label" as a string of length ùëò). We also assume that we have
access to conditional (i.e., if statements), which we can emulate using
syntactic sugar in the same way as we did in NAND-CIRC.
To emulate a GOTO statement, we will first modify a program P of
the form
do foo
do bar
do blah
to have the following form (using syntactic sugar for if):
pc = "line1"
if (pc=="line1"):
do foo
pc = "line2"
if (pc=="line2"):
do bar
pc = "line3"
if (pc=="line3"):
do blah
These two programs do the same thing. The variable pc cor-
responds to the ‚Äúprogram counter‚Äù and tells the program which
line to execute next. We can see that if we wanted to emulate a
GOTO("line3") then we could simply modify the instruction pc =
"line2" to be pc = "line3".


--- Page 273 ---

loops and infinity
273
In NAND-CIRC we could only have GOTOs that go forward in the
code, but since in NAND-TM everything is encompassed within a
large outer loop, we can use the same ideas to implement GOTO‚Äôs that
can go backwards, as well as conditional loops.
Other loops.
Once we have GOTO, we can emulate all the standard loop
constructs such as while, do .. until or for in NAND-TM as well.
For example, we can replace the code
while foo:
do blah
do bar
with
"loop":
if NOT(foo): GOTO("next")
do blah
GOTO("loop")
"next":
do bar
R
Remark 7.14 ‚Äî GOTO‚Äôs in programming languages. The
GOTO statement was a staple of most early program-
ming languages, but has largely fallen out of favor and
is not included in many modern languages such as
Python, Java, Javascript. In 1968, Edsger Dijsktra wrote a
famous letter titled ‚ÄúGo to statement considered harm-
ful.‚Äù (see also Fig. 7.10). The main trouble with GOTO
is that it makes analysis of programs more difficult
by making it harder to argue about invariants of the
program.
When a program contains a loop of the form:
for j in range(100):
do something
do blah
you know that the line of code do blah can only be
reached if the loop ended, in which case you know
that j is equal to 100, and might also be able to argue
other properties of the state of the program. In con-
trast, if the program might jump to do blah from any
other point in the code, then it‚Äôs very hard for you as
the programmer to know what you can rely upon in
this code. As Dijkstra said, such invariants are impor-
tant because ‚Äúour intellectual powers are rather geared
to master static relations and .. our powers to visualize
processes evolving in time are relatively poorly developed‚Äù


--- Page 274 ---

274
introduction to theoretical computer science
Figure 7.10: XKCD‚Äôs take on the GOTO statement.
and so ‚Äúwe should ‚Ä¶ do ‚Ä¶our utmost best to shorten the
conceptual gap between the static program and the dynamic
process.‚Äù
That said, GOTO is still a major part of lower level lan-
guages where it is used to implement higher level
looping constructs such as while and for loops.
For example, even though Java doesn‚Äôt have a GOTO
statement, the Java Bytecode (which is a lower level
representation of Java) does have such a statement.
Similarly, Python bytecode has instructions such as
POP_JUMP_IF_TRUE that implement the GOTO function-
ality, and similar instructions are included in many
assembly languages. The way we use GOTO to imple-
ment a higher level functionality in NAND-TM is
reminiscent of the way these various jump instructions
are used to implement higher level looping constructs.
7.5 UNIFORMITY, AND NAND VS NAND-TM (DISCUSSION)
While NAND-TM adds extra operations over NAND-CIRC, it is not
exactly accurate to say that NAND-TM programs or Turing machines
are ‚Äúmore powerful‚Äù than NAND-CIRC programs or Boolean circuits.
NAND-CIRC programs, having no loops, are simply not applicable
for computing functions with an unbounded number of inputs. Thus,
to compute a function ùêπ‚à∂{0, 1}‚àó‚à∂‚Üí{0, 1}‚àóusing NAND-CIRC (or
equivalently, Boolean circuits) we need a collection of programs/cir-
cuits: one for every input length.
The key difference between NAND-CIRC and NAND-TM is that
NAND-TM allows us to express the fact that the algorithm for com-
puting parities of length-100 strings is really the same one as the al-
gorithm for computing parities of length-5 strings (or similarly the
fact that the algorithm for adding ùëõ-bit numbers is the same for every
ùëõ, etc.). That is, one can think of the NAND-TM program for general
parity as the ‚Äúseed‚Äù out of which we can grow NAND-CIRC programs
for length 10, length 100, or length 1000 parities as needed.
This notion of a single algorithm that can compute functions of
all input lengths is known as uniformity of computation and hence
we think of Turing machines / NAND-TM as uniform model of com-
putation, as opposed to Boolean circuits or NAND-CIRC which is a
nonuniform model, where we have to specify a different program for
every input length.
Looking ahead, we will see that this uniformity leads to another
crucial difference between Turing machines and circuits. Turing ma-
chines can have inputs and outputs that are longer than the descrip-
tion of the machine as a string and in particular there exists a Turing
machine that can ‚Äúself replicate‚Äù in the sense that it can print its own


--- Page 275 ---

loops and infinity
275
code. This notion of ‚Äúself replication‚Äù, and the related notion of ‚Äúself
reference‚Äù is crucial to many aspects of computation, as well of course
to life itself, whether in the form of digital or biological programs.
For now, what you ought to remember is the following differences
between uniform and non uniform computational models:
‚Ä¢ Non uniform computational models: Examples are NAND-CIRC
programs and Boolean circuits. These are models where each indi-
vidual program/circuit can compute a finite function ùëì‚à∂{0, 1}ùëõ‚Üí
{0, 1}ùëö. We have seen that every finite function can be computed by
some program/circuit. To discuss computation of an infinite func-
tion ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}‚àówe need to allow a sequence {ùëÉùëõ}ùëõ‚àà‚Ñïof
programs/circuits (one for every input length), but this does not
capture the notion of a single algorithm to compute the function ùêπ.
‚Ä¢ Uniform computational models: Examples are Turing machines and
NAND-TM programs. These are models where a single program/-
machine can take inputs of arbitrary length and hence compute an
infinite function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}‚àó. The number of steps that
a program/machine takes on some input is not a priori bounded
in advance and in particular there is a chance that it will enter into
an infinite loop. Unlike the nonuniform case, we have not shown
that every infinite function can be computed by some NAND-TM
program/Turing Machine. We will come back to this point in Chap-
ter 9.
‚úì
Chapter Recap
‚Ä¢ Turing machines capture the notion of a single al-
gorithm that can evaluate functions of every input
length.
‚Ä¢ They are equivalent to NAND-TM programs, which
add loops and arrays to NAND-CIRC.
‚Ä¢ Unlike NAND-CIRC or Boolean circuits, the num-
ber of steps that a Turing machine takes on a given
input is not fixed in advance. In fact, a Turing ma-
chine or a NAND-TM program can enter into an
infinite loop on certain inputs, and not halt at all.
7.6 EXERCISES
Exercise 7.1 ‚Äî Explicit NAND TM programming. Produce the code of a
(syntactic-sugar free) NAND-TM program ùëÉthat computes the (un-
bounded input length) Majority function ùëÄùëéùëó‚à∂{0, 1}‚àó‚Üí{0, 1} where
for every ùë•‚àà{0, 1}‚àó, ùëÄùëéùëó(ùë•) = 1 if and only if ‚àë
|ùë•|
ùëñ=0 ùë•ùëñ> |ùë•|/2. We
say ‚Äúproduce‚Äù rather than ‚Äúwrite‚Äù because you do not have to write


--- Page 276 ---

276
introduction to theoretical computer science
the code of ùëÉby hand, but rather can use the programming language
of your choice to compute this code.
‚ñ†
Exercise 7.2 ‚Äî Computable functions examples. Prove that the following
functions are computable. For all of these functions, you do not have
to fully specify the Turing Machine or the NAND-TM program that
computes the function, but rather only prove that such a machine or
program exists:
1. INC ‚à∂{0, 1}‚àó‚Üí{0, 1}‚àówhich takes as input a representation of a
natural number ùëõand outputs the representation of ùëõ+ 1.
2. ADD ‚à∂{0, 1}‚àó‚Üí{0, 1}‚àówhich takes as input a representation of
a pair of natural numbers (ùëõ, ùëö) and outputs the representation of
ùëõ+ ùëö.
3. MULT ‚à∂{0, 1}‚àó‚Üí{0, 1}‚àó, which takes a representation of a pair of
natural numbers (ùëõ, ùëö) and outputs the representation of ùëõÃáùëö.
4. SORT ‚à∂{0, 1}‚àó‚Üí{0, 1}‚àówhich takes as input the representation of
a list of natural numbers (ùëé0, ‚Ä¶ , ùëéùëõ‚àí1) and returns its sorted version
(ùëè0, ‚Ä¶ , ùëèùëõ‚àí1) such that for every ùëñ‚àà[ùëõ] there is some ùëó‚àà[ùëõ] with
ùëèùëñ= ùëéùëóand ùëè0 ‚â§ùëè1 ‚â§‚ãØ‚â§ùëèùëõ‚àí1.
‚ñ†
Exercise 7.3 ‚Äî Two index NAND-TM. Define NAND-TM‚Äô to be the variant of
NAND-TM where there are two index variables i and j. Arrays can be
indexed by either i or j. The operation MODANDJMP takes four variables
ùëé, ùëè, ùëê, ùëëand uses the values of ùëê, ùëëto decide whether to increment j,
decrement j or keep it in the same value (corresponding to 01, 10, and
00 respectively). Prove that for every function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}‚àó, ùêπ
is computable by a NAND-TM program if and only if ùêπis computable
by a NAND-TM‚Äô program.
‚ñ†
Exercise 7.4 ‚Äî Two tape Turing machines. Define a two tape Turing machine
to be a Turing machine which has two separate tapes and two separate
heads. At every step, the transition function gets as input the location
of the cells in the two tapes, and can decide whether to move each
head independently. Prove that for every function ùêπ‚à∂{0, 1}‚àó‚Üí
{0, 1}‚àó, ùêπis computable by a standard Turing Machine if and only if ùêπ
is computable by a two-tape Turing machine.
‚ñ†
Exercise 7.5 ‚Äî Two dimensional arrays. Define NAND-TM‚Äô ‚Äô to be the vari-
ant of NAND-TM where just like NAND-TM‚Äô defined in Exercise 7.3


--- Page 277 ---

loops and infinity
277
3 You can use the sequence R, L,R, R, L, L, R,R,R, L, L, L,
‚Ä¶.
there are two index variables i and j, but now the arrays are two di-
mensional and so we index an array Foo by Foo[i][j]. Prove that for
every function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}‚àó, ùêπis computable by a NAND-TM
program if and only if ùêπis computable by a NAND-TM‚Äô ‚Äô program.
‚ñ†
Exercise 7.6 ‚Äî Two dimensional Turing machines. Define a two-dimensional
Turing machine to be a Turing machine in which the tape is two dimen-
sional. At every step the machine can move Up, Down, Left, Right, or
Stay. Prove that for every function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}‚àó, ùêπis com-
putable by a standard Turing Machine if and only if ùêπis computable
by a two-dimensional Turing machine.
‚ñ†
Exercise 7.7 Prove the following closure properties of the set R defined
in Definition 7.3:
1. If ùêπ‚ààR then the function ùê∫(ùë•) = 1 ‚àíùêπ(ùë•) is in R.
2. If ùêπ, ùê∫‚ààR then the function ùêª(ùë•) = ùêπ(ùë•) ‚à®ùê∫(ùë•) is in R.
3. If ùêπ‚ààR then the function ùêπ‚àóin in R where ùêπ‚àóis defined as fol-
lows: ùêπ‚àó(ùë•) = 1 iff there exist some strings ùë§0, ‚Ä¶ , ùë§ùëò‚àí1 such that
ùë•= ùë§0ùë§1 ‚ãØùë§ùëò‚àí1 and ùêπ(ùë§ùëñ) = 1 for every ùëñ‚àà[ùëò].
4. If ùêπ‚ààR then the function
ùê∫(ùë•) =
‚éß
{
‚é®
{
‚é©
‚àÉùë¶‚àà{0,1}|ùë•|ùêπ(ùë•ùë¶) = 1
0
otherwise
(7.3)
is in R.
‚ñ†
Exercise 7.8 ‚Äî Oblivious Turing Machines (challenging). Define a Turing Ma-
chine ùëÄto be oblivious if its head movements are independent of its
input. That is, we say that ùëÄis oblivious if there exists an infinite
sequence MOVE ‚àà{L, R, S}‚àûsuch that for every ùë•‚àà{0, 1}‚àó, the
movements of ùëÄwhen given input ùë•(up until the point it halts, if
such point exists) are given by MOVE0, MOVE1, MOVE2, ‚Ä¶.
Prove that for every function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}‚àó, if ùêπis com-
putable then it is computable by an oblivious Turing machine. See
footnote for hint.3
‚ñ†
Exercise 7.9 ‚Äî Single vs multiple bit. Prove that for every ùêπ‚à∂{0, 1}‚àó‚Üí
{0, 1}‚àó, the function ùêπis computable if and only if the following func-


--- Page 278 ---

278
introduction to theoretical computer science
tion ùê∫‚à∂{0, 1}‚àó‚Üí{0, 1} is computable, where ùê∫is defined as follows:
ùê∫(ùë•, ùëñ, ùúé) =
‚éß
{
{
‚é®
{
{
‚é©
ùêπ(ùë•)ùëñ
ùëñ< |ùêπ(ùë•)|, ùúé= 0
1
ùëñ< |ùêπ(ùë•)|, ùúé= 1
0
ùëñ‚â•|ùêπ(ùë•)|
‚ñ†
Exercise 7.10 ‚Äî Uncomputability via counting. Recall that R is the set of all
total functions from {0, 1}‚àóto {0, 1} that are computable by a Turing
machine (see Definition 7.3). Prove that R is countable. That is, prove
that there exists a one-to-one map ùê∑ùë°ùëÅ‚à∂R ‚Üí‚Ñï. You can use the
equivalence between Turing machines and NAND-TM programs.
‚ñ†
Exercise 7.11 ‚Äî Not every function is computable. Prove that the set of all
total functions from {0, 1}‚àó‚Üí{0, 1} is not countable. You can use the
results of Section 2.4. (We will see an explicit uncomputable function
in Chapter 9.)
‚ñ†
7.7 BIBLIOGRAPHICAL NOTES
Augusta Ada Byron, countess of Lovelace (1815-1852) lived a short
but turbulent life, though is today most well known for her collabo-
ration with Charles Babbage (see [Ste87] for a biography). Ada took
an immense interest in Babbage‚Äôs analytical engine, which we men-
tioned in Chapter 3. In 1842-3, she translated from Italian a paper of
Menabrea on the engine, adding copious notes (longer than the paper
itself). The quote in the chapter‚Äôs beginning is taken from Nota A in
this text. Lovelace‚Äôs notes contain several examples of programs for the
analytical engine, and because of this she has been called ‚Äúthe world‚Äôs
first computer programmer‚Äù though it is not clear whether they were
written by Lovelace or Babbage himself [Hol01]. Regardless, Ada was
clearly one of very few people (perhaps the only one outside of Bab-
bage himself) to fully appreciate how significant and revolutionary
the idea of mechanizing computation truly is.
The books of Shetterly [She16] and Sobel [Sob17] discuss the his-
tory of human computers (who were female, more often than not)
and their important contributions to scientific discoveries in astron-
omy and space exploration.
Alan Turing was one of the intellectual giants of the 20th century.
He was not only the first person to define the notion of computation,
but also invented and used some of the world‚Äôs earliest computational
devices as part of the effort to break the Enigma cipher during World
War II, saving millions of lives. Tragically, Turing committed suicide
in 1954, following his conviction in 1952 for homosexual acts and a
court-mandated hormonal treatment. In 2009, British prime minister


--- Page 279 ---

loops and infinity
279
Gordon Brown made an official public apology to Turing, and in 2013
Queen Elizabeth II granted Turing a posthumous pardon. Turing‚Äôs life
is the subject of a great book and a mediocre movie.
Sipser‚Äôs text [Sip97] defines a Turing machine as a seven tuple con-
sisting of the state space, input alphabet, tape alphabet, transition
function, starting state, accepting state, and rejecting state. Superfi-
cially this looks like a very different definition than Definition 7.1 but
it is simply a different representation of the same concept, just as a
graph can be represented in either adjacency list or adjacency matrix
form.
One difference is that Sipser considers a general set of states ùëÑthat
is not necessarily of the form ùëÑ= {0, 1, 2, ‚Ä¶ , ùëò‚àí1} for some natural
number ùëò> 0. Sipser also restricts his attention to Turing machines
that output only a single bit and therefore designates two special halt-
ing states: the ‚Äú0 halting state‚Äù (often known as the rejecting state) and
the other as the ‚Äú1 halting state‚Äù (often known as the accepting state).
Thus instead of writing 0 or 1 on an output tape, the machine will en-
ter into one of these states and halt. This again makes no difference
to the computational power, though we prefer to consider the more
general model of multi-bit outputs. (Sipser presents the basic task of a
Turing machine as that of deciding a language as opposed to computing
a function, but these are equivalent, see Remark 7.4.)
Sipser considers also functions with input in Œ£‚àófor an arbitrary
alphabet Œ£ (and hence distinguishes between the input alphabet which
he denotes as Œ£ and the tape alphabet which he denotes as Œì), while we
restrict attention to functions with binary strings as input. Again this
is not a major issue, since we can always encode an element of Œ£ using
a binary string of length log‚åà|Œ£|‚åâ. Finally (and this is a very minor
point) Sipser requires the machine to either move left or right in every
step, without the Stay operation, though staying in place is very easy
to emulate by simply moving right and then back left.
Another definition used in the literature is that a Turing machine
ùëÄrecognizes a language ùêøif for every ùë•‚ààùêø, ùëÄ(ùë•) = 1 and for
every ùë•‚àâùêø, ùëÄ(ùë•) ‚àà{0, ‚ä•}. A language ùêøis recursively enumerable if
there exists a Turing machine ùëÄthat recognizes it, and the set of all
recursively enumerable languages is often denoted by RE. We will not
use this terminology in this book.
One of the first programming-language formulations of Turing
machines was given by Wang [Wan57]. Our formulation of NAND-
TM is aimed at making the connection with circuits more direct, with
the eventual goal of using it for the Cook-Levin Theorem, as well as
results such as P ‚äÜP/poly and BPP ‚äÜP/poly. The website esolangs.org
features a large variety of esoteric Turing-complete programming
languages. One of the most famous of them is Brainf*ck.


--- Page 280 ---



--- Page 281 ---

8
Equivalent models of computation
‚ÄúAll problems in computer science can be solved by another level of indirec-
tion‚Äù, attributed to David Wheeler.
‚ÄúBecause we shall later compute with expressions for functions, we need a
distinction between functions and forms and a notation for expressing this
distinction. This distinction and a notation for describing it, from which we
deviate trivially, is given by Church.‚Äù, John McCarthy, 1960 (in paper
describing the LISP programming language)
So far we have defined the notion of computing a function using
Turing machines, which are not a close match to the way computation
is done in practice. In this chapter we justify this choice by showing
that the definition of computable functions will remain the same
under a wide variety of computational models. This notion is known
as Turing completeness or Turing equivalence and is one of the most
fundamental facts of computer science. In fact, a widely believed
claim known as the Church-Turing Thesis holds that every ‚Äúreasonable‚Äù
definition of computable function is equivalent to being computable
by a Turing machine. We discuss the Church-Turing Thesis and the
potential definitions of ‚Äúreasonable‚Äù in Section 8.8.
Some of the main computational models we discuss in this chapter
include:
‚Ä¢ RAM Machines: Turing Machines do not correspond to standard
computing architectures that have Random Access Memory (RAM).
The mathematical model of RAM machines is much closer to actual
computers, but we will see that it is equivalent in power to Turing
Machines. We also discuss a programming language variant of
RAM machines, which we call NAND-RAM. The equivalence of
Turing Machines and RAM machines enables demonstrating the
Turing Equivalence of many popular programming languages, in-
cluding all general-purpose languages used in practice such as C,
Python, JavaScript, etc.
Compiled on 8.26.2020 18:10
Learning Objectives:
‚Ä¢ Learn about RAM machines and the Œª
calculus.
‚Ä¢ Equivalence between these and other models
and Turing machines.
‚Ä¢ Cellular automata and configurations of
Turing machines.
‚Ä¢ Understand the Church-Turing thesis.


--- Page 282 ---

282
introduction to theoretical computer science
‚Ä¢ Cellular Automata: Many natural and artificial systems can be
modeled as collections of simple components, each evolving ac-
cording to simple rules based on its state and the state of its imme-
diate neighbors. One well-known such example is Conway‚Äôs Game
of Life. To prove that cellular automata are equivalent to Turing
machines we introduce the tool of configurations of Turing Ma-
chines. These have other applications, and in particular are used in
Chapter 11 to prove G√∂del‚Äôs Incompleteness Theorem: a central result
in mathematics.
‚Ä¢ ùúÜcalculus: The ùúÜcalculus is a model for expressing computation
that originates from the 1930‚Äôs, though it is closely connected to
functional programming languages widely used today. Showing
the equivalence of ùúÜcalculus to Turing Machines involves a beauti-
ful technique to eliminate recursion known as the ‚ÄúY Combinator‚Äù.
This chapter: A non-mathy overview
In this chapter we study equivalence between models. Two
computational models are equivalent (also known as Turing
equivalent) if they can compute the same set of functions. For
example, we have seen that Turing Machines and NAND-TM
programs are equivalent since we can transform every Tur-
ing Machine into a NAND-TM program that computes the
same function, and similarly can transform every NAND-
TM program into a Turing Machine that computes the same
function.
In this chapter we show this extends far beyond Turing Ma-
chines. The techniques we develop allow us to show that
all general-purpose programming languages (i.e., Python,
C, Java, etc.) are Turing Complete, in the sense that they can
simulate Turing Machines and hence compute all functions
that can be computed by a TM. We will also show the other
direction- Turing Machines can be used to simulate a pro-
gram in any of these languages and hence compute any
function computable by them. This means that all these pro-
gramming language are Turing equivalent: they are equivalent
in power to Turing Machines and to each other. This is a
powerful principle, which underlies behind the vast reach of
Computer Science. Moreover, it enables us to ‚Äúhave our cake
and eat it too‚Äù- since all these models are equivalent, we can
choose the model of our convenience for the task at hand.
To achieve this equivalence, we define a new computational
model known as RAM machines. RAM Machines capture the


--- Page 283 ---

equivalent models of computation
283
architecture of modern computers more closely than Turing
Machines, but are still computationally equivalent to Turing
machines.
Finally, we will show that Turing equivalence extends far
beyond traditional programming languages. We will see
that cellular automata which are a mathematical model of ex-
tremely simple natural systems is also Turing equivalent, and
also see the Turing equivalence of the ùúÜcalculus - a logical
system for expressing functions that is the basis for functional
programming languages such as Lisp, OCaml, and more.
See Fig. 8.1 for an overview of the results of this chapter.
Figure 8.1: Some Turing-equivalent models. All of
these are equivalent in power to Turing Machines
(or equivalently NAND-TM programs) in the sense
that they can compute exactly the same class of
functions. All of these are models for computing
infinite functions that take inputs of unbounded
length. In contrast, Boolean circuits / NAND-CIRC
programs can only compute finite functions and hence
are not Turing complete.
8.1 RAM MACHINES AND NAND-RAM
One of the limitations of Turing Machines (and NAND-TM programs)
is that we can only access one location of our arrays/tape at a time. If
the head is at position 22 in the tape and we want to access the 957-th
position then it will take us at least 923 steps to get there. In contrast,
almost every programming language has a formalism for directly
accessing memory locations. Actual physical computers also provide
so called Random Access Memory (RAM) which can be thought of as a
large array Memory, such that given an index ùëù(i.e., memory address,
or a pointer), we can read from and write to the ùëùùë°‚Ñélocation of Memory.
(‚ÄúRandom access memory‚Äù is quite a misnomer since it has nothing to
do with probability, but since it is a standard term in both the theory
and practice of computing, we will use it as well.)
The computational model that models access to such a memory is
the RAM machine (sometimes also known as the Word RAM model),
as depicted in Fig. 8.2. The memory of a RAM machine is an array
of unbounded size where each cell can store a single word, which
we think of as a string in {0, 1}ùë§and also (equivalently) as a num-
ber in [2ùë§]. For example, many modern computing architectures


--- Page 284 ---

284
introduction to theoretical computer science
Figure 8.2: A RAM Machine contains a finite number of
local registers, each of which holds an integer, and an
unbounded memory array. It can perform arithmetic
operations on its register as well as load to a register ùëü
the contents of the memory at the address indexed by
the number in register ùëü‚Ä≤.
Figure 8.3: Different aspects of RAM machines and
Turing machines. RAM machines can store integers
in their local registers, and can read and write to
their memory at a location specified by a register.
In contrast, Turing machines can only access their
memory in the head location, which moves at most
one position to the right or left in each step.
use 64 bit words, in which every memory location holds a string in
{0, 1}64 which can also be thought of as a number between 0 and
264 ‚àí1 = 18, 446, 744, 073, 709, 551, 615. The parameter ùë§is known
as the word size. In practice often ùë§is a fixed number such as 64, but
when doing theory we model ùë§as a parameter that can depend on
the input length or number of steps. (You can think of 2ùë§as roughly
corresponding to the largest memory address that we use in the com-
putation.) In addition to the memory array, a RAM machine also
contains a constant number of registers ùëü0, ‚Ä¶ , ùëüùëò‚àí1, each of which can
also contain a single word.
The operations a RAM machine can carry out include:
‚Ä¢ Data movement: Load data from a certain cell in memory into
a register or store the contents of a register into a certain cell of
memory. RAM machine can directly access any cell of memory
without having to move the ‚Äúhead‚Äù (as Turing machines do) to that
location. That is, in one step a RAM machine can load into register
ùëüùëñthe contents of the memory cell indexed by register ùëüùëó, or store
into the memory cell indexed by register ùëüùëóthe contents of register
ùëüùëñ.
‚Ä¢ Computation: RAM machines can carry out computation on regis-
ters such as arithmetic operations, logical operations, and compar-
isons.
‚Ä¢ Control flow: As in the case of Turing machines, the choice of what
instruction to perform next can depend on the state of the RAM
machine, which is captured by the contents of its register.
We will not give a formal definition of RAM Machines, though the
bibliographical notes section (Section 8.10) contains sources for such
definitions. Just as the NAND-TM programming language models
Turing machines, we can also define a NAND-RAM programming lan-
guage that models RAM machines. The NAND-RAM programming
language extends NAND-TM by adding the following features:
‚Ä¢ The variables of NAND-RAM are allowed to be (non negative)
integer valued rather than only Boolean as is the case in NAND-
TM. That is, a scalar variable foo holds a non negative integer in ‚Ñï
(rather than only a bit in {0, 1}), and an array variable Bar holds
an array of integers. As in the case of RAM machines, we will not
allow integers of unbounded size. Concretely, each variable holds
a number between 0 and ùëá‚àí1, where ùëáis the number of steps
that have been executed by the program so far. (You can ignore
this restriction for now: if we want to hold larger numbers, we
can simply execute dummy instructions; it will be useful in later
chapters.)


--- Page 285 ---

equivalent models of computation
285
Figure 8.4: Overview of the steps in the proof of The-
orem 8.1 simulating NANDRAM with NANDTM.
We first use the inner loop syntactic sugar of Sec-
tion 7.4.1 to enable loading an integer from an array
to the index variable i of NANDTM. Once we can do
that, we can simulate indexed access in NANDTM. We
then use an embedding of ‚Ñï2 in ‚Ñïto simulate two
dimensional bit arrays in NANDTM. Finally, we use
the binary representation to encode one-dimensional
arrays of integers as two dimensional arrays of bits
hence completing the simulation of NANDRAM with
NANDTM.
‚Ä¢ We allow indexed access to arrays. If foo is a scalar and Bar is an
array, then Bar[foo] refers to the location of Bar indexed by the
value of foo. (Note that this means we don‚Äôt need to have a special
index variable i anymore.)
‚Ä¢ As is often the case in programming languages, we will assume
that for Boolean operations such as NAND, a zero valued integer is
considered as false, and a nonzero valued integer is considered as
true.
‚Ä¢ In addition to NAND, NAND-RAM also includes all the basic arith-
metic operations of addition, subtraction, multiplication, (integer)
division, as well as comparisons (equal, greater than, less than,
etc..).
‚Ä¢ NAND-RAM includes conditional statements if/then as part of
the language.
‚Ä¢ NAND-RAM contains looping constructs such as while and do as
part of the language.
A full description of the NAND-RAM programming language is
in the appendix. However, the most important fact you need to know
about NAND-RAM is that you actually don‚Äôt need to know much
about NAND-RAM at all, since it is equivalent in power to Turing
machines:
Theorem 8.1 ‚Äî Turing Machines (aka NAND-TM programs) and RAM ma-
chines (aka NAND-RAM programs) are equivalent. For every function
ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}‚àó, ùêπis computable by a NAND-TM program if
and only if ùêπis computable by a NAND-RAM program.
Since NAND-TM programs are equivalent to Turing machines, and
NAND-RAM programs are equivalent to RAM machines, Theorem 8.1
shows that all these four models are equivalent to one another.
Proof Idea:
Clearly NAND-RAM is only more powerful than NAND-TM, and
so if a function ùêπis computable by a NAND-TM program then it can
be computed by a NAND-RAM program. The challenging direction is
to transform a NAND-RAM program ùëÉto an equivalent NAND-TM
program ùëÑ. To describe the proof in full we will need to cover the full
formal specification of the NAND-RAM language, and show how we
can implement every one of its features as syntactic sugar on top of
NAND-TM.
This can be done but going over all the operations in detail is rather
tedious. Hence we will focus on describing the main ideas behind this


--- Page 286 ---

286
introduction to theoretical computer science
transformation. (See also Fig. 8.4.) NAND-RAM generalizes NAND-
TM in two main ways: (a) adding indexed access to the arrays (ie..,
Foo[bar] syntax) and (b) moving from Boolean valued variables to
integer valued ones. The transformation has two steps:
1. Indexed access of bit arrays: We start by showing how to handle (a).
Namely, we show how we can implement in NAND-TM the op-
eration Setindex(Bar) such that if Bar is an array that encodes
some integer ùëó, then after executing Setindex(Bar) the value of
i will equal to ùëó. This will allow us to simulate syntax of the form
Foo[Bar] by Setindex(Bar) followed by Foo[i].
2. Two dimensional bit arrays: We then show how we can use ‚Äúsyntactic
sugar‚Äù to augment NAND-TM with two dimensional arrays. That is,
have two indices i and j and two dimensional arrays, such that we can
use the syntax Foo[i][j] to access the (i,j)-th location of Foo.
3. Arrays of integers: Finally we will encode a one dimensional array
Arr of integers by a two dimensional Arrbin of bits. The idea is
simple: if ùëéùëñ,0, ‚Ä¶ , ùëéùëñ,‚Ñìis a binary (prefix-free) representation of
Arr[ùëñ], then Arrbin[ùëñ][ùëó] will be equal to ùëéùëñ,ùëó.
Once we have arrays of integers, we can use our usual syntactic
sugar for functions, GOTO etc. to implement the arithmetic and control
flow operations of NAND-RAM.
‚ãÜ
The above approach is not the only way to obtain a proof of Theo-
rem 8.1, see for example Exercise 8.1
R
Remark 8.2 ‚Äî RAM machines / NAND-RAM and assembly
language (optional). RAM machines correspond quite
closely to actual microprocessors such as those in the
Intel x86 series that also contains a large primary mem-
ory and a constant number of small registers. This is of
course no accident: RAM machines aim at modeling
more closely than Turing machines the architecture of
actual computing systems, which largely follows the
so called von Neumann architecture as described in
the report [Neu45]. As a result, NAND-RAM is sim-
ilar in its general outline to assembly languages such
as x86 or NIPS. These assembly languages all have
instructions to (1) move data from registers to mem-
ory, (2) perform arithmetic or logical computations
on registers, and (3) conditional execution and loops
(‚Äúif‚Äù and ‚Äúgoto‚Äù, commonly known as ‚Äúbranches‚Äù and
‚Äújumps‚Äù in the context of assembly languages).
The main difference between RAM machines and
actual microprocessors (and correspondingly between


--- Page 287 ---

equivalent models of computation
287
NAND-RAM and assembly languages) is that actual
microprocessors have a fixed word size ùë§so that all
registers and memory cells hold numbers in [2ùë§] (or
equivalently strings in {0, 1}ùë§). This number ùë§can
vary among different processors, but common values
are either 32 or 64. As a theoretical model, RAM ma-
chines do not have this limitation, but we rather let ùë§
be the logarithm of our running time (which roughly
corresponds to its value in practice as well). Actual
microprocessors also have a fixed number of registers
(e.g., 14 general purpose registers in x86-64) but this
does not make a big difference with RAM machines.
It can be shown that RAM machines with as few as
two registers are as powerful as full-fledged RAM ma-
chines that have an arbitrarily large constant number
of registers.
Of course actual microprocessors have many features
not shared with RAM machines as well, including
parallelism, memory hierarchies, and many others.
However, RAM machines do capture actual comput-
ers to a first approximation and so (as we will see), the
running time of an algorithm on a RAM machine (e.g.,
ùëÇ(ùëõ) vs ùëÇ(ùëõ2)) is strongly correlated with its practical
efficiency.
8.2 THE GORY DETAILS (OPTIONAL)
We do not show the full formal proof of Theorem 8.1 but focus on the
most important parts: implementing indexed access, and simulating
two dimensional arrays with one dimensional ones. Even these are
already quite tedious to describe, as will not be surprising to anyone
that has ever written a compiler. Hence you can feel free to merely
skim this section. The important point is not for you to know all de-
tails by heart but to be convinced that in principle it is possible to
transform a NAND-RAM program to an equivalent NAND-TM pro-
gram, and even be convinced that, with sufficient time and effort, you
could do it if you wanted to.
8.2.1 Indexed access in NAND-TM
In NAND-TM we can only access our arrays in the position of the in-
dex variable i, while NAND-RAM has integer-valued variables and
can use them for indexed access to arrays, of the form Foo[bar]. To im-
plement indexed access in NAND-TM, we will encode integers in our
arrays using some prefix-free representation (see Section 2.5.2)), and
then have a procedure Setindex(Bar) that sets i to the value encoded
by Bar. We can simulate the effect of Foo[Bar] using Setindex(Bar)
followed by Foo[i].
Implementing Setindex(Bar) can be achieved as follows:


--- Page 288 ---

288
introduction to theoretical computer science
1. We initialize an array Arzero such that Atzero[0]= 1 and
Atzero[ùëó]= 0 for all ùëó> 0. (This can be easily done in NAND-TM
as all uninitialized variables default to zero.)
2. Set i to zero, by decrementing it until we reach the point where
Atzero[i]= 1.
3. Let Temp be an array encoding the number 0.
4. We use GOTO to simulate an inner loop of of the form: while Temp ‚â†
Bar, increment Temp.
5. At the end of the loop, i is equal to the value encoded by Bar.
In NAND-TM code (using some syntactic sugar), we can imple-
ment the above operations as follows:
# assume Atzero is an array such that Atzero[0]=1
# and Atzero[j]=0 for all j>0
# set i to 0.
LABEL("zero_idx")
dir0 = zero
dir1 = one
# corresponds to i <- i-1
GOTO("zero_idx",NOT(Atzero[i]))
...
# zero out temp
#(code below assumes a specific prefix-free encoding in
which 10 is the "end marker")
‚Ü™
Temp[0] = 1
Temp[1] = 0
# set i to Bar, assume we know how to increment, compare
LABEL("increment_temp")
cond = EQUAL(Temp,Bar)
dir0 = one
dir1 = one
# corresponds to i <- i+1
INC(Temp)
GOTO("increment_temp",cond)
# if we reach this point, i is number encoded by Bar
...
# final instruction of program
MODANDJUMP(dir0,dir1)
8.2.2 Two dimensional arrays in NAND-TM
To implement two dimensional arrays, we want to embed them in a
one dimensional array. The idea is that we come up with a one to one


--- Page 289 ---

equivalent models of computation
289
Figure 8.5: Illustration of the map ùëíùëöùëèùëíùëë(ùë•, ùë¶) =
1
2 (ùë•+ ùë¶)(ùë•+ ùë¶+ 1) + ùë•for ùë•, ùë¶‚àà[10], one can
see that for every distinct pairs (ùë•, ùë¶) and (ùë•‚Ä≤, ùë¶‚Ä≤),
ùëíùëöùëèùëíùëë(ùë•, ùë¶) ‚â†ùëíùëöùëèùëíùëë(ùë•‚Ä≤, ùë¶‚Ä≤).
function ùëíùëöùëèùëíùëë‚à∂‚Ñï√ó ‚Ñï‚Üí‚Ñï, and so embed the location (ùëñ, ùëó) of the
two dimensional array Two in the location ùëíùëöùëèùëíùëë(ùëñ, ùëó) of the array One.
Since the set ‚Ñï√ó ‚Ñïseems ‚Äúmuch bigger‚Äù than the set ‚Ñï, a priori it
might not be clear that such a one to one mapping exists. However,
once you think about it more, it is not that hard to construct. For ex-
ample, you could ask a child to use scissors and glue to transform a
10‚Äù by 10‚Äù piece of paper into a 1‚Äù by 100‚Äù strip. This is essentially
a one to one map from [10] √ó [10] to [100]. We can generalize this to
obtain a one to one map from [ùëõ] √ó [ùëõ] to [ùëõ2] and more generally a one
to one map from ‚Ñï√ó ‚Ñïto ‚Ñï. Specifically, the following map ùëíùëöùëèùëíùëë
would do (see Fig. 8.5):
ùëíùëöùëèùëíùëë(ùë•, ùë¶) = 1
2(ùë•+ ùë¶)(ùë•+ ùë¶+ 1) + ùë•.
(8.1)
Exercise 8.3 asks you to prove that ùëíùëöùëèùëíùëëis indeed one to one, as
well as computable by a NAND-TM program. (The latter can be done
by simply following the grade-school algorithms for multiplication,
addition, and division.) This means that we can replace code of the
form Two[Foo][Bar] = something (i.e., access the two dimensional
array Two at the integers encoded by the one dimensional arrays Foo
and Bar) by code of the form:
Blah = embed(Foo,Bar)
Setindex(Blah)
Two[i] = something
8.2.3 All the rest
Once we have two dimensional arrays and indexed access, simulating
NAND-RAM with NAND-TM is just a matter of implementing the
standard algorithms for arithmetic operations and comparisons in
NAND-TM. While this is cumbersome, it is not difficult, and the end
result is to show that every NAND-RAM program ùëÉcan be simulated
by an equivalent NAND-TM program ùëÑ, thus completing the proof of
Theorem 8.1.
R
Remark 8.3 ‚Äî Recursion in NAND-RAM (advanced). One
concept that appears in many programming languages
but we did not include in NAND-RAM programs is
recursion. However, recursion (and function calls in
general) can be implemented in NAND-RAM using
the stack data structure. A stack is a data structure con-
taining a sequence of elements, where we can ‚Äúpush‚Äù
elements into it and ‚Äúpop‚Äù them from it in ‚Äúfirst in last
out‚Äù order.
We can implement a stack using an array of integers
Stack and a scalar variable stackpointer that will


--- Page 290 ---

290
introduction to theoretical computer science
Figure 8.6: A punched card corresponding to a Fortran
statement.
1 Some programming language have fixed (even if
extremely large) bounds on the amount of memory
they can access, which formally prevent them from
being applicable to computing infinite functions and
hence simulating Turing machines. We ignore such
issues in this discussion and assume access to some
storage device without a fixed upper bound on its
capacity.
be the number of items in the stack. We implement
push(foo) by
Stack[stackpointer]=foo
stackpointer += one
and implement bar = pop() by
bar = Stack[stackpointer]
stackpointer -= one
We implement a function call to ùêπby pushing the
arguments for ùêπinto the stack. The code of ùêπwill
‚Äúpop‚Äù the arguments from the stack, perform the com-
putation (which might involve making recursive or
non recursive calls) and then ‚Äúpush‚Äù its return value
into the stack. Because of the ‚Äúfirst in last out‚Äù na-
ture of a stack, we do not return control to the calling
procedure until all the recursive calls are done.
The fact that we can implement recursion using a non-
recursive language is not surprising. Indeed, machine
languages typically do not have recursion (or function
calls in general), and hence a compiler implements
function calls using a stack and GOTO. You can find
online tutorials on how recursion is implemented
via stack in your favorite programming language,
whether it‚Äôs Python , JavaScript, or Lisp/Scheme.
8.3 TURING EQUIVALENCE (DISCUSSION)
Any of the standard programming language such as C, Java, Python,
Pascal, Fortran have very similar operations to NAND-RAM. (In-
deed, ultimately they can all be executed by machines which have a
fixed number of registers and a large memory array.) Hence using
Theorem 8.1, we can simulate any program in such a programming
language by a NAND-TM program. In the other direction, it is a fairly
easy programming exercise to write an interpreter for NAND-TM in
any of the above programming languages. Hence we can also simulate
NAND-TM programs (and so by Theorem 7.11, Turing machines) us-
ing these programming languages. This property of being equivalent
in power to Turing Machines / NAND-TM is called Turing Equivalent
(or sometimes Turing Complete). Thus all programming languages we
are familiar with are Turing equivalent.1
8.3.1 The ‚ÄúBest of both worlds‚Äù paradigm
The equivalence between Turing Machines and RAM machines allows
us to choose the most convenient language for the task at hand:
‚Ä¢ When we want to prove a theorem about all programs/algorithms,
we can use Turing machines (or NAND-TM) since they are sim-


--- Page 291 ---

equivalent models of computation
291
Figure 8.7: By having the two equivalent languages
NAND-TM and NAND-RAM, we can ‚Äúhave our cake
and eat it too‚Äù, using NAND-TM when we want to
prove that programs can‚Äôt do something, and using
NAND-RAM or other high level languages when we
want to prove that programs can do something.
pler and easier to analyze. In particular, if we want to show that
a certain function can not be computed, then we will use Turing
machines.
‚Ä¢ When we want to show that a function can be computed we can use
RAM machines or NAND-RAM, because they are easier to pro-
gram in and correspond more closely to high level programming
languages we are used to. In fact, we will often describe NAND-
RAM programs in an informal manner, trusting that the reader
can fill in the details and translate the high level description to the
precise program. (This is just like the way people typically use in-
formal or ‚Äúpseudocode‚Äù descriptions of algorithms, trusting that
their audience will know to translate these descriptions to code if
needed.)
Our usage of Turing Machines / NAND-TM and RAM Machines
/ NAND-RAM is very similar to the way people use in practice high
and low level programming languages. When one wants to produce
a device that executes programs, it is convenient to do so for very
simple and ‚Äúlow level‚Äù programming language. When one wants to
describe an algorithm, it is convenient to use as high level a formalism
as possible.
ÔÉ´Big Idea 10 Using equivalence results such as those between
Turing and RAM machines, we can ‚Äúhave our cake and eat it too‚Äù.
We can use a simpler model such as Turing machines when we want
to prove something can‚Äôt be done, and use a feature-rich model such as
RAM machines when we want to prove something can be done.
8.3.2 Let‚Äôs talk about abstractions
‚ÄúThe programmer is in the unique position that ‚Ä¶ he has to be able
to think in terms of conceptual hierarchies that are much deeper than
a single mind ever needed to face before.‚Äù, Edsger Dijkstra, ‚ÄúOn the
cruelty of really teaching computing science‚Äù, 1988.
At some point in any theory of computation course, the instructor
and students need to have the talk. That is, we need to discuss the level
of abstraction in describing algorithms. In algorithms courses, one
typically describes algorithms in English, assuming readers can ‚Äúfill
in the details‚Äù and would be able to convert such an algorithm into an
implementation if needed. For example, Algorithm 8.4 is a high level
description of the breadth first search algorithm.


--- Page 292 ---

292
introduction to theoretical computer science
Algorithm 8.4 ‚Äî Breadth First Search.
Input: Graph ùê∫, vertices ùë¢, ùë£
Output: ‚Äùconnected‚Äù when ùë¢is connected to ùë£in ùê∫, ‚Äùdis-
connected‚Äù
1: Initialize empty queue ùëÑ.
2: Put ùë¢in ùëÑ
3: while ùëÑis not empty do
4:
Remove top vertex ùë§from ùëÑ
5:
if ùë§= ùë£then return ‚Äùconnected‚Äù
6:
end if
7:
Mark ùë§
8:
Add all unmarked neighbors of ùë§to ùëÑ.
9: end while
10: return ‚Äùdisconnected‚Äù
If we wanted to give more details on how to implement breadth
first search in a programming language such as Python or C (or
NAND-RAM / NAND-TM for that matter), we would describe how
we implement the queue data structure using an array, and similarly
how we would use arrays to mark vertices. We call such an ‚Äúinterme-
diate level‚Äù description an implementation level or pseudocode descrip-
tion. Finally, if we want to describe the implementation precisely, we
would give the full code of the program (or another fully precise rep-
resentation, such as in the form of a list of tuples). We call this a formal
or low level description.
Figure 8.8: We can describe an algorithm at different
levels of granularity/detail and precision. At the
highest level we just write the idea in words, omitting
all details on representation and implementation.
In the intermediate level (also known as implemen-
tation or pseudocode) we give enough details of the
implementation that would allow someone to de-
rive it, though we still fall short of providing the full
code. The lowest level is where the actual code or
mathematical description is fully spelled out. These
different levels of detail all have their uses, and mov-
ing between them is one of the most important skills
for a computer scientist.
While we started off by describing NAND-CIRC, NAND-TM, and
NAND-RAM programs at the full formal level, as we progress in this


--- Page 293 ---

equivalent models of computation
293
book we will move to implementation and high level description.
After all, our goal is not to use these models for actual computation,
but rather to analyze the general phenomenon of computation. That
said, if you don‚Äôt understand how the high level description translates
to an actual implementation, going ‚Äúdown to the metal‚Äù is often an
excellent exercise. One of the most important skills for a computer
scientist is the ability to move up and down hierarchies of abstractions.
A similar distinction applies to the notion of representation of objects
as strings. Sometimes, to be precise, we give a low level specification
of exactly how an object maps into a binary string. For example, we
might describe an encoding of ùëõvertex graphs as length ùëõ2 binary
strings, by saying that we map a graph ùê∫over the vertices [ùëõ] to a
string ùë•‚àà{0, 1}ùëõ2 such that the ùëõ‚ãÖùëñ+ ùëó-th coordinate of ùë•is 1 if and
only if the edge ‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó
ùëñùëóis present in ùê∫. We can also use an intermediate or
implementation level description, by simply saying that we represent a
graph using the adjacency matrix representation.
Finally, because we are translating between the various represen-
tations of graphs (and objects in general) can be done via a NAND-
RAM (and hence a NAND-TM) program, when talking in a high level
we also suppress discussion of representation altogether. For example,
the fact that graph connectivity is a computable function is true re-
gardless of whether we represent graphs as adjacency lists, adjacency
matrices, list of edge-pairs, and so on and so forth. Hence, in cases
where the precise representation doesn‚Äôt make a difference, we would
often talk about our algorithms as taking as input an object ùëã(that
can be a graph, a vector, a program, etc.) without specifying how ùëãis
encoded as a string.
Defining ‚ÄùAlgorithms‚Äù.
Up until now we have used the term ‚Äúalgo-
rithm‚Äù informally. However, Turing Machines and the range of equiv-
alent models yield a way to precisely and formally define algorithms.
Hence whenever we refer to an algorithm in this book, we will mean
that it is an instance of one of the Turing equivalent models, such as
Turing machines, NAND-TM, RAM machines, etc. Because of the
equivalence of all these models, in many contexts, it will not matter
which of these we use.
8.3.3 Turing completeness and equivalence, a formal definition (optional)
A computational model is some way to define what it means for a pro-
gram (which is represented by a string) to compute a (partial) func-
tion. A computational model ‚Ñ≥is Turing complete, if we can map ev-
ery Turing machine (or equivalently NAND-TM program) ùëÅinto a
program ùëÉfor ‚Ñ≥that computes the same function as ùëÑ. It is Turing
equivalent if the other direction holds as well (i.e., we can map every
program in ‚Ñ≥to a Turing machine that computes the same function).


--- Page 294 ---

294
introduction to theoretical computer science
We can define this notion formally as follows. (This formal definition
is not crucial for the remainder of this book so feel to skip it as long
as you understand the general concept of Turing equivalence; This
notion is sometimes referred to in the literature as G√∂del numbering
or admissible numbering.)
Definition 8.5 ‚Äî Turing completeness and equivalence (optional). Let ‚Ñ±be
the set of all partial functions from {0, 1}‚àóto {0, 1}‚àó. A computa-
tional model is a map ‚Ñ≥‚à∂{0, 1}‚àó‚Üí‚Ñ±.
We say that a program ùëÉ‚àà{0, 1}‚àó‚Ñ≥-computes a function ùêπ‚àà‚Ñ±
if ‚Ñ≥(ùëÉ) = ùêπ.
A computational model ‚Ñ≥is Turing complete if there is a com-
putable map ENCODE‚Ñ≥
‚à∂
{0, 1}‚àó
‚Üí
{0, 1}‚àófor every Turing
machine ùëÅ(represented as a string), ‚Ñ≥(ENCODE‚Ñ≥(ùëÅ)) is equal
to the partial function computed by ùëÅ.
A computational model ‚Ñ≥is Turing equivalent if it is Tur-
ing complete and there exists a computable map DECODE‚Ñ≥
‚à∂
{0, 1}‚àó
‚Üí
{0, 1}‚àósuch that or every string ùëÉ
‚àà
{0, 1}‚àó, ùëÅ
=
DECODE‚Ñ≥(ùëÉ) is a string representation of a Turing machine that
computes the function ‚Ñ≥(ùëÉ).
Some examples of Turing equivalent models (some of which we
have already seen, and some are discussed below) include:
‚Ä¢ Turing machines
‚Ä¢ NAND-TM programs
‚Ä¢ NAND-RAM programs
‚Ä¢ Œª calculus
‚Ä¢ Game of life (mapping programs and inputs/outputs to starting
and ending configurations)
‚Ä¢ Programming languages such as Python/C/Javascript/OCaml‚Ä¶
(allowing for unbounded storage)
8.4 CELLULAR AUTOMATA
Many physical systems can be described as consisting of a large num-
ber of elementary components that interact with one another. One
way to model such systems is using cellular automata. This is a system
that consists of a large (or even infinite) number of cells. Each cell
only has a constant number of possible states. At each time step, a cell
updates to a new state by applying some simple rule to the state of
itself and its neighbors.
A canonical example of a cellular automaton is Conway‚Äôs Game
of Life. In this automata the cells are arranged in an infinite two di-
mensional grid. Each cell has only two states: ‚Äúdead‚Äù (which we can


--- Page 295 ---

equivalent models of computation
295
Figure 8.9: Rules for Conway‚Äôs Game of Life. Image
from this blog post.
encode as 0 and identify with ‚àÖ) or ‚Äúalive‚Äù (which we can encode
as 1). The next state of a cell depends on its previous state and the
states of its 8 vertical, horizontal and diagonal neighbors (see Fig. 8.9).
A dead cell becomes alive only if exactly three of its neighbors are
alive. A live cell continues to live if it has two or three live neighbors.
Even though the number of cells is potentially infinite, we can en-
code the state using a finite-length string by only keeping track of the
live cells. If we initialize the system in a configuration with a finite
number of live cells, then the number of live cells will stay finite in all
future steps. The Wikipedia page for the Game of Life contains some
beautiful figures and animations of configurations that produce very
interesting evolutions.
Figure 8.10: In a two dimensional cellular automaton
every cell is in position ùëñ, ùëófor some integers ùëñ, ùëó‚àà‚Ñ§.
The state of a cell is some value ùê¥ùëñ,ùëó‚ààŒ£ for some
finite alphabet Œ£. At a given time step, the state of the
cell is adjusted according to some function applied to
the state of (ùëñ, ùëó) and all its neighbors (ùëñ¬± 1, ùëó¬± 1).
In a one dimensional cellular automaton every cell is in
position ùëñ‚àà‚Ñ§and the state ùê¥ùëñof ùëñat the next time
step depends on its current state and the state of its
two neighbors ùëñ‚àí1 and ùëñ+ 1.
Since the cells in the game of life are are arranged in an infinite two-
dimensional grid, it is an example of a two dimensional cellular automa-
ton. We can also consider the even simpler setting of a one dimensional
cellular automaton, where the cells are arranged in an infinite line, see
Fig. 8.10. It turns out that even this simple model is enough to achieve


--- Page 296 ---

296
introduction to theoretical computer science
Figure 8.11: A Game-of-Life configuration simulating
a Turing Machine. Figure by Paul Rendell.
Turing-completeness. We will now formally define one-dimensional
cellular automata and then prove their Turing completeness.
Definition 8.6 ‚Äî One dimensional cellular automata. Let Œ£ be a finite set
containing the symbol ‚àÖ. A one dimensional cellular automation over
alphabet Œ£ is described by a transition rule ùëü
‚à∂
Œ£3
‚Üí
Œ£, which
satisfies ùëü(‚àÖ, ‚àÖ, ‚àÖ) = ‚àÖ.
A configuration of the automaton ùëüis a function ùê¥‚à∂‚Ñ§‚ÜíŒ£. If
an automaton with rule ùëüis in configuration ùê¥, then its next config-
uration, denoted by ùê¥‚Ä≤
=
NEXTùëü(ùê¥), is the function ùê¥‚Ä≤ such that
ùê¥‚Ä≤(ùëñ) = ùëü(ùê¥(ùëñ‚àí1), ùê¥(ùëñ), ùê¥(ùëñ+ 1)) for every ùëñ‚àà‚Ñ§. In other words,
the next state of the automaton ùëüat point ùëñobtained by applying
the rule ùëüto the values of ùê¥at ùëñand its two neighbors.
Finite configuration.
We say that a configuration of an automaton ùëü
is finite if there is only some finite number of indices ùëñ0, ‚Ä¶ , ùëñùëó‚àí1 in ‚Ñ§
such that ùê¥(ùëñùëó) ‚â†‚àÖ. (That is, for every ùëñ‚àâ{ùëñ0, ‚Ä¶ , ùëñùëó‚àí1}, ùê¥(ùëñ) = ‚àÖ.)
Such a configuration can be represented using a finite string that
encodes the indices ùëñ0, ‚Ä¶ , ùëñùëõ‚àí1 and the values ùê¥(ùëñ0), ‚Ä¶ , ùê¥(ùëñùëõ‚àí1). Since
ùëÖ(‚àÖ, ‚àÖ, ‚àÖ) = ‚àÖ, if ùê¥is a finite configuration then NEXTùëü(ùê¥) is finite
as well. We will only be interested in studying cellular automata that
are initialized in finite configurations, and hence remain in a finite
configuration throughout their evolution.
8.4.1 One dimensional cellular automata are Turing complete
We can write a program (for example using NAND-RAM) that sim-
ulates the evolution of any cellular automaton from an initial finite
configuration by simply storing the values of the cells with state not
equal to ‚àÖand repeatedly applying the rule ùëü. Hence cellular au-
tomata can be simulated by Turing Machines. What is more surprising
that the other direction holds as well. For example, as simple as its
rules seem, we can simulate a Turing machine using the game of life
(see Fig. 8.11).
In fact, even one dimensional cellular automata can be Turing com-
plete:
Theorem 8.7 ‚Äî One dimensional automata are Turing complete. For every
Turing machine ùëÄ, there is a one dimensional cellular automaton
that can simulate ùëÄon every input ùë•.
To make the notion of ‚Äúsimulating a Turing machine‚Äù more precise
we will need to define configurations of Turing machines. We will
do so in Section 8.4.2 below, but at a high level a configuration of a
Turing machine is a string that encodes its full state at a given step in


--- Page 297 ---

equivalent models of computation
297
its computation. That is, the contents of all (non empty) cells of its
tape, its current state, as well as the head position.
The key idea in the proof of Theorem 8.7 is that at every point in
the computation of a Turing machine ùëÄ, the only cell in ùëÄ‚Äôs tape that
can change is the one where the head is located, and the value this
cell changes to is a function of its current state and the finite state of
ùëÄ. This observation allows us to encode the configuration of a Turing
machine ùëÄas a finite configuration of a cellular automaton ùëü, and
ensure that a one-step evolution of this encoded configuration under
the rules of ùëücorresponds to one step in the execution of the Turing
machine ùëÄ.
8.4.2 Configurations of Turing machines and the next-step function
To turn the above ideas into a rigorous proof (and even statement!)
of Theorem 8.7 we will need to precisely define the notion of config-
urations of Turing machines. This notion will be useful for us in later
chapters as well.
Figure 8.12: A configuration of a Turing machine ùëÄ
with alphabet Œ£ and state space [ùëò] encodes the state
of ùëÄat a particular step in its execution as a string ùõº
over the alphabet Œ£ = Œ£ √ó ({‚ãÖ} √ó [ùëò]). The string is
of length ùë°where ùë°is such that ùëÄ‚Äôs tape contains ‚àÖin
all positions ùë°and larger and ùëÄ‚Äôs head is in a position
smaller than ùë°. If ùëÄ‚Äôs head is in the ùëñ-th position, then
for ùëó‚â†ùëñ, ùõºùëóencodes the value of the ùëó-th cell of ùëÄ‚Äôs
tape, while ùõºùëñencodes both this value as well as the
current state of ùëÄ. If the machine writes the value ùúè,
changes state to ùë°, and moves right, then in the next
configuration will contain at position ùëñthe value (ùúè, ‚ãÖ)
and at position ùëñ+ 1 the value (ùõºùëñ+1, ùë°).
Definition 8.8 ‚Äî Configuration of Turing Machines.. Let ùëÄbe a Turing ma-
chine with tape alphabet Œ£ and state space [ùëò]. A configuration of ùëÄ
is a string ùõº‚ààŒ£
‚àówhere Œ£ = Œ£ √ó ({‚ãÖ} ‚à™[ùëò]) that satisfies that there
is exactly one coordinate ùëñfor which ùõºùëñ= (ùúé, ùë†) for some ùúé‚ààŒ£ and
ùë†‚àà[ùëò]. For all other coordinates ùëó, ùõºùëó= (ùúé‚Ä≤, ‚ãÖ) for some ùúé‚Ä≤ ‚ààŒ£.
A configuration ùõº‚ààŒ£
‚àóof ùëÄcorresponds to the following state
of its execution:
‚Ä¢ ùëÄ‚Äôs tape contains ùõºùëó,0 for all ùëó
<
|ùõº| and contains ‚àÖfor all po-
sitions that are at least |ùõº|, where we let ùõºùëó,0 be the value ùúésuch
that ùõºùëó= (ùúé, ùë°) with ùúé‚ààŒ£ and ùë°‚àà{‚ãÖ} ‚à™[ùëò]. (In other words,


--- Page 298 ---

298
introduction to theoretical computer science
since ùõºùëóis a pair of an alphabet symbol ùúéand either a state in [ùëò]
or the symbol ‚ãÖ, ùõºùëó,0 is the first component ùúéof this pair.)
‚Ä¢ ùëÄ‚Äôs head is in the unique position ùëñfor which ùõºùëñhas the form
(ùúé, ùë†) for ùë†‚àà[ùëò], and ùëÄ‚Äôs state is equal to ùë†.
P
Definition 8.8 below has some technical details, but
is not actually that deep or complicated. Try to take a
moment to stop and think how you would encode as a
string the state of a Turing machine at a given point in
an execution.
Think what are all the components that you need to
know in order to be able to continue the execution
from this point onwards, and what is a simple way
to encode them using a list of finite symbols. In par-
ticular, with an eye towards our future applications,
try to think of an encoding which will make it as sim-
ple as possible to map a configuration at step ùë°to the
configuration at step ùë°+ 1.
Definition 8.8 is a little cumbersome, but ultimately a configuration
is simply a string that encodes a snapshot of the Turing machine at a
given point in the execution. (In operating-systems lingo, it is a ‚Äúcore
dump‚Äù.) Such a snapshot needs to encode the following components:
1. The current head position.
2. The full contents of the large scale memory, that is the tape.
3. The contents of the ‚Äúlocal registers‚Äù, that is the state of the ma-
chine.
The precise details of how we encode a configuration are not impor-
tant, but we do want to record the following simple fact:
Lemma 8.9 Let ùëÄbe a Turing machine and let NEXTùëÄ‚à∂Œ£
‚àó‚ÜíŒ£
‚àó
be the function that maps a configuration of ùëÄto the configuration
at the next step of the execution. Then for every ùëñ‚àà‚Ñï, the value of
NEXTùëÄ(ùõº)ùëñonly depends on the coordinates ùõºùëñ‚àí1, ùõºùëñ, ùõºùëñ+1.
(For simplicity of notation, above we use the convention that if ùëñ
is ‚Äúout of bounds‚Äù, such as ùëñ< 0 or ùëñ> |ùõº|, then we assume that
ùõºùëñ= (‚àÖ, ‚ãÖ).) We leave proving Lemma 8.9 as Exercise 8.7. The idea
behind the proof is simple: if the head is neither in position ùëñnor
positions ùëñ‚àí1 and ùëñ+ 1, then the next-step configuration at ùëñwill be
the same as it was before. Otherwise, we can ‚Äúread off‚Äù the state of the
Turing machine and the value of the tape at the head location from the


--- Page 299 ---

equivalent models of computation
299
Figure 8.13: Evolution of a one dimensional automata.
Each row in the figure corresponds to the configura-
tion. The initial configuration corresponds to the top
row and contains only a single ‚Äúlive‚Äù cell. This figure
corresponds to the ‚ÄúRule 110‚Äù automaton of Stephen
Wolfram which is Turing Complete. Figure taken
from Wolfram MathWorld.
configuration at ùëñor one of its neighbors and use that to update what
the new state at ùëñshould be. Completing the full proof is not hard,
but doing it is a great way to ensure that you are comfortable with the
definition of configurations.
Completing the proof of Theorem 8.7.
We can now restate Theorem 8.7
more formally, and complete its proof:
Theorem 8.10 ‚Äî One dimensional automata are Turing complete (formal state-
ment). For every Turing Machine ùëÄ, if we denote by Œ£ the alphabet
of its configuration strings, then there is a one-dimensional cellular
automaton ùëüover the alphabet Œ£
‚àósuch that
(NEXTùëÄ(ùõº)) = NEXTùëü(ùõº)
(8.2)
for every configuration ùõº
‚àà
Œ£
‚àóof ùëÄ(again using the convention
that we consider ùõºùëñ= ‚àÖif ùëñis ‚Äùout of bounds).
Proof. We consider the element (‚àÖ, ‚ãÖ) of Œ£ to correspond to the ‚àÖ
element of the automaton ùëü. In this case, by Lemma 8.9, the function
NEXTùëÄthat maps a configuration of ùëÄinto the next one is in fact a
valid rule for a one dimensional automata.
‚ñ†
The automaton arising from the proof of Theorem 8.10 has a large
alphabet, and furthermore one whose size that depends on the ma-
chine ùëÄthat is being simulated. It turns out that one can obtain an
automaton with an alphabet of fixed size that is independent of the
program being simulated, and in fact the alphabet of the automaton
can be the minimal set {0, 1}! See Fig. 8.13 for an example of such an
Turing-complete automaton.
R
Remark 8.11 ‚Äî Configurations of NAND-TM programs.
We can use the same approach as Definition 8.8 to
define configurations of a NAND-TM program. Such a
configuration will need to encode:
1. The current value of the variable i.
2. For every scalar variable foo, the value of foo.
3. For every array variable Bar, the value Bar[ùëó] for
every ùëó
‚àà
{0, ‚Ä¶ , ùë°‚àí1} where ùë°‚àí1 is the largest
value that the index variable i ever achieved in the
computation.


--- Page 300 ---

300
introduction to theoretical computer science
8.5 LAMBDA CALCULUS AND FUNCTIONAL PROGRAMMING LAN-
GUAGES
The Œª calculus is another way to define computable functions. It was
proposed by Alonzo Church in the 1930‚Äôs around the same time as
Alan Turing‚Äôs proposal of the Turing Machine. Interestingly, while
Turing Machines are not used for practical computation, the Œª calculus
has inspired functional programming languages such as LISP, ML and
Haskell, and indirectly the development of many other programming
languages as well. In this section we will present the Œª calculus and
show that its power is equivalent to NAND-TM programs (and hence
also to Turing machines). Our Github repository contains a Jupyter
notebook with a Python implementation of the Œª calculus that you can
experiment with to get a better feel for this topic.
The Œª operator.
At the core of the Œª calculus is a way to define ‚Äúanony-
mous‚Äù functions. For example, instead of giving a name ùëìto a func-
tion and defining it as
ùëì(ùë•) = ùë•√ó ùë•
(8.3)
we can write it as
ùúÜùë•.ùë•√ó ùë•
(8.4)
and so (ùúÜùë•.ùë•√ó ùë•)(7) = 49. That is, you can think of ùúÜùë•.ùëíùë•ùëù(ùë•),
where ùëíùë•ùëùis some expression as a way of specifying the anonymous
function ùë•‚Ü¶ùëíùë•ùëù(ùë•). Anonymous functions, using either ùúÜùë•.ùëì(ùë•), ùë•‚Ü¶
ùëì(ùë•) or other closely related notation, appear in many programming
languages. For example, in Python we can define the squaring function
using lambda x: x*x while in JavaScript we can use x => x*x or
(x) => x*x. In Scheme we would define it as (lambda (x) (* x x)).
Clearly, the name of the argument to a function doesn‚Äôt matter, and so
ùúÜùë¶.ùë¶√ó ùë¶is the same as ùúÜùë•.ùë•√ó ùë•, as both correspond to the squaring
function.
Dropping parenthesis. To reduce notational clutter, when writing
ùúÜcalculus expressions we often drop the parentheses for function
evaluation. Hence instead of writing ùëì(ùë•) for the result of applying
the function ùëìto the input ùë•, we can also write this as simply ùëìùë•.
Therefore we can write (ùúÜùë•.ùë•√ó ùë•)7 = 49. In this chapter, we will use
both the ùëì(ùë•) and ùëìùë•notations for function application. Function
evaluations are associative and bind from left to right, and hence ùëìùëî‚Ñé
is the same as (ùëìùëî)‚Ñé.
8.5.1 Applying functions to functions
A key feature of the Œª calculus is that functions are ‚Äúfirst-class objects‚Äù
in the sense that we can use functions as arguments to other functions.


--- Page 301 ---

equivalent models of computation
301
For example, can you guess what number is the following expression
equal to?
(((ùúÜùëì.(ùúÜùë¶.(ùëì(ùëìùë¶))))(ùúÜùë•.ùë•√ó ùë•)) 3)
(8.5)
P
The expression (8.5) might seem daunting, but before
you look at the solution below, try to break it apart
to its components, and evaluate each component at a
time. Working out this example would go a long way
toward understanding the Œª calculus.
Let‚Äôs evaluate (8.5) one step at a time. As nice as it is for the Œª
calculus to allow anonymous functions, adding names can be very
helpful for understanding complicated expressions. So, let us write
ùêπ= ùúÜùëì.(ùúÜùë¶.(ùëì(ùëìùë¶))) and ùëî= ùúÜùë•.ùë•√ó ùë•.
Therefore (8.5) becomes
((ùêπùëî) 3) .
(8.6)
On input a function ùëì, ùêπoutputs the function ùúÜùë¶.(ùëì(ùëìùë¶)), or in
other words ùêπùëìis the function ùë¶‚Ü¶ùëì(ùëì(ùë¶)). Our function ùëîis simply
ùëî(ùë•) = ùë•2 and so (ùêπùëî) is the function that maps ùë¶to (ùë¶2)2 = ùë¶4. Hence
((ùêπùëî)3) = 34 = 81.
Solved Exercise 8.1 What number does the following expression equal
to?
((ùúÜùë•.(ùúÜùë¶.ùë•)) 2) 9) .
(8.7)
‚ñ†
Solution:
ùúÜùë¶.ùë•is the function that on input ùë¶ignores its input and outputs
ùë•. Hence (ùúÜùë•.(ùúÜùë¶.ùë•))2 yields the function ùë¶‚Ü¶2 (or, using ùúÜnota-
tion, the function ùúÜùë¶.2). Hence (8.7) is equivalent to (ùúÜùë¶.2)9 = 2.
‚ñ†
8.5.2 Obtaining multi-argument functions via Currying
In a Œª expression of the form ùúÜùë•.ùëí, the expression ùëícan itself involve
the Œª operator. Thus for example the function
ùúÜùë•.(ùúÜùë¶.ùë•+ ùë¶)
(8.8)
maps ùë•to the function ùë¶‚Ü¶ùë•+ ùë¶.
In particular, if we invoke the function (8.8) on ùëéto obtain some
function ùëì, and then invoke ùëìon ùëè, we obtain the value ùëé+ ùëè. We


--- Page 302 ---

302
introduction to theoretical computer science
Figure 8.14: In the ‚Äúcurrying‚Äù transformation, we can
create the effect of a two parameter function ùëì(ùë•, ùë¶)
with the Œª expression ùúÜùë•.(ùúÜùë¶.ùëì(ùë•, ùë¶)) which on input
ùë•outputs a one-parameter function ùëìùë•that has ùë•
‚Äúhardwired‚Äù into it and such that ùëìùë•(ùë¶) = ùëì(ùë•, ùë¶).
This can be illustrated by a circuit diagram; see
Chelsea Voss‚Äôs site.
can see that the one-argument function (8.8) corresponding to ùëé‚Ü¶
(ùëè‚Ü¶ùëé+ ùëè) can also be thought of as the two-argument function
(ùëé, ùëè) ‚Ü¶ùëé+ ùëè. Generally, we can use the Œª expression ùúÜùë•.(ùúÜùë¶.ùëì(ùë•, ùë¶))
to simulate the effect of a two argument function (ùë•, ùë¶) ‚Ü¶ùëì(ùë•, ùë¶). This
technique is known as Currying. We will use the shorthand ùúÜùë•, ùë¶.ùëí
for ùúÜùë•.(ùúÜùë¶.ùëí). If ùëì= ùúÜùë•.(ùúÜùë¶.ùëí) then (ùëìùëé)ùëècorresponds to applying ùëìùëé
and then invoking the resulting function on ùëè, obtaining the result of
replacing in ùëíthe occurrences of ùë•with ùëéand occurrences of ùëèwith
ùë¶. By our rules of associativity, this is the same as (ùëìùëéùëè) which we‚Äôll
sometimes also write as ùëì(ùëé, ùëè).
8.5.3 Formal description of the Œª calculus
We now provide a formal description of the Œª calculus. We start with
‚Äúbasic expressions‚Äù that contain a single variable such as ùë•or ùë¶and
build more complex expressions of the form (ùëíùëí‚Ä≤) and ùúÜùë•.ùëíwhere ùëí, ùëí‚Ä≤
are expressions and ùë•is a variable idenifier. Formally Œª expressions
are defined as follows:
Definition 8.12 ‚Äî Œª expression.. A Œª expression is either a single variable
identifier or an expression ùëíof the one of the following forms:
‚Ä¢ Application: ùëí= (ùëí‚Ä≤ ùëí‚Ä≥), where ùëí‚Ä≤ and ùëí‚Ä≥ are Œª expressions.
‚Ä¢ Abstraction: ùëí= ùúÜùë•.(ùëí‚Ä≤) where ùëí‚Ä≤ is a Œª expression.
Definition 8.12 is a recursive definition since we defined the concept
of Œª expressions in terms of itself. This might seem confusing at first,
but in fact you have known recursive definitions since you were an
elementary school student. Consider how we define an arithmetic
expression: it is an expression that is either just a number, or has one of
the forms (ùëí+ ùëí‚Ä≤), (ùëí‚àíùëí‚Ä≤), (ùëí√ó ùëí‚Ä≤), or (ùëí√∑ ùëí‚Ä≤), where ùëíand ùëí‚Ä≤ are other
arithmetic expressions.
Free and bound variables. Variables in a Œª expression can either be
free or bound to a ùúÜoperator (in the sense of Section 1.4.7). In a single-
variable Œª expression ùë£ùëéùëü, the variable ùë£ùëéùëüis free. The set of free and
bound variables in an application expression ùëí= (ùëí‚Ä≤ ùëí‚Ä≥) is the same
as that of the underlying expressions ùëí‚Ä≤ and ùëí‚Ä≥. In an abstraction ex-
pression ùëí= ùúÜùë£ùëéùëü.(ùëí‚Ä≤), all free occurences of ùë£ùëéùëüin ùëí‚Ä≤ are bound to
the ùúÜoperator of ùëí. If you find the notion of free and bound variables
confusing, you can avoid all these issues by using unique identifiers
for all variables.
Precedence and parenthesis. We will use the following rules to allow
us to drop some parenthesis. Function application associates from
left to right, and so ùëìùëî‚Ñéis the same as (ùëìùëî)‚Ñé. Function application has
a higher precedence than the Œª operator, and so ùúÜùë•.ùëìùëîùë•is the same


--- Page 303 ---

equivalent models of computation
303
as ùúÜùë•.((ùëìùëî)ùë•). This is similar to how we use the precedence rules in
arithmetic operations to allow us to use fewer parenthesis and so write
the expression (7√ó3)+2 as 7√ó3+2. As mentioned in Section 8.5.2, we
also use the shorthand ùúÜùë•, ùë¶.ùëífor ùúÜùë•.(ùúÜùë¶.ùëí) and the shorthand ùëì(ùë•, ùë¶)
for (ùëìùë•) ùë¶. This plays nicely with the ‚ÄúCurrying‚Äù transformation of
simulating multi-input functions using Œª expressions.
Equivalence of Œª expressions.
As we have seen in Solved Exercise 8.1,
the rule that (ùúÜùë•.ùëíùë•ùëù)ùëíùë•ùëù‚Ä≤ is equivalent to ùëíùë•ùëù[ùë•‚Üíùëíùë•ùëù‚Ä≤] enables us
to modify Œª expressions and obtain simpler equivalent form for them.
Another rule that we can use is that the parameter does not matter
and hence for example ùúÜùë¶.ùë¶is the same as ùúÜùëß.ùëß. Together these rules
define the notion of equivalence of Œª expressions:
Definition 8.13 ‚Äî Equivalence of Œª expressions. Two Œª expressions are
equivalent if they can be made into the same expression by repeated
applications of the following rules:
1. Evaluation (aka ùõΩreduction): The expression (ùúÜùë•.ùëíùë•ùëù)ùëíùë•ùëù‚Ä≤ is
equivalent to ùëíùë•ùëù[ùë•‚Üíùëíùë•ùëù‚Ä≤].
2. Variable renaming (aka ùõºconversion): The expression ùúÜùë•.ùëíùë•ùëù
is equivalent to ùúÜùë¶.ùëíùë•ùëù[ùë•‚Üíùë¶].
If ùëíùë•ùëùis a Œª expression of the form ùúÜùë•.ùëíùë•ùëù‚Ä≤ then it naturally corre-
sponds to the function that maps any input ùëßto ùëíùë•ùëù‚Ä≤[ùë•‚Üíùëß]. Hence
the Œª calculus naturally implies a computational model. Since in the Œª
calculus the inputs can themselves be functions, we need to decide in
what order we evaluate an expression such as
(ùúÜùë•.ùëì)(ùúÜùë¶.ùëîùëß) .
(8.9)
There are two natural conventions for this:
‚Ä¢ Call by name (aka ‚Äúlazy evaluation‚Äù): We evaluate (8.9) by first plug-
ging in the righthand expression (ùúÜùë¶.ùëîùëß) as input to the lefthand
side function, obtaining ùëì[ùë•‚Üí(ùúÜùë¶.ùëîùëß)] and then continue from
there.
‚Ä¢ Call by value (aka ‚Äúeager evaluation‚Äù): We evaluate (8.9) by first
evaluating the righthand side and obtaining ‚Ñé= ùëî[ùë¶‚Üíùëß], and then
plugging this into the lefthandside to obtain ùëì[ùë•‚Üí‚Ñé].
Because the Œª calculus has only pure functions, that do not have
‚Äúside effects‚Äù, in many cases the order does not matter. In fact, it can
be shown that if we obtain a definite irreducible expression (for ex-
ample, a number) in both strategies, then it will be the same one.


--- Page 304 ---

304
introduction to theoretical computer science
However, for concreteness we will always use the ‚Äúcall by name‚Äù (i.e.,
lazy evaluation) order. (The same choice is made in the programming
language Haskell, though many other programming languages use
eager evaluation.) Formally, the evaluation of a Œª expression using
‚Äúcall by name‚Äù is captured by the following process:
Definition 8.14 ‚Äî Simplification of Œª expressions. Let ùëíbe a Œª expres-
sion. The simplification of ùëíis the result of the following recursive
process:
1. If ùëíis a single variable ùë•then the simplification of ùëíis ùëí.
2. If ùëíhas the form ùëí
=
ùúÜùë•.ùëí‚Ä≤ then the simplification of ùëíis ùúÜùë•.ùëì‚Ä≤
where ùëì‚Ä≤ is the simplification of ùëí‚Ä≤.
3. (Evaluation / ùõΩreduction.) If ùëíhas the form ùëí= (ùúÜùë•.ùëí‚Ä≤ ùëí‚Ä≥) then
the simplification of ùëíis the simplification of ùëí‚Ä≤[ùë•
‚Üí
ùëí‚Ä≥], which
denotes replacing all copies of ùë•in ùëí‚Ä≤ bound to the ùúÜoperator
with ùëí‚Ä≥
4. (Renaming / ùõºconversion.) The canonical simplification of ùëíis
obtained by taking the simplification of ùëíand renaming the vari-
ables so that the first bound variable in the expression is ùë£0, the
second one is ùë£1, and so on and so forth.
We say that two Œª expressions ùëíand ùëí‚Ä≤ are equivalent, denoted by
ùëí‚âÖùëí‚Ä≤, if they have the same canonical simplification.
Solved Exercise 8.2 ‚Äî Equivalence of Œª expressions. Prove that the following
two expressions ùëíand ùëìare equivalent:
ùëí= ùúÜùë•.ùë•
(8.10)
ùëì= (ùúÜùëé.(ùúÜùëè.ùëè))(ùúÜùëß.ùëßùëß)
(8.11)
‚ñ†
Solution:
The canonical simplification of ùëíis simply ùúÜùë£0.ùë£0. To do the
canonical simplification of ùëìwe first use ùõΩreduction to plug in
ùúÜùëß.ùëßùëßinstead of ùëéin (ùúÜùëè.ùëè) but since ùëéis not used in this function at
all, we simply obtained ùúÜùëè.ùëèwhich simplifies to ùúÜùë£0.ùë£0 as well.
‚ñ†


--- Page 305 ---

equivalent models of computation
305
2 In Lisp, the PAIR, HEAD and TAIL functions are
traditionally called cons, car and cdr.
8.5.4 Infinite loops in the Œª calculus
Like Turing machines and NAND-TM programs, the simplification
process in the Œª calculus can also enter into an infinite loop. For exam-
ple, consider the Œª expression
ùúÜùë•.ùë•ùë•ùúÜùë•.ùë•ùë•
(8.12)
If we try to simplify (8.12) by invoking the lefthand function on
the righthand one, then we get another copy of (8.12) and hence this
never ends. There are examples where the order of evaluation can
matter for whether or not an expression can be simplified, see Exer-
cise 8.9.
8.6 THE ‚ÄúENHANCED‚Äù Œõ CALCULUS
We now discuss the Œª calculus as a computational model. We will
start by describing an ‚Äúenhanced‚Äù version of the Œª calculus that con-
tains some ‚Äúsuperfluous features‚Äù but is easier to wrap your head
around. We will first show how the enhanced Œª calculus is equiva-
lent to Turing machines in computational power. Then we will show
how all the features of ‚Äúenhanced Œª calculus‚Äù can be implemented as
‚Äúsyntactic sugar‚Äù on top of the ‚Äúpure‚Äù (i.e., non enhanced) Œª calculus.
Hence the pure Œª calculus is equivalent in power to Turing machines
(and hence also to RAM machines and all other Turing-equivalent
models).
The enhanced Œª calculus includes the following set of objects and
operations:
‚Ä¢ Boolean constants and IF function: There are Œª expressions 0, 1
and IF that satisfy the following conditions: for every Œª expression
ùëíand ùëì, IF 1 ùëíùëì= ùëíand IF 0 ùëíùëì= ùëì. That is, IF is the function that
given three arguments ùëé, ùëí, ùëìoutputs ùëíif ùëé= 1 and ùëìif ùëé= 0.
‚Ä¢ Pairs: There is a Œª expression PAIR which we will think of as the
pairing function. For every Œª expressions ùëí, ùëì, PAIR ùëíùëìis the
pair ‚ü®ùëí, ùëì‚ü©that contains ùëías its first member and ùëìas its second
member. We also have Œª expressions HEAD and TAIL that extract
the first and second member of a pair respectively. Hence, for every
Œª expressions ùëí, ùëì, HEAD (PAIR ùëíùëì) = ùëíand TAIL (PAIR ùëíùëì) = ùëì.2
‚Ä¢ Lists and strings: There is Œª expression NIL that corresponds to
the empty list, which we also denote by ‚ü®‚ä•‚ü©. Using PAIR and NIL
we construct lists. The idea is that if ùêøis a ùëòelement list of the
form ‚ü®ùëí1, ùëí2, ‚Ä¶ , ùëíùëò, ‚ä•‚ü©then for every Œª expression ùëí0 we can obtain
the ùëò+ 1 element list ‚ü®ùëí0, ùëí1, ùëí2, ‚Ä¶ , ùëíùëò, ‚ä•‚ü©using the expression
PAIR ùëí0 ùêø. For example, for every three Œª expressions ùëí, ùëì, ùëî, the


--- Page 306 ---

306
introduction to theoretical computer science
following corresponds to the three element list ‚ü®ùëí, ùëì, ùëî, ‚ä•‚ü©:
PAIR ùëí(PAIR ùëì(PAIR ùëîNIL)) .
(8.13)
The Œª expression ISEMPTY returns 1 on NIL and returns 0 on every
other list. A string is simply a list of bits.
‚Ä¢ List operations: The enhanced Œª calculus also contains the
list-processing functions MAP, REDUCE, and FILTER. Given
a list ùêø= ‚ü®ùë•0, ‚Ä¶ , ùë•ùëõ‚àí1, ‚ä•‚ü©and a function ùëì, MAP ùêøùëìap-
plies ùëìon every member of the list to obtain the new list
ùêø‚Ä≤ = ‚ü®ùëì(ùë•0), ‚Ä¶ , ùëì(ùë•ùëõ‚àí1), ‚ä•‚ü©. Given a list ùêøas above and an
expression ùëìwhose output is either 0 or 1, FILTER ùêøùëìreturns the
list ‚ü®ùë•ùëñ‚ü©ùëìùë•ùëñ=1 containing all the elements of ùêøfor which ùëìoutputs
1. The function REDUCE applies a ‚Äúcombining‚Äù operation to a
list. For example, REDUCE ùêø+ 0 will return the sum of all the
elements in the list ùêø. More generally, REDUCE takes a list ùêø, an
operation ùëì(which we think of as taking two arguments) and a Œª
expression ùëß(which we think of as the ‚Äúneutral element‚Äù for the
operation ùëì, such as 0 for addition and 1 for multiplication). The
output is defined via
REDUCE ùêøùëìùëß=
‚éß
{
‚é®
{
‚é©
ùëß
ùêø= NIL
ùëì(HEAD ùêø) (REDUCE (TAIL ùêø) ùëìùëß)
otherwise
.
(8.14)
See Fig. 8.16 for an illustration of the three list-processing operations.
‚Ä¢ Recursion: Finally, we want to be able to execute recursive func-
tions. Since in Œª calculus functions are anonymous, we can‚Äôt write
a definition of the form ùëì(ùë•) = ùëèùëôùëé‚Ñéwhere ùëèùëôùëé‚Ñéincludes calls to
ùëì. Instead we use functions ùëìthat take an additional input ùëöùëías a
parameter. The operator RECURSE will take such a function ùëìas
input and return a ‚Äúrecursive version‚Äù of ùëìwhere all the calls to ùëöùëí
are replaced by recursive calls to this function. That is, if we have a
function ùêπtaking two parameters ùëöùëíand ùë•, then RECURSE ùêπwill
be the function ùëìtaking one parameter ùë•such that ùëì(ùë•) = ùêπ(ùëì, ùë•)
for every ùë•.
Solved Exercise 8.3 ‚Äî Compute NAND using Œª calculus. Give a Œª expression
ùëÅsuch that ùëÅùë•ùë¶= NAND(ùë•, ùë¶) for every ùë•, ùë¶‚àà{0, 1}.
‚ñ†


--- Page 307 ---

equivalent models of computation
307
Solution:
The NAND of ùë•, ùë¶is equal to 1 unless ùë•= ùë¶= 1. Hence we can
write
ùëÅ= ùúÜùë•, ùë¶.IF(ùë•, IF(ùë¶, 0, 1), 1)
(8.15)
‚ñ†
Solved Exercise 8.4 ‚Äî Compute XOR using Œª calculus. Give a Œª expression
XOR such that for every list ùêø= ‚ü®ùë•0, ‚Ä¶ , ùë•ùëõ‚àí1, ‚ä•‚ü©where ùë•ùëñ‚àà{0, 1} for
ùëñ‚àà[ùëõ], XORùêøevaluates to ‚àëùë•ùëñmod 2.
‚ñ†
Solution:
First, we note that we can compute XOR of two bits as follows:
NOT = ùúÜùëé.IF(ùëé, 0, 1)
(8.16)
and
XOR2 = ùúÜùëé, ùëè.IF(ùëè, NOT(ùëé), ùëé)
(8.17)
(We are using here a bit of syntactic sugar to describe the func-
tions. To obtain the Œª expression for XOR we will simply replace
the expression (8.16) in (8.17).) Now recursively we can define the
XOR of a list as follows:
XOR(ùêø) =
‚éß
{
‚é®
{
‚é©
0
ùêøis empty
XOR2(HEAD(ùêø), XOR(TAIL(ùêø)))
otherwise
(8.18)
This means that XOR is equal to
RECURSE (ùúÜùëöùëí, ùêø.IF(ISEMPTY(ùêø), 0, XOR2(HEAD ùêø, ùëöùëí(TAIL ùêø)))) .
(8.19)
That is, XOR is obtained by applying the RECURSE operator
to the function that on inputs ùëöùëí, ùêø, returns 0 if ISEMPTY(ùêø) and
otherwise returns XOR2 applied to HEAD(ùêø) and to ùëöùëí(TAIL(ùêø)).
We could have also computed XOR using the REDUCE opera-
tion, we leave working this out as an exercise to the reader.
‚ñ†
8.6.1 Computing a function in the enhanced Œª calculus
An enhanced Œª expression is obtained by composing the objects above
with the application and abstraction rules. The result of simplifying a Œª


--- Page 308 ---

308
introduction to theoretical computer science
Figure 8.15: A list ‚ü®ùë•0, ùë•1, ùë•2‚ü©in the Œª calculus is con-
structed from the tail up, building the pair ‚ü®ùë•2, NIL‚ü©,
then the pair ‚ü®ùë•1, ‚ü®ùë•2, NIL‚ü©‚ü©and finally the pair
‚ü®ùë•0, ‚ü®ùë•1, ‚ü®ùë•2, NIL‚ü©‚ü©‚ü©. That is, a list is a pair where
the first element of the pair is the first element of the
list and the second element is the rest of the list. The
figure on the left renders this ‚Äúpairs inside pairs‚Äù
construction, though it is often easier to think of a list
as a ‚Äúchain‚Äù, as in the figure on the right, where the
second element of each pair is thought of as a link,
pointer or reference to the remainder of the list.
Figure 8.16: Illustration of the MAP, FILTER and
REDUCE operations.
expression is an equivalent expression, and hence if two expressions
have the same simplification then they are equivalent.
Definition 8.15 ‚Äî Computing a function via Œª calculus. Let ùêπ
‚à∂
{0, 1}‚àó
‚Üí
{0, 1}‚àó
We say that ùëíùë•ùëùcomputes ùêπif for every ùë•‚àà{0, 1}‚àó,
ùëíùë•ùëù‚ü®ùë•0, ‚Ä¶ , ùë•ùëõ‚àí1, ‚ä•‚ü©‚âÖ‚ü®ùë¶0, ‚Ä¶ , ùë¶ùëö‚àí1, ‚ä•‚ü©
(8.20)
where ùëõ= |ùë•|, ùë¶= ùêπ(ùë•), and ùëö= |ùë¶|, and the notion of equiva-
lence is defined as per Definition 8.14.
8.6.2 Enhanced Œª calculus is Turing-complete
The basic operations of the enhanced Œª calculus more or less amount
to the Lisp or Scheme programming languages. Given that, it is per-
haps not surprising that the enhanced Œª-calculus is equivalent to
Turing machines:
Theorem 8.16 ‚Äî Lambda calculus and NAND-TM. For every function
ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}‚àó, ùêπis computable in the enhanced Œª calculus if
and only if it is computable by a Turing machine.


--- Page 309 ---

equivalent models of computation
309
Proof Idea:
To prove the theorem, we need to show that (1) if ùêπis computable
by a Œª calculus expression then it is computable by a Turing machine,
and (2) if ùêπis computable by a Turing machine, then it is computable
by an enhanced Œª calculus expression.
Showing (1) is fairly straightforward. Applying the simplification
rules to a Œª expression basically amounts to ‚Äúsearch and replace‚Äù
which we can implement easily in, say, NAND-RAM, or for that
matter Python (both of which are equivalent to Turing machines in
power). Showing (2) essentially amounts to simulating a Turing ma-
chine (or writing a NAND-TM interpreter) in a functional program-
ming language such as LISP or Scheme. We give the details below but
how this can be done is a good exercise in mastering some functional
programming techniques that are useful in their own right.
‚ãÜ
Proof of Theorem 8.16. We only sketch the proof. The ‚Äúif‚Äù direction
is simple. As mentioned above, evaluating Œª expressions basically
amounts to ‚Äúsearch and replace‚Äù. It is also a fairly straightforward
programming exercise to implement all the above basic operations in
an imperative language such as Python or C, and using the same ideas
we can do so in NAND-RAM as well, which we can then transform to
a NAND-TM program.
For the ‚Äúonly if‚Äù direction we need to simulate a Turing machine
using a Œª expression. We will do so by first showing that showing
for every Turing machine ùëÄa Œª expression to compute the next-step
function NEXTùëÄ‚à∂Œ£
‚àó‚ÜíŒ£
‚àóthat maps a configuration of ùëÄto the next
one (see Section 8.4.2).
A configuration of ùëÄis a string ùõº‚ààŒ£
‚àófor a finite set Œ£. We can
encode every symbol ùúé‚ààŒ£ by a finite string {0, 1}‚Ñì, and so we will
encode a configuration ùõºin the Œª calculus as a list ‚ü®ùõº0, ùõº1, ‚Ä¶ , ùõºùëö‚àí1, ‚ä•‚ü©
where ùõºùëñis an ‚Ñì-length string (i.e., an ‚Ñì-length list of 0‚Äôs and 1‚Äôs) en-
coding a symbol in Œ£.
By Lemma 8.9, for every ùõº‚ààŒ£
‚àó, NEXTùëÄ(ùõº)ùëñis equal to
ùëü(ùõºùëñ‚àí1, ùõºùëñ, ùõºùëñ+1) for some finite function ùëü‚à∂Œ£
3 ‚ÜíŒ£. Using our
encoding of Œ£ as {0, 1}‚Ñì, we can also think of ùëüas mapping {0, 1}3‚Ñìto
{0, 1}‚Ñì. By Solved Exercise 8.3, we can compute the NAND function,
and hence every finite function, including ùëü, using the Œª calculus.
Using this insight, we can compute NEXTùëÄusing the Œª calculus as
follows. Given a list ùêøencoding the configuration ùõº0 ‚ãØùõºùëö‚àí1, we
define the lists ùêøùëùùëüùëíùë£and ùêøùëõùëíùë•ùë°encoding the configuration ùõºshifted
by one step to the right and left respectively. The next configuration
ùõº‚Ä≤ is defined as ùõº‚Ä≤
ùëñ= ùëü(ùêøùëùùëüùëíùë£[ùëñ], ùêø[ùëñ], ùêøùëõùëíùë•ùë°[ùëñ]) where we let ùêø‚Ä≤[ùëñ] denote


--- Page 310 ---

310
introduction to theoretical computer science
the ùëñ-th element of ùêø‚Ä≤. This can be computed by recursion (and hence
using the enhanced Œª calculus‚Äô RECURSE operator) as follows:
Algorithm 8.17 ‚Äî ùëÅùê∏ùëãùëáùëÄusing the Œª calculus.
Input: List ùêø= ‚ü®ùõº0, ùõº1, ‚Ä¶ , ùõºùëö‚àí1, ‚ä•‚ü©encoding a configura-
tion ùõº.
Output: List ùêø‚Ä≤ encoding ùëÅùê∏ùëãùëáùëÄ(ùõº)
1: procedure ComputeNext(ùêøùëùùëüùëíùë£, ùêø, ùêøùëõùëíùë•ùë°)
2:
if thenùêºùëÜùê∏ùëÄùëÉùëáùëåùêøùëùùëüùëíùë£
3:
return ùëÅùêºùêø
4:
end if
5:
ùëé‚Üêùêªùê∏ùê¥ùê∑ùêøùëùùëüùëíùë£
6:
if thenùêºùëÜùê∏ùëÄùëÉùëáùëåùêø
7:
ùëè‚Üê‚àÖ
# Encoding of ‚àÖin {0, 1}‚Ñì
8:
else
9:
ùëè‚Üêùêªùê∏ùê¥ùê∑ùêø
10:
end if
11:
if thenùêºùëÜùê∏ùëÄùëÉùëáùëåùêøùëõùëíùë•ùë°
12:
ùëê‚Üê‚àÖ
13:
else
14:
ùëê‚Üêùêªùê∏ùê¥ùê∑ùêøùëõùëíùë•ùë°
15:
end if
16:
return ùëÉùê¥ùêºùëÖùëü(ùëé, ùëè, ùëê) ComputeNext(ùëáùê¥ùêºùêøùêøùëùùëüùëíùë£, ùëáùê¥ùêºùêøùêø, ùëáùê¥ùêºùêøùêøùëõùëíùë•ùë°)
17: end procedure
18: ùêøùëùùëüùëíùë£‚ÜêùëÉùê¥ùêºùëÖ‚àÖùêø
# ùêøùëùùëüùëíùë£= ‚ü®‚àÖ, ùõº0, ‚Ä¶ , ùõºùëö‚àí1, ‚ä•‚ü©
19: ùêøùëõùëíùë•ùë°‚Üêùëáùê¥ùêºùêøùêø
# ùêøùëõùëíùë•ùë°= ‚ü®ùõº1, ‚Ä¶ , ùõºùëö‚àí1, ‚ä•}
20: return ComputeNext(ùêøùëùùëüùëíùë£, ùêø, ùêøùëõùëíùë•ùë°)
Once we can compute NEXTùëÄ, we can simulate the execution of
ùëÄon input ùë•using the following recursion. Define FINAL(ùõº) to be
the final configuration of ùëÄwhen initialized at configuration ùõº. The
function FINAL can be defined recursively as follows:
FINAL(ùõº) =
‚éß
{
‚é®
{
‚é©
ùõº
ùõºis halting configuration
NEXTùëÄ(ùõº)
otherwise
.
(8.21)
Checking whether a configuration is halting (i.e., whether it is
one in which the transition function would output Halt) can be easily
implemented in the ùúÜcalculus, and hence we can use the RECURSE
to compute FINAL. If we let ùõº0 be the initial configuration of ùëÄon
input ùë•then we can obtain the output ùëÄ(ùë•) from FINAL(ùõº0), hence
completing the proof.
‚ñ†


--- Page 311 ---

equivalent models of computation
311
8.7 FROM ENHANCED TO PURE Œõ CALCULUS
While the collection of ‚Äúbasic‚Äù functions we allowed for the enhanced
Œª calculus is smaller than what‚Äôs provided by most Lisp dialects, com-
ing from NAND-TM it still seems a little ‚Äúbloated‚Äù. Can we make do
with less? In other words, can we find a subset of these basic opera-
tions that can implement the rest?
It turns out that there is in fact a proper subset of the operations of
the enhanced Œª calculus that can be used to implement the rest. That
subset is the empty set. That is, we can implement all the operations
above using the Œª formalism only, even without using 0‚Äôs and 1‚Äôs. It‚Äôs
Œª‚Äôs all the way down!
P
This is a good point to pause and think how
you would implement these operations your-
self. For example, start by thinking how you
could implement MAP using REDUCE, and
then REDUCE using RECURSE combined with
0, 1, IF, PAIR, HEAD, TAIL, NIL, ISEMPTY. You can
also PAIR, HEAD and TAIL based on 0, 1, IF. The most
challenging part is to implement RECURSE using only
the operations of the pure Œª calculus.
Theorem 8.18 ‚Äî Enhanced Œª calculus equivalent to pure Œª calculus.. There
are Œª expressions that implement the functions 0,1,IF,PAIR, HEAD,
TAIL, NIL, ISEMPTY, MAP, REDUCE, and RECURSE.
The idea behind Theorem 8.18 is that we encode 0 and 1 them-
selves as Œª expressions, and build things up from there. This is known
as Church encoding, as it was originated by Church in his effort to
show that the Œª calculus can be a basis for all computation. We will
not write the full formal proof of Theorem 8.18 but outline the ideas
involved in it:
‚Ä¢ We define 0 to be the function that on two inputs ùë•, ùë¶outputs ùë¶,
and 1 to be the function that on two inputs ùë•, ùë¶outputs ùë•. We use
Currying to achieve the effect of two-input functions and hence
0 = ùúÜùë•.ùúÜùë¶.ùë¶and 1 = ùúÜùë•.ùúÜùë¶.ùë•. (This representation scheme is the
common convention for representing false and true but there are
many other alternative representations for 0 and 1 that would have
worked just as well.)
‚Ä¢ The above implementation makes the IF function trivial:
IF(ùëêùëúùëõùëë, ùëé, ùëè) is simply ùëêùëúùëõùëëùëéùëèsince 0ùëéùëè= ùëèand 1ùëéùëè= ùëé. We
can write IF = ùúÜùë•.ùë•to achieve IF(ùëêùëúùëõùëë, ùëé, ùëè) = (((IFùëêùëúùëõùëë)ùëé)ùëè) =
ùëêùëúùëõùëëùëéùëè.


--- Page 312 ---

312
introduction to theoretical computer science
‚Ä¢ To encode a pair (ùë•, ùë¶) we will produce a function ùëìùë•,ùë¶that has ùë•
and ùë¶‚Äúin its belly‚Äù and satisfies ùëìùë•,ùë¶ùëî= ùëîùë•ùë¶for every function ùëî.
That is, PAIR = ùúÜùë•, ùë¶. (ùúÜùëî.ùëîùë•ùë¶). We can extract the first element of
a pair ùëùby writing ùëù1 and the second element by writing ùëù0, and so
HEAD = ùúÜùëù.ùëù1 and TAIL = ùúÜùëù.ùëù0.
‚Ä¢ We define NIL to be the function that ignores its input and always
outputs 1. That is, NIL = ùúÜùë•.1. The ISEMPTY function checks,
given an input ùëù, whether we get 1 if we apply ùëùto the function
ùëßùëíùëüùëú= ùúÜùë•, ùë¶.0 that ignores both its inputs and always outputs 0. For
every valid pair of the form ùëù= PAIRùë•ùë¶, ùëùùëßùëíùëüùëú= ùëùùë•ùë¶= 0 while
NILùëßùëíùëüùëú= 1. Formally, ISEMPTY = ùúÜùëù.ùëù(ùúÜùë•, ùë¶.0).
R
Remark 8.19 ‚Äî Church numerals (optional). There is
nothing special about Boolean values. You can use
similar tricks to implement natural numbers using
Œª terms. The standard way to do so is to represent
the number ùëõby the function ITERùëõthat on input a
function ùëìoutputs the function ùë•
‚Ü¶
ùëì(ùëì(‚ãØùëì(ùë•))) (ùëõ
times). That is, we represent the natural number 1 as
ùúÜùëì.ùëì, the number 2 as ùúÜùëì.(ùúÜùë•.ùëì(ùëìùë•)), the number 3 as
ùúÜùëì.(ùúÜùë•.ùëì(ùëì(ùëìùë•))), and so on and so forth. (Note that
this is not the same representation we used for 1 in
the Boolean context: this is fine; we already know that
the same object can be represented in more than one
way.) The number 0 is represented by the function
that maps any function ùëìto the identity function ùúÜùë•.ùë•.
(That is, 0 = ùúÜùëì.(ùúÜùë•.ùë•).)
In this representation, we can compute PLUS(ùëõ, ùëö)
as ùúÜùëì.ùúÜùë•.(ùëõùëì)((ùëöùëì)ùë•) and TIMES(ùëõ, ùëö) as ùúÜùëì.ùëõ(ùëöùëì).
Subtraction and division are trickier, but can be
achieved using recursion. (Working this out is a great
exercise.)
8.7.1 List processing
Now we come to a bigger hurdle, which is how to implement
MAP, FILTER, REDUCE and RECURSE in the pure Œª calculus. It
turns out that we can build MAP and FILTER from REDUCE, and
REDUCE from RECURSE. For example MAP(ùêø, ùëì) is the same as
REDUCE(ùêø, ùëî) where ùëîis the operation that on input ùë•and ùë¶, outputs
PAIR(ùëì(ùë•), NIL) if ùë¶is NIL and otherwise outputs PAIR(ùëì(ùë•), ùë¶). (I
leave checking this as a (recommended!) exercise for you, the reader.)
We can define REDUCE(ùêø, ùëî) recursively, by setting
REDUCE(NIL, ùëî) = NIL and stipulating that given a non-
empty list ùêø, which we can think of as a pair (‚Ñéùëíùëéùëë, ùëüùëíùë†ùë°),


--- Page 313 ---

equivalent models of computation
313
REDUCE(ùêø, ùëî) = ùëî(‚Ñéùëíùëéùëë, REDUCE(ùëüùëíùë†ùë°, ùëî))). Thus, we might
try to write a recursive Œª expression for REDUCE as follows
REDUCE = ùúÜùêø, ùëî.IF(ISEMPTY(ùêø), NIL, ùëîHEAD(ùêø)REDUCE(TAIL(ùêø), ùëî)) .
(8.22)
The only fly in this ointment is that the Œª calculus does not have the
notion of recursion, and so this is an invalid definition. But of course
we can use our RECURSE operator to solve this problem. We will
replace the recursive call to ‚ÄúREDUCE‚Äù with a call to a function ùëöùëí
that is given as an extra argument, and then apply RECURSE to this.
Thus REDUCE = RECURSE ùëöùë¶ùëÖùê∏ùê∑ùëàùê∂ùê∏where
ùëöùë¶ùëÖùê∏ùê∑ùëàùê∂ùê∏= ùúÜùëöùëí, ùêø, ùëî.IF(ISEMPTY(ùêø), NIL, ùëîHEAD(ùêø)ùëöùëí(TAIL(ùêø), ùëî)) .
(8.23)
8.7.2 The Y combinator, or recursion without recursion
Eq. (8.23) means that implementing MAP, FILTER, and REDUCE boils
down to implementing the RECURSE operator in the pure Œª calculus.
This is what we do now.
How can we implement recursion without recursion? We will
illustrate this using a simple example - the XOR function. As shown in
Solved Exercise 8.4, we can write the XOR function of a list recursively
as follows:
XOR(ùêø) =
‚éß
{
‚é®
{
‚é©
0
ùêøis empty
XOR2(HEAD(ùêø), XOR(TAIL(ùêø)))
otherwise
(8.24)
where XOR2 ‚à∂{0, 1}2 ‚Üí{0, 1} is the XOR on two bits. In Python we
would write this as
def xor2(a,b): return 1-b if a else b
def head(L): return L[0]
def tail(L): return L[1:]
def xor(L): return xor2(head(L),xor(tail(L))) if L else 0
print(xor([0,1,1,0,0,1]))
# 1
Now, how could we eliminate this recursive call? The main idea is
that since functions can take other functions as input, it is perfectly
legal in Python (and the Œª calculus of course) to give a function itself
as input. So, our idea is to try to come up with a non recursive function
tempxor that takes two inputs: a function and a list, and such that
tempxor(tempxor,L) will output the XOR of L!


--- Page 314 ---

314
introduction to theoretical computer science
P
At this point you might want to stop and try to im-
plement this on your own in Python or any other
programming language of your choice (as long as it
allows functions as inputs).
Our first attempt might be to simply use the idea of replacing the
recursive call by me. Let‚Äôs define this function as myxor
def myxor(me,L): return xor2(head(L),me(tail(L))) if L
else 0
‚Ü™
Let‚Äôs test this out:
myxor(myxor,[1,0,1])
If you do this, you will get the following complaint from the inter-
preter:
TypeError: myxor() missing 1 required positional argu-
ment
The problem is that myxor expects two inputs- a function and a
list- while in the call to me we only provided a list. To correct this, we
modify the call to also provide the function itself:
def tempxor(me,L): return xor2(head(L),me(me,tail(L))) if
L else 0
‚Ü™
Note the call me(me,..) in the definition of tempxor: given a func-
tion me as input, tempxor will actually call the function me with itself
as the first input. If we test this out now, we see that we actually get
the right result!
tempxor(tempxor,[1,0,1])
# 0
tempxor(tempxor,[1,0,1,1])
# 1
and so we can define xor(L) as simply return tem-
pxor(tempxor,L).
The approach above is not specific to XOR. Given a recursive func-
tion f that takes an input x, we can obtain a non recursive version as
follows:
1. Create the function myf that takes a pair of inputs me and x, and
replaces recursive calls to f with calls to me.
2. Create the function tempf that converts calls in myf of the form
me(x) to calls of the form me(me,x).


--- Page 315 ---

equivalent models of computation
315
3 Because of specific issues of Python syntax, in this
implementation we use f * g for applying f to g
rather than fg, and use Œªx(exp) rather than Œªx.exp
for abstraction. We also use _0 and _1 for the Œª terms
for 0 and 1 so as not to confuse with the Python
constants.
3. The function f(x) will be defined as tempf(tempf,x)
Here is the way we implement the RECURSE operator in Python. It
will take a function myf as above, and replace it with a function g such
that g(x)=myf(g,x) for every x.
def RECURSE(myf):
def tempf(me,x): return myf(lambda y: me(me,y),x)
return lambda x: tempf(tempf,x)
xor = RECURSE(myxor)
print(xor([0,1,1,0,0,1]))
# 1
print(xor([1,1,0,0,1,1,1,1]))
# 0
From Python to the ‡∏Äcalculus.
In the Œª calculus, a two input function
ùëîthat takes a pair of inputs ùëöùëí, ùë¶is written as ùúÜùëöùëí.(ùúÜùë¶.ùëî). So the
function ùë¶‚Ü¶ùëöùëí(ùëöùëí, ùë¶) is simply written as ùëöùëíùëöùëíand similarly
the function ùë•‚Ü¶ùë°ùëíùëöùëùùëì(ùë°ùëíùëöùëùùëì, ùë•) is simply ùë°ùëíùëöùëùùëìùë°ùëíùëöùëùùëì. (Can
you see why?) Therefore the function tempf defined above can be
written as Œª me. myf(me me). This means that if we denote the input
of RECURSE by ùëì, then RECURSE ùëöùë¶ùëì= ùë°ùëíùëöùëùùëìùë°ùëíùëöùëùùëìwhere ùë°ùëíùëöùëùùëì=
ùúÜùëö.ùëì(ùëöùëö) or in other words
RECURSE = ùúÜùëì.((ùúÜùëö.ùëì(ùëöùëö)) (ùúÜùëö.ùëì(ùëöùëö)))
(8.25)
The online appendix contains an implementation of the Œª calcu-
lus using Python. Here is an implementation of the recursive XOR
function from that appendix:3
# XOR of two bits
XOR2 = Œª(a,b)(IF(a,IF(b,_0,_1),b))
# Recursive XOR with recursive calls replaced by m
parameter
‚Ü™
myXOR = Œª(m,l)(IF(ISEMPTY(l),_0,XOR2(HEAD(l),m(TAIL(l)))))
# Recurse operator (aka Y combinator)
RECURSE = Œªf((Œªm(f(m*m)))(Œªm(f(m*m))))
# XOR function


--- Page 316 ---

316
introduction to theoretical computer science
XOR = RECURSE(myXOR)
#TESTING:
XOR(PAIR(_1,NIL)) # List [1]
# equals 1
XOR(PAIR(_1,PAIR(_0,PAIR(_1,NIL)))) # List [1,0,1]
# equals 0
R
Remark 8.20 ‚Äî The Y combinator. The RECURSE opera-
tor above is better known as the Y combinator.
It is one of a family of a fixed point operators that given
a lambda expression ùêπ, find a fixed point ùëìof ùêπsuch
that ùëì
=
ùêπùëì. If you think about it, XOR is the fixed
point of ùëöùë¶ùëãùëÇùëÖabove. XOR is the function such
that for every ùë•, if plug in XOR as the first argument
of ùëöùë¶ùëãùëÇùëÖthen we get back XOR, or in other words
XOR = ùëöùë¶ùëãùëÇùëÖXOR. Hence finding a fixed point for
ùëöùë¶ùëãùëÇùëÖis the same as applying RECURSE to it.
8.8 THE CHURCH-TURING THESIS (DISCUSSION)
‚Äú[In 1934], Church had been speculating, and finally definitely proposed, that
the Œª-definable functions are all the effectively calculable functions ‚Ä¶. When
Church proposed this thesis, I sat down to disprove it ‚Ä¶ but, quickly realizing
that [my approach failed], I became overnight a supporter of the thesis.‚Äù,
Stephen Kleene, 1979.
‚Äú[The thesis is] not so much a definition or to an axiom but ‚Ä¶ a natural law.‚Äù,
Emil Post, 1936.
We have defined functions to be computable if they can be computed
by a NAND-TM program, and we‚Äôve seen that the definition would
remain the same if we replaced NAND-TM programs by Python pro-
grams, Turing machines, Œª calculus, cellular automata, and many
other computational models. The Church-Turing thesis is that this is
the only sensible definition of ‚Äúcomputable‚Äù functions. Unlike the
‚ÄúPhysical Extended Church-Turing Thesis‚Äù (PECTT) which we saw
before, the Church-Turing thesis does not make a concrete physical
prediction that can be experimentally tested, but it certainly motivates
predictions such as the PECTT. One can think of the Church-Turing
Thesis as either advocating a definitional choice, making some pre-
diction about all potential computing devices, or suggesting some
laws of nature that constrain the natural world. In Scott Aaronson‚Äôs


--- Page 317 ---

equivalent models of computation
317
words, ‚Äúwhatever it is, the Church-Turing thesis can only be regarded
as extremely successful‚Äù. No candidate computing device (including
quantum computers, and also much less reasonable models such as
the hypothetical ‚Äúclosed time curve‚Äù computers we mentioned before)
has so far mounted a serious challenge to the Church-Turing thesis.
These devices might potentially make some computations more effi-
cient, but they do not change the difference between what is finitely
computable and what is not. (The extended Church-Turing thesis,
which we discuss in Section 13.3, stipulates that Turing machines cap-
ture also the limit of what can be efficiently computable. Just like its
physical version, quantum computing presents the main challenge to
this thesis.)
8.8.1 Different models of computation
We can summarize the models we have seen in the following table:
Table 8.1: Different models for computing finite functions and
functions with arbitrary input length.
Computational
problems
Type of model
Examples
Finite functions
ùëì‚à∂{0, 1}ùëõ‚Üí{0, 1}ùëö
Non uniform
computation
(algorithm
depends on input
length)
Boolean circuits,
NAND circuits,
straight-line programs
(e.g., NAND-CIRC)
Functions with
unbounded inputs
ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}‚àó
Sequential access
to memory
Turing machines,
NAND-TM programs
‚Äì
Indexed access /
RAM
RAM machines,
NAND-RAM, modern
programming
languages
‚Äì
Other
Lambda calculus,
cellular automata
Later on in Chapter 17 we will study memory bounded computa-
tion. It turns out that NAND-TM programs with a constant amount
of memory are equivalent to the model of finite automata (the adjec-
tives ‚Äúdeterministic‚Äù or ‚Äúnondeterministic‚Äù are sometimes added as
well, this model is also known as finite state machines) which in turn
captures the notion of regular languages (those that can be described by
regular expressions), which is a concept we will see in Chapter 10.


--- Page 318 ---

318
introduction to theoretical computer science
‚úì
Chapter Recap
‚Ä¢ While we defined computable functions using
Turing machines, we could just as well have done
so using many other models, including not just
NAND-TM programs but also RAM machines,
NAND-RAM, the Œª-calculus, cellular automata and
many other models.
‚Ä¢ Very simple models turn out to be ‚ÄúTuring com-
plete‚Äù in the sense that they can simulate arbitrarily
complex computation.
8.9 EXERCISES
Exercise 8.1 ‚Äî Alternative proof for TM/RAM equivalence. Let SEARCH ‚à∂
{0, 1}‚àó‚Üí{0, 1}‚àóbe the following function. The input is a pair
(ùêø, ùëò) where ùëò‚àà{0, 1}‚àó, ùêøis an encoding of a list of key value pairs
(ùëò0, ùë£1), ‚Ä¶ , (ùëòùëö‚àí1, ùë£ùëö‚àí1) where ùëò0, ‚Ä¶ , ùëòùëö‚àí1, ùë£0, ‚Ä¶ , ùë£ùëö‚àí1 are binary
strings. The output is ùë£ùëñfor the smallest ùëñsuch that ùëòùëñ= ùëò, if such ùëñ
exists, and otherwise the empty string.
1. Prove that SEARCH is computable by a Turing machine.
2. Let UPDATE(ùêø, ùëò, ùë£) be the function whose input is a list ùêøof pairs,
and whose output is the list ùêø‚Ä≤ obtained by prepending the pair
(ùëò, ùë£) to the beginning of ùêø. Prove that UPDATE is computable by a
Turing machine.
3. Suppose we encode the configuration of a NAND-RAM program
by a list ùêøof key/value pairs where the key is either the name of
a scalar variable foo or of the form Bar[<num>] for some num-
ber <num> and it contains all the nonzero values of variables. Let
NEXT(ùêø) be the function that maps a configuration of a NAND-
RAM program at one step to the configuration in the next step.
Prove that NEXT is computable by a Turing machine (you don‚Äôt
have to implement each one of the arithmetic operations: it is
enough to implement addition and multiplication).
4. Prove that for every ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}‚àóthat is computable by a
NAND-RAM program, ùêπis computable by a Turing machine.
‚ñ†
Exercise 8.2 ‚Äî NAND-TM lookup. This exercise shows part of the proof that
NAND-TM can simulate NAND-RAM. Produce the code of a NAND-
TM program that computes the function LOOKUP ‚à∂{0, 1}‚àó‚Üí{0, 1}
that is defined as follows. On input ùëùùëì(ùëñ)ùë•, where ùëùùëì(ùëñ) denotes a
prefix-free encoding of an integer ùëñ, LOOKUP(ùëùùëì(ùëñ)ùë•) = ùë•ùëñif ùëñ< |ùë•|


--- Page 319 ---

equivalent models of computation
319
4 You don‚Äôt have to give a full description of a Turing
machine: use our ‚Äúhave the cake and eat it too‚Äù
paradigm to show the existence of such a machine by
arguing from more powerful equivalent models.
5 Same hint as Exercise 8.5 applies. Note that for
showing that LONGPATH is computable you don‚Äôt
have to give an efficient algorithm.
and LOOKUP(ùëùùëì(ùëñ)ùë•) = 0 otherwise. (We don‚Äôt care what LOOKUP
outputs on inputs that are not of this form.) You can choose any
prefix-free encoding of your choice, and also can use your favorite
programming language to produce this code.
‚ñ†
Exercise 8.3 ‚Äî Pairing. Let ùëíùëöùëèùëíùëë‚à∂‚Ñï2 ‚Üí‚Ñïbe the function defined as
ùëíùëöùëèùëíùëë(ùë•0, ùë•1) = 1
2(ùë•0 + ùë•1)(ùë•0 + ùë•1 + 1) + ùë•1.
1. Prove that for every ùë•0, ùë•1 ‚àà‚Ñï, ùëíùëöùëèùëíùëë(ùë•0, ùë•1) is indeed a natural
number.
2. Prove that ùëíùëöùëèùëíùëëis one-to-one
3. Construct a NAND-TM program ùëÉsuch that for every ùë•0, ùë•1 ‚àà‚Ñï,
ùëÉ(ùëùùëì(ùë•0)ùëùùëì(ùë•1)) = ùëùùëì(ùëíùëöùëèùëíùëë(ùë•0, ùë•1)), where ùëùùëìis the prefix-free
encoding map defined above. You can use the syntactic sugar for
inner loops, conditionals, and incrementing/decrementing the
counter.
4. Construct NAND-TM programs ùëÉ0, ùëÉ1 such that for for every
ùë•0, ùë•1 ‚àà‚Ñïand ùëñ‚ààùëÅ, ùëÉùëñ(ùëùùëì(ùëíùëöùëèùëíùëë(ùë•0, ùë•1))) = ùëùùëì(ùë•ùëñ). You can
use the syntactic sugar for inner loops, conditionals, and increment-
ing/decrementing the counter.
‚ñ†
Exercise 8.4 ‚Äî Shortest Path. Let SHORTPATH ‚à∂{0, 1}‚àó‚Üí{0, 1}‚àó
be the function that on input a string encoding a triple (ùê∫, ùë¢, ùë£) out-
puts a string encoding ‚àûif ùë¢and ùë£are disconnected in ùê∫or a string
encoding the length ùëòof the shortest path from ùë¢to ùë£. Prove that
SHORTPATH is computable by a Turing machine. See footnote for
hint.4
‚ñ†
Exercise 8.5 ‚Äî Longest Path. Let LONGPATH ‚à∂{0, 1}‚àó‚Üí{0, 1}‚àóbe
the function that on input a string encoding a triple (ùê∫, ùë¢, ùë£) outputs
a string encoding ‚àûif ùë¢and ùë£are disconnected in ùê∫or a string en-
coding the length ùëòof the longest simple path from ùë¢to ùë£. Prove that
LONGPATH is computable by a Turing machine. See footnote for
hint.5
‚ñ†
Exercise 8.6 ‚Äî Shortest path Œª expression. Let SHORTPATH be as in
Exercise 8.4. Prove that there exists a ùúÜexpression that computes
SHORTPATH. You can use Exercise 8.4
‚ñ†
Exercise 8.7 ‚Äî Next-step function is local. Prove Lemma 8.9 and use it to
complete the proof of Theorem 8.7.


--- Page 320 ---

320
introduction to theoretical computer science
6 Hint: You can reduce the number of variables a
function takes by ‚Äúpairing them up‚Äù. That is, define a
Œª expression PAIR such that for every ùë•, ùë¶PAIRùë•ùë¶is
some function ùëìsuch that ùëì0 = ùë•and ùëì1 = ùë¶. Then
use PAIR to iteratively reduce the number of variables
used.
7 Use structural induction on the expression ùëí.
8 The name ùëßùëñùëùis a common name for this operation,
for example in Python. It should not be confused with
the zip compression file format.
9 Use MAP and REDUCE (and potentially FILTER).
You might also find the function ùëßùëñùëùof Exercise 8.10
useful.
10 Try to set up a procedure such that if array Left
contains an encoding of a Œª expression ùúÜùë•.ùëíand
array Right contains an encoding of another Œª expres-
sion ùëí‚Ä≤, then the array Result will contain ùëí[ùë•‚Üíùëí‚Ä≤].
‚ñ†
Exercise 8.8 ‚Äî Œª calculus requires at most three variables. Prove that for ev-
ery Œª-expression ùëíwith no free variables there is an equivalent Œª-
expression ùëìthat only uses the variables ùë•,ùë¶, and ùëß.6
‚ñ†
Exercise 8.9 ‚Äî Evaluation order example in Œª calculus. 1. Let ùëí=
ùúÜùë•.7 ((ùúÜùë•.ùë•ùë•)(ùúÜùë•.ùë•ùë•)). Prove that the simplification process of ùëí
ends in a definite number if we use the ‚Äúcall by name‚Äù evaluation
order while it never ends if we use the ‚Äúcall by value‚Äù order.
2. (bonus, challenging) Let ùëíbe any Œª expression. Prove that if the
simplification process ends in a definite number if we use the ‚Äúcall
by value‚Äù order then it also ends in such a number if we use the
‚Äúcall by name‚Äù order. See footnote for hint.7
‚ñ†
Exercise 8.10 ‚Äî Zip function. Give an enhanced Œª calculus expression to
compute the function ùëßùëñùëùthat on input a pair of lists ùêºand ùêøof the
same length ùëõ, outputs a list of ùëõpairs ùëÄsuch that the ùëó-th element
of ùëÄ(which we denote by ùëÄùëó) is the pair (ùêºùëó, ùêøùëó). Thus ùëßùëñùëù‚Äúzips
together‚Äù these two lists of elements into a single list of pairs.8
‚ñ†
Exercise 8.11 ‚Äî Next-step function without ùëÖùê∏ùê∂ùëàùëÖùëÜùê∏. Let ùëÄbe a Turing
machine. Give an enhanced Œª calculus expression to compute the
next-step function NEXTùëÄof ùëÄ(as in the proof of Theorem 8.16)
without using RECURSE. See footnote for hint.9
‚ñ†
Exercise 8.12 ‚Äî Œª calculus to NAND-TM compiler (challenging). Give a program
in the programming language of your choice that takes as input a Œª
expression ùëíand outputs a NAND-TM program ùëÉthat computes the
same function as ùëí. For partial credit you can use the GOTO and all
NAND-CIRC syntactic sugar in your output program. You can use
any encoding of Œª expressions as binary string that is convenient for
you. See footnote for hint.10
‚ñ†
Exercise 8.13 ‚Äî At least two in ùúÜcalculus. Let 1 = ùúÜùë•, ùë¶.ùë•and 0 = ùúÜùë•, ùë¶.ùë¶as
before. Define
ALT = ùúÜùëé, ùëè, ùëê.(ùëé(ùëè1(ùëê10))(ùëèùëê0))
(8.26)
Prove that ALT is a ùúÜexpression that computes the at least two func-
tion. That is, for every ùëé, ùëè, ùëê‚àà{0, 1} (as encoded above) ALTùëéùëèùëê= 1
if and only at least two of {ùëé, ùëè, ùëê} are equal to 1.
‚ñ†


--- Page 321 ---

equivalent models of computation
321
Exercise 8.14 ‚Äî Locality of next-step function. This question will help you
get a better sense of the notion of locality of the next step function of Tur-
ing Machines. This locality plays an important role in results such as
the Turing completeness of ùúÜcalculus and one dimensional cellular
automata, as well as results such as Godel‚Äôs Incompleteness Theorem
and the Cook Levin theorem that we will see later in this course. De-
fine STRINGS to be the a programming language that has the following
semantics:
‚Ä¢ A STRINGS program ùëÑhas a single string variable str that is both
the input and the output of ùëÑ. The program has no loops and no
other variables, but rather consists of a sequence of conditional
search and replace operations that modify str.
‚Ä¢ The operations of a STRINGS program are:
‚Äì REPLACE(pattern1,pattern2) where pattern1 and pattern2
are fixed strings. This replaces the first occurrence of pattern1
in str with pattern2
‚Äì if search(pattern) { code } executes code if pattern is a
substring of str. The code code can itself include nested if‚Äôs.
(One can also add an else { ... } to execute if pattern is not
a substring of condf).
‚Äì the returned value is str
‚Ä¢ A STRING program ùëÑcomputes a function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}‚àóif
for every ùë•‚àà{0, 1}‚àó, if we initialize str to ùë•and then execute the
sequence of instructions in ùëÑ, then at the end of the execution str
equals ùêπ(ùë•).
For example, the following is a STRINGS program that computes
the function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}‚àósuch that for every ùë•‚àà{0, 1}‚àó, if ùë•
contains a substring of the form ùë¶= 11ùëéùëè11 where ùëé, ùëè‚àà{0, 1}, then
ùêπ(ùë•) = ùë•‚Ä≤ where ùë•‚Ä≤ is obtained by replacing the first occurrence of ùë¶in
ùë•with 00.
if search('110011') {
replace('110011','00')
} else if search('110111') {
replace('110111','00')
} else if search('111011') {
replace('111011','00')
} else if search('111111') {
replace('1111111','00')
}


--- Page 322 ---

322
introduction to theoretical computer science
Prove that for every Turing Machine program ùëÄ, there exists a
STRINGS program ùëÑthat computes the NEXTùëÄfunction that maps
every string encoding a valid configuration of ùëÄto the string encoding
the configuration of the next step of ùëÄ‚Äôs computation. (We don‚Äôt
care what the function will do on strings that do not encode a valid
configuration.) You don‚Äôt have to write the STRINGS program fully,
but you do need to give a convincing argument that such a program
exists.
‚ñ†
8.10 BIBLIOGRAPHICAL NOTES
Chapters 7 in the wonderful book of Moore and Mertens [MM11]
contains a great exposition much of this material. .
The RAM model can be very useful in studying the concrete com-
plexity of practical algorithms. Its theoretical study was initiated in
[CR73]. However, the exact set of operations that are allowed in the
RAM model and their costs vary between texts and contexts. One
needs to be careful in making such definitions, especially if the word
size grows, as was already shown by Shamir [Sha79]. Chapter 3 in
Savage‚Äôs book [Sav98] contains a more formal description of RAM
machines, see also the paper [Hag98]. A study of RAM algorithms
that are independent of the input size (known as the ‚Äútransdichoto-
mous RAM model‚Äù) was initiated by [FW93]
The models of computation we considered so far are inherently
sequential, but these days much computation happens in parallel,
whether using multi-core processors or in massively parallel dis-
tributed computation in data centers or over the Internet. Parallel
computing is important in practice, but it does not really make much
difference for the question of what can and can‚Äôt be computed. After
all, if a computation can be performed using ùëömachines in ùë°time,
then it can be computed by a single machine in time ùëöùë°.
The Œª-calculus was described by Church in [Chu41]. Pierce‚Äôs book
[Pie02] is a canonical textbook, see also [Bar84]. The ‚ÄúCurrying tech-
nique‚Äù is named after the logician Haskell Curry (the Haskell pro-
gramming language is named after Haskell Curry as well). Curry
himself attributed this concept to Moses Sch√∂nfinkel, though for some
reason the term ‚ÄúSch√∂nfinkeling‚Äù never caught on.
Unlike most programming languages, the pure Œª-calculus doesn‚Äôt
have the notion of types. Every object in the Œª calculus can also be
thought of as a Œª expression and hence as a function that takes one
input and returns one output. All functions take one input and re-
turn one output, and if you feed a function an input of a form it didn‚Äôt
expect, it still evaluates the Œª expression via ‚Äúsearch and replace‚Äù,
replacing all instances of its parameter with copies of the input expres-


--- Page 323 ---

equivalent models of computation
323
sion you fed it. Typed variants of the Œª calculus are objects of intense
research, and are strongly related to type systems for programming
language and computer-verifiable proof systems, see [Pie02]. Some of
the typed variants of the Œª calculus do not have infinite loops, which
makes them very useful as ways of enabling static analysis of pro-
grams as well as computer-verifiable proofs. We will come back to this
point in Chapter 10 and Chapter 22.
Tao has proposed showing the Turing completeness of fluid dy-
namics (a ‚Äúwater computer‚Äù) as a way of settling the question of the
behavior of the Navier-Stokes equations, see this popular article.


--- Page 324 ---



--- Page 325 ---

9
Universality and uncomputability
‚ÄúA function of a variable quantity is an analytic expression composed in any
way whatsoever of the variable quantity and numbers or constant quantities.‚Äù,
Leonhard Euler, 1748.
‚ÄúThe importance of the universal machine is clear. We do not need to have an
infinity of different machines doing different jobs. ‚Ä¶ The engineering problem
of producing various machines for various jobs is replaced by the office work of
‚Äòprogramming‚Äô the universal machine‚Äù, Alan Turing, 1948
One of the most significant results we showed for Boolean circuits
(or equivalently, straight-line programs) is the notion of universality:
there is a single circuit that can evaluate all other circuits. However,
this result came with a significant caveat. To evaluate a circuit of ùë†
gates, the universal circuit needed to use a number of gates larger
than ùë†. It turns out that uniform models such as Turing machines or
NAND-TM programs allow us to ‚Äúbreak out of this cycle‚Äù and obtain
a truly universal Turing machine ùëàthat can evaluate all other machines,
including machines that are more complex (e.g., more states) than ùëà
itself. (Similarly, there is a Universal NAND-TM program ùëà‚Ä≤ that can
evaluate all NAND-TM programs, including programs that have more
lines than ùëà‚Ä≤.)
It is no exaggeration to say that the existence of such a universal
program/machine underlies the information technology revolution
that began in the latter half of the 20th century (and is still ongoing).
Up to that point in history, people have produced various special-
purpose calculating devices such as the abacus, the slide ruler, and
machines that compute various trigonometric series. But as Turing
(who was perhaps the one to see most clearly the ramifications of
universality) observed, a general purpose computer is much more pow-
erful. Once we build a device that can compute the single universal
function, we have the ability, via software, to extend it to do arbitrary
computations. For example, if we want to simulate a new Turing ma-
chine ùëÄ, we do not need to build a new physical machine, but rather
Compiled on 8.26.2020 18:10
Learning Objectives:
‚Ä¢ The universal machine/program - ‚Äúone
program to rule them all‚Äù
‚Ä¢ A fundamental result in computer science and
mathematics: the existence of uncomputable
functions.
‚Ä¢ The halting problem: the canonical example of
an uncomputable function.
‚Ä¢ Introduction to the technique of reductions.
‚Ä¢ Rice‚Äôs Theorem: A ‚Äúmeta tool‚Äù for
uncomputability results, and a starting point
for much of the research on compilers,
programming languages, and software
verification.


--- Page 326 ---

326
introduction to theoretical computer science
can represent ùëÄas a string (i.e., using code) and then input ùëÄto the
universal machine ùëà.
Beyond the practical applications, the existence of a universal algo-
rithm also has surprising theoretical ramifications, and in particular
can be used to show the existence of uncomputable functions, upend-
ing the intuitions of mathematicians over the centuries from Euler
to Hilbert. In this chapter we will prove the existence of the univer-
sal program, and also show its implications for uncomputability, see
Fig. 9.1
This chapter: A non-mathy overview
In this chapter we will see two of the most important results
in Computer Science:
1. The existence of a universal Turing machine: a single algo-
rithm that can evaluate all other algorithms,
2. The existence of uncomputable functions: functions (includ-
ing the famous ‚ÄúHalting problem‚Äù) that cannot be computed
by any algorithm.
Along the way, we develop the technique of reductions as a
way to show hardness of computing a function. A reduction
gives a way to compute a certain function using ‚Äúwishful
thinking‚Äù and assuming that another function can be com-
puted. Reductions are of course widely used in program-
ming - we often obtain an algorithm for one task by using
another task as a ‚Äúblack box‚Äù subroutine. However we will
use it in the ‚Äúcontra positive‚Äù: rather than using a reduction
to show that the former task is ‚Äúeasy‚Äù, we use them to show
that the latter task is ‚Äúhard‚Äù. Don‚Äôt worry if you find this
confusing - reductions are initially confusing - but they can be
mastered with time and practice.
9.1 UNIVERSALITY OR A META-CIRCULAR EVALUATOR
We start by proving the existence of a universal Turing machine. This is
a single Turing machine ùëàthat can evaluate arbitrary Turing machines
ùëÄon arbitrary inputs ùë•, including machines ùëÄthat can have more
states and larger alphabet than ùëàitself. In particular, ùëàcan even be
used to evaluate itself! This notion of self reference will appear time and
again in this book, and as we will see, leads to several counter-intuitive
phenomena in computing.


--- Page 327 ---

universality and uncomputability
327
Figure 9.1: In this chapter we will show the existence
of a universal Turing machine and then use this to de-
rive first the existence of some uncomputable function.
We then use this to derive the uncomputability of
Turing‚Äôs famous ‚Äúhalting problem‚Äù (i.e., the HALT
function), from which we a host of other uncom-
putability results follow. We also introduce reductions,
which allow us to use the uncomputability of a
function ùêπto derive the uncomputability of a new
function ùê∫.
Figure 9.2: A Universal Turing Machine is a single
Turing Machine ùëàthat can evaluate, given input the
(description as a string of) arbitrary Turing machine
ùëÄand input ùë•, the output of ùëÄon ùë•. In contrast to
the universal circuit depicted in Fig. 5.6, the machine
ùëÄcan be much more complex (e.g., more states or
tape alphabet symbols) than ùëà.
Theorem 9.1 ‚Äî Universal Turing Machine. There exists a Turing machine
ùëàsuch that on every string ùëÄwhich represents a Turing machine,
and ùë•‚àà{0, 1}‚àó, ùëà(ùëÄ, ùë•) = ùëÄ(ùë•).
That is, if the machine ùëÄhalts on ùë•and outputs some ùë¶
‚àà
{0, 1}‚àóthen ùëà(ùëÄ, ùë•)
=
ùë¶, and if ùëÄdoes not halt on ùë•(i.e.,
ùëÄ(ùë•) = ‚ä•) then ùëà(ùëÄ, ùë•) = ‚ä•.
ÔÉ´Big Idea 11 There is a ‚Äúuniversal‚Äù algorithm that can evaluate
arbitrary algorithms on arbitrary inputs.
Proof Idea:
Once you understand what the theorem says, it is not that hard to
prove. The desired program ùëàis an interpreter for Turing machines.
That is, ùëàgets a representation of the machine ùëÄ(think of it as source
code), and some input ùë•, and needs to simulate the execution of ùëÄon
ùë•.
Think of how you would code ùëàin your favorite programming
language. First, you would need to decide on some representation
scheme for ùëÄ. For example, you can use an array or a dictionary
to encode ùëÄ‚Äôs transition function. Then you would use some data
structure, such as a list, to store the contents of ùëÄ‚Äôs tape. Now you can
simulate ùëÄstep by step, updating the data structure as you go along.
The interpreter will continue the simulation until the machine halts.
Once you do that, translating this interpreter from your favorite
programming language to a Turing machine can be done just as we
have seen in Chapter 8. The end result is what‚Äôs known as a ‚Äúmeta-


--- Page 328 ---

328
introduction to theoretical computer science
circular evaluator‚Äù: an interpreter for a programming language in the
same one. This is a concept that has a long history in computer science
starting from the original universal Turing machine. See also Fig. 9.3.
‚ãÜ
9.1.1 Proving the existence of a universal Turing Machine
To prove (and even properly state) Theorem 9.1, we need to fix some
representation for Turing machines as strings. One potential choice
for such a representation is to use the equivalence between Turing
machines and NAND-TM programs and hence represent a Turing
machine ùëÄusing the ASCII encoding of the source code of the corre-
sponding NAND-TM program ùëÉ. However, we will use a more direct
encoding.
Definition 9.2 ‚Äî String representation of Turing Machine. Let ùëÄbe a Turing
machine with ùëòstates and a size ‚Ñìalphabet Œ£ = {ùúé0, ‚Ä¶ , ùúé‚Ñì‚àí1} (we
use the convention ùúé0 = 0,ùúé1 = 1, ùúé2 = ‚àÖ, ùúé3 = ‚ñ∑). We represent
ùëÄas the triple (ùëò, ‚Ñì, ùëá) where ùëáis the table of values for ùõøùëÄ:
ùëá= (ùõøùëÄ(0, ùúé0), ùõøùëÄ(0, ùúé1), ‚Ä¶ , ùõøùëÄ(ùëò‚àí1, ùúé‚Ñì‚àí1)) ,
(9.1)
where each value ùõøùëÄ(ùë†, ùúé) is a triple (ùë†‚Ä≤, ùúé‚Ä≤, ùëë) with ùë†‚Ä≤
‚àà
[ùëò],
ùúé‚Ä≤ ‚ààŒ£ and ùëëa number {0, 1, 2, 3} encoding one of {L, R, S, H}. Thus
such a machine ùëÄis encoded by a list of 2 + 3ùëò‚ãÖ‚Ñìnatural num-
bers. The string representation of ùëÄis obtained by concatenating
prefix free representation of all these integers. If a string ùõº‚àà{0, 1}‚àó
does not represent a list of integers in the form above, then we treat
it as representing the trivial Turing machine with one state that
immediately halts on every input.
R
Remark 9.3 ‚Äî Take away points of representation. The
details of the representation scheme of Turing ma-
chines as strings are immaterial for almost all applica-
tions. What you need to remember are the following
points:
1. We can represent every Turing machine as a string.
2. Given the string representation of a Turing ma-
chine ùëÄand an input ùë•, we can simulate ùëÄ‚Äôs
execution on the input ùë•. (This is the content of
Theorem 9.1.)
An additional minor issue is that for convenience we
make the assumption that every string represents some
Turing machine. This is very easy to ensure by just
mapping strings that would otherwise not represent a


--- Page 329 ---

universality and uncomputability
329
Turing machine into some fixed trivial machine. This
assumption is not very important, but does make a
few results (such as Rice‚Äôs Theorem: Theorem 9.15) a
little less cumbersome to state.
Using this representation, we can formally prove Theorem 9.1.
Proof of Theorem 9.1. We will only sketch the proof, giving the major
ideas. First, we observe that we can easily write a Python program
that, on input a representation (ùëò, ‚Ñì, ùëá) of a Turing machine ùëÄand
an input ùë•, evaluates ùëÄon ùëã. Here is the code of this program for
concreteness, though you can feel free to skip it if you are not familiar
with (or interested in) Python:
# constants
def EVAL(Œ¥,x):
'''Evaluate TM given by transition table Œ¥
on input x'''
Tape = ["‡∏Ä"] + [a for a in x]
i = 0; s = 0 # i = head pos, s = state
while True:
s, Tape[i], d = Œ¥[(s,Tape[i])]
if d == "H": break
if d == "L": i = max(i-1,0)
if d == "R": i += 1
if i>= len(Tape): Tape.append('Œ¶')
j = 1; Y = [] # produce output
while Tape[j] != 'Œ¶':
Y.append(Tape[j])
j += 1
return Y
On input a transition table ùõøthis program will simulate the cor-
responding machine ùëÄstep by step, at each point maintaining the
invariant that the array Tape contains the contents of ùëÄ‚Äôs tape, and
the variable s contains ùëÄ‚Äôs current state.
The above does not prove the theorem as stated, since we need
to show a Turing machine that computes EVAL rather than a Python
program. With enough effort, we can translate this Python code
line by line to a Turing machine. However, to prove the theorem we
don‚Äôt need to do this, but can use our ‚Äúeat the cake and have it too‚Äù
paradigm. That is, while we need to evaluate a Turing machine, in
writing the code for the interpreter we are allowed to use a richer
model such as NAND-RAM since it is equivalent in power to Turing
machines per Theorem 8.1).


--- Page 330 ---

330
introduction to theoretical computer science
Translating the above Python code to NAND-RAM is truly straight-
forward. The only issue is that NAND-RAM doesn‚Äôt have the dictio-
nary data structure built in, which we have used above to store the
transition function Œ¥. However, we can represent a dictionary ùê∑of
the form {ùëòùëíùë¶0 ‚à∂ùë£ùëéùëô0, ‚Ä¶ , ùëòùëíùë¶ùëö‚àí1 ‚à∂ùë£ùëéùëôùëö‚àí1} as simply a list of pairs.
To compute ùê∑[ùëò] we can scan over all the pairs until we find one of
the form (ùëò, ùë£) in which case we return ùë£. Similarly we scan the list
to update the dictionary with a new value, either modifying it or ap-
pending the pair (ùëòùëíùë¶, ùë£ùëéùëô) at the end.
‚ñ†
R
Remark 9.4 ‚Äî Efficiency of the simulation. The argu-
ment in the proof of Theorem 9.1 is a very inefficient
way to implement the dictionary data structure in
practice, but it suffices for the purpose of proving the
theorem. Reading and writing to a dictionary of ùëö
values in this implementation takes Œ©(ùëö) steps, but
it is in fact possible to do this in ùëÇ(log ùëö) steps using
a search tree data structure or even ùëÇ(1) (for ‚Äútypical‚Äù
instances) using a hash table. NAND-RAM and RAM
machines correspond to the architecture of modern
electronic computers, and so we can implement hash
tables and search trees in NAND-RAM just as they are
implemented in other programming languages.
The construction above yields a universal Turing Machine with a
very large number of states. However, since universal Turing machines
have such a philosophical and technical importance, researchers have
attempted to find the smallest possible universal Turing machines, see
Section 9.7.
9.1.2 Implications of universality (discussion)
There is more than one Turing machine ùëàthat satisfies the condi-
tions of Theorem 9.1, but the existence of even a single such machine
is already extremely fundamental to both the theory and practice of
computer science. Theorem 9.1‚Äôs impact reaches beyond the particu-
lar model of Turing machines. Because we can simulate every Turing
Machine by a NAND-TM program and vice versa, Theorem 9.1 im-
mediately implies there exists a universal NAND-TM program ùëÉùëà
such that ùëÉùëà(ùëÉ, ùë•) = ùëÉ(ùë•) for every NAND-TM program ùëÉ. We can
also ‚Äúmix and match‚Äù models. For example since we can simulate
every NAND-RAM program by a Turing machine, and every Turing
Machine by the ùúÜcalculus, Theorem 9.1 implies that there exists a ùúÜ
expression ùëísuch that for every NAND-RAM program ùëÉand input ùë•
on which ùëÉ(ùë•) = ùë¶, if we encode (ùëÉ, ùë•) as a ùúÜ-expression ùëì(using the


--- Page 331 ---

universality and uncomputability
331
Figure 9.3: a) A particularly elegant example of a
‚Äúmeta-circular evaluator‚Äù comes from John Mc-
Carthy‚Äôs 1960 paper, where he defined the Lisp
programming language and gave a Lisp function that
evaluates an arbitrary Lisp program (see above). Lisp
was not initially intended as a practical program-
ming language and this example was merely meant
as an illustration that the Lisp universal function is
more elegant than the universal Turing machine. It
was McCarthy‚Äôs graduate student Steve Russell who
suggested that it can be implemented. As McCarthy
later recalled, ‚ÄúI said to him, ho, ho, you‚Äôre confusing
theory with practice, this eval is intended for reading, not
for computing. But he went ahead and did it. That is, he
compiled the eval in my paper into IBM 704 machine code,
fixing a bug, and then advertised this as a Lisp interpreter,
which it certainly was‚Äù. b) A self-replicating C program
from the classic essay of Thompson [Tho84].
ùúÜ-calculus encoding of strings as lists of 0‚Äôs and 1‚Äôs) then (ùëíùëì) eval-
uates to an encoding of ùë¶. More generally we can say that for every
ùí≥and ùí¥in the set { Turing Machines, RAM Machines, NAND-TM,
NAND-RAM, ùúÜ-calculus, JavaScript, Python, ‚Ä¶ } of Turing equivalent
models, there exists a program/machine in ùí≥that computes the map
(ùëÉ, ùë•) ‚Ü¶ùëÉ(ùë•) for every program/machine ùëÉ‚ààùí¥.
The idea of a ‚Äúuniversal program‚Äù is of course not limited to theory.
For example compilers for programming languages are often used to
compile themselves, as well as programs more complicated than the
compiler. (An extreme example of this is Fabrice Bellard‚Äôs Obfuscated
Tiny C Compiler which is a C program of 2048 bytes that can compile
a large subset of the C programming language, and in particular can
compile itself.) This is also related to the fact that it is possible to write
a program that can print its own source code, see Fig. 9.3. There are
universal Turing machines known that require a very small number
of states or alphabet symbols, and in particular there is a universal
Turing machine (with respect to a particular choice of representing
Turing machines as strings) whose tape alphabet is {‚ñ∑, ‚àÖ, 0, 1} and
has fewer than 25 states (see Section 9.7).
9.2 IS EVERY FUNCTION COMPUTABLE?
In Theorem 4.12, we saw that NAND-CIRC programs can compute
every finite function ùëì‚à∂{0, 1}ùëõ‚Üí{0, 1}. Therefore a natural guess is
that NAND-TM programs (or equivalently, Turing Machines) could
compute every infinite function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}. However, this
turns out to be false. That is, there exists a function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}
that is uncomputable!


--- Page 332 ---

332
introduction to theoretical computer science
The existence of uncomputable functions is quite surprising. Our
intuitive notion of a ‚Äúfunction‚Äù (and the notion most mathematicians
had until the 20th century) is that a function ùëìdefines some implicit
or explicit way of computing the output ùëì(ùë•) from the input ùë•. The
notion of an ‚Äúuncomputable function‚Äù thus seems to be a contradic-
tion in terms, but yet the following theorem shows that such creatures
do exist:
Theorem 9.5 ‚Äî Uncomputable functions. There exists a function ùêπ‚àó
‚à∂
{0, 1}‚àó‚Üí{0, 1} that is not computable by any Turing machine.
Proof Idea:
The idea behind the proof follows quite closely Cantor‚Äôs proof that
the reals are uncountable (Theorem 2.5), and in fact the theorem can
also be obtained fairly directly from that result (see Exercise 7.11).
However, it is instructive to see the direct proof. The idea is to con-
struct ùêπ‚àóin a way that will ensure that every possible machine ùëÄwill
in fact fail to compute ùêπ‚àó. We do so by defining ùêπ‚àó(ùë•) to equal 0 if ùë•
describes a Turing machine ùëÄwhich satisfies ùëÄ(ùë•) = 1 and defining
ùêπ‚àó(ùë•) = 1 otherwise. By construction, if ùëÄis any Turing machine and
ùë•is the string describing it, then ùêπ‚àó(ùë•) ‚â†ùëÄ(ùë•) and therefore ùëÄdoes
not compute ùêπ‚àó.
‚ãÜ
Proof of Theorem 9.5. The proof is illustrated in Fig. 9.4. We start by
defining the following function ùê∫‚à∂{0, 1}‚àó‚Üí{0, 1}:
For every string ùë•‚àà{0, 1}‚àó, if ùë•satisfies (1) ùë•is a valid repre-
sentation of some Turing machine ùëÄ(per the representation scheme
above) and (2) when the program ùëÄis executed on the input ùë•it
halts and produces an output, then we define ùê∫(ùë•) as the first bit of
this output. Otherwise (i.e., if ùë•is not a valid representation of a Tur-
ing machine, or the machine ùëÄùë•never halts on ùë•) we define ùê∫(ùë•) = 0.
We define ùêπ‚àó(ùë•) = 1 ‚àíùê∫(ùë•).
We claim that there is no Turing machine that computes ùêπ‚àó. In-
deed, suppose, towards the sake of contradiction, there exists a ma-
chine ùëÄthat computes ùêπ‚àó, and let ùë•be the binary string that rep-
resents the machine ùëÄ. On one hand, since by our assumption ùëÄ
computes ùêπ‚àó, on input ùë•the machine ùëÄhalts and outputs ùêπ‚àó(ùë•). On
the other hand, by the definition of ùêπ‚àó, since ùë•is the representation
of the machine ùëÄ, ùêπ‚àó(ùë•) = 1 ‚àíùê∫(ùë•) = 1 ‚àíùëÄ(ùë•), hence yielding a
contradiction.
‚ñ†


--- Page 333 ---

universality and uncomputability
333
Figure 9.4: We construct an uncomputable function
by defining for every two strings ùë•, ùë¶the value
1 ‚àíùëÄùë¶(ùë•) which equals 0 if the machine described
by ùë¶outputs 1 on ùë•, and 1 otherwise. We then define
ùêπ‚àó(ùë•) to be the ‚Äúdiagonal‚Äù of this table, namely
ùêπ‚àó(ùë•) = 1 ‚àíùëÄùë•(ùë•) for every ùë•. The function ùêπ‚àó
is uncomputable, because if it was computable by
some machine whose string description is ùë•‚àóthen we
would get that ùëÄùë•‚àó(ùë•‚àó) = ùêπ(ùë•‚àó) = 1 ‚àíùëÄùë•‚àó(ùë•‚àó).
ÔÉ´Big Idea 12 There are some functions that can not be computed by
any algorithm.
P
The proof of Theorem 9.5 is short but subtle. I suggest
that you pause here and go back to read it again and
think about it - this is a proof that is worth reading at
least twice if not three or four times. It is not often the
case that a few lines of mathematical reasoning estab-
lish a deeply profound fact - that there are problems
we simply cannot solve.
The type of argument used to prove Theorem 9.5 is known as di-
agonalization since it can be described as defining a function based
on the diagonal entries of a table as in Fig. 9.4. The proof can be
thought of as an infinite version of the counting argument we used
for showing lower bound for NAND-CIRC programs in Theorem 5.3.
Namely, we show that it‚Äôs not possible to compute all functions from
{0, 1}‚àó‚Üí{0, 1} by Turing machines simply because there are more
functions like that then there are Turing machines.
As mentioned in Remark 7.4, many texts use the ‚Äúlanguage‚Äù ter-
minology and so will call a set ùêø‚äÜ{0, 1}‚àóan undecidable or non
recursive language if the function ùêπ‚à∂{0, 1}‚àó‚à∂‚Üí{0, 1} such that
ùêπ(ùë•) = 1 ‚Üîùë•‚ààùêøis uncomputable.


--- Page 334 ---

334
introduction to theoretical computer science
9.3 THE HALTING PROBLEM
Theorem 9.5 shows that there is some function that cannot be com-
puted. But is this function the equivalent of the ‚Äútree that falls in the
forest with no one hearing it‚Äù? That is, perhaps it is a function that
no one actually wants to compute. It turns out that there are natural
uncomputable functions:
Theorem 9.6 ‚Äî Uncomputability of Halting function. Let HALT ‚à∂{0, 1}‚àó‚Üí
{0, 1} be the function such that for every string ùëÄ
‚àà
{0, 1}‚àó,
HALT(ùëÄ, ùë•)
=
1 if Turing machine ùëÄhalts on the input ùë•and
HALT(ùëÄ, ùë•) = 0 otherwise. Then HALT is not computable.
Before turning to prove Theorem 9.6, we note that HALT is a very
natural function to want to compute. For example, one can think of
HALT as a special case of the task of managing an ‚ÄúApp store‚Äù. That
is, given the code of some application, the gatekeeper for the store
needs to decide if this code is safe enough to allow in the store or not.
At a minimum, it seems that we should verify that the code would not
go into an infinite loop.
Proof Idea:
One way to think about this proof is as follows:
Uncomputability of ùêπ‚àó+ Universality = Uncomputability of HALT
(9.2)
That is, we will use the universal Turing machine that computes EVAL
to derive the uncomputability of HALT from the uncomputability of
ùêπ‚àóshown in Theorem 9.5. Specifically, the proof will be by contra-
diction. That is, we will assume towards a contradiction that HALT is
computable, and use that assumption, together with the universal Tur-
ing machine of Theorem 9.1, to derive that ùêπ‚àóis computable, which
will contradict Theorem 9.5.
‚ãÜ
ÔÉ´Big Idea 13 If a function ùêπis uncomputable we can show that
another function ùêªis uncomputable by giving a way to reduce the task
of computing ùêπto computing ùêª.
Proof of Theorem 9.6. The proof will use the previously established
result Theorem 9.5. Recall that Theorem 9.5 shows that the following
function ùêπ‚àó‚à∂{0, 1}‚àó‚Üí{0, 1} is uncomputable:
ùêπ‚àó(ùë•) =
‚éß
{
‚é®
{
‚é©
1
ùë•(ùë•) = 0
0
otherwise
(9.3)


--- Page 335 ---

universality and uncomputability
335
where ùë•(ùë•) denotes the output of the Turing machine described by the
string ùë•on the input ùë•(with the usual convention that ùë•(ùë•) = ‚ä•if this
computation does not halt).
We will show that the uncomputability of ùêπ‚àóimplies the uncom-
putability of HALT. Specifically, we will assume, towards a contra-
diction, that there exists a Turing machine ùëÄthat can compute the
HALT function, and use that to obtain a Turing machine ùëÄ‚Ä≤ that com-
putes the function ùêπ‚àó. (This is known as a proof by reduction, since we
reduce the task of computing ùêπ‚àóto the task of computing HALT. By
the contrapositive, this means the uncomputability of ùêπ‚àóimplies the
uncomputability of HALT.)
Indeed, suppose that ùëÄis a Turing machine that computes HALT.
Algorithm 9.7 describes a Turing Machine ùëÄ‚Ä≤ that computes ùêπ‚àó. (We
use ‚Äúhigh level‚Äù description of Turing machines, appealing to the
‚Äúhave your cake and eat it too‚Äù paradigm, see Big Idea 10.)
Algorithm 9.7 ‚Äî ùêπ‚àóto ùêªùê¥ùêøùëáreduction.
Input: ùë•‚àà{0, 1}‚àó
Output: ùêπ‚àó(ùë•)
1:
# Assume T.M. ùëÄùêªùê¥ùêøùëácomputes ùêªùê¥ùêøùëá
2: Let ùëß‚ÜêùëÄùêªùê¥ùêøùëá(ùë•, ùë•).
# Assume ùëß= ùêªùê¥ùêøùëá(ùë•, ùë•).
3: if ùëß= 0 then
4:
return 0
5: end if
6: Let ùë¶‚Üêùëà(ùë•, ùë•)
# ùëàuniversal TM, i.e., ùë¶= ùë•(ùë•)
7: if ùë¶= 0 then
8:
return 1
9: end if
10: return 0
We claim that Algorithm 9.7 computes the function ùêπ‚àó. In-
deed, suppose that ùë•(ùë•) = 0 (and hence ùêπ‚àó(ùë•) = 1). In this
case, HALT(ùë•, ùë•) = 1 and hence, under our assumption that
ùëÄ(ùë•, ùë•) = HALT(ùë•, ùë•), the value ùëßwill equal 1, and hence Al-
gorithm 9.7 will set ùë¶= ùë•(ùë•) = 0, and output the correct value
1.
Suppose otherwise that ùë•(ùë•) ‚â†0 (and hence ùêπ‚àó(ùë•) = 0). In this
case there are two possibilities:
‚Ä¢ Case 1: The machine described by ùë•does not halt on the input ùë•.
In this case, HALT(ùë•, ùë•) = 0. Since we assume that ùëÄcomputes
HALT it means that on input ùë•, ùë•, the machine ùëÄmust halt and
output the value 0. This means that Algorithm 9.7 will set ùëß= 0
and output 0.


--- Page 336 ---

336
introduction to theoretical computer science
1 This argument has also been connected to the
issues of consciousness and free will. I am personally
skeptical of its relevance to these issues. Perhaps the
reasoning is that humans have the ability to solve the
halting problem but they exercise their free will and
consciousness by choosing not to do so.
‚Ä¢ Case 2: The machine described by ùë•halts on the input ùë•and out-
puts some ùë¶‚Ä≤ ‚â†0. In this case, since HALT(ùë•, ùë•) = 1, under our
assumptions, Algorithm 9.7 will set ùë¶= ùë¶‚Ä≤ ‚â†0 and so output 0.
We see that in all cases, ùëÄ‚Ä≤(ùë•) = ùêπ‚àó(ùë•), which contradicts the
fact that ùêπ‚àóis uncomputable. Hence we reach a contradiction to our
original assumption that ùëÄcomputes HALT.
‚ñ†
P
Once again, this is a proof that‚Äôs worth reading more
than once. The uncomputability of the halting prob-
lem is one of the fundamental theorems of computer
science, and is the starting point for much of the in-
vestigations we will see later. An excellent way to get
a better understanding of Theorem 9.6 is to go over
Section 9.3.2, which presents an alternative proof of
the same result.
9.3.1 Is the Halting problem really hard? (discussion)
Many people‚Äôs first instinct when they see the proof of Theorem 9.6
is to not believe it. That is, most people do believe the mathematical
statement, but intuitively it doesn‚Äôt seem that the Halting problem is
really that hard. After all, being uncomputable only means that HALT
cannot be computed by a Turing machine.
But programmers seem to solve HALT all the time by informally or
formally arguing that their programs halt. It‚Äôs true that their programs
are written in C or Python, as opposed to Turing machines, but that
makes no difference: we can easily translate back and forth between
this model and any other programming language.
While every programmer encounters at some point an infinite loop,
is there really no way to solve the halting problem? Some people
argue that they personally can, if they think hard enough, determine
whether any concrete program that they are given will halt or not.
Some have even argued that humans in general have the ability to
do that, and hence humans have inherently superior intelligence to
computers or anything else modeled by Turing machines.1
The best answer we have so far is that there truly is no way to solve
HALT, whether using Macs, PCs, quantum computers, humans, or
any other combination of electronic, mechanical, and biological de-
vices. Indeed this assertion is the content of the Church-Turing Thesis.
This of course does not mean that for every possible program ùëÉ, it
is hard to decide if ùëÉenters an infinite loop. Some programs don‚Äôt
even have loops at all (and hence trivially halt), and there are many
other far less trivial examples of programs that we can certify to never


--- Page 337 ---

universality and uncomputability
337
Figure 9.5: SMBC‚Äôs take on solving the Halting prob-
lem.
enter an infinite loop (or programs that we know for sure that will
enter such a loop). However, there is no general procedure that would
determine for an arbitrary program ùëÉwhether it halts or not. More-
over, there are some very simple programs for which no one knows
whether they halt or not. For example, the following Python program
will halt if and only if Goldbach‚Äôs conjecture is false:
def isprime(p):
return all(p % i for i in range(2,p-1))
def Goldbach(n):
return any( (isprime(p) and isprime(n-p))
for p in range(2,n-1))
n = 4
while True:
if not Goldbach(n): break
n+= 2
Given that Goldbach‚Äôs Conjecture has been open since 1742, it is
unclear that humans have any magical ability to say whether this (or
other similar programs) will halt or not.
9.3.2 A direct proof of the uncomputability of HALT (optional)
It turns out that we can combine the ideas of the proofs of Theo-
rem 9.5 and Theorem 9.6 to obtain a short proof of the latter theorem,
that does not appeal to the uncomputability of ùêπ‚àó. This short proof
appeared in print in a 1965 letter to the editor of Christopher Strachey:
To the Editor, The Computer Journal.
An Impossible Program
Sir,
A well-known piece of folk-lore among programmers holds that it is
impossible to write a program which can examine any other program
and tell, in every case, if it will terminate or get into a closed loop when
it is run. I have never actually seen a proof of this in print, and though
Alan Turing once gave me a verbal proof (in a railway carriage on the
way to a Conference at the NPL in 1953), I unfortunately and promptly
forgot the details. This left me with an uneasy feeling that the proof
must be long or complicated, but in fact it is so short and simple that it
may be of interest to casual readers. The version below uses CPL, but
not in any essential way.
Suppose T[R] is a Boolean function taking a routine (or program) R
with no formal or free variables as its arguments and that for all R,
T[R] = True if R terminates if run and that T[R] = False if R does not
terminate.
Consider the routine P defined as follows


--- Page 338 ---

338
introduction to theoretical computer science
rec routine P
¬ßL: if T[P] go to L
Return ¬ß
If T[P] = True the routine P will loop, and it will only terminate if
T[P] = False. In each case ‚ÄòT[P]‚Äú has exactly the wrong value, and this
contradiction shows that the function T cannot exist.
Yours faithfully,
C. Strachey
Churchill College, Cambridge
P
Try to stop and extract the argument for proving
Theorem 9.6 from the letter above.
Since CPL is not as common today, let us reproduce this proof. The
idea is the following: suppose for the sake of contradiction that there
exists a program T such that T(f,x) equals True iff f halts on input
x. (Strachey‚Äôs letter considers the no-input variant of HALT, but as
we‚Äôll see, this is an immaterial distinction.) Then we can construct a
program P and an input x such that T(P,x) gives the wrong answer.
The idea is that on input x, the program P will do the following: run
T(x,x), and if the answer is True then go into an infinite loop, and
otherwise halt. Now you can see that T(P,P) will give the wrong
answer: if P halts when it gets its own code as input, then T(P,P) is
supposed to be True, but then P(P) will go into an infinite loop. And
if P does not halt, then T(P,P) is supposed to be False but then P(P)
will halt. We can also code this up in Python:
def CantSolveMe(T):
"""
Gets function T that claims to solve HALT.
Returns a pair (P,x) of code and input on which
T(P,x) ‚â†HALT(x)
"""
def fool(x):
if T(x,x):
while True: pass
return "I halted"
return (fool,fool)
For example, consider the following Naive Python program T that
guesses that a given function does not halt if its input contains while
or for


--- Page 339 ---

universality and uncomputability
339
def T(f,x):
"""Crude halting tester - decides it doesn't halt if it
contains a loop."""
‚Ü™
import inspect
source = inspect.getsource(f)
if source.find("while"): return False
if source.find("for"): return False
return True
If we now set (f,x) = CantSolveMe(T), then T(f,x)=False but
f(x) does in fact halt. This is of course not specific to this particular T:
for every program T, if we run (f,x) = CantSolveMe(T) then we‚Äôll
get an input on which T gives the wrong answer to HALT.
9.4 REDUCTIONS
The Halting problem turns out to be a linchpin of uncomputability, in
the sense that Theorem 9.6 has been used to show the uncomputabil-
ity of a great many interesting functions. We will see several examples
of such results in this chapter and the exercises, but there are many
more such results (see Fig. 9.6).
Figure 9.6: Some uncomputability results. An arrow
from problem X to problem Y means that we use the
uncomputability of X to prove the uncomputability
of Y by reducing computing X to computing Y.
All of these results except for the MRDP Theorem
appear in either the text or exercises. The Halting
Problem HALT serves as our starting point for all
these uncomputability results as well as many others.
The idea behind such uncomputability results is conceptually sim-
ple but can at first be quite confusing. If we know that HALT is un-
computable, and we want to show that some other function BLAH is
uncomputable, then we can do so via a contrapositive argument (i.e.,
proof by contradiction). That is, we show that if there exists a Turing
machine that computes BLAH then there exists a Turing machine that
computes HALT. (Indeed, this is exactly how we showed that HALT
itself is uncomputable, by reducing this fact to the uncomputability of
the function ùêπ‚àófrom Theorem 9.5.)


--- Page 340 ---

340
introduction to theoretical computer science
For example, to prove that BLAH is uncomputable, we could show
that there is a computable function ùëÖ‚à∂{0, 1}‚àó‚Üí{0, 1}‚àósuch that for
every pair ùëÄand ùë•, HALT(ùëÄ, ùë•) = BLAH(ùëÖ(ùëÄ, ùë•)). The existence of
such a function ùëÖimplies that if BLAH was computable then HALT
would be computable as well, hence leading to a contradiction! The
confusing part about reductions is that we are assuming something
we believe is false (that BLAH has an algorithm) to derive something
that we know is false (that HALT has an algorithm). Michael Sipser
describes such results as having the form ‚ÄúIf pigs could whistle then
horses could fly‚Äù.
A reduction-based proof has two components. For starters, since
we need ùëÖto be computable, we should describe the algorithm to
compute it. The algorithm to compute ùëÖis known as a reduction since
the transformation ùëÖmodifies an input to HALT to an input to BLAH,
and hence reduces the task of computing HALT to the task of comput-
ing BLAH. The second component of a reduction-based proof is the
analysis of the algorithm ùëÖ: namely a proof that ùëÖdoes indeed satisfy
the desired properties.
Reduction-based proofs are just like other proofs by contradiction,
but the fact that they involve hypothetical algorithms that don‚Äôt really
exist tends to make reductions quite confusing. The one silver lining
is that at the end of the day the notion of reductions is mathematically
quite simple, and so it‚Äôs not that bad even if you have to go back to
first principles every time you need to remember what is the direction
that a reduction should go in.
R
Remark 9.8 ‚Äî Reductions are algorithms. A reduction
is an algorithm, which means that, as discussed in
Remark 0.3, a reduction has three components:
‚Ä¢ Specification (what): In the case of a reduction
from HALT to BLAH, the specification is that func-
tion ùëÖ
‚à∂
{0, 1}‚àó
‚Üí
{0, 1}‚àóshould satisfy that
HALT(ùëÄ, ùë•)
=
BLAH(ùëÖ(ùëÄ, ùë•)) for every Tur-
ing machine ùëÄand input ùë•. In general, to reduce
a function ùêπto ùê∫, the reduction should satisfy
ùêπ(ùë§) = ùê∫(ùëÖ(ùë§)) for every input ùë§to ùêπ.
‚Ä¢ Implementation (how): The algorithm‚Äôs descrip-
tion: the precise instructions how to transform an
input ùë§to the output ùëÖ(ùë§).
‚Ä¢ Analysis (why): A proof that the algorithm meets
the specification. In particular, in a reduction
from ùêπto ùê∫this is a proof that for every input
ùë§, the output ùë¶of the algorithm satisfies that
ùêπ(ùë§) = ùê∫(ùë¶).


--- Page 341 ---

universality and uncomputability
341
9.4.1 Example: Halting on the zero problem
Here is a concrete example for a proof by reduction. We define the
function HALTONZERO ‚à∂{0, 1}‚àó‚Üí{0, 1} as follows. Given any
string ùëÄ, HALTONZERO(ùëÄ) = 1 if and only if ùëÄdescribes a Turing
machine that halts when it is given the string 0 as input. A priori
HALTONZERO seems like a potentially easier function to compute
than the full-fledged HALT function, and so we could perhaps hope
that it is not uncomputable. Alas, the following theorem shows that
this is not the case:
Theorem 9.9 ‚Äî Halting without input. HALTONZERO is uncomputable.
P
The proof of Theorem 9.9 is below, but before reading
it you might want to pause for a couple of minutes
and think how you would prove it yourself. In partic-
ular, try to think of what a reduction from HALT to
HALTONZERO would look like. Doing so is an excel-
lent way to get some initial comfort with the notion
of proofs by reduction, which a technique we will be
using time and again in this book.
Figure 9.7: To prove Theorem 9.9, we show that
HALTONZERO is uncomputable by giving a reduction
from the task of computing HALT to the task of com-
puting HALTONZERO. This shows that if there was a
hypothetical algorithm ùê¥computing HALTONZERO,
then there would be an algorithm ùêµcomputing
HALT, contradicting Theorem 9.6. Since neither ùê¥nor
ùêµactually exists, this is an example of an implication
of the form ‚Äúif pigs could whistle then horses could
fly‚Äù.
Proof of Theorem 9.9. The proof is by reduction from HALT, see
Fig. 9.7. We will assume, towards the sake of contradiction, that
HALTONZERO is computable by some algorithm ùê¥, and use this
hypothetical algorithm ùê¥to construct an algorithm ùêµto compute
HALT, hence obtaining a contradiction to Theorem 9.6. (As discussed


--- Page 342 ---

342
introduction to theoretical computer science
in Big Idea 10, following our ‚Äúhave your cake and eat it too‚Äù paradigm,
we just use the generic name ‚Äúalgorithm‚Äù rather than worrying
whether we model them as Turing machines, NAND-TM programs,
NAND-RAM, etc.; this makes no difference since all these models are
equivalent to one another.)
Since this is our first proof by reduction from the Halting prob-
lem, we will spell it out in more details than usual. Such a proof by
reduction consists of two steps:
1. Description of the reduction: We will describe the operation of our
algorithm ùêµ, and how it makes ‚Äúfunction calls‚Äù to the hypothetical
algorithm ùê¥.
2. Analysis of the reduction: We will then prove that under the hypoth-
esis that Algorithm ùê¥computes HALTONZERO, Algorithm ùêµwill
compute HALT.
Algorithm 9.10 ‚Äî ùêªùê¥ùêøùëáto ùêªùê¥ùêøùëáùëÇùëÅùëçùê∏ùëÖùëÇreduction.
Input: Turing machine ùëÄand string ùë•.
Output: Turing machine ùëÄ‚Ä≤ such that ùëÄhalts on ùë•iff ùëÄ‚Ä≤
halts on zero
1: procedure ùëÅùëÄ,ùë•(ùë§)
# Description of the T.M. ùëÅùëÄ,ùë•
2:
return ùê∏ùëâùê¥ùêø(ùëÄ, ùë•)
# Ignore the
Input: ùë§, evaluate ùëÄon ùë•.
3: end procedure
4: return ùëÅùëÄ,ùë•
# We do not execute ùëÅùëÄ,ùë•: only return its
description
Our Algorithm ùêµworks as follows: on input ùëÄ, ùë•, it runs Algo-
rithm 9.10 to obtain a Turing Machine ùëÄ‚Ä≤, and then returns ùê¥(ùëÄ‚Ä≤).
The machine ùëÄ‚Ä≤ ignores its input ùëßand simply runs ùëÄon ùë•.
In pseudocode, the program ùëÅùëÄ,ùë•will look something like the
following:
def N(z):
M = r'.......'
# a string constant containing desc. of M
x = r'.......'
# a string constant containing x
return eval(M,x)
# note that we ignore the input z
That is, if we think of ùëÅùëÄ,ùë•as a program, then it is a program that
contains ùëÄand ùë•as ‚Äúhardwired constants‚Äù, and given any input ùëß, it
simply ignores the input and always returns the result of evaluating


--- Page 343 ---

universality and uncomputability
343
ùëÄon ùë•. The algorithm ùêµdoes not actually execute the machine ùëÅùëÄ,ùë•.
ùêµmerely writes down the description of ùëÅùëÄ,ùë•as a string (just as we
did above) and feeds this string as input to ùê¥.
The above completes the description of the reduction. The analysis is
obtained by proving the following claim:
Claim: For every strings ùëÄ, ùë•, ùëß, the machine ùëÅùëÄ,ùë•constructed by
Algorithm ùêµin Step 1 satisfies that ùëÅùëÄ,ùë•halts on ùëßif and only if the
program described by ùëÄhalts on the input ùë•.
Proof of Claim: Since ùëÅùëÄ,ùë•ignores its input and evaluates ùëÄon ùë•
using the universal Turing machine, it will halt on ùëßif and only if ùëÄ
halts on ùë•.
In particular if we instantiate this claim with the input ùëß= 0 to
ùëÅùëÄ,ùë•, we see that HALTONZERO(ùëÅùëÄ,ùë•) = HALT(ùëÄ, ùë•). Thus if
the hypothetical algorithm ùê¥satisfies ùê¥(ùëÄ) = HALTONZERO(ùëÄ)
for every ùëÄthen the algorithm ùêµwe construct satisfies ùêµ(ùëÄ, ùë•) =
HALT(ùëÄ, ùë•) for every ùëÄ, ùë•, contradicting the uncomputability of
HALT.
‚ñ†
R
Remark 9.11 ‚Äî The hardwiring technique. In the proof of
Theorem 9.9 we used the technique of ‚Äúhardwiring‚Äù
an input ùë•to a program/machine ùëÉ. That is, modify-
ing a program ùëÉthat it uses ‚Äúhardwired constants‚Äù
for some of all of its input. This technique is quite
common in reductions and elsewhere, and we will
often use it again in this course.
9.5 RICE‚ÄôS THEOREM AND THE IMPOSSIBILITY OF GENERAL
SOFTWARE VERIFICATION
The uncomputability of the Halting problem turns out to be a special
case of a much more general phenomenon. Namely, that we cannot
certify semantic properties of general purpose programs. ‚ÄúSemantic prop-
erties‚Äù mean properties of the function that the program computes, as
opposed to properties that depend on the particular syntax used by
the program.
An example for a semantic property of a program ùëÉis the property
that whenever ùëÉis given an input string with an even number of 1‚Äôs,
it outputs 0. Another example is the property that ùëÉwill always halt
whenever the input ends with a 1. In contrast, the property that a C
program contains a comment before every function declaration is not
a semantic property, since it depends on the actual source code as
opposed to the input/output relation.


--- Page 344 ---

344
introduction to theoretical computer science
Checking semantic properties of programs is of great interest, as it
corresponds to checking whether a program conforms to a specifica-
tion. Alas it turns out that such properties are in general uncomputable.
We have already seen some examples of uncomputable semantic func-
tions, namely HALT and HALTONZERO, but these are just the ‚Äútip of
the iceberg‚Äù. We start by observing one more such example:
Theorem 9.12 ‚Äî Computing all zero function. Let ZEROFUNC ‚à∂{0, 1}‚àó‚Üí
{0, 1} be the function such that for every ùëÄ‚àà{0, 1}‚àó, ZEROFUNC(ùëÄ) =
1 if and only if ùëÄrepresents a Turing machine such that ùëÄoutputs
0 on every input ùë•‚àà{0, 1}‚àó. Then ZEROFUNC is uncomputable.
P
Despite the similarity in their names, ZEROFUNC and
HALTONZERO are two different functions. For exam-
ple, if ùëÄis a Turing machine that on input ùë•‚àà{0, 1}‚àó,
halts and outputs the OR of all of ùë•‚Äôs coordinates, then
HALTONZERO(ùëÄ)
=
1 (since ùëÄdoes halt on the
input 0) but ZEROFUNC(ùëÄ)
=
0 (since ùëÄdoes not
compute the constant zero function).
Proof of Theorem 9.12. The proof is by reduction to HALTONZERO.
Suppose, towards the sake of contradiction, that there was an algo-
rithm ùê¥such that ùê¥(ùëÄ) = ZEROFUNC(ùëÄ) for every ùëÄ‚àà{0, 1}‚àó.
Then we will construct an algorithm ùêµthat solves HALTONZERO,
contradicting Theorem 9.9.
Given a Turing machine ùëÅ(which is the input to HALTONZERO),
our Algorithm ùêµdoes the following:
1. Construct a Turing Machine ùëÄwhich on input ùë•‚àà{0, 1}‚àó, first
runs ùëÅ(0) and then outputs 0.
2. Return ùê¥(ùëÄ).
Now if ùëÅhalts on the input 0 then the Turing machine ùëÄcom-
putes the constant zero function, and hence under our assumption
that ùê¥computes ZEROFUNC, ùê¥(ùëÄ) = 1. If ùëÅdoes not halt on the
input 0, then the Turing machine ùëÄwill not halt on any input, and
so in particular will not compute the constant zero function. Hence
under our assumption that ùê¥computes ZEROFUNC, ùê¥(ùëÄ) = 0.
We see that in both cases, ZEROFUNC(ùëÄ) = HALTONZERO(ùëÅ)
and hence the value that Algorithm ùêµreturns in step 2 is equal to
HALTONZERO(ùëÅ) which is what we needed to prove.
‚ñ†
Another result along similar lines is the following:


--- Page 345 ---

universality and uncomputability
345
Theorem 9.13 ‚Äî Uncomputability of verifying parity. The following func-
tion is uncomputable
COMPUTES-PARITY(ùëÉ) =
‚éß
{
‚é®
{
‚é©
1
ùëÉcomputes the parity function
0
otherwise
(9.4)
P
We leave the proof of Theorem 9.13 as an exercise
(Exercise 9.6). I strongly encourage you to stop here
and try to solve this exercise.
9.5.1 Rice‚Äôs Theorem
Theorem 9.13 can be generalized far beyond the parity function. In
fact, this generalization rules out verifying any type of semantic spec-
ification on programs. We define a semantic specification on programs
to be some property that does not depend on the code of the program
but just on the function that the program computes.
For example, consider the following two C programs
int First(int n) {
if (n<0) return 0;
return 2*n;
}
int Second(int n) {
int i = 0;
int j = 0
if (n<0) return 0;
while (j<n) {
i = i + 2;
j=
j + 1;
}
return i;
}
First and Second are two distinct C programs, but they compute
the same function. A semantic property, would be either true for both
programs or false for both programs, since it depends on the function
the programs compute and not on their code. An example for a se-
mantic property that both First and Second satisfy is the following:
‚ÄúThe program ùëÉcomputes a function ùëìmapping integers to integers satisfy-
ing that ùëì(ùëõ) ‚â•ùëõfor every input ùëõ‚Äù.


--- Page 346 ---

346
introduction to theoretical computer science
A property is not semantic if it depends on the source code rather
than the input/output behavior. For example, properties such as ‚Äúthe
program contains the variable k‚Äù or ‚Äúthe program uses the while op-
eration‚Äù are not semantic. Such properties can be true for one of the
programs and false for others. Formally, we define semantic proper-
ties as follows:
Definition 9.14 ‚Äî Semantic properties. A pair of Turing machines
ùëÄand ùëÄ‚Ä≤ are functionally equivalent if for every ùë•
‚àà
{0, 1}‚àó,
ùëÄ(ùë•) = ùëÄ‚Ä≤(ùë•). (In particular, ùëÄ(ùë•) = ‚ä•iff ùëÄ‚Ä≤(ùë•) = ‚ä•for all
ùë•.)
A function ùêπ
‚à∂
{0, 1}‚àó
‚Üí
{0, 1} is semantic if for every pair of
strings ùëÄ, ùëÄ‚Ä≤ that represent functionally equivalent Turing ma-
chines, ùêπ(ùëÄ)
=
ùêπ(ùëÄ‚Ä≤). (Recall that we assume that every string
represents some Turing machine, see Remark 9.3)
There are two trivial examples of semantic functions: the constant
one function and the constant zero function. For example, if ùëçis the
constant zero function (i.e., ùëç(ùëÄ) = 0 for every ùëÄ) then clearly
ùêπ(ùëÄ) = ùêπ(ùëÄ‚Ä≤) for every pair of Turing machines ùëÄand ùëÄ‚Ä≤ that are
functionally equivalent ùëÄand ùëÄ‚Ä≤. Here is a non-trivial example
Solved Exercise 9.1 ‚Äî ùëçùê∏ùëÖùëÇùêπùëàùëÅùê∂is semantic. Prove that the function
ZEROFUNC is semantic.
‚ñ†
Solution:
Recall that ZEROFUNC(ùëÄ)
=
1 if and only if ùëÄ(ùë•)
=
0 for
every ùë•‚àà{0, 1}‚àó. If ùëÄand ùëÄ‚Ä≤ are functionally equivalent, then for
every ùë•, ùëÄ(ùë•) = ùëÄ‚Ä≤(ùë•). Hence ZEROFUNC(ùëÄ) = 1 if and only if
ZEROFUNC(ùëÄ‚Ä≤) = 1.
‚ñ†
Often the properties of programs that we are most interested in
computing are the semantic ones, since we want to understand the
programs‚Äô functionality. Unfortunately, Rice‚Äôs Theorem tells us that
these properties are all uncomputable:
Theorem 9.15 ‚Äî Rice‚Äôs Theorem. Let ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}. If ùêπis seman-
tic and non-trivial then it is uncomputable.
Proof Idea:
The idea behind the proof is to show that every semantic non-
trivial function ùêπis at least as hard to compute as HALTONZERO.
This will conclude the proof since by Theorem 9.9, HALTONZERO
is uncomputable. If a function ùêπis non trivial then there are two


--- Page 347 ---

universality and uncomputability
347
machines ùëÄ0 and ùëÄ1 such that ùêπ(ùëÄ0) = 0 and ùêπ(ùëÄ1) = 1. So,
the goal would be to take a machine ùëÅand find a way to map it into
a machine ùëÄ= ùëÖ(ùëÅ), such that (i) if ùëÅhalts on zero then ùëÄis
functionally equivalent to ùëÄ1 and (ii) if ùëÅdoes not halt on zero then
ùëÄis functionally equivalent ùëÄ0.
Because ùêπis semantic, if we achieved this, then we would be guar-
anteed that HALTONZERO(ùëÅ) = ùêπ(ùëÖ(ùëÅ)), and hence would show
that if ùêπwas computable, then HALTONZERO would be computable
as well, contradicting Theorem 9.9.
‚ãÜ
Proof of Theorem 9.15. We will not give the proof in full formality, but
rather illustrate the proof idea by restricting our attention to a particu-
lar semantic function ùêπ. However, the same techniques generalize to
all possible semantic functions. Define MONOTONE ‚à∂{0, 1}‚àó‚Üí{0, 1}
as follows: MONOTONE(ùëÄ) = 1 if there does not exist ùëõ‚àà‚Ñïand
two inputs ùë•, ùë•‚Ä≤ ‚àà{0, 1}ùëõsuch that for every ùëñ‚àà[ùëõ] ùë•ùëñ‚â§ùë•‚Ä≤
ùëñbut ùëÄ(ùë•)
outputs 1 and ùëÄ(ùë•‚Ä≤) = 0. That is, MONOTONE(ùëÄ) = 1 if it‚Äôs not
possible to find an input ùë•such that flipping some bits of ùë•from 0 to
1 will change ùëÄ‚Äôs output in the other direction from 1 to 0. We will
prove that MONOTONE is uncomputable, but the proof will easily
generalize to any semantic function.
We start by noting that MONOTONE is neither the constant zero
nor the constant one function:
‚Ä¢ The machine INF that simply goes into an infinite loop on every
input satisfies MONOTONE(INF) = 1, since INF is not defined
anywhere and so in particular there are no two inputs ùë•, ùë•‚Ä≤ where
ùë•ùëñ‚â§ùë•‚Ä≤
ùëñfor every ùëñbut INF(ùë•) = 0 and INF(ùë•‚Ä≤) = 1.
‚Ä¢ The machine PAR that computes the XOR or parity of its input, is
not monotone (e.g., PAR(1, 1, 0, 0, ‚Ä¶ , 0) = 0 but PAR(1, 0, 0, ‚Ä¶ , 0) =
0) and hence MONOTONE(PAR) = 0.
(Note that INF and PAR are machines and not functions.)
We will now give a reduction from HALTONZERO to
MONOTONE. That is, we assume towards a contradiction that
there exists an algorithm ùê¥that computes MONOTONE and we will
build an algorithm ùêµthat computes HALTONZERO. Our algorithm ùêµ
will work as follows:
Algorithm ùêµ:
Input: String ùëÅdescribing a Turing machine. (Goal: Compute
HALTONZERO(ùëÅ))
Assumption: Access to Algorithm ùê¥to compute MONOTONE.
Operation:


--- Page 348 ---

348
introduction to theoretical computer science
1. Construct the following machine ùëÄ: ‚ÄúOn input ùëß‚àà{0, 1}‚àódo: (a)
Run ùëÅ(0), (b) Return PAR(ùëß)‚Äù.
2. Return 1 ‚àíùê¥(ùëÄ).
To complete the proof we need to show that ùêµoutputs the cor-
rect answer, under our assumption that ùê¥computes MONOTONE.
In other words, we need to show that HALTONZERO(ùëÅ) = 1 ‚àí
ùëÄùëÇùëÅùëÇùëáùëÇùëÅùê∏(ùëÄ). Suppose that ùëÅdoes not halt on zero. In this
case the program ùëÄconstructed by Algorithm ùêµenters into an in-
finite loop in step (a) and will never reach step (b). Hence in this
case ùëÅis functionally equivalent to INF. (The machine ùëÅis not
the same machine as INF: its description or code is different. But it
does have the same input/output behavior (in this case) of never
halting on any input. Also, while the program ùëÄwill go into an in-
finite loop on every input, Algorithm ùêµnever actually runs ùëÄ: it
only produces its code and feeds it to ùê¥. Hence Algorithm ùêµwill
not enter into an infinite loop even in this case.) Thus in this case,
MONOTONE(ùëÅ) = MONOTONE(INF) = 1.
If ùëÅdoes halt on zero, then step (a) in ùëÄwill eventually conclude
and ùëÄ‚Äôs output will be determined by step (b), where it simply out-
puts the parity of its input. Hence in this case, ùëÄcomputes the non-
monotone parity function (i.e., is functionally equivalent to PAR), and
so we get that MONOTONE(ùëÄ) = MONOTONE(PAR) = 0. In both
cases, MONOTONE(ùëÄ) = 1 ‚àíùêªùê¥ùêøùëáùëÇùëÅùëçùê∏ùëÖùëÇ(ùëÅ), which is what
we wanted to prove.
An examination of this proof shows that we did not use anything
about MONOTONE beyond the fact that it is semantic and non-trivial.
For every semantic non-trivial ùêπ, we can use the same proof, replacing
PAR and INF with two machines ùëÄ0 and ùëÄ1 such that ùêπ(ùëÄ0) = 0 and
ùêπ(ùëÄ1) = 1. Such machines must exist if ùêπis non trivial.
‚ñ†
R
Remark 9.16 ‚Äî Semantic is not the same as uncom-
putable. Rice‚Äôs Theorem is so powerful and such a
popular way of proving uncomputability that peo-
ple sometimes get confused and think that it is the
only way to prove uncomputability. In particular, a
common misconception is that if a function ùêπis not
semantic then it is computable. This is not at all the
case.
For example, consider the following function
HALTNOYALE ‚à∂{0, 1}‚àó‚Üí{0, 1}. This is a function
that on input a string that represents a NAND-TM
program ùëÉ, outputs 1 if and only if both (i) ùëÉhalts
on the input 0, and (ii) the program ùëÉdoes not con-
tain a variable with the identifier Yale. The function


--- Page 349 ---

universality and uncomputability
349
HALTNOYALE is clearly not semantic, as it will out-
put two different values when given as input one of
the following two functionally equivalent programs:
Yale[0] = NAND(X[0],X[0])
Y[0] = NAND(X[0],Yale[0])
and
Harvard[0] = NAND(X[0],X[0])
Y[0] = NAND(X[0],Harvard[0])
However, HALTNOYALE is uncomputable since every
program ùëÉcan be transformed into an equivalent
(and in fact improved :)) program ùëÉ‚Ä≤ that does not
contain the variable Yale. Hence if we could compute
HALTNOYALE then determine halting on zero for
NAND-TM programs (and hence for Turing machines
as well).
Moreover, as we will see in Chapter 11, there are un-
computable functions whose inputs are not programs,
and hence for which the adjective ‚Äúsemantic‚Äù is not
applicable.
Properties such as ‚Äúthe program contains the variable
Yale‚Äù are sometimes known as syntactic properties.
The terms ‚Äúsemantic‚Äù and ‚Äúsyntactic‚Äù are used be-
yond the realm of programming languages: a famous
example of a syntactically correct but semantically
meaningless sentence in English is Chomsky‚Äôs ‚ÄúColor-
less green ideas sleep furiously.‚Äù However, formally
defining ‚Äúsyntactic properties‚Äù is rather subtle and we
will not use this terminology in this book, sticking to
the terms ‚Äúsemantic‚Äù and ‚Äúnon semantic‚Äù only.
9.5.2 Halting and Rice‚Äôs Theorem for other Turing-complete models
As we saw before, many natural computational models turn out to be
equivalent to one another, in the sense that we can transform a ‚Äúpro-
gram‚Äù of one model (such as a ùúÜexpression, or a game-of-life config-
urations) into another model (such as a NAND-TM program). This
equivalence implies that we can translate the uncomputability of the
Halting problem for NAND-TM programs into uncomputability for
Halting in other models. For example:
Theorem 9.17 ‚Äî NAND-TM Machine Halting. Let NANDTMHALT
‚à∂
{0, 1}‚àó
‚Üí
{0, 1} be the function that on input strings ùëÉ
‚àà
{0, 1}‚àóand ùë•
‚àà
{0, 1}‚àóoutputs 1 if the NAND-TM program de-
scribed by ùëÉhalts on the input ùë•and outputs 0 otherwise. Then
NANDTMHALT is uncomputable.


--- Page 350 ---

350
introduction to theoretical computer science
P
Once again, this is a good point for you to stop and try
to prove the result yourself before reading the proof
below.
Proof. We have seen in Theorem 7.11 that for every Turing machine
ùëÄ, there is an equivalent NAND-TM program ùëÉùëÄsuch that for ev-
ery ùë•, ùëÉùëÄ(ùë•) = ùëÄ(ùë•). In particular this means that HALT(ùëÄ) =
NANDTMHALT(ùëÉùëÄ).
The transformation ùëÄ‚Ü¶ùëÉùëÄthat is obtained from the proof
of Theorem 7.11 is constructive. That is, the proof yields a way to
compute the map ùëÄ‚Ü¶ùëÉùëÄ. This means that this proof yields a
reduction from task of computing HALT to the task of computing
NANDTMHALT, which means that since HALT is uncomputable,
neither is NANDTMHALT.
‚ñ†
The same proof carries over to other computational models such as
the ùúÜcalculus, two dimensional (or even one-dimensional) automata etc.
Hence for example, there is no algorithm to decide if a ùúÜexpression
evaluates the identity function, and no algorithm to decide whether
an initial configuration of the game of life will result in eventually
coloring the cell (0, 0) black or not.
Indeed, we can generalize Rice‚Äôs Theorem to all these models. For
example, if ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1} is a non-trivial function such that
ùêπ(ùëÉ) = ùêπ(ùëÉ‚Ä≤) for every functionally equivalent NAND-TM programs
ùëÉ, ùëÉ‚Ä≤ then ùêπis uncomputable, and the same holds for NAND-RAM
programs, ùúÜ-expressions, and all other Turing complete models (as
defined in Definition 8.5), see also Exercise 9.12.
9.5.3 Is software verification doomed? (discussion)
Programs are increasingly being used for mission critical purposes,
whether it‚Äôs running our banking system, flying planes, or monitoring
nuclear reactors. If we can‚Äôt even give a certification algorithm that
a program correctly computes the parity function, how can we ever
be assured that a program does what it is supposed to do? The key
insight is that while it is impossible to certify that a general program
conforms with a specification, it is possible to write a program in
the first place in a way that will make it easier to certify. As a trivial
example, if you write a program without loops, then you can certify
that it halts. Also, while it might not be possible to certify that an
arbitrary program computes the parity function, it is quite possible to
write a particular program ùëÉfor which we can mathematically prove
that ùëÉcomputes the parity. In fact, writing programs or algorithms


--- Page 351 ---

universality and uncomputability
351
and providing proofs for their correctness is what we do all the time in
algorithms research.
The field of software verification is concerned with verifying that
given programs satisfy certain conditions. These conditions can be
that the program computes a certain function, that it never writes
into a dangerous memory location, that is respects certain invari-
ants, and others. While the general tasks of verifying this may be
uncomputable, researchers have managed to do so for many inter-
esting cases, especially if the program is written in the first place in
a formalism or programming language that makes verification eas-
ier. That said, verification, especially of large and complex programs,
remains a highly challenging task in practice as well, and the num-
ber of programs that have been formally proven correct is still quite
small. Moreover, even phrasing the right theorem to prove (i.e., the
specification) if often a highly non-trivial endeavor.
Figure 9.8: The set R of computable Boolean functions
(Definition 7.3) is a proper subset of the set of all
functions mapping {0, 1}‚àóto {0, 1}. In this chapter
we saw a few examples of elements in the latter set
that are not in the former.
‚úì
Chapter Recap
‚Ä¢ There is a universal Turing machine (or NAND-TM
program) ùëàsuch that on input a description of a
Turing machine ùëÄand some input ùë•, ùëà(ùëÄ, ùë•) halts
and outputs ùëÄ(ùë•) if (and only if) ùëÄhalts on input
ùë•. Unlike in the case of finite computation (i.e.,
NAND-CIRC programs / circuits), the input to
the program ùëàcan be a machine ùëÄthat has more
states than ùëàitself.
‚Ä¢ Unlike the finite case, there are actually functions
that are inherently uncomputable in the sense that
they cannot be computed by any Turing machine.
‚Ä¢ These include not only some ‚Äúdegenerate‚Äù or ‚Äúeso-
teric‚Äù functions but also functions that people have


--- Page 352 ---

352
introduction to theoretical computer science
2 A machine with alphabet Œ£ can have at most |Œ£|ùëá
choices for the contents of the first ùëálocations of
its tape. What happens if the machine repeats a
previously seen configuration, in the sense that the
tape contents, the head location, and the current state,
are all identical to what they were in some previous
state of the execution?
deeply care about and conjectured that could be
computed.
‚Ä¢ If the Church-Turing thesis holds then a function
ùêπthat is uncomputable according to our definition
cannot be computed by any means in our physical
world.
9.6 EXERCISES
Exercise 9.1 ‚Äî NAND-RAM Halt. Let NANDRAMHALT ‚à∂{0, 1}‚àó‚Üí{0, 1}
be the function such that on input (ùëÉ, ùë•) where ùëÉrepresents a NAND-
RAM program, NANDRAMHALT(ùëÉ, ùë•) = 1 iff ùëÉhalts on the input ùë•.
Prove that NANDRAMHALT is uncomputable.
‚ñ†
Exercise 9.2 ‚Äî Timed halting. Let TIMEDHALT ‚à∂{0, 1}‚àó‚Üí{0, 1} be
the function that on input (a string representing) a triple (ùëÄ, ùë•, ùëá),
TIMEDHALT(ùëÄ, ùë•, ùëá) = 1 iff the Turing machine ùëÄ, on input ùë•,
halts within at most ùëásteps (where a step is defined as one sequence
of reading a symbol from the tape, updating the state, writing a new
symbol and (potentially) moving the head).
Prove that TIMEDHALT is computable.
‚ñ†
Exercise 9.3 ‚Äî Space halting (challenging). Let SPACEHALT ‚à∂{0, 1}‚àó‚Üí
{0, 1} be the function that on input (a string representing) a triple
(ùëÄ, ùë•, ùëá), SPACEHALT(ùëÄ, ùë•, ùëá) = 1 iff the Turing machine ùëÄ, on
input ùë•, halts before its head reached the ùëá-th location of its tape. (We
don‚Äôt care how many steps ùëÄmakes, as long as the head stays inside
locations {0, ‚Ä¶ , ùëá‚àí1}.)
Prove that SPACEHALT is computable. See footnote for hint2
‚ñ†
Exercise 9.4 ‚Äî Computable compositions. Suppose that ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}
and ùê∫‚à∂{0, 1}‚àó‚Üí{0, 1} are computable functions. For each one of the
following functions ùêª, either prove that ùêªis necessarily computable or
give an example of a pair ùêπand ùê∫of computable functions such that
ùêªwill not be computable. Prove your assertions.
1. ùêª(ùë•) = 1 iff ùêπ(ùë•) = 1 OR ùê∫(ùë•) = 1.
2. ùêª(ùë•) = 1 iff there exist two nonempty strings ùë¢, ùë£‚àà{0, 1}‚àósuch
that ùë•= ùë¢ùë£(i.e., ùë•is the concatenation of ùë¢and ùë£), ùêπ(ùë¢) = 1 and
ùê∫(ùë£) = 1.
3. ùêª(ùë•) = 1 iff there exist a list ùë¢0, ‚Ä¶ , ùë¢ùë°‚àí1 of non empty strings such
that stringsùêπ(ùë¢ùëñ) = 1 for every ùëñ‚àà[ùë°] and ùë•= ùë¢0ùë¢1 ‚ãØùë¢ùë°‚àí1.


--- Page 353 ---

universality and uncomputability
353
3 Hint: You can use Rice‚Äôs Theorem.
4 Hint: While it cannot be applied directly, with a
little ‚Äúmassaging‚Äù you can prove this using Rice‚Äôs
Theorem.
4. ùêª(ùë•) = 1 iff ùë•is a valid string representation of a NAND++
program ùëÉsuch that for every ùëß‚àà{0, 1}‚àó, on input ùëßthe program
ùëÉoutputs ùêπ(ùëß).
5. ùêª(ùë•) = 1 iff ùë•is a valid string representation of a NAND++
program ùëÉsuch that on input ùë•the program ùëÉoutputs ùêπ(ùë•).
6. ùêª(ùë•) = 1 iff ùë•is a valid string representation of a NAND++
program ùëÉsuch that on input ùë•, ùëÉoutputs ùêπ(ùë•) after executing at
most 100 ‚ãÖ|ùë•|2 lines.
‚ñ†
Exercise 9.5 Prove that the following function FINITE ‚à∂{0, 1}‚àó‚Üí{0, 1}
is uncomputable. On input ùëÉ‚àà{0, 1}‚àó, we define FINITE(ùëÉ) = 1
if and only if ùëÉis a string that represents a NAND++ program such
that there only a finite number of inputs ùë•‚àà{0, 1}‚àós.t. ùëÉ(ùë•) = 1.3
‚ñ†
Exercise 9.6 ‚Äî Computing parity. Prove Theorem 9.13 without using Rice‚Äôs
Theorem.
‚ñ†
Exercise 9.7 ‚Äî TM Equivalence. Let EQ ‚à∂{0, 1}‚àó‚à∂‚Üí{0, 1} be the func-
tion defined as follows: given a string representing a pair (ùëÄ, ùëÄ‚Ä≤)
of Turing machines, EQ(ùëÄ, ùëÄ‚Ä≤) = 1 iff ùëÄand ùëÄ‚Ä≤ are functionally
equivalent as per Definition 9.14. Prove that EQ is uncomputable.
Note that you cannot use Rice‚Äôs Theorem directly, as this theorem
only deals with functions that take a single Turing machine as input,
and EQ takes two machines.
‚ñ†
Exercise 9.8 For each of the following two functions, say whether it is
computable or not:
1. Given a NAND-TM program ùëÉ, an input ùë•, and a number ùëò, when
we run ùëÉon ùë•, does the index variable i ever reach ùëò?
2. Given a NAND-TM program ùëÉ, an input ùë•, and a number ùëò, when
we run ùëÉon ùë•, does ùëÉever write to an array at index ùëò?
‚ñ†
Exercise 9.9 Let ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1} be the function that is defined as
follows. On input a string ùëÉthat represents a NAND-RAM program
and a String ùëÄthat represents a Turing machine, ùêπ(ùëÉ, ùëÄ) = 1 if and
only if there exists some input ùë•such ùëÉhalts on ùë•but ùëÄdoes not halt
on ùë•. Prove that ùêπis uncomputable. See footnote for hint.4
‚ñ†


--- Page 354 ---

354
introduction to theoretical computer science
5 HALT has this property.
6 You can either use the diagonalization method to
prove this directly or show that the set of all recur-
sively enumerable functions is countable.
7 HALT has this property: show that if both HALT
and ùêªùê¥ùêøùëáwere recursively enumerable then HALT
would be in fact computable.
8 Show that any ùê∫satisfying (b) must be semantic.
Exercise 9.10 ‚Äî Recursively enumerable. Define a function ùêπ‚à∂{0, 1}‚àó‚à∂‚Üí
{0, 1} to be recursively enumerable if there exists a Turing machine ùëÄ
such that such that for every ùë•‚àà{0, 1}‚àó, if ùêπ(ùë•) = 1 then ùëÄ(ùë•) = 1,
and if ùêπ(ùë•) = 0 then ùëÄ(ùë•) = ‚ä•. (i.e., if ùêπ(ùë•) = 0 then ùëÄdoes not halt
on ùë•.)
1. Prove that every computable ùêπis also recursively enumerable.
2. Prove that there exists ùêπthat is not computable but is recursively
enumerable. See footnote for hint.5
3. Prove that there exists a function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1} such that ùêπis
not recursively enumerable. See footnote for hint.6
4. Prove that there exists a function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1} such that
ùêπis recursively enumerable but the function ùêπdefined as ùêπ(ùë•) =
1 ‚àíùêπ(ùë•) is not recursively enumerable. See footnote for hint.7
‚ñ†
Exercise 9.11 ‚Äî Rice‚Äôs Theorem: standard form. In this exercise we will
prove Rice‚Äôs Theorem in the form that it is typically stated in the litera-
ture.
For a Turing machine ùëÄ, define ùêø(ùëÄ) ‚äÜ{0, 1}‚àóto be the set of all
ùë•‚àà{0, 1}‚àósuch that ùëÄhalts on the input ùë•and outputs 1. (The set
ùêø(ùëÄ) is known in the literature as the language recognized by ùëÄ. Note
that ùëÄmight either output a value other than 1 or not halt at all on
inputs ùë•‚àâùêø(ùëÄ). )
1. Prove that for every Turing Machine ùëÄ, if we define ùêπùëÄ‚à∂{0, 1}‚àó‚Üí
{0, 1} to be the function such that ùêπùëÄ(ùë•) = 1 iff ùë•‚ààùêø(ùëÄ) then ùêπùëÄ
is recursively enumerable as defined in Exercise 9.10.
2. Use Theorem 9.15 to prove that for every ùê∫‚à∂{0, 1}‚àó‚Üí{0, 1}, if (a)
ùê∫is neither the constant zero nor the constant one function, and
(b) for every ùëÄ, ùëÄ‚Ä≤ such that ùêø(ùëÄ) = ùêø(ùëÄ‚Ä≤), ùê∫(ùëÄ) = ùê∫(ùëÄ‚Ä≤),
then ùê∫is uncomputable. See footnote for hint.8
‚ñ†
Exercise 9.12 ‚Äî Rice‚Äôs Theorem for general Turing-equivalent models (optional).
Let ‚Ñ±be the set of all partial functions from {0, 1}‚àóto {0, 1} and ‚Ñ≥‚à∂
{0, 1}‚àó‚Üí‚Ñ±be a Turing-equivalent model as defined in Definition 8.5.
We define a function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1} to be ‚Ñ≥-semantic if there
exists some ùí¢‚à∂‚Ñ±‚Üí{0, 1} such that ùêπ(ùëÉ) = ùí¢(‚Ñ≥(ùëÉ)) for every
ùëÉ‚àà{0, 1}‚àó.
Prove that for every ‚Ñ≥-semantic ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1} that is neither
the constant one nor the constant zero function, ùêπis uncomputable.
‚ñ†


--- Page 355 ---

universality and uncomputability
355
9 You will not need to use very specific properties
of the TOWER function in this exercise. For exam-
ple, NBB(ùëõ) also grows faster than the Ackerman
function.
Exercise 9.13 ‚Äî Busy Beaver. In this question we define the NAND-
TM variant of the busy beaver function (see Aaronson‚Äôs 1999 essay,
2017 blog post and 2020 survey [Aar20]; see also Tao‚Äôs highly recom-
mended presentation on how civilization‚Äôs scientific progress can be
measured by the quantities we can grasp).
1. Let ùëáùêµùêµ‚à∂{0, 1}‚àó‚Üí‚Ñïbe defined as follows. For every string ùëÉ‚àà
{0, 1}‚àó, if ùëÉrepresents a NAND-TM program such that when ùëÉis
executed on the input 0 then it halts within ùëÄsteps then ùëáùêµùêµ(ùëÉ) =
ùëÄ. Otherwise (if ùëÉdoes not represent a NAND-TM program, or it
is a program that does not halt on 0), ùëáùêµùêµ(ùëÉ) = 0. Prove that ùëáùêµùêµ
is uncomputable.
2. Let TOWER(ùëõ) denote the number 2‚ãÖ‚ãÖ‚ãÖ2
‚èü
ùëõtimes
(that is, a ‚Äútower of pow-
ers of two‚Äù of height ùëõ). To get a sense of how fast this function
grows, TOWER(1) = 2, TOWER(2) = 22 = 4, TOWER(3) = 222 =
16, TOWER(4) = 216 = 65536 and TOWER(5) = 265536 which
is about 1020000. TOWER(6) is already a number that is too big to
write even in scientific notation. Define NBB ‚à∂‚Ñï‚Üí‚Ñï(for ‚ÄúNAND-
TM Busy Beaver‚Äù) to be the function NBB(ùëõ) = maxùëÉ‚àà{0,1}ùëõùëáùêµùêµ(ùëÉ)
where ùëáùêµùêµis as defined in Question 6.1. Prove that NBB grows
faster than TOWER, in the sense that TOWER(ùëõ) = ùëú(NBB(ùëõ)). See
footnote for hint9
‚ñ†
9.7 BIBLIOGRAPHICAL NOTES
The cartoon of the Halting problem in Fig. 9.1 and taken from Charles
Cooper‚Äôs website.
Section 7.2 in [MM11] gives a highly recommended overview of
uncomputability. G√∂del, Escher, Bach [Hof99] is a classic popular
science book that touches on uncomputability, and unprovability, and
specifically G√∂del‚Äôs Theorem that we will see in Chapter 11. See also
the recent book by Holt [Hol18].
The history of the definition of a function is intertwined with the
development of mathematics as a field. For many years, a function
was identified (as per Euler‚Äôs quote above) with the means to calcu-
late the output from the input. In the 1800‚Äôs, with the invention of
the Fourier series and with the systematic study of continuity and
differentiability, people have started looking at more general kinds of
functions, but the modern definition of a function as an arbitrary map-
ping was not yet universally accepted. For example, in 1899 Poincare
wrote ‚Äúwe have seen a mass of bizarre functions which appear to be forced
to resemble as little as possible honest functions which serve some purpose.


--- Page 356 ---

356
introduction to theoretical computer science
‚Ä¶ they are invented on purpose to show that our ancestor‚Äôs reasoning was at
fault, and we shall never get anything more than that out of them‚Äù. Some of
this fascinating history is discussed in [Gra83; Kle91; L√ºt02; Gra05].
The existence of a universal Turing machine, and the uncomputabil-
ity of HALT was first shown by Turing in his seminal paper [Tur37],
though closely related results were shown by Church a year before.
These works built on G√∂del‚Äôs 1931 incompleteness theorem that we will
discuss in Chapter 11.
Some universal Turing Machines with a small alphabet and number
of states are given in [Rog96], including a single-tape universal Turing
machine with the binary alphabet and with less than 25 states; see
also the survey [WN09]. Adam Yedidia has written software to help
in producing Turing machines with a small number of states. This is
related to the recreational pastime of ‚ÄúCode Golfing‚Äù which is about
solving a certain computational task using the as short as possible
program. Finding ‚Äúhighly complex‚Äù small Turing machine is also
related to the ‚ÄúBusy Beaver‚Äù problem, see Exercise 9.13 and the survey
[Aar20].
The diagonalization argument used to prove uncomputability of ùêπ‚àó
is derived from Cantor‚Äôs argument for the uncountability of the reals
discussed in Chapter 2.
Christopher Strachey was an English computer scientist and the
inventor of the CPL programming language. He was also an early
artificial intelligence visionary, programming a computer to play
Checkers and even write love letters in the early 1950‚Äôs, see this New
Yorker article and this website.
Rice‚Äôs Theorem was proven in [Ric53]. It is typically stated in a
form somewhat different than what we used, see Exercise 9.11.
We do not discuss in the chapter the concept of recursively enumer-
able languages, but it is covered briefly in Exercise 9.10. As usual, we
use function, as opposed to language, notation.
The cartoon of the Halting problem in Fig. 9.1 is copyright 2019
Charles F. Cooper.


--- Page 357 ---

10
Restricted computational models
‚ÄúHappy families are all alike; every unhappy family is unhappy in its own
way‚Äù, Leo Tolstoy (opening of the book ‚ÄúAnna Karenina‚Äù).
We have seen that many models of computation are Turing equiva-
lent, including Turing machines, NAND-TM/NAND-RAM programs,
standard programming languages such as C/Python/Javascript, as
well as other models such as the ùúÜcalculus and even the game of life.
The flip side of this is that for all these models, Rice‚Äôs theorem (The-
orem 9.15) holds as well, which means that any semantic property of
programs in such a model is uncomputable.
The uncomputability of halting and other semantic specification
problems for Turing equivalent models motivates restricted com-
putational models that are (a) powerful enough to capture a set of
functions useful for certain applications but (b) weak enough that we
can still solve semantic specification problems on them. In this chapter
we discuss several such examples.
ÔÉ´Big Idea 14 We can use restricted computational models to bypass
limitations such as uncomputability of the Halting problem and Rice‚Äôs
Theorem. Such models can compute only a restricted subclass of
functions, but allow to answer at least some semantic questions on
programs.
10.1 TURING COMPLETENESS AS A BUG
We have seen that seemingly simple computational models or sys-
tems can turn out to be Turing complete. The following webpage lists
several examples of formalisms that ‚Äúaccidentally‚Äù turned out to Tur-
ing complete, including supposedly limited languages such as the C
preprocessor, CSS, (certain variants of) SQL, sendmail configuration,
as well as games such as Minecraft, Super Mario, and the card game
Compiled on 8.26.2020 18:10
Learning Objectives:
‚Ä¢ See that Turing completeness is not always a
good thing.
‚Ä¢ Another example of an always-halting
formalism: context-free grammars and simply
typed ùúÜcalculus.
‚Ä¢ The pumping lemma for non context-free
functions.
‚Ä¢ Examples of computable and uncomputable
semantic properties of regular expressions and
context-free grammars.


--- Page 358 ---

358
introduction to theoretical computer science
Figure 10.1: Some restricted computational models.
We have already seen two equivalent restricted
models of computation: regular expressions and
deterministic finite automata. We show a more
powerful model: context-free grammars. We also
present tools to demonstrate that some functions can
not be computed in these models.
‚ÄúMagic: The Gathering‚Äù. Turing completeness is not always a good
thing, as it means that such formalisms can give rise to arbitrarily
complex behavior. For example, the postscript format (a precursor of
PDF) is a Turing-complete programming language meant to describe
documents for printing. The expressive power of postscript can allow
for short descriptions of very complex images, but it also gave rise to
some nasty surprises, such as the attacks described in this page rang-
ing from using infinite loops as a denial of service attack, to accessing
the printer‚Äôs file system.
‚ñ†Example 10.1 ‚Äî The DAO Hack. An interesting recent example of the
pitfalls of Turing-completeness arose in the context of the cryp-
tocurrency Ethereum. The distinguishing feature of this currency
is the ability to design ‚Äúsmart contracts‚Äù using an expressive (and
in particular Turing-complete) programming language. In our
current ‚Äúhuman operated‚Äù economy, Alice and Bob might sign a
contract to agree that if condition X happens then they will jointly
invest in Charlie‚Äôs company. Ethereum allows Alice and Bob to
create a joint venture where Alice and Bob pool their funds to-
gether into an account that will be governed by some program ùëÉ
that decides under what conditions it disburses funds from it. For
example, one could imagine a piece of code that interacts between
Alice, Bob, and some program running on Bob‚Äôs car that allows
Alice to rent out Bob‚Äôs car without any human intervention or
overhead.
Specifically Ethereum uses the Turing-complete programming
language solidity which has a syntax similar to JavaScript. The
flagship of Ethereum was an experiment known as The ‚ÄúDecen-
tralized Autonomous Organization‚Äù or The DAO. The idea was
to create a smart contract that would create an autonomously run
decentralized venture capital fund, without human managers,
where shareholders could decide on investment opportunities. The


--- Page 359 ---

restricted computational models
359
DAO was at the time the biggest crowdfunding success in history.
At its height the DAO was worth 150 million dollars, which was
more than ten percent of the total Ethereum market. Investing in
the DAO (or entering any other ‚Äúsmart contract‚Äù) amounts to pro-
viding your funds to be run by a computer program. i.e., ‚Äúcode
is law‚Äù, or to use the words the DAO described itself: ‚ÄúThe DAO
is borne from immutable, unstoppable, and irrefutable computer code‚Äù.
Unfortunately, it turns out that (as we saw in Chapter 9) under-
standing the behavior of computer programs is quite a hard thing
to do. A hacker (or perhaps, some would say, a savvy investor)
was able to fashion an input that caused the DAO code to enter
into an infinite recursive loop in which it continuously transferred
funds into the hacker‚Äôs account, thereby cleaning out about 60 mil-
lion dollars out of the DAO. While this transaction was ‚Äúlegal‚Äù in
the sense that it complied with the code of the smart contract, it
was obviously not what the humans who wrote this code had in
mind. The Ethereum community struggled with the response to
this attack. Some tried the ‚ÄúRobin Hood‚Äù approach of using the
same loophole to drain the DAO funds into a secure account, but
it only had limited success. Eventually, the Ethereum community
decided that the code can be mutable, stoppable, and refutable.
Specifically, the Ethereum maintainers and miners agreed on a
‚Äúhard fork‚Äù (also known as a ‚Äúbailout‚Äù) to revert history to be-
fore the hacker‚Äôs transaction occurred. Some community members
strongly opposed this decision, and so an alternative currency
called Ethereum Classic was created that preserved the original
history.
10.2 CONTEXT FREE GRAMMARS
If you have ever written a program, you‚Äôve experienced a syntax error.
You probably also had the experience of your program entering into
an infinite loop. What is less likely is that the compiler or interpreter
entered an infinite loop while trying to figure out if your program has
a syntax error.
When a person designs a programming language, they need to
determine its syntax. That is, the designer decides which strings corre-
sponds to valid programs, and which ones do not (i.e., which strings
contain a syntax error). To ensure that a compiler or interpreter al-
ways halts when checking for syntax errors, language designers typi-
cally do not use a general Turing-complete mechanism to express their
syntax. Rather they use a restricted computational model. One of the
most popular choices for such models is context free grammars.


--- Page 360 ---

360
introduction to theoretical computer science
To explain context free grammars, let us begin with a canonical ex-
ample. Consider the function ARITH ‚à∂Œ£‚àó‚Üí{0, 1} that takes as input
a string ùë•over the alphabet Œ£ = {(, ), +, ‚àí, √ó, √∑, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9}
and returns 1 if and only if the string ùë•represents a valid arithmetic
expression. Intuitively, we build expressions by applying an opera-
tion such as +,‚àí,√ó or √∑ to smaller expressions, or enclosing them in
parenthesis, where the ‚Äúbase case‚Äù corresponds to expressions that are
simply numbers. More precisely, we can make the following defini-
tions:
‚Ä¢ A digit is one of the symbols 0, 1, 2, 3, 4, 5, 6, 7, 8, 9.
‚Ä¢ A number is a sequence of digits. (For simplicity we drop the condi-
tion that the sequence does not have a leading zero, though it is not
hard to encode it in a context-free grammar as well.)
‚Ä¢ An operation is one of +, ‚àí, √ó, √∑
‚Ä¢ An expression has either the form ‚Äúnumber‚Äù, the form ‚Äúsub-
expression1 operation sub-expression2‚Äù, or the form ‚Äú(sub-
expression1)‚Äù, where ‚Äúsub-expression1‚Äù and ‚Äúsub-expression2‚Äù are
themselves expressions. (Note that this is a recursive definition.)
A context free grammar (CFG) is a formal way of specifying such
conditions. A CFG consists of a set of rules that tell us how to generate
strings from smaller components. In the above example, one of the
rules is ‚Äúif ùëíùë•ùëù1 and ùëíùë•ùëù2 are valid expressions, then ùëíùë•ùëù1 √ó ùëíùë•ùëù2 is
also a valid expression‚Äù; we can also write this rule using the short-
hand ùëíùë•ùëùùëüùëíùë†ùë†ùëñùëúùëõ‚áíùëíùë•ùëùùëüùëíùë†ùë†ùëñùëúùëõ√ó ùëíùë•ùëùùëüùëíùë†ùë†ùëñùëúùëõ. As in the above ex-
ample, the rules of a context-free grammar are often recursive: the rule
ùëíùë•ùëùùëüùëíùë†ùë†ùëñùëúùëõ‚áíùëíùë•ùëùùëüùëíùë†ùë†ùëñùëúùëõ√ó ùëíùë•ùëùùëüùëíùë†ùë†ùëñùëúùëõdefines valid expressions in
terms of itself. We now formally define context-free grammars:
Definition 10.2 ‚Äî Context Free Grammar. Let Œ£ be some finite set. A
context free grammar (CFG) over Œ£ is a triple (ùëâ, ùëÖ, ùë†) such that:
‚Ä¢ ùëâ, known as the variables, is a set disjoint from Œ£.
‚Ä¢ ùë†‚ààùëâis known as the initial variable.
‚Ä¢ ùëÖis a set of rules. Each rule is a pair (ùë£, ùëß) with ùë£
‚àà
ùëâand
ùëß‚àà(Œ£ ‚à™ùëâ)‚àó. We often write the rule (ùë£, ùëß) as ùë£‚áíùëßand say that
the string ùëßcan be derived from the variable ùë£.


--- Page 361 ---

restricted computational models
361
‚ñ†Example 10.3 ‚Äî Context free grammar for arithmetic expressions. The
example above of well-formed arithmetic expressions can be cap-
tured formally by the following context free grammar:
‚Ä¢ The alphabet Œ£ is {(, ), +, ‚àí, √ó, √∑, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9}
‚Ä¢ The variables are ùëâ= {ùëíùë•ùëùùëüùëíùë†ùë†ùëñùëúùëõ, ùëõùë¢ùëöùëèùëíùëü, ùëëùëñùëîùëñùë°, ùëúùëùùëíùëüùëéùë°ùëñùëúùëõ}.
‚Ä¢ The rules are the set ùëÖcontaining the following 19 rules:
‚Äì The 4 rules ùëúùëùùëíùëüùëéùë°ùëñùëúùëõ‚áí+, ùëúùëùùëíùëüùëéùë°ùëñùëúùëõ‚áí‚àí, ùëúùëùùëíùëüùëéùë°ùëñùëúùëõ‚áí√ó,
and ùëúùëùùëíùëüùëéùë°ùëñùëúùëõ‚áí√∑.
‚Äì The 10 rules ùëëùëñùëîùëñùë°‚áí0,‚Ä¶, ùëëùëñùëîùëñùë°‚áí9.
‚Äì The rule ùëõùë¢ùëöùëèùëíùëü‚áíùëëùëñùëîùëñùë°.
‚Äì The rule ùëõùë¢ùëöùëèùëíùëü‚áíùëëùëñùëîùëñùë°ùëõùë¢ùëöùëèùëíùëü.
‚Äì The rule ùëíùë•ùëùùëüùëíùë†ùë†ùëñùëúùëõ‚áíùëõùë¢ùëöùëèùëíùëü.
‚Äì The rule ùëíùë•ùëùùëüùëíùë†ùë†ùëñùëúùëõ‚áíùëíùë•ùëùùëüùëíùë†ùë†ùëñùëúùëõùëúùëùùëíùëüùëéùë°ùëñùëúùëõùëíùë•ùëùùëüùëíùë†ùë†ùëñùëúùëõ.
‚Äì The rule ùëíùë•ùëùùëüùëíùë†ùë†ùëñùëúùëõ‚áí(ùëíùë•ùëùùëüùëíùë†ùë†ùëñùëúùëõ).
‚Ä¢ The starting variable is ùëíùë•ùëùùëüùëíùë†ùë†ùëñùëúùëõ
People use many different notations to write context free grammars.
One of the most common notations is the Backus‚ÄìNaur form. In this
notation we write a rule of the form ùë£‚áíùëé(where ùë£is a variable and ùëé
is a string) in the form <v> := a. If we have several rules of the form
ùë£‚Ü¶ùëé, ùë£‚Ü¶ùëè, and ùë£‚Ü¶ùëêthen we can combine them as <v> := a|b|c.
(In words we say that ùë£can derive either ùëé, ùëè, or ùëê.) For example, the
Backus-Naur description for the context free grammar of Example 10.3
is the following (using ASCII equivalents for operations):
operation
:= +|-|*|/
digit
:= 0|1|2|3|4|5|6|7|8|9
number
:= digit|digit number
expression := number|expression operation
expression|(expression)
‚Ü™
Another example of a context free grammar is the ‚Äúmatching paren-
thesis‚Äù grammar, which can be represented in Backus-Naur as fol-
lows:
match
:= ""|match match|(match)
A string over the alphabet { (,) } can be generated from this gram-
mar (where match is the starting expression and "" corresponds to the
empty string) if and only if it consists of a matching set of parenthesis.


--- Page 362 ---

362
introduction to theoretical computer science
1 As in the case of Definition 6.7 we can also use
language rather than function notation and say that a
language ùêø‚äÜŒ£‚àóis context free if the function ùêπsuch
that ùêπ(ùë•) = 1 iff ùë•‚ààùêøis context free.
In contrast, by Lemma 6.19 there is no regular expression that matches
a string ùë•if and only if ùë•contains a valid sequence of matching paren-
thesis.
10.2.1 Context-free grammars as a computational model
We can think of a context-free grammar over the alphabet Œ£ as defin-
ing a function that maps every string ùë•in Œ£‚àóto 1 or 0 depending on
whether ùë•can be generated by the rules of the grammars. We now
make this definition formally.
Definition 10.4 ‚Äî Deriving a string from a grammar. If ùê∫
=
(ùëâ, ùëÖ, ùë†) is a
context-free grammar over Œ£, then for two strings ùõº, ùõΩ‚àà(Œ£ ‚à™ùëâ)‚àó
we say that ùõΩcan be derived in one step from ùõº, denoted by ùõº‚áíùê∫ùõΩ,
if we can obtain ùõΩfrom ùõºby applying one of the rules of ùê∫. That is,
we obtain ùõΩby replacing in ùõºone occurrence of the variable ùë£with
the string ùëß, where ùë£‚áíùëßis a rule of ùê∫.
We say that ùõΩcan be derived from ùõº, denoted by ùõº
‚áí‚àó
ùê∫
ùõΩ, if it
can be derived by some finite number ùëòof steps. That is, if there
are ùõº1, ‚Ä¶ , ùõºùëò‚àí1 ‚àà(Œ£ ‚à™ùëâ)‚àó, so that ùõº‚áíùê∫ùõº1 ‚áíùê∫ùõº2 ‚áíùê∫‚ãØ‚áíùê∫
ùõºùëò‚àí1 ‚áíùê∫ùõΩ.
We say that ùë•‚ààŒ£‚àóis matched by ùê∫= (ùëâ, ùëÖ, ùë†) if ùë•can be de-
rived from the starting variable ùë†(i.e., if ùë†
‚áí‚àó
ùê∫
ùë•). We define the
function computed by (ùëâ, ùëÖ, ùë†) to be the map Œ¶ùëâ,ùëÖ,ùë†‚à∂Œ£‚àó‚Üí{0, 1}
such that Œ¶ùëâ,ùëÖ,ùë†(ùë•)
=
1 iff ùë•is matched by (ùëâ, ùëÖ, ùë†). A function
ùêπ‚à∂Œ£‚àó‚Üí{0, 1} is context free if ùêπ= Œ¶ùëâ,ùëÖ,ùë†for some CFG (ùëâ, ùëÖ, ùë†).
1
A priori it might not be clear that the map Œ¶ùëâ,ùëÖ,ùë†is computable,
but it turns out that this is the case.
Theorem 10.5 ‚Äî Context-free grammars always halt. For every CFG
(ùëâ, ùëÖ, ùë†) over {0, 1}, the function Œ¶ùëâ,ùëÖ,ùë†
‚à∂
{0, 1}‚àó
‚Üí
{0, 1} is
computable.
As usual we restrict attention to grammars over {0, 1} although the
proof extends to any finite alphabet Œ£.
Proof. We only sketch the proof. We start with the observation we can
convert every CFG to an equivalent version of Chomsky normal form,
where all rules either have the form ùë¢‚Üíùë£ùë§for variables ùë¢, ùë£, ùë§or the
form ùë¢‚Üíùúéfor a variable ùë¢and symbol ùúé‚ààŒ£, plus potentially the
rule ùë†‚Üí"" where ùë†is the starting variable.
The idea behind such a transformation is to simply add new vari-
ables as needed, and so for example we can translate a rule such as
ùë£‚Üíùë¢ùúéùë§into the three rules ùë£‚Üíùë¢ùëü, ùëü‚Üíùë°ùë§and ùë°‚Üíùúé.


--- Page 363 ---

restricted computational models
363
Using the Chomsky Normal form we get a natural recursive algo-
rithm for computing whether ùë†‚áí‚àó
ùê∫ùë•for a given grammar ùê∫and
string ùë•. We simply try all possible guesses for the first rule ùë†‚Üíùë¢ùë£
that is used in such a derivation, and then all possible ways to par-
tition ùë•as a concatenation ùë•= ùë•‚Ä≤ùë•‚Ä≥. If we guessed the rule and the
partition correctly, then this reduces our task to checking whether
ùë¢‚áí‚àó
ùê∫ùë•‚Ä≤ and ùë£‚áí‚àó
ùê∫ùë•‚Ä≥, which (as it involves shorter strings) can
be done recursively. The base cases are when ùë•is empty or a single
symbol, and can be easily handled.
‚ñ†
R
Remark 10.6 ‚Äî Parse trees. While we focus on the
task of deciding whether a CFG matches a string, the
algorithm to compute Œ¶ùëâ,ùëÖ,ùë†actually gives more in-
formation than that. That is, on input a string ùë•, if
Œ¶ùëâ,ùëÖ,ùë†(ùë•) = 1 then the algorithm yields the sequence
of rules that one can apply from the starting vertex ùë†
to obtain the final string ùë•. We can think of these rules
as determining a tree with ùë†being the root vertex and
the sinks (or leaves) corresponding to the substrings
of ùë•that are obtained by the rules that do not have a
variable in their second element. This tree is known
as the parse tree of ùë•, and often yields very useful
information about the structure of ùë•.
Often the first step in a compiler or interpreter for a
programming language is a parser that transforms the
source into the parse tree (also known as the abstract
syntax tree). There are also tools that can automati-
cally convert a description of a context-free grammars
into a parser algorithm that computes the parse tree of
a given string. (Indeed, the above recursive algorithm
can be used to achieve this, but there are much more
efficient versions, especially for grammars that have
particular forms, and programming language design-
ers often try to ensure their languages have these more
efficient grammars.)
10.2.2 The power of context free grammars
Context free grammars can capture every regular expression:
Theorem 10.7 ‚Äî Context free grammars and regular expressions. Let ùëíbe a
regular expression over {0, 1}, then there is a CFG (ùëâ, ùëÖ, ùë†) over
{0, 1} such that Œ¶ùëâ,ùëÖ,ùë†= Œ¶ùëí.
Proof. We prove the theorem by induction on the length of ùëí. If ùëíis
an expression of one bit length, then ùëí= 0 or ùëí= 1, in which case
we leave it to the reader to verify that there is a (trivial) CFG that


--- Page 364 ---

364
introduction to theoretical computer science
computes it. Otherwise, we fall into one of the following case: case
1: ùëí= ùëí‚Ä≤ùëí‚Ä≥, case 2: ùëí= ùëí‚Ä≤|ùëí‚Ä≥ or case 3: ùëí= (ùëí‚Ä≤)‚àówhere in all cases
ùëí‚Ä≤, ùëí‚Ä≥ are shorter regular expressions. By the induction hypothesis
have grammars (ùëâ‚Ä≤, ùëÖ‚Ä≤, ùë†‚Ä≤) and (ùëâ‚Ä≥, ùëÖ‚Ä≥, ùë†‚Ä≥) that compute Œ¶ùëí‚Ä≤ and Œ¶ùëí‚Ä≥
respectively. By renaming of variables, we can also assume without
loss of generality that ùëâ‚Ä≤ and ùëâ‚Ä≥ are disjoint.
In case 1, we can define the new grammar as follows: we add a new
starting variable ùë†‚àâùëâ‚à™ùëâ‚Ä≤ and the rule ùë†‚Ü¶ùë†‚Ä≤ùë†‚Ä≥. In case 2, we can
define the new grammar as follows: we add a new starting variable
ùë†‚àâùëâ‚à™ùëâ‚Ä≤ and the rules ùë†‚Ü¶ùë†‚Ä≤ and ùë†‚Ü¶ùë†‚Ä≥. Case 3 will be the
only one that uses recursion. As before we add a new starting variable
ùë†‚àâùëâ‚à™ùëâ‚Ä≤, but now add the rules ùë†‚Ü¶"" (i.e., the empty string) and
also add, for every rule of the form (ùë†‚Ä≤, ùõº) ‚ààùëÖ‚Ä≤, the rule ùë†‚Ü¶ùë†ùõºto ùëÖ.
We leave it to the reader as (a very good!) exercise to verify that in
all three cases the grammars we produce capture the same function as
the original expression.
‚ñ†
It turns out that CFG‚Äôs are strictly more powerful than regular
expressions. In particular, as we‚Äôve seen, the ‚Äúmatching parenthesis‚Äù
function MATCHPAREN can be computed by a context free grammar,
whereas, as shown in Lemma 6.19, it cannot be computed by regular
expressions. Here is another example:
Solved Exercise 10.1 ‚Äî Context free grammar for palindromes. Let PAL ‚à∂
{0, 1, ; }‚àó‚Üí{0, 1} be the function defined in Solved Exercise 6.4 where
PAL(ùë§) = 1 iff ùë§has the form ùë¢; ùë¢ùëÖ. Then PAL can be computed by a
context-free grammar
‚ñ†
Solution:
A simple grammar computing PAL can be described using
Backus‚ÄìNaur notation:
start
:= ; | 0 start 0 | 1 start 1
One can prove by induction that this grammar generates exactly
the strings ùë§such that PAL(ùë§) = 1.
‚ñ†
A more interesting example is computing the strings of the form
ùë¢; ùë£that are not palindromes:
Solved Exercise 10.2 ‚Äî Non palindromes. Prove that there is a context free
grammar that computes NPAL ‚à∂{0, 1, ; }‚àó‚Üí{0, 1} where NPAL(ùë§) =
1 if ùë§= ùë¢; ùë£but ùë£‚â†ùë¢ùëÖ.
‚ñ†


--- Page 365 ---

restricted computational models
365
Solution:
Using Backus‚ÄìNaur notation we can describe such a grammar as
follows
palindrome
:= ; | 0 palindrome 0 | 1 palindrome 1
different
:= 0 palindrome 1 | 1 palindrome 0
start
:= different | 0 start | 1 start | start
0 | start 1
‚Ü™
In words, this means that we can characterize a string ùë§such
that NPAL(ùë§) = 1 as having the following form
ùë§= ùõºùëèùë¢; ùë¢ùëÖùëè‚Ä≤ùõΩ
(10.1)
where ùõº, ùõΩ, ùë¢are arbitrary strings and ùëè
‚â†
ùëè‚Ä≤. Hence we can
generate such a string by first generating a palindrome ùë¢; ùë¢ùëÖ
(palindrome variable), then adding 0 on either the left or right and
1 on the opposite side to get something that is not a palindrome
(different variable), and then we can add arbitrary number of 0‚Äôs
and 1‚Äôs on either end (the start variable).
‚ñ†
10.2.3 Limitations of context-free grammars (optional)
Even though context-free grammars are more powerful than regular
expressions, there are some simple languages that are not captured
by context free grammars. One tool to show this is the context-free
grammar analog of the ‚Äúpumping lemma‚Äù (Theorem 6.20):
Theorem 10.8 ‚Äî Context-free pumping lemma. Let (ùëâ, ùëÖ, ùë†) be a CFG
over Œ£, then there is some numbers ùëõ0, ùëõ1
‚àà‚Ñïsuch that for every
ùë•‚ààŒ£‚àówith |ùë•| > ùëõ0, if Œ¶ùëâ,ùëÖ,ùë†(ùë•) = 1 then ùë•= ùëéùëèùëêùëëùëísuch that
|ùëè| + |ùëê| + |ùëë| ‚â§ùëõ1, |ùëè| + |ùëë| ‚â•1, and Œ¶ùëâ,ùëÖ,ùë†(ùëéùëèùëòùëêùëëùëòùëí) = 1 for every
ùëò‚àà‚Ñï.
P
The context-free pumping lemma is even more cum-
bersome to state than its regular analog, but you can
remember it as saying the following: ‚ÄúIf a long enough
string is matched by a grammar, there must be a variable
that is repeated in the derivation.‚Äù
Proof of Theorem 10.8. We only sketch the proof. The idea is that if
the total number of symbols in the rules of the grammar is ùëò0, then
the only way to get |ùë•| > ùëõ0 with Œ¶ùëâ,ùëÖ,ùë†(ùë•) = 1 is to use recursion.
That is, there must be some variable ùë£‚ààùëâsuch that we are able to


--- Page 366 ---

366
introduction to theoretical computer science
derive from ùë£the value ùëèùë£ùëëfor some strings ùëè, ùëë‚ààŒ£‚àó, and then further
on derive from ùë£some string ùëê‚ààŒ£‚àósuch that ùëèùëêùëëis a substring of
ùë•(in other words, ùë•= ùëéùëèùëêùëëùëífor some ùëé, ùëí‚àà{0, 1}‚àó). If we take
the variable ùë£satisfying this requirement with a minimum number
of derivation steps, then we can ensure that |ùëèùëêùëë| is at most some
constant depending on ùëõ0 and we can set ùëõ1 to be that constant (ùëõ1 =
10 ‚ãÖ|ùëÖ| ‚ãÖùëõ0 will do, since we will not need more than |ùëÖ| applications
of rules, and each such application can grow the string by at most ùëõ0
symbols).
Thus by the definition of the grammar, we can repeat the derivation
to replace the substring ùëèùëêùëëin ùë•with ùëèùëòùëêùëëùëòfor every ùëò‚àà‚Ñïwhile
retaining the property that the output of Œ¶ùëâ,ùëÖ,ùë†is still one. Since ùëèùëêùëë
is a substring of ùë•, we can write ùë•= ùëéùëèùëêùëëùëíand are guaranteed that
ùëéùëèùëòùëêùëëùëòùëíis matched by the grammar for every ùëò.
‚ñ†
Using Theorem 10.8 one can show that even the simple function
ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1} defined as follows:
ùêπ(ùë•) =
‚éß
{
‚é®
{
‚é©
1
ùë•= ùë§ùë§for some ùë§‚àà{0, 1}‚àó
0
otherwise
(10.2)
is not context free. (In contrast, the function ùê∫‚à∂{0, 1}‚àó‚Üí{0, 1}
defined as ùê∫(ùë•) = 1 iff ùë•= ùë§0ùë§1 ‚ãØùë§ùëõ‚àí1ùë§ùëõ‚àí1ùë§ùëõ‚àí2 ‚ãØùë§0 for some
ùë§‚àà{0, 1}‚àóand ùëõ= |ùë§| is context free, can you see why?.)
Solved Exercise 10.3 ‚Äî Equality is not context-free. Let EQ ‚à∂{0, 1, ; }‚àó‚Üí
{0, 1} be the function such that EQ(ùë•) = 1 if and only if ùë•= ùë¢; ùë¢for
some ùë¢‚àà{0, 1}‚àó. Then EQ is not context free.
‚ñ†
Solution:
We use the context-free pumping lemma. Suppose towards the
sake of contradiction that there is a grammar ùê∫that computes EQ,
and let ùëõ0 be the constant obtained from Theorem 10.8.
Consider the string ùë•= 1ùëõ00ùëõ0; 1ùëõ00ùëõ0, and write it as ùë•= ùëéùëèùëêùëëùëí
as per Theorem 10.8, with |ùëèùëêùëë| ‚â§ùëõ0 and with |ùëè| + |ùëë| ‚â•1. By The-
orem 10.8, it should hold that EQ(ùëéùëêùëí) = 1. However, by case anal-
ysis this can be shown to be a contradiction.
Firstly, unless ùëèis on the left side of the ; separator and ùëëis on
the right side, dropping ùëèand ùëëwill definitely make the two parts
different. But if it is the case that ùëèis on the left side and ùëëis on the
right side, then by the condition that |ùëèùëêùëë| ‚â§ùëõ0 we know that ùëèis a
string of only zeros and ùëëis a string of only ones. If we drop ùëèand
ùëëthen since one of them is non empty, we get that there are either


--- Page 367 ---

restricted computational models
367
less zeroes on the left side than on the right side, or there are less
ones on the right side than on the left side. In either case, we get
that EQ(ùëéùëêùëí) = 0, obtaining the desired contradiction.
‚ñ†
10.3 SEMANTIC PROPERTIES OF CONTEXT FREE LANGUAGES
As in the case of regular expressions, the limitations of context free
grammars do provide some advantages. For example, emptiness of
context free grammars is decidable:
Theorem 10.9 ‚Äî Emptiness for CFG‚Äôs is decidable. There is an algorithm
that on input a context-free grammar ùê∫, outputs 1 if and only if Œ¶ùê∫
is the constant zero function.
Proof Idea:
The proof is easier to see if we transform the grammar to Chomsky
Normal Form as in Theorem 10.5. Given a grammar ùê∫, we can recur-
sively define a non-terminal variable ùë£to be non empty if there is either
a rule of the form ùë£‚áíùúé, or there is a rule of the form ùë£‚áíùë¢ùë§where
both ùë¢and ùë§are non empty. Then the grammar is non empty if and
only if the starting variable ùë†is non-empty.
‚ãÜ
Proof of Theorem 10.9. We assume that the grammar ùê∫in Chomsky
Normal Form as in Theorem 10.5. We consider the following proce-
dure for marking variables as ‚Äúnon empty‚Äù:
1. We start by marking all variables ùë£that are involved in a rule of the
form ùë£‚áíùúéas non empty.
2. We then continue to mark ùë£as non empty if it is involved in a rule
of the form ùë£‚áíùë¢ùë§where ùë¢, ùë§have been marked before.
We continue this way until we cannot mark any more variables. We
then declare that the grammar is empty if and only if ùë†has not been
marked. To see why this is a valid algorithm, note that if a variable ùë£
has been marked as ‚Äúnon empty‚Äù then there is some string ùõº‚ààŒ£‚àóthat
can be derived from ùë£. On the other hand, if ùë£has not been marked,
then every sequence of derivations from ùë£will always have a variable
that has not been replaced by alphabet symbols. Hence in particular
Œ¶ùê∫is the all zero function if and only if the starting variable ùë†is not
marked ‚Äúnon empty‚Äù.
‚ñ†


--- Page 368 ---

368
introduction to theoretical computer science
10.3.1 Uncomputability of context-free grammar equivalence (optional)
By analogy to regular expressions, one might have hoped to get an
algorithm for deciding whether two given context free grammars
are equivalent. Alas, no such luck. It turns out that the equivalence
problem for context free grammars is uncomputable. This is a direct
corollary of the following theorem:
Theorem 10.10 ‚Äî Fullness of CFG‚Äôs is uncomputable. For every set Œ£, let
CFGFULLŒ£ be the function that on input a context-free grammar ùê∫
over Œ£, outputs 1 if and only if ùê∫computes the constant 1 function.
Then there is some finite Œ£ such that CFGFULLŒ£ is uncomputable.
Theorem 10.10 immediately implies that equivalence for context-
free grammars is uncomputable, since computing ‚Äúfullness‚Äù of a
grammar ùê∫over some alphabet Œ£ = {ùúé0, ‚Ä¶ , ùúéùëò‚àí1} corresponds to
checking whether ùê∫is equivalent to the grammar ùë†‚áí""|ùë†ùúé0| ‚ãØ|ùë†ùúéùëò‚àí1.
Note that Theorem 10.10 and Theorem 10.9 together imply that
context-free grammars, unlike regular expressions, are not closed
under complement. (Can you see why?) Since we can encode every
element of Œ£ using ‚åàlog |Œ£|‚åâbits (and this finite encoding can be easily
carried out within a grammar) Theorem 10.10 implies that fullness is
also uncomputable for grammars over the binary alphabet.
Proof Idea:
We prove the theorem by reducing from the Halting problem. To
do that we use the notion of configurations of NAND-TM programs, as
defined in Definition 8.8. Recall that a configuration of a program ùëÉis a
binary string ùë†that encodes all the information about the program in
the current iteration.
We define Œ£ to be {0, 1} plus some separator characters and define
INVALIDùëÉ‚à∂Œ£‚àó‚Üí{0, 1} to be the function that maps every string ùêø‚àà
Œ£‚àóto 1 if and only if ùêødoes not encode a sequence of configurations
that correspond to a valid halting history of the computation of ùëÉon
the empty input.
The heart of the proof is to show that INVALIDùëÉis context-free.
Once we do that, we see that ùëÉhalts on the empty input if and only if
INVALIDùëÉ(ùêø) = 1 for every ùêø. To show that, we will encode the list
in a special way that makes it amenable to deciding via a context-free
grammar. Specifically we will reverse all the odd-numbered strings.
‚ãÜ
Proof of Theorem 10.10. We only sketch the proof. We will show that if
we can compute CFGFULL then we can solve HALTONZERO, which
has been proven uncomputable in Theorem 9.9. Let ùëÄbe an input


--- Page 369 ---

restricted computational models
369
Turing machine for HALTONZERO. We will use the notion of configu-
rations of a Turing machine, as defined in Definition 8.8.
Recall that a configuration of Turing machine ùëÄand input ùë•cap-
tures the full state of ùëÄat some point of the computation. The partic-
ular details of configurations are not so important, but what you need
to remember is that:
‚Ä¢ A configuration can be encoded by a binary string ùúé‚àà{0, 1}‚àó.
‚Ä¢ The initial configuration of ùëÄon the input 0 is some fixed string.
‚Ä¢ A halting configuration will have the value a certain state (which can
be easily ‚Äúread off‚Äù from it) set to 1.
‚Ä¢ If ùúéis a configuration at some step ùëñof the computation, we denote
by NEXTùëÄ(ùúé) as the configuration at the next step. NEXTùëÄ(ùúé) is
a string that agrees with ùúéon all but a constant number of coor-
dinates (those encoding the position corresponding to the head
position and the two adjacent ones). On those coordinates, the
value of NEXTùëÄ(ùúé) can be computed by some finite function.
We will let the alphabet Œ£ = {0, 1} ‚à™{‚Äñ, #}. A computation his-
tory of ùëÄon the input 0 is a string ùêø‚ààŒ£ that corresponds to a list
‚Äñùúé0#ùúé1‚Äñùúé2#ùúé3 ‚ãØùúéùë°‚àí2‚Äñùúéùë°‚àí1# (i.e., ‚Äñ comes before an even numbered
block, and # comes before an odd numbered one) such that if ùëñis
even then ùúéùëñis the string encoding the configuration of ùëÉon input 0
at the beginning of its ùëñ-th iteration, and if ùëñis odd then it is the same
except the string is reversed. (That is, for odd ùëñ, ùëüùëíùë£(ùúéùëñ) encodes the
configuration of ùëÉon input 0 at the beginning of its ùëñ-th iteration.)
Reversing the odd-numbered blocks is a technical trick to ensure that
the function INVALIDùëÄwe define below is context free.
We now define INVALIDùëÄ‚à∂Œ£‚àó‚Üí{0, 1} as follows:
INVALIDùëÄ(ùêø) =
‚éß
{
‚é®
{
‚é©
0
ùêøis a valid computation history of ùëÄon 0
1
otherwise
(10.3)
We will show the following claim:
CLAIM: INVALIDùëÄis context-free.
The claim implies the theorem. Since ùëÄhalts on 0 if and only if
there exists a valid computation history, INVALIDùëÄis the constant
one function if and only if ùëÄdoes not halt on 0. In particular, this
allows us to reduce determining whether ùëÄhalts on 0 to determining
whether the grammar ùê∫ùëÄcorresponding to INVALIDùëÄis full.
We now turn to the proof of the claim. We will not show all the
details, but the main point INVALIDùëÄ(ùêø) = 1 if at least one of the
following three conditions hold:


--- Page 370 ---

370
introduction to theoretical computer science
1. ùêøis not of the right format, i.e. not of the form ‚ü®binary-string‚ü©#‚ü®binary-string‚ü©‚Äñ‚ü®binary-string‚ü©# ‚ãØ.
2. ùêøcontains a substring of the form ‚Äñùúé#ùúé‚Ä≤‚Äñ such that
ùúé‚Ä≤ ‚â†ùëüùëíùë£(NEXTùëÉ(ùúé))
3. ùêøcontains a substring of the form #ùúé‚Äñùúé‚Ä≤# such that
ùúé‚Ä≤ ‚â†NEXTùëÉ(ùëüùëíùë£(ùúé))
Since context-free functions are closed under the OR operation, the
claim will follow if we show that we can verify conditions 1, 2 and 3
via a context-free grammar.
For condition 1 this is very simple: checking that ùêøis of the correct
format can be done using a regular expression. Since regular expres-
sions are closed under negation, this means that checking that ùêøis not
of this format can also be done by a regular expression and hence by a
context-free grammar.
For conditions 2 and 3, this follows via very similar reasoning to
that showing that the function ùêπsuch that ùêπ(ùë¢#ùë£) = 1 iff ùë¢‚â†ùëüùëíùë£(ùë£)
is context-free, see Solved Exercise 10.2. After all, the NEXTùëÄfunction
only modifies its input in a constant number of places. We leave filling
out the details as an exercise to the reader. Since INVALIDùëÄ(ùêø) = 1
if and only if ùêøsatisfies one of the conditions 1., 2. or 3., and all three
conditions can be tested for via a context-free grammar, this completes
the proof of the claim and hence the theorem.
‚ñ†
10.4 SUMMARY OF SEMANTIC PROPERTIES FOR REGULAR EX-
PRESSIONS AND CONTEXT-FREE GRAMMARS
To summarize, we can often trade expressiveness of the model for
amenability to analysis. If we consider computational models that are
not Turing complete, then we are sometimes able to bypass Rice‚Äôs The-
orem and answer certain semantic questions about programs in such
models. Here is a summary of some of what is known about semantic
questions for the different models we have seen.
Table 10.1: Computability of semantic properties
Model
Halting
Emptiness
Equivalence
Regular expressions
Computable
Computable
Computable
Context free grammars
Computable
Computable
Uncomputable
Turing-complete models
UncomputableUncomputable Uncomputable


--- Page 371 ---

restricted computational models
371
‚úì
Chapter Recap
‚Ä¢ The uncomputability of the Halting problem for
general models motivates the definition of re-
stricted computational models.
‚Ä¢ In some restricted models we can answer semantic
questions such as: does a given program terminate,
or do two programs compute the same function?
‚Ä¢ Regular expressions are a restricted model of com-
putation that is often useful to capture tasks of
string matching. We can test efficiently whether
an expression matches a string, as well as answer
questions such as Halting and Equivalence.
‚Ä¢ Context free grammars is a stronger, yet still not Tur-
ing complete, model of computation. The halting
problem for context free grammars is computable,
but equivalence is not computable.
10.5 EXERCISES
Exercise 10.1 ‚Äî Closure properties of context-free functions. Suppose that
ùêπ, ùê∫‚à∂{0, 1}‚àó‚Üí{0, 1} are context free. For each one of the following
definitions of the function ùêª, either prove that ùêªis always context
free or give a counterexample for regular ùêπ, ùê∫that would make ùêªnot
context free.
1. ùêª(ùë•) = ùêπ(ùë•) ‚à®ùê∫(ùë•).
2. ùêª(ùë•) = ùêπ(ùë•) ‚àßùê∫(ùë•)
3. ùêª(ùë•) = NAND(ùêπ(ùë•), ùê∫(ùë•)).
4. ùêª(ùë•) = ùêπ(ùë•ùëÖ) where ùë•ùëÖis the reverse of ùë•: ùë•ùëÖ= ùë•ùëõ‚àí1ùë•ùëõ‚àí2 ‚ãØùë•ùëúfor
ùëõ= |ùë•|.
5. ùêª(ùë•) =
‚éß
{
‚é®
{
‚é©
1
ùë•= ùë¢ùë£s.t. ùêπ(ùë¢) = ùê∫(ùë£) = 1
0
otherwise
6. ùêª(ùë•) =
‚éß
{
‚é®
{
‚é©
1
ùë•= ùë¢ùë¢s.t. ùêπ(ùë¢) = ùê∫(ùë¢) = 1
0
otherwise
7. ùêª(ùë•) =
‚éß
{
‚é®
{
‚é©
1
ùë•= ùë¢ùë¢ùëÖs.t. ùêπ(ùë¢) = ùê∫(ùë¢) = 1
0
otherwise
‚ñ†
Exercise 10.2 Prove that the function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1} such that
ùêπ(ùë•) = 1 if and only if |ùë•| is a power of two is not context free.
‚ñ†


--- Page 372 ---

372
introduction to theoretical computer science
2 Try to see if you can ‚Äúembed‚Äù in some way a func-
tion that looks similar to MATCHPAREN in SYN, so
you can use a similar proof. Of course for a function
to be non-regular, it does not need to utilize literal
parentheses symbols.
Exercise 10.3 ‚Äî Syntax for programming languages. Consider the following
syntax of a ‚Äúprogramming language‚Äù whose source can be written
using the ASCII character set:
‚Ä¢ Variables are obtained by a sequence of letters, numbers and under-
scores, but can‚Äôt start with a number.
‚Ä¢ A statement has either the form foo = bar; where foo and bar are
variables, or the form IF (foo) BEGIN ... END where ... is list
of one or more statements, potentially separated by newlines.
A program in our language is simply a sequence of statements (pos-
sibly separated by newlines or spaces).
1. Let VAR ‚à∂{0, 1}‚àó‚Üí{0, 1} be the function that given a string
ùë•‚àà{0, 1}‚àó, outputs 1 if and only if ùë•corresponds to an ASCII
encoding of a valid variable identifier. Prove that VAR is regular.
2. Let SYN ‚à∂{0, 1}‚àó‚Üí{0, 1} be the function that given a string
ùë†‚àà{0, 1}‚àó, outputs 1 if and only if ùë†is an ASCII encoding of a valid
program in our language. Prove that SYN is context free. (You do
not have to specify the full formal grammar for SYN, but you need
to show that such a grammar exists.)
3. Prove that SYN is not regular. See footnote for hint2
‚ñ†
10.6 BIBLIOGRAPHICAL NOTES
As in the case of regular expressions, there are many resources avail-
able that cover context-free grammar in great detail. Chapter 2 of
[Sip97] contains many examples of context-free grammars and their
properties. There are also websites such as Grammophone where you
can input grammars, and see what strings they generate, as well as
some of the properties that they satisfy.
The adjective ‚Äúcontext free‚Äù is used for CFG‚Äôs because a rule of
the form ùë£‚Ü¶ùëémeans that we can always replace ùë£with the string
ùëé, no matter what is the context in which ùë£appears. More generally,
we might want to consider cases where the replacement rules depend
on the context. This gives rise to the notion of general (aka ‚ÄúType 0‚Äù)
grammars that allow rules of the form ùëé‚áíùëèwhere both ùëéand ùëèare
strings over (ùëâ‚à™Œ£)‚àó. The idea is that if, for example, we wanted to
enforce the condition that we only apply some rule such as ùë£‚Ü¶0ùë§1
when ùë£is surrounded by three zeroes on both sides, then we could do
so by adding a rule of the form 000ùë£000 ‚Ü¶0000ùë§1000 (and of course
we can add much more general conditions). Alas, this generality


--- Page 373 ---

restricted computational models
373
comes at a cost - general grammars are Turing complete and hence
their halting problem is uncomputable. That is, there is no algorithm
ùê¥that can determine for every general grammar ùê∫and a string ùë•,
whether or not the grammar ùê∫generates ùë•.
The Chomsky Hierarchy is a hierarchy of grammars from the least
restrictive (most powerful) Type 0 grammars, which correspond to
recursively enumerable languages (see Exercise 9.10) to the most re-
strictive Type 3 grammars, which correspond to regular languages.
Context-free languages correspond to Type 2 grammars. Type 1 gram-
mars are context sensitive grammars. These are more powerful than
context-free grammars but still less powerful than Turing machines.
In particular functions/languages corresponding to context-sensitive
grammars are always computable, and in fact can be computed by a
linear bounded automatons which are non-deterministic algorithms
that take ùëÇ(ùëõ) space. For this reason, the class of functions/languages
corresponding to context-sensitive grammars is also known as the
complexity class NSPACEùëÇ(ùëõ); we discuss space-bounded com-
plexity in Chapter 17). While Rice‚Äôs Theorem implies that we cannot
compute any non-trivial semantic property of Type 0 grammars, the
situation is more complex for other types of grammars: some seman-
tic properties can be determined and some cannot, depending on the
grammar‚Äôs place in the hierarchy.


--- Page 374 ---



--- Page 375 ---

11
Is every theorem provable?
‚ÄúTake any definite unsolved problem, such as ‚Ä¶ the existence of an infinite
number of prime numbers of the form 2ùëõ+ 1. However unapproachable these
problems may seem to us and however helpless we stand before them, we have,
nevertheless, the firm conviction that their solution must follow by a finite
number of purely logical processes‚Ä¶‚Äù
‚Äú‚Ä¶This conviction of the solvability of every mathematical problem is a pow-
erful incentive to the worker. We hear within us the perpetual call: There is the
problem. Seek its solution. You can find it by pure reason, for in mathematics
there is no ignorabimus.‚Äù, David Hilbert, 1900.
‚ÄúThe meaning of a statement is its method of verification.‚Äù, Moritz Schlick,
1938 (aka ‚ÄúThe verification principle‚Äù of logical positivism)
The problems shown uncomputable in Chapter 9, while natural
and important, still intimately involved NAND-TM programs or other
computing mechanisms in their definitions. One could perhaps hope
that as long as we steer clear of functions whose inputs are themselves
programs, we can avoid the ‚Äúcurse of uncomputability‚Äù. Alas, we have
no such luck.
In this chapter we will see an example of a natural and seemingly
‚Äúcomputation free‚Äù problem that nevertheless turns out to be uncom-
putable: solving Diophantine equations. As a corollary, we will see
one of the most striking results of 20th century mathematics: G√∂del‚Äôs
Incompleteness Theorem, which showed that there are some mathemat-
ical statements (in fact, in number theory) that are inherently unprov-
able. We will actually start with the latter result, and then show the
former.
11.1 HILBERT‚ÄôS PROGRAM AND G√ñDEL‚ÄôS INCOMPLETENESS
THEOREM
‚ÄúAnd what are these ‚Ä¶vanishing increments? They are neither finite quanti-
ties, nor quantities infinitely small, nor yet nothing. May we not call them the
ghosts of departed quantities?‚Äù, George Berkeley, Bishop of Cloyne, 1734.
Compiled on 8.26.2020 18:10
Learning Objectives:
‚Ä¢ More examples of uncomputable functions
that are not as tied to computation.
‚Ä¢ G√∂del‚Äôs incompleteness theorem - a result
that shook the world of mathematics in the
early 20th century.


--- Page 376 ---

376
introduction to theoretical computer science
Figure 11.1: Outline of the results of this chapter. One
version of G√∂del‚Äôs Incompleteness Theorem is an
immediate consequence of the uncomputability of the
Halting problem. To obtain the theorem as originally
stated (for statements about the integers) we first
prove that the QMS problem of determining truth
of quantified statements involving both integers and
strings is uncomputable. We do so using the notion of
Turing Machine configurations but there are alternative
approaches to do so as well, see Remark 11.14.
The 1700‚Äôs and 1800‚Äôs were a time of great discoveries in mathe-
matics but also of several crises. The discovery of calculus by Newton
and Leibnitz in the late 1600‚Äôs ushered a golden age of problem solv-
ing. Many longstanding challenges succumbed to the new tools that
were discovered, and mathematicians got ever better at doing some
truly impressive calculations. However, the rigorous foundations
behind these calculations left much to be desired. Mathematicians
manipulated infinitesimal quantities and infinite series cavalierly, and
while most of the time they ended up with the correct results, there
were a few strange examples (such as trying to calculate the value
of the infinite series 1 ‚àí1 + 1 ‚àí1 + 1 + ‚Ä¶) which seemed to give
out different answers depending on the method of calculation. This
led to a growing sense of unease in the foundations of the subject
which was addressed in the works of mathematicians such as Cauchy,
Weierstrass, and Riemann, who eventually placed analysis on firmer
foundations, giving rise to the ùúñ‚Äôs and ùõø‚Äôs that students taking honors
calculus grapple with to this day.
In the beginning of the 20th century, there was an effort to replicate
this effort, in greater rigor, to all parts of mathematics. The hope was
to show that all the true results of mathematics can be obtained by
starting with a number of axioms, and deriving theorems from them
using logical rules of inference. This effort was known as the Hilbert
program, named after the influential mathematician David Hilbert.
Alas, it turns out the results we‚Äôve seen dealt a devastating blow to
this program, as was shown by Kurt G√∂del in 1931:
Theorem 11.1 ‚Äî G√∂del‚Äôs Incompleteness Theorem: informal version. For
every sound proof system ùëâfor sufficiently rich mathematical
statements, there is a mathematical statement that is true but is not
provable in ùëâ.


--- Page 377 ---

is every theorem provable?
377
1 This happens to be a false statement.
2 It is unknown whether this statement is true or false.
11.1.1 Defining ‚ÄúProof Systems‚Äù
Before proving Theorem 11.1, we need to define ‚Äúproof systems‚Äù and
even formally define the notion of a ‚Äúmathematical statement‚Äù. In
geometry and other areas of mathematics, proof systems are often
defined by starting with some basic assumptions or axioms and then
deriving more statements by using inference rules such as the famous
Modus Ponens, but what axioms shall we use? What rules? We will
use an extremely general notion of proof systems, not even restricting
ourselves to ones that have the form of axioms and inference.
Mathematical statements.
At the highest level, a mathematical statement
is simply a piece of text, which we can think of as a string ùë•‚àà{0, 1}‚àó.
Mathematical statements contain assertions whose truth does not
depend on any empirical fact, but rather only on properties of abstract
objects. For example, the following is a mathematical statement:1
‚ÄúThe number 2,696,635,869,504,783,333,238,805,675,613, 588,278,597,832,162,617,892,474,670,798,113
is prime‚Äù.
Mathematical statements do not have to involve numbers. They
can assert properties of any other mathematical object including sets,
strings, functions, graphs and yes, even programs. Thus, another exam-
ple of a mathematical statement is the following:2
The following Python function halts on every positive integer n
def f(n):
if n==1: return 1
return f(3*n+1) if n % 2 else f(n//2)
Proof systems.
A proof for a statement ùë•‚àà{0, 1}‚àóis another piece of
text ùë§‚àà{0, 1}‚àóthat certifies the truth of the statement asserted in ùë•.
The conditions for a valid proof system are:
1. (Effectiveness) Given a statement ùë•and a proof ùë§, there is an algo-
rithm to verify whether or not ùë§is a valid proof for ùë•. (For exam-
ple, by going line by line and checking that each line follows from
the preceding ones using one of the allowed inference rules.)
2. (Soundness) If there is a valid proof ùë§for ùë•then ùë•is true.
These are quite minimal requirements for a proof system. Require-
ment 2 (soundness) is the very definition of a proof system: you
shouldn‚Äôt be able to prove things that are not true. Requirement 1
is also essential. If there is no set of rules (i.e., an algorithm) to check
that a proof is valid then in what sense is it a proof system? We could


--- Page 378 ---

378
introduction to theoretical computer science
replace it with a system where the ‚Äúproof‚Äù for a statement ùë•is ‚Äútrust
me: it‚Äôs true‚Äù.
We formally define proof systems as an algorithm ùëâwhere
ùëâ(ùë•, ùë§) = 1 holds if the string ùë§is a valid proof for the statement ùë•.
Even if ùë•is true, the string ùë§does not have to be a valid proof for it
(there are plenty of wrong proofs for true statements such as 4=2+2)
but if ùë§is a valid proof for ùë•then ùë•must be true.
Definition 11.2 ‚Äî Proof systems. Let ùíØ
‚äÜ
{0, 1}‚àóbe some set (which
we consider the ‚Äútrue‚Äù statements). A proof system for ùíØis an algo-
rithm ùëâthat satisfies:
1. (Effectiveness) For every ùë•, ùë§‚àà{0, 1}‚àó, ùëâ(ùë•, ùë§) halts with an out-
put of either 0 or 1.
2. (Soundness) For every ùë•‚àâùíØand ùë§‚àà{0, 1}‚àó, ùëâ(ùë•, ùë§) = 0.
A true statement ùë•
‚àà
ùíØis unprovable (with respect to ùëâ) if for
every ùë§‚àà{0, 1}‚àó, ùëâ(ùë•, ùë§) = 0. We say that ùëâis complete if there
does not exist a true statement ùë•that is unprovable with respect to
ùëâ.
ÔÉ´Big Idea 15 A proof is just a string of text whose meaning is given
by a verification algorithm.
11.2 G√ñDEL‚ÄôS INCOMPLETENESS THEOREM: COMPUTATIONAL
VARIANT
Our first formalization of Theorem 11.1 involves statements about
Turing machines. We let ‚Ñãbe the set of strings ùë•‚àà{0, 1}‚àóthat have
the form ‚ÄúTuring machine ùëÄhalts on the zero input‚Äù.
Theorem 11.3 ‚Äî G√∂del‚Äôs Incompleteness Theorem: computational variant.
There does not exist a complete proof system for ‚Ñã.
Proof Idea:
If we had such a complete and sound proof system then we could
solve the HALTONZERO problem. On input a Turing machine ùëÄ,
we would search all purported proofs ùë§and halt as soon as we find
a proof of either ‚ÄúùëÄhalts on zero‚Äù or ‚ÄúùëÄdoes not halt on zero‚Äù. If
the system is sound and complete then we will eventually find such a
proof, and it will provide us with the correct output.
‚ãÜ


--- Page 379 ---

is every theorem provable?
379
Proof of Theorem 11.3. Assume for the sake of contradiction that there
was such a proof system ùëâ. We will use ùëâto build an algorithm ùê¥
that computes HALTONZERO, hence contradicting Theorem 9.9. Our
algorithm ùê¥will will work as follows:
Algorithm 11.4 ‚Äî Halting from proofs.
Input: Turing Machine ùëÄ
Output: 1 ùëÄif halts on the
Input: 0; 0 otherwise.
1: for ùëõ= 1, 2, 3, ‚Ä¶ do
2:
for ùë§‚àà{0, 1}ùëõdo
3:
if ùëâ(‚ÄùùëÄhalts on 0‚Äù, ùë§) = 1 then
4:
return 1
5:
end if
6:
if ùëâ(‚ÄùùëÄdoes not halt on 0‚Äù, ùë§) = 1 then
7:
return 0
8:
end if
9:
end for
10: end for
If ùëÄhalts on 0 then under our assumption there exists ùë§that
proves this fact, and so when Algorithm ùê¥reaches ùëõ= |ùë§| we will
eventually find this ùë§and output 1, unless we already halted be-
fore. But we cannot halt before and output a wrong answer because
it would contradict the soundness of the proof system. Similarly, this
shows that if ùëÄdoes not halt on 0 then (since we assume there is a
proof of this fact too) our algorithm ùê¥will eventually halt and output
0.
‚ñ†
R
Remark 11.5 ‚Äî The G√∂del statement (optional). One can
extract from the proof of Theorem 11.3 a procedure
that for every proof system ùëâ, yields a true statement
ùë•‚àóthat cannot be proven in ùëâ. But G√∂del‚Äôs proof
gave a very explicit description of such a statement ùë•‚àó
which is closely related to the ‚ÄúLiar‚Äôs paradox‚Äù. That
is, G√∂del‚Äôs statement ùë•‚àówas designed to be true if and
only if ‚àÄùë§‚àà{0,1}‚àóùëâ(ùë•, ùë§) = 0. In other words, it satisfied
the following property
ùë•‚àóis true ‚áîùë•‚àódoes not have a proof in ùëâ
(11.1)
One can see that if ùë•‚àóis true, then it does not have a
proof, but it is false then (assuming the proof system
is sound) then it cannot have a proof, and hence ùë•‚àó


--- Page 380 ---

380
introduction to theoretical computer science
must be both true and unprovable. One might wonder
how is it possible to come up with an ùë•‚àóthat satisfies
a condition such as (11.1) where the same string ùë•‚àó
appears on both the righthand side and the lefthand
side of the equation. The idea is that the proof of The-
orem 11.3 yields a way to transform every statement ùë•
into a statement ùêπ(ùë•) that is true if and only if ùë•does
not have a proof in ùëâ. Thus ùë•‚àóneeds to be a fixed point
of ùêπ: a sentence such that ùë•‚àó= ùêπ(ùë•‚àó). It turns out that
we can always find such a fixed point of ùêπ. We‚Äôve al-
ready seen this phenomenon in the ùúÜcalculus, where
the ùëåcombinator maps every ùêπinto a fixed point ùëåùêπ
of ùêπ. This is very related to the idea of programs that
can print their own code. Indeed, Scott Aaronson likes
to describe G√∂del‚Äôs statement as follows:
The following sentence repeated twice, the sec-
ond time in quotes, is not provable in the formal
system ùëâ. ‚ÄúThe following sentence repeated
twice, the second time in quotes, is not provable
in the formal system ùëâ.‚Äù
In the argument above we actually showed that ùë•‚àóis
true, under the assumption that ùëâis sound. Since ùë•‚àó
is true and does not have a proof in ùëâ, this means that
we cannot carry the above argument in the system ùëâ,
which means that ùëâcannot prove its own soundness
(or even consistency: that there is no proof of both a
statement and its negation). Using this idea, it‚Äôs not
hard to get G√∂del‚Äôs second incompleteness theorem,
which says that every sufficiently rich ùëâcannot prove
its own consistency. That is, if we formalize the state-
ment ùëê‚àóthat is true if and only if ùëâis consistent (i.e.,
ùëâcannot prove both a statement and the statement‚Äôs
negation), then ùëê‚àócannot be proven in ùëâ.
11.3 QUANTIFIED INTEGER STATEMENTS
There is something ‚Äúunsatisfying‚Äù about Theorem 11.3. Sure, it shows
there are statements that are unprovable, but they don‚Äôt feel like ‚Äúreal‚Äù
statements about math. After all, they talk about programs rather than
numbers, matrices, or derivatives, or whatever it is they teach in math
courses. It turns out that we can get an analogous result for statements
such as ‚Äúthere are no positive integers ùë•and ùë¶such that ùë•2 ‚àí2 =
ùë¶7‚Äù, or ‚Äúthere are positive integers ùë•, ùë¶, ùëßsuch that ùë•2 + ùë¶6 = ùëß11‚Äù
that only talk about natural numbers. It doesn‚Äôt get much more ‚Äúreal
math‚Äù than this. Indeed, the 19th century mathematician Leopold
Kronecker famously said that ‚ÄúGod made the integers, all else is the
work of man.‚Äù (By the way, the status of the above two statements is
unknown.)


--- Page 381 ---

is every theorem provable?
381
To make this more precise, let us define the notion of quantified
integer statements:
Definition 11.6 ‚Äî Quantified integer statements. A quantified integer state-
ment is a well-formed statement with no unbound variables involv-
ing integers, variables, the operators >, <, √ó, +, ‚àí, =, the logical
operations ¬¨ (NOT), ‚àß(AND), and ‚à®(OR), as well as quantifiers
of the form ‚àÉùë•‚àà‚Ñïand ‚àÄùë¶‚àà‚Ñïwhere ùë•, ùë¶are variable names.
We often care deeply about determining the truth of quantified
integer statements. For example, the statement that Fermat‚Äôs Last
Theorem is true for ùëõ= 3 can be phrased as the quantified integer
statement
¬¨‚àÉùëé‚àà‚Ñï‚àÉùëè‚àà‚Ñï‚àÉùëê‚àà‚Ñï(ùëé> 0)‚àß(ùëè> 0)‚àß(ùëê> 0)‚àß(ùëé√ó ùëé√ó ùëé+ ùëè√ó ùëè√ó ùëè= ùëê√ó ùëê√ó ùëê) .
(11.2)
The twin prime conjecture, that states that there is an infinite num-
ber of numbers ùëùsuch that both ùëùand ùëù+ 2 are primes can be phrased
as the quantified integer statement
‚àÄùëõ‚àà‚Ñï‚àÉùëù‚àà‚Ñï(ùëù> ùëõ) ‚àßPRIME(ùëù) ‚àßPRIME(ùëù+ 2)
(11.3)
where we replace an instance of PRIME(ùëû) with the statement (ùëû>
1) ‚àß‚àÄùëé‚àà‚Ñï‚àÄùëè‚àà‚Ñï(ùëé= 1) ‚à®(ùëé= ùëû) ‚à®¬¨(ùëé√ó ùëè= ùëû).
The claim (mentioned in Hilbert‚Äôs quote above) that are infinitely
many primes of the form ùëù= 2ùëõ+ 1 can be phrased as follows:
‚àÄùëõ‚àà‚Ñï‚àÉùëù‚àà‚Ñï(ùëù> ùëõ) ‚àßPRIME(ùëù)‚àß
(‚àÄùëò‚àà‚Ñï(ùëò‚â†2 ‚àßPRIME(ùëò)) ‚áí¬¨DIVIDES(ùëò, ùëù‚àí1))
(11.4)
where DIVIDES(ùëé, ùëè) is the statement ‚àÉùëê‚àà‚Ñïùëè√ó ùëê= ùëé. In English, this
corresponds to the claim that for every ùëõthere is some ùëù> ùëõsuch that
all of ùëù‚àí1‚Äôs prime factors are equal to 2.
R
Remark 11.7 ‚Äî Syntactic sugar for quantified integer
statements. To make our statements more readable,
we often use syntactic sugar and so write ùë•
‚â†
ùë¶as
shorthand for ¬¨(ùë•
=
ùë¶), and so on. Similarly, the
‚Äúimplication operator‚Äù ùëé
‚áí
ùëèis ‚Äúsyntactic sugar‚Äù or
shorthand for ¬¨ùëé‚à®ùëè, and the ‚Äúif and only if operator‚Äù
ùëé
‚áîis shorthand for (ùëé
‚áí
ùëè) ‚àß(ùëè
‚áí
ùëé). We will
also allow ourselves the use of ‚Äúmacros‚Äù: plugging in
one quantified integer statement in another, as we did
with DIVIDES and PRIME above.


--- Page 382 ---

382
introduction to theoretical computer science
Much of number theory is concerned with determining the truth
of quantified integer statements. Since our experience has been that,
given enough time (which could sometimes be several centuries) hu-
manity has managed to do so for the statements that it cared enough
about, one could (as Hilbert did) hope that eventually we would be
able to prove or disprove all such statements. Alas, this turns out to be
impossible:
Theorem 11.8 ‚Äî G√∂del‚Äôs Incompleteness Theorem for quantified integer state-
ments. Let ùëâ‚à∂{0, 1}‚àó‚Üí{0, 1} a computable purported verification
procedure for quantified integer statements. Then either:
‚Ä¢ ùëâis not sound: There exists a false statement ùë•and a string
ùë§‚àà{0, 1}‚àósuch that ùëâ(ùë•, ùë§) = 1.
or
‚Ä¢ ùëâis not complete: There exists a true statement ùë•such that for
every ùë§‚àà{0, 1}‚àó, ùëâ(ùë•, ùë§) = 0.
Theorem 11.8 is a direct corollary of the following result, just
as Theorem 11.3 was a direct corollary of the uncomputability of
HALTONZERO:
Theorem 11.9 ‚Äî Uncomputability of quantified integer statements. Let
QIS
‚à∂
{0, 1}‚àó
‚Üí
{0, 1} be the function that given a (string rep-
resentation of) a quantified integer statement outputs 1 if it is true
and 0 if it is false. Then QIS is uncomputable.
Since a quantified integer statement is simply a sequence of sym-
bols, we can easily represent it as a string. For simplicity we will as-
sume that every string represents some quantified integer statement,
by mapping strings that do not correspond to such a statement to an
arbitrary statement such as ‚àÉùë•‚àà‚Ñïùë•= 1.
P
Please stop here and make sure you understand
why the uncomputability of QIS (i.e., Theorem 11.9)
means that there is no sound and complete proof
system for proving quantified integer statements (i.e.,
Theorem 11.8). This follows in the same way that
Theorem 11.3 followed from the uncomputability of
HALTONZERO, but working out the details is a great
exercise (see Exercise 11.1)
In the rest of this chapter, we will show the proof of Theorem 11.8,
following the outline illustrated in Fig. 11.1.


--- Page 383 ---

is every theorem provable?
383
Figure 11.2: Diophantine equations such as finding
a positive integer solution to the equation ùëé(ùëé+
ùëè)(ùëé+ ùëê) + ùëè(ùëè+ ùëé)(ùëè+ ùëê) + ùëê(ùëê+ ùëé)(ùëê+ ùëè) =
4(ùëé+ ùëè)(ùëé+ ùëê)(ùëè+ ùëê) (depicted more compactly
and whimsically above) can be surprisingly difficult.
There are many equations for which we do not know
if they have a solution, and there is no algorithm to
solve them in general. The smallest solution for this
equation has 80 digits! See this Quora post for more
information, including the credits for this image.
3 This is a special case of what‚Äôs known as ‚ÄúFermat‚Äôs
Last Theorem‚Äù which states that ùëéùëõ+ ùëèùëõ= ùëêùëõhas no
solution in integers for ùëõ> 2. This was conjectured in
1637 by Pierre de Fermat but only proven by Andrew
Wiles in 1991. The case ùëõ= 11 (along with all other
so called ‚Äúregular prime exponents‚Äù) was established
by Kummer in 1850.
11.4 DIOPHANTINE EQUATIONS AND THE MRDP THEOREM
Many of the functions people wanted to compute over the years in-
volved solving equations. These have a much longer history than
mechanical computers. The Babylonians already knew how to solve
some quadratic equations in 2000BC, and the formula for all quadrat-
ics appears in the Bakhshali Manuscript that was composed in India
around the 3rd century. During the Renaissance, Italian mathemati-
cians discovered generalization of these formulas for cubic and quar-
tic (degrees 3 and 4) equations. Many of the greatest minds of the
17th and 18th century, including Euler, Lagrange, Leibniz and Gauss
worked on the problem of finding such a formula for quintic equations
to no avail, until in the 19th century Ruffini, Abel and Galois showed
that no such formula exists, along the way giving birth to group theory.
However, the fact that there is no closed-form formula does
not mean we can not solve such equations. People have been
solving higher degree equations numerically for ages. The Chinese
manuscript Jiuzhang Suanshu from the first century mentions such
approaches. Solving polynomial equations is by no means restricted
only to ancient history or to students‚Äô homework. The gradient
descent method is the workhorse powering many of the machine
learning tools that have revolutionized Computer Science over the last
several years.
But there are some equations that we simply do not know how to
solve by any means. For example, it took more than 200 years until peo-
ple succeeded in proving that the equation ùëé11 + ùëè11 = ùëê11 has no
solution in integers.3 The notorious difficulty of so called Diophantine
equations (i.e., finding integer roots of a polynomial) motivated the
mathematician David Hilbert in 1900 to include the question of find-
ing a general procedure for solving such equations in his famous list
of twenty-three open problems for mathematics of the 20th century. I
don‚Äôt think Hilbert doubted that such a procedure exists. After all, the
whole history of mathematics up to this point involved the discovery
of ever more powerful methods, and even impossibility results such
as the inability to trisect an angle with a straightedge and compass, or
the non-existence of an algebraic formula for quintic equations, merely
pointed out to the need to use more general methods.
Alas, this turned out not to be the case for Diophantine equations.
In 1970, Yuri Matiyasevich, building on a decades long line of work by
Martin Davis, Hilary Putnam and Julia Robinson, showed that there is
simply no method to solve such equations in general:
Theorem 11.10 ‚Äî MRDP Theorem. Let DIO
‚à∂
{0, 1}‚àó
‚Üí
{0, 1} be the
function that takes as input a string describing a 100-variable poly-


--- Page 384 ---

384
introduction to theoretical computer science
nomial with integer coefficients ùëÉ(ùë•0, ‚Ä¶ , ùë•99) and outputs 1 if and
only if there exists ùëß0, ‚Ä¶ , ùëß99 ‚àà‚Ñïs.t. ùëÉ(ùëß0, ‚Ä¶ , ùëß99) = 0.
Then DIO is uncomputable.
As usual, we assume some standard way to express numbers and
text as binary strings. The constant 100 is of course arbitrary; the prob-
lem is known to be uncomputable even for polynomials of degree
four and at most 58 variables. In fact the number of variables can be
reduced to nine, at the expense of the polynomial having a larger (but
still constant) degree. See Jones‚Äôs paper for more about this issue.
R
Remark 11.11 ‚Äî Active code vs static data. The diffi-
culty in finding a way to distinguish between ‚Äúcode‚Äù
such as NAND-TM programs, and ‚Äústatic content‚Äù
such as polynomials is just another manifestation of
the phenomenon that code is the same as data. While
a fool-proof solution for distinguishing between the
two is inherently impossible, finding heuristics that do
a reasonable job keeps many firewall and anti-virus
manufacturers very busy (and finding ways to bypass
these tools keeps many hackers busy as well).
11.5 HARDNESS OF QUANTIFIED INTEGER STATEMENTS
We will not prove the MRDP Theorem (Theorem 11.10). However, as
we mentioned, we will prove the uncomputability of QIS (i.e., Theo-
rem 11.9), which is a special case of the MRDP Theorem. The reason
is that a Diophantine equation is a special case of a quantified integer
statement where the only quantifier is ‚àÉ. This means that deciding the
truth of quantified integer statements is a potentially harder problem
than solving Diophantine equations, and so it is potentially easier to
prove that QIS is uncomputable.
P
If you find the last sentence confusing, it is worth-
while to reread it until you are sure you follow its
logic. We are so accustomed to trying to find solu-
tions for problems that it can sometimes be hard to
follow the arguments for showing that problems are
uncomputable.
Our proof of the uncomputability of QIS (i.e. Theorem 11.9) will, as
usual, go by reduction from the Halting problem, but we will do so in
two steps:


--- Page 385 ---

is every theorem provable?
385
1. We will first use a reduction from the Halting problem to show that
deciding the truth of quantified mixed statements is uncomputable.
Quantified mixed statements involve both strings and integers.
Since quantified mixed statements are a more general concept than
quantified integer statements, it is easier to prove the uncomputabil-
ity of deciding their truth.
2. We will then reduce the problem of quantified mixed statements to
quantifier integer statements.
11.5.1 Step 1: Quantified mixed statements and computation histories
We define quantified mixed statements as statements involving not just
integers and the usual arithmetic operators, but also string variables as
well.
Definition 11.12 ‚Äî Quantified mixed statements. A quantified mixed state-
ment is a well-formed statement with no unbound variables involv-
ing integers, variables, the operators >, <, √ó, +, ‚àí, =, the logical
operations ¬¨ (NOT), ‚àß(AND), and ‚à®(OR), as well as quanti-
fiers of the form ‚àÉùë•‚àà‚Ñï, ‚àÉùëé‚àà{0,1}‚àó, ‚àÄùë¶‚àà‚Ñï, ‚àÄùëè‚àà{0,1}‚àówhere ùë•, ùë¶, ùëé, ùëèare
variable names. These also include the operator |ùëé| which returns
the length of a string valued variable ùëé, as well as the operator ùëéùëñ
where ùëéis a string-valued variable and ùëñis an integer valued ex-
pression which is true if ùëñis smaller than the length of ùëéand the ùëñùë°‚Ñé
coordinate of ùëéis 1, and is false otherwise.
For example, the true statement that for every string ùëéthere is a
string ùëèthat corresponds to ùëéin reverse order can be phrased as the
following quantified mixed statement
‚àÄùëé‚àà{0,1}‚àó‚àÉùëè‚àà{0,1}‚àó(|ùëé| = |ùëè|) ‚àß(‚àÄùëñ‚àà‚Ñïùëñ< |ùëé| ‚áí(ùëéùëñ‚áîùëè|ùëé|‚àíùëñ)) .
(11.5)
Quantified mixed statements are more general than quantified
integer statements, and so the following theorem is potentially easier
to prove than Theorem 11.9:
Theorem 11.13 ‚Äî Uncomputability of quantified mixed statements. Let
QMS
‚à∂
{0, 1}‚àó
‚Üí
{0, 1} be the function that given a (string rep-
resentation of) a quantified mixed statement outputs 1 if it is true
and 0 if it is false. Then QMS is uncomputable.
Proof Idea:
The idea behind the proof is similar to that used in showing that
one-dimensional cellular automata are Turing complete (Theorem 8.7)
as well as showing that equivalence (or even ‚Äúfullness‚Äù) of context
free grammars is uncomputable (Theorem 10.10). We use the notion


--- Page 386 ---

386
introduction to theoretical computer science
of a configuration of a NAND-TM program as in Definition 8.8. Such
a configuration can be thought of as a string ùõºover some large-but-
finite alphabet Œ£ describing its current state, including the values
of all arrays, scalars, and the index variable i. It can be shown that
if ùõºis the configuration at a certain step of the execution and ùõΩis
the configuration at the next step, then ùõΩùëó= ùõºùëófor all ùëóoutside of
{ùëñ‚àí1, ùëñ, ùëñ+ 1} where ùëñis the value of i. In particular, every value ùõΩùëóis
simply a function of ùõºùëó‚àí1,ùëó,ùëó+1. Using these observations we can write
a quantified mixed statement NEXT(ùõº, ùõΩ) that will be true if and only if
ùõΩis the configuration encoding the next step after ùõº. Since a program
ùëÉhalts on input ùë•if and only if there is a sequence of configurations
ùõº0, ‚Ä¶ , ùõºùë°‚àí1 (known as a computation history) starting with the initial
configuration with input ùë•and ending in a halting configuration, we
can define a quantified mixed statement to determine if there is such
a statement by taking a universal quantifier over all strings ùêª(for
history) that encode a tuple (ùõº0, ùõº1, ‚Ä¶ , ùõºùë°‚àí1) and then checking that
ùõº0 and ùõºùë°‚àí1 are valid starting and halting configurations, and that
NEXT(ùõºùëó, ùõºùëó+1) is true for every ùëó‚àà{0, ‚Ä¶ , ùë°‚àí2}.
‚ãÜ
Proof of Theorem 11.13. The proof is obtained by a reduction from the
Halting problem. Specifically, we will use the notion of a configura-
tion of a Turing Machines (Definition 8.8) that we have seen in the
context of proving that one dimensional cellular automata are Turing
complete. We need the following facts about configurations:
‚Ä¢ For every Turing Machine ùëÄ, there is a finite alphabet Œ£, and a
configuration of ùëÄis a string ùõº‚ààŒ£‚àó.
‚Ä¢ A configuration ùõºencodes all the state of the program at a particu-
lar iteration, including the array, scalar, and index variables.
‚Ä¢ If ùõºis a configuration, then ùõΩ= NEXTùëÉ(ùõº) denotes the configura-
tion of the computation after one more iteration. ùõΩis a string over Œ£
of length either |ùõº| or |ùõº| + 1, and every coordinate of ùõΩis a function
of just three coordinates in ùõº. That is, for every ùëó‚àà{0, ‚Ä¶ , |ùõΩ| ‚àí1},
ùõΩùëó= MAPùëÉ(ùõºùëó‚àí1, ùõºùëó, ùõºùëó+1) where MAPùëÉ‚à∂Œ£3 ‚ÜíŒ£ is some function
depending on ùëÉ.
‚Ä¢ There are simple conditions to check whether a string ùõºis a valid
starting configuration corresponding to an input ùë•, as well as to
check whether a string ùõºis a halting configuration. In particular
these conditions can be phrased as quantified mixed statements.
‚Ä¢ A program ùëÄhalts on input ùë•if and only if there exists a sequence
of configurations ùêª= (ùõº0, ùõº1, ‚Ä¶ , ùõºùëá‚àí1) such that (i) ùõº0 is a valid


--- Page 387 ---

is every theorem provable?
387
starting configuration of ùëÄwith input ùë•, (ii) ùõºùëá‚àí1 is a valid halting
configuration of ùëÉ, and (iii) ùõºùëñ+1 = NEXTùëÉ(ùõºùëñ) for every ùëñ‚àà
{0, ‚Ä¶ , ùëá‚àí2}.
We can encode such a sequence ùêªof configuration as a binary
string. For concreteness, we let ‚Ñì= ‚åàlog(|Œ£| + 1)‚åâand encode each
symbol ùúéin Œ£ ‚à™{"; "} by a string in {0, 1}‚Ñì. We use ‚Äú;‚Äù as a ‚Äúseparator‚Äù
symbol, and so encode ùêª= (ùõº0, ùõº1, ‚Ä¶ , ùõºùëá‚àí1) as the concatenation
of the encodings of each configuration, using ‚Äú;‚Äù to separate the en-
coding of ùõºùëñand ùõºùëñ+1 for every ùëñ‚àà[ùëá]. In particular for every Turing
Machine ùëÄ, ùëÄhalts on the input 0 if and only if the following state-
ment ùúëùëÄis true
‚àÉùêª‚àà{0,1}‚àóùêªencodes halting configuration sequence starting with input 0 .
(11.6)
If we can encode the statement ùúëùëÄas a mixed-integer statement
then, since ùúëùëÄis true if and only if HALTONZERO(ùëÄ) = 1, this
would reduce the task of computing HALTONZERO to computing
MIS, and hence imply (using Theorem 9.9 ) that MIS is uncomputable,
completing the proof. Indeed, ùúëùëÄcan be encoded as a mixed-integer
statement for the following reasons:
1. Let ùõº, ùõΩ‚àà{0, 1}‚àóbe two strings that encode configurations of ùëÄ.
We can define a quantified mixed predicate NEXT(ùõº, ùõΩ) that is true
if and only if ùõΩ= NEXTùëÄ(ùõΩ) (i.e., ùõΩencodes the configuration
obtained by proceeding from ùõºin one computational step). Indeed
NEXT(ùõº, ùõΩ) is true if for every ùëñ‚àà{0, ‚Ä¶ , |ùõΩ|} which is a multiple
of ‚Ñì, ùõΩùëñ,‚Ä¶,ùëñ+‚Ñì‚àí1 = MAPùëÄ(ùõºùëñ‚àí‚Ñì,‚ãØ,ùëñ+2‚Ñì‚àí1) where MAPùëÄ‚à∂{0, 1}3‚Ñì‚Üí
{0, 1}‚Ñìis the finite function above (identifying elements of Œ£ with
their encoding in {0, 1}‚Ñì). Since MAPùëÄis a finite function, we can
express it using the logical operations AND,OR, NOT (for example
by computing MAPùëÄwith NAND‚Äôs).
2. Using the above we can now write the condition that for every
substring of ùêªthat has the form ùõºENC(; )ùõΩwith ùõº, ùõΩ‚àà{0, 1}‚Ñì
and ENC(; ) being the encoding of the separator ‚Äú;‚Äù, it holds that
NEXT(ùõº, ùõΩ) is true.
3. Finally, if ùõº0 is a binary string encoding the initial configuration of
ùëÄon input 0, checking that the first |ùõº0| bits of ùêªequal ùõº0 can be
expressed using AND,OR, and NOT‚Äôs. Similarly checking that the
last configuration encoded by ùêªcorresponds to a state in which ùëÄ
will halt can also be expressed as a quantified statement.
Together the above yields a computable procedure that maps every
Turing Machine ùëÄinto a quantified mixed statement ùúëùëÄsuch that


--- Page 388 ---

388
introduction to theoretical computer science
HALTONZERO(ùëÄ) = 1 if and only if QMS(ùúëùëÄ) = 1. This reduces
computing HALTONZERO to computing QMS, and hence the uncom-
putability of HALTONZERO implies the uncomputability of QMS.
‚ñ†
R
Remark 11.14 ‚Äî Alternative proofs. There are sev-
eral other ways to show that QMS is uncomputable.
For example, we can express the condition that a 1-
dimensional cellular automaton eventually writes a
‚Äú1‚Äù to a given cell from a given initial configuration
as a quantified mixed statement over a string encod-
ing the history of all configurations. We can then use
the fact that cellular automatons can simulate Tur-
ing machines (Theorem 8.7) to reduce the halting
problem to QMS. We can also use other well known
uncomputable problems such as tiling or the post cor-
respondence problem. Exercise 11.5 and Exercise 11.6
explore two alternative proofs of Theorem 11.13.
11.5.2 Step 2: Reducing mixed statements to integer statements
We now show how to prove Theorem 11.9 using Theorem 11.13. The
idea is again a proof by reduction. We will show a transformation of
every quantifier mixed statement ùúëinto a quantified integer statement
ùúâthat does not use string-valued variables such that ùúëis true if and
only if ùúâis true.
To remove string-valued variables from a statement, we encode
every string by a pair integer. We will show that we can encode a
string ùë•‚àà{0, 1}‚àóby a pair of numbers (ùëã, ùëõ) ‚àà‚Ñïs.t.
‚Ä¢ ùëõ= |ùë•|
‚Ä¢ There is a quantified integer statement COORD(ùëã, ùëñ) that for every
ùëñ< ùëõ, will be true if ùë•ùëñ= 1 and will be false otherwise.
This will mean that we can replace a ‚Äúfor all‚Äù quantifier over strings
such as ‚àÄùë•‚àà{0,1}‚àówith a pair of quantifiers over integers of the form
‚àÄùëã‚àà‚Ñï‚àÄùëõ‚àà‚Ñï(and similarly replace an existential quantifier of the form
‚àÉùë•‚àà{0,1}‚àówith a pair of quantifiers ‚àÉùëã‚àà‚Ñï‚àÉùëõ‚àà‚Ñï) . We can then replace all
calls to |ùë•| by ùëõand all calls to ùë•ùëñby COORD(ùëã, ùëñ). This means that
if we are able to define COORD via a quantified integer statement,
then we obtain a proof of Theorem 11.9, since we can use it to map
every mixed quantified statement ùúëto an equivalent quantified inte-
ger statement ùúâsuch that ùúâis true if and only if ùúëis true, and hence
QMS(ùúë) = QIS(ùúâ). Such a procedure implies that the task of comput-
ing QMS reduces to the task of computing QIS, which means that the
uncomputability of QMS implies the uncomputability of QIS.


--- Page 389 ---

is every theorem provable?
389
The above shows that proof of Theorem 11.9 all boils down to find-
ing the right encoding of strings as integers, and the right way to
implement COORD as a quantified integer statement. To achieve this
we use the following technical result :
Lemma 11.15 ‚Äî Constructible prime sequence. There is a sequence of prime
numbers ùëù0 < ùëù1 < ùëù2 < ‚ãØsuch that there is a quantified integer
statement PSEQ(ùëù, ùëñ) that is true if and only if ùëù= ùëùùëñ.
Using Lemma 11.15 we can encode a ùë•‚àà{0, 1}‚àóby the numbers
(ùëã, ùëõ) where ùëã= ‚àèùë•ùëñ=1 ùëùùëñand ùëõ= |ùë•|. We can then define the
statement COORD(ùëã, ùëñ) as
COORD(ùëã, ùëñ) = ‚àÉùëù‚àà‚ÑïPSEQ(ùëù, ùëñ) ‚àßDIVIDES(ùëù, ùëã)
(11.7)
where DIVIDES(ùëé, ùëè), as before, is defined as ‚àÉùëê‚àà‚Ñïùëé√ó ùëê= ùëè. Note that
indeed if ùëã, ùëõencodes the string ùë•‚àà{0, 1}‚àó, then for every ùëñ< ùëõ,
COORD(ùëã, ùëñ) = ùë•ùëñ, since ùëùùëñdivides ùëãif and only if ùë•ùëñ= 1.
Thus all that is left to conclude the proof of Theorem 11.9 is to
prove Lemma 11.15, which we now proceed to do.
Proof. The sequence of prime numbers we consider is the following:
We fix ùê∂to be a sufficiently large constant (ùê∂= 2234 will do) and
define ùëùùëñto be the smallest prime number that is in the interval [(ùëñ+
ùê∂)3 + 1, (ùëñ+ ùê∂+ 1)3 ‚àí1]. It is known that there exists such a prime
number for every ùëñ‚àà‚Ñï. Given this, the definition of PSEQ(ùëù, ùëñ) is
simple:
(ùëù> (ùëñ+ùê∂)√ó(ùëñ+ùê∂)√ó(ùëñ+ùê∂))‚àß(ùëù< (ùëñ+ùê∂+1)√ó(ùëñ+ùê∂+1)√ó(ùëñ+ùê∂+1))‚àß(‚àÄùëù‚Ä≤¬¨PRIME(ùëù‚Ä≤) ‚à®(ùëù‚Ä≤ ‚â§ùëñ) ‚à®(ùëù‚Ä≤ ‚â•ùëù)) ,
(11.8)
We leave it to the reader to verify that PSEQ(ùëù, ùëñ) is true iff ùëù= ùëùùëñ.
‚ñ†
To sum up we have shown that for every quantified mixed state-
ment ùúë, we can compute a quantified integer statement ùúâsuch that
QMS(ùúë) = 1 if and only if QIS(ùúâ) = 1. Hence the uncomputability
of QMS (Theorem 11.13) implies the uncomputability of QIS, com-
pleting the proof of Theorem 11.9, and so also the proof of G√∂del‚Äôs
Incompleteness Theorem for quantified integer statements (Theo-
rem 11.8).
‚úì
Chapter Recap
‚Ä¢ Uncomputable functions include also functions
that seem to have nothing to do with NAND-TM
programs or other computational models such
as determining the satisfiability of Diophantine
equations.


--- Page 390 ---

390
introduction to theoretical computer science
4 Hint: think of ùë•as saying ‚ÄúTuring Machine ùëÄhalts
on input ùë¢‚Äù and ùë§being a proof that is the number of
steps that it will take for this to happen. Can you find
an always-halting ùëâthat will verify such statements?
Figure 11.3: In the puzzle problem, the input can be
thought of as a finite collection Œ£ of types of puz-
zle pieces and the goal is to find out whether or not
find a way to arrange pieces from these types in a
rectangle. Formally, we model the input as a pair of
functions ùëöùëéùë°ùëê‚Ñé‚Üî, ùëöùëéùë°ùëê‚Ñé‚Üï‚à∂Œ£2 ‚Üí{0, 1} that
such that ùëöùëéùë°ùëê‚Ñé‚Üî(ùëôùëíùëìùë°, ùëüùëñùëî‚Ñéùë°) = 1 (respectively
ùëöùëéùë°ùëê‚Ñé‚Üï(ùë¢ùëù, ùëëùëúùë§ùëõ) = 1 ) if the pair of pieces are
compatible when placed in their respective posi-
tions. We assume Œ£ contains a special symbol ‚àÖ
corresponding to having no piece, and an arrange-
ment of puzzle pieces by an (ùëö‚àí2) √ó (ùëõ‚àí2)
rectangle is modeled by a string ùë•‚ààŒ£ùëö‚ãÖùëõwhose
‚Äúouter coordinates‚Äô ‚Äô are ‚àÖand such that for every
ùëñ‚àà[ùëõ‚àí1], ùëó‚àà[ùëö‚àí1], ùëöùëéùë°ùëê‚Ñé‚Üï(ùë•ùëñ,ùëó, ùë•ùëñ+1,ùëó) = 1 and
ùëöùëéùë°ùëê‚Ñé‚Üî(ùë•ùëñ,ùëó, ùë•ùëñ,ùëó+1) = 1.
‚Ä¢ This also implies that for any sound proof system
(and in particular every finite axiomatic system) ùëÜ,
there are interesting statements ùëã(namely of the
form ‚Äúùêπ(ùë•)
=
0‚Äù for an uncomputable function
ùêπ) such that ùëÜis not able to prove either ùëãor its
negation.
11.6 EXERCISES
Exercise 11.1 ‚Äî G√∂del‚Äôs Theorem from uncomputability of ùëÑùêºùëÜ. Prove Theo-
rem 11.8 using Theorem 11.9.
‚ñ†
Exercise 11.2 ‚Äî Proof systems and uncomputability. Let FINDPROOF ‚à∂
{0, 1}‚àó‚Üí{0, 1} be the following function. On input a Turing machine
ùëâ(which we think of as the verifying algorithm for a proof system)
and a string ùë•‚àà{0, 1}‚àó, FINDPROOF(ùëâ, ùë•) = 1 if and only if there
exists ùë§‚àà{0, 1}‚àósuch that ùëâ(ùë•, ùë§) = 1.
1. Prove that FINDPROOF is uncomputable.
2. Prove that there exists a Turing machine ùëâsuch that ùëâhalts
on every input ùë•, ùë£but the function FINDPROOFùëâdefined as
FINDPROOFùëâ(ùë•) = FINDPROOF(ùëâ, ùë•) is uncomputable. See
footnote for hint.4
‚ñ†
Exercise 11.3 ‚Äî Expression for floor. Let FSQRT(ùëõ, ùëö) = ‚àÄùëó‚àà‚Ñï((ùëó√ó ùëó) >
ùëö) ‚à®(ùëó‚â§ùëõ). Prove that FSQRT(ùëõ, ùëö) is true if and only if ùëõ= ‚åä‚àöùëö‚åã.
‚ñ†
Exercise 11.4 ‚Äî axiomatic proof systems. For every representation of logical
statements as strings, we can define an axiomatic proof system to
consist of a finite set of strings ùê¥and a finite set of rules ùêº0, ‚Ä¶ , ùêºùëö‚àí1
with ùêºùëó‚à∂({0, 1}‚àó)ùëòùëó‚Üí{0, 1}‚àósuch that a proof (ùë†1, ‚Ä¶ , ùë†ùëõ) that ùë†ùëõ
is true is valid if for every ùëñ, either ùë†ùëñ‚ààùê¥or is some ùëó‚àà[ùëö] and
are ùëñ1, ‚Ä¶ , ùëñùëòùëó< ùëñsuch that ùë†ùëñ= ùêºùëó(ùë†ùëñ1, ‚Ä¶ , ùëñùëòùëó). A system is sound if
whenever there is no false ùë†such that there is a proof that ùë†is true.
Prove that for every uncomputable function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}
and every sound axiomatic proof system ùëÜ(that is characterized by a
finite number of axioms and inference rules), there is some input ùë•for
which the proof system ùëÜis not able to prove neither that ùêπ(ùë•) = 0
nor that ùêπ(ùë•) ‚â†0.
‚ñ†
Exercise 11.5 ‚Äî Post Corrrespondence Problem. In the Post Correspondence
Problem the input is a set ùëÜ= {(ùõº0, ùõΩ0), ‚Ä¶ , (ùõΩùëê‚àí1, ùõΩùëê‚àí1)} where each


--- Page 391 ---

is every theorem provable?
391
ùõºùëñand ùõΩùëóis a string in {0, 1}‚àó. We say that PCP(ùëÜ) = 1 if and only if
there exists a list (ùõº0, ùõΩ0), ‚Ä¶ , (ùõºùëö‚àí1, ùõΩùëö‚àí1) of pairs in ùëÜsuch that
ùõº0ùõº1 ‚ãØùõºùëö‚àí1 = ùõΩ0ùõΩ1 ‚ãØùõΩùëö‚àí1 .
(11.9)
(We can think of each pair (ùõº, ùõΩ) ‚ààùëÜas a ‚Äúdomino tile‚Äù and the ques-
tion is whether we can stack a list of such tiles so that the top and the
bottom yield the same string.) It can be shown that the PCP is uncom-
putable by a fairly straightforward though somewhat tedious proof
(see for example the Wikipedia page for the Post Correspondence
Problem or Section 5.2 in [Sip97]).
Use this fact to provide a direct proof that QMS is uncomputable by
showing that there exists a computable map ùëÖ‚à∂{0, 1}‚àó‚Üí{0, 1}‚àósuch
that PCP(ùëÜ) = QMS(ùëÖ(ùëÜ)) for every string ùëÜencoding an instance of
the post correspondence problem.
‚ñ†
Exercise 11.6 ‚Äî Uncomputability of puzzle. Let PUZZLE ‚à∂{0, 1}‚àó‚Üí{0, 1} be
the problem of determining, given a finite collection of types of ‚Äúpuz-
zle pieces‚Äù, whether it is possible to put them together in a rectangle,
see Fig. 11.3. Formally, we think of such a collection as a finite set Œ£
(see Fig. 11.3). We model the criteria as to which pieces ‚Äúfit together‚Äù
by a pair of finite function ùëöùëéùë°ùëê‚Ñé‚Üï, ùëöùëéùë°ùëê‚Ñé‚Üî‚à∂Œ£2 ‚Üí{0, 1} such that a
piece ùëéfits above a piece ùëèif and only if ùëöùëéùë°ùëê‚Ñé‚Üï(ùëé, ùëè) = 1 and a piece
ùëêfits to the left of a piece ùëëif and only if ùëöùëéùë°ùëê‚Ñé‚Üî(ùëê, ùëë) = 1. To model
the ‚Äústraight edge‚Äù pieces that can be placed next to a ‚Äúblank spot‚Äù
we assume that Œ£ contains the symbol ‚àÖand the matching functions
are defined accordingly. A square tiling of Œ£ is an ùëö√ó ùëõlong string
ùë•‚ààŒ£ùëöùëõ, such that for every ùëñ‚àà{1, ‚Ä¶ , ùëö‚àí2} and ùëó‚àà{1, ‚Ä¶ , ùëõ‚àí2},
ùëöùëéùë°ùëê‚Ñé(ùë•ùëñ,ùëó, ùë•ùëñ‚àí1,ùëó, ùë•ùëñ+1,ùëó, ùë•ùëñ,ùëó‚àí1, ùë•ùëñ,ùëó+1) = 1 (i.e., every ‚Äúinternal pieve‚Äù
fits in with the pieces adjacent to it). We also require all of the ‚Äúouter
pieces‚Äù (i.e., ùë•ùëñ,ùëówhere ùëñ‚àà{0, ùëö‚àí1} of ùëó‚àà{0, ùëõ‚àí1}) are ‚Äúblank‚Äù
or equal to ‚àÖ. The function PUZZLE takes as input a string describing
the set Œ£ and the function ùëöùëéùë°ùëê‚Ñéand outputs 1 if and only if there is
some square tiling of Œ£: some not all blank string ùë•‚ààŒ£ùëöùëõsatisfying
the above condition.
1. Prove that PUZZLE is uncomputable.
2. Give a reduction from PUZZLE to QMS.
‚ñ†
Exercise 11.7 ‚Äî MRDP exercise. The MRDP theorem states that the
problem of determining, given a ùëò-variable polynomial ùëùwith integer
coefficients, whether there exists integers ùë•0, ‚Ä¶ , ùë•ùëò‚àí1 such that
ùëù(ùë•0, ‚Ä¶ , ùë•ùëò‚àí1) = 0 is uncomputable. Consider the following quadratic


--- Page 392 ---

392
introduction to theoretical computer science
5 You can replace the equation ùë¶= ùë•4 with the pair
of equations ùë¶= ùëß2 and ùëß= ùë•2. Also, you can
replace the equation ùë§= ùë•6 with the three equations
ùë§= ùë¶ùë¢, ùë¶= ùë•4 and ùë¢= ùë•2.
6 You will not need to use very specific properties of
the TOWER function in this exercise. For example,
NBB(ùëõ) also grows faster than the Ackerman func-
tion. You might find Aaronson‚Äôs blog post on the
same topic to be quite interesting, and relevant to this
book at large. If you like it then you might also enjoy
this piece by Terence Tao.
integer equation problem: the input is a list of polynomials ùëù0, ‚Ä¶ , ùëùùëö‚àí1
over ùëòvariables with integer coefficients, where each of the polynomi-
als is of degree at most two (i.e., it is a quadratic function). The goal
is to determine whether there exist integers ùë•0, ‚Ä¶ , ùë•ùëò‚àí1 that solve the
equations ùëù0(ùë•) = ‚ãØ= ùëùùëö‚àí1(ùë•) = 0.
Use the MRDP Theorem to prove that this problem is uncom-
putable. That is, show that the function QUADINTEQ ‚à∂{0, 1}‚àó‚Üí
{0, 1} is uncomputable, where this function gets as input a string de-
scribing the polynomials ùëù0, ‚Ä¶ , ùëùùëö‚àí1 (each with integer coefficients
and degree at most two), and outputs 1 if and only if there exists
ùë•0, ‚Ä¶ , ùë•ùëò‚àí1 ‚àà‚Ñ§such that for every ùëñ‚àà[ùëö], ùëùùëñ(ùë•0, ‚Ä¶ , ùë•ùëò‚àí1) = 0. See
footnote for hint5
‚ñ†
Exercise 11.8 ‚Äî The Busy Beaver problem. In this question we define the
NAND-TM variant of the busy beaver function.
1. We define the function ùëá‚à∂{0, 1}‚àó‚Üí‚Ñïas follows: for every
string ùëÉ‚àà{0, 1}‚àó, if ùëÉrepresents a NAND-TM program such that
when ùëÉis executed on the input 0 (i.e., the string of length 1 that is
simply 0), a total of ùëÄlines are executed before the program halts,
then ùëá(ùëÉ) = ùëÄ. Otherwise (if ùëÉdoes not represent a NAND-TM
program, or it is a program that does not halt on 0), ùëá(ùëÉ) = 0.
Prove that ùëáis uncomputable.
2. Let TOWER(ùëõ) denote the number 222...2
‚èü
ùëõtimes
(that is, a ‚Äútower of pow-
ers of two‚Äù of height ùëõ). To get a sense of how fast this function
grows, TOWER(1) = 2, TOWER(2) = 22 = 4, TOWER(3) = 222 =
16, TOWER(4) = 216 = 65536 and TOWER(5) = 265536 which
is about 1020000. TOWER(6) is already a number that is too big to
write even in scientific notation. Define NBB ‚à∂‚Ñï‚Üí‚Ñï(for ‚ÄúNAND-
TM Busy Beaver‚Äù) to be the function NBB(ùëõ) = maxùëÉ‚àà{0,1}ùëõùëá(ùëÉ)
where ùëá‚à∂‚Ñï‚Üí‚Ñïis the function defined in Item 1. Prove that
NBB grows faster than TOWER, in the sense that TOWER(ùëõ) =
ùëú(NBB(ùëõ)) (i.e., for every ùúñ> 0, there exists ùëõ0 such that for every
ùëõ> ùëõ0, TOWER(ùëõ) < ùúñ‚ãÖNBB(ùëõ).).6
‚ñ†
11.7 BIBLIOGRAPHICAL NOTES
As mentioned before, G√∂del, Escher, Bach [Hof99] is a highly recom-
mended book covering G√∂del‚Äôs Theorem. A classic popular science
book about Fermat‚Äôs Last Theorem is [Sin97].
Cantor‚Äôs are used for both Turing and G√∂del‚Äôs theorems. In a twist
of fate, using techniques originating from the works of G√∂del and Tur-


--- Page 393 ---

is every theorem provable?
393
ing, Paul Cohen showed in 1963 that Cantor‚Äôs Continuum Hypothesis
is independent of the axioms of set theory, which means that neither
it nor its negation is provable from these axioms and hence in some
sense can be considered as ‚Äúneither true nor false‚Äù (see [Coh08]). The
Continuum Hypothesis is the conjecture that for every subset ùëÜof ‚Ñù,
either there is a one-to-one and onto map between ùëÜand ‚Ñïor there
is a one-to-one and onto map between ùëÜand ‚Ñù. It was conjectured
by Cantor and listed by Hilbert in 1900 as one of the most important
problems in mathematics. See also the non-conventional survey of
Shelah [She03]. See here for recent progress on a related question.
Thanks to Alex Lombardi for pointing out an embarrassing mistake
in the description of Fermat‚Äôs Last Theorem. (I said that it was open
for exponent 11 before Wiles‚Äô work.)


--- Page 394 ---



--- Page 395 ---

III
EFFICIENT ALGORITHMS


--- Page 396 ---



--- Page 397 ---

12
Efficient computation: An informal introduction
‚ÄúThe problem of distinguishing prime numbers from composite and of resolving
the latter into their prime factors is ‚Ä¶ one of the most important and useful
in arithmetic ‚Ä¶ Nevertheless we must confess that all methods ‚Ä¶ are either
restricted to very special cases or are so laborious ‚Ä¶ they try the patience of
even the practiced calculator ‚Ä¶ and do not apply at all to larger numbers.‚Äù,
Carl Friedrich Gauss, 1798
‚ÄúFor practical purposes, the difference between algebraic and exponential order
is often more crucial than the difference between finite and non-finite.‚Äù, Jack
Edmunds, ‚ÄúPaths, Trees, and Flowers‚Äù, 1963
‚ÄúWhat is the most efficient way to sort a million 32-bit integers?‚Äù, Eric
Schmidt to Barack Obama, 2008
‚ÄúI think the bubble sort would be the wrong way to go.‚Äù, Barack Obama.
So far we have been concerned with which functions are computable
and which ones are not. In this chapter we look at the finer question
of the time that it takes to compute functions, as a function of their input
length. Time complexity is extremely important to both the theory and
practice of computing, but in introductory courses, coding interviews,
and software development, terms such as ‚ÄúùëÇ(ùëõ) running time‚Äù are of-
ten used in an informal way. People don‚Äôt have a precise definition of
what a linear-time algorithm is, but rather assume that ‚Äúthey‚Äôll know
it when they see it‚Äù. In this book we will define running time pre-
cisely, using the mathematical models of computation we developed
in the previous chapters. This will allow us to ask (and sometimes
answer) questions such as:
‚Ä¢ ‚ÄúIs there a function that can be computed in ùëÇ(ùëõ2) time but not in
ùëÇ(ùëõ) time?‚Äù
‚Ä¢ ‚ÄúAre there natural problems for which the best algorithm (and not
just the best known) requires 2‚Ñ¶(ùëõ) time?‚Äù
Compiled on 8.26.2020 18:10
Learning Objectives:
‚Ä¢ Describe at a high level some interesting
computational problems.
‚Ä¢ The difference between polynomial and
exponential time.
‚Ä¢ Examples of techniques for obtaining efficient
algorithms
‚Ä¢ Examples of how seemingly small differences
in problems can potentially make huge
differences in their computational complexity.


--- Page 398 ---

398
introduction to theoretical computer science
ÔÉ´Big Idea 16 The running time of an algorithm is not a number, it is
a function of the length of the input.
This chapter: A non-mathy overview
In this chapter, we informally survey examples of compu-
tational problems. For some of these problems we know
efficient (i.e., ùëÇ(ùëõùëê)-time for a small constant ùëê) algorithms,
and for others the best known algorithms are exponential.
We present these examples to get a feel as to the kinds of
problems that lie on each side of this divide and also see how
sometimes seemingly minor changes in problem formula-
tion can make the (known) complexity of a problem ‚Äújump‚Äù
from polynomial to exponential. We do not formally define
the notion of running time in this chapter, but use the same
‚ÄúI know it when I see it‚Äù notion of an ùëÇ(ùëõ) or ùëÇ(ùëõ2) time
algorithms as the one you‚Äôve seen in introduction to com-
puter science courses. We will see the precise definition of
running time (using Turing machines and RAM machines /
NAND-RAM) in Chapter 13.
While the difference between ùëÇ(ùëõ) and ùëÇ(ùëõ2) time can be crucial in
practice, in this book we focus on the even bigger difference between
polynomial and exponential running time. As we will see, the difference
between polynomial versus exponential time is typically insensitive to
the choice of the particular computational model, a polynomial-time
algorithm is still polynomial whether you use Turing machines, RAM
machines, or parallel cluster as your model of computation, and sim-
ilarly an exponential-time algorithm will remain exponential in all of
these platforms. One of the interesting phenomena of computing is
that there is often a kind of a ‚Äúthreshold phenomenon‚Äù or ‚Äúzero-one
law‚Äù for running time. Many natural problems can either be solved
in polynomial running time with a not-too-large exponent (e.g., some-
thing like ùëÇ(ùëõ2) or ùëÇ(ùëõ3)), or require exponential (e.g., at least 2‚Ñ¶(ùëõ)
or 2‚Ñ¶(‚àöùëõ)) time to solve. The reasons for this phenomenon are still not
fully understood, but some light on it is shed by the concept of NP
completeness, which we will see in Chapter 15.
This chapter is merely a tiny sample of the landscape of computa-
tional problems and efficient algorithms. If you want to explore the
field of algorithms and data structures more deeply (which I very
much hope you do!), the bibliographical notes contain references to
some excellent texts, some of which are available freely on the web.


--- Page 399 ---

efficient computation: an informal introduction
399
R
Remark 12.1 ‚Äî Relations between parts of this book.
Part I of this book contained a quantitative study of
computation of finite functions. We asked what are
the resources (in terms of gates of Boolean circuits or
lines in straight-line programs) required to compute
various finite functions.
Part II of the book contained a qualitative study of
computation of infinite functions (i.e., functions of
unbounded input length). In that part we asked the
qualitative question of whether or not a function is com-
putable at all, regardless of the number of operations.
Part III of the book, beginning with this chapter,
merges the two approaches and contains a quantitative
study of computation of infinite functions. In this part
we ask how do resources for computing a function
scale with the length of the input. In Chapter 13 we
define the notion of running time, and the class P of
functions that can be computed using a number of
steps that scales polynomially with the input length.
In Section 13.6 we will relate this class to the models
of Boolean circuits and straightline programs that we
studied in Part I.
12.1 PROBLEMS ON GRAPHS
In this chapter we discuss several examples of important computa-
tional problems. Many of the problems will involve graphs. We have
already encountered graphs before (see Section 1.4.4) but now quickly
recall the basic notation. A graph ùê∫consists of a set of vertices ùëâand
edges ùê∏where each edge is a pair of vertices. We typically denote by
ùëõthe number of vertices (and in fact often consider graphs where the
set of vertices ùëâequals the set [ùëõ] of the integers between 0 and ùëõ‚àí1).
In a directed graph, an edge is an ordered pair (ùë¢, ùë£), which we some-
times denote as ‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó
ùë¢ùë£. In an undirected graph, an edge is an unordered
pair (or simply a set) {ùë¢, ùë£} which we sometimes denote as ùë¢ùë£or
ùë¢‚àºùë£. An equivalent viewpoint is that an undirected graph corre-
sponds to a directed graph satisfying the property that whenever the
edge ‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó
ùë¢ùë£is present then so is the edge ‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó
ùë£ùë¢. In this chapter we restrict
our attention to graphs that are undirected and simple (i.e., containing
no parallel edges or self-loops). Graphs can be represented either in
the adjacency list or adjacency matrix representation. We can transform
between these two representations using ùëÇ(ùëõ2) operations, and hence
for our purposes we will mostly consider them as equivalent.
Graphs are so ubiquitous in computer science and other sciences
because they can be used to model a great many of the data that we
encounter. These are not just the ‚Äúobvious‚Äù data such as the road


--- Page 400 ---

400
introduction to theoretical computer science
Figure 12.1: Some examples of graphs found on the
Internet.
1 A queue is a data structure for storing a list of el-
ements in ‚ÄúFirst In First Out (FIFO)‚Äù order. Each
‚Äúpop‚Äù operation removes an element from the queue
in the order that they were ‚Äúpushed‚Äù into it; see the
Wikipedia page.
network (which can be thought of as a graph of whose vertices are
locations with edges corresponding to road segments), or the web
(which can be thought of as a graph whose vertices are web pages
with edges corresponding to links), or social networks (which can
be thought of as a graph whose vertices are people and the edges
correspond to friend relation). Graphs can also denote correlations in
data (e.g., graph of observations of features with edges corresponding
to features that tend to appear together), causal relations (e.g., gene
regulatory networks, where a gene is connected to gene products it
derives), or the state space of a system (e.g., graph of configurations
of a physical system, with edges corresponding to states that can be
reached from one another in one step).
12.1.1 Finding the shortest path in a graph
The shortest path problem is the task of finding, given a graph ùê∫=
(ùëâ, ùê∏) and two vertices ùë†, ùë°‚ààùëâ, the length of the shortest path
between ùë†and ùë°(if such a path exists). That is, we want to find the
smallest number ùëòsuch that there are vertices ùë£0, ùë£1, ‚Ä¶ , ùë£ùëòwith ùë£0 = ùë†,
ùë£ùëò= ùë°and for every ùëñ‚àà{0, ‚Ä¶ , ùëò‚àí1} an edge between ùë£ùëñand ùë£ùëñ+1. For-
mally, we define MINPATH ‚à∂{0, 1}‚àó‚Üí{0, 1}‚àóto be the function that
on input a triple (ùê∫, ùë†, ùë°) (represented as a string) outputs the number
ùëòwhich is the length of the shortest path in ùê∫between ùë†and ùë°or a
string representing no path if no such path exists. (In practice people
often want to also find the actual path and not just its length; it turns
out that the algorithms to compute the length of the path often yield
the actual path itself as a byproduct, and so everything we say about
the task of computing the length also applies to the task of finding the
path.)
If each vertex has at least two neighbors then there can be an expo-
nential number of paths from ùë†to ùë°, but fortunately we do not have to
enumerate them all to find the shortest path. We can find the short-
est path using a breadth first search (BFS), enumerating ùë†‚Äôs neigh-
bors, and then neighbors‚Äô neighbors, etc.. in order. If we maintain
the neighbors in a list we can perform a BFS in ùëÇ(ùëõ2) time, while us-
ing a queue we can do this in ùëÇ(ùëö) time.1 Dijkstra‚Äôs algorithm is a
well-known generalization of BFS to weighted graphs. More formally,
the algorithm for computing the function MINPATH is described in
Algorithm 12.2.


--- Page 401 ---

efficient computation: an informal introduction
401
Algorithm 12.2 ‚Äî Shortest path via BFS.
Input: Graph ùê∫= (ùëâ, ùê∏) and vertices ùë†, ùë°‚ààùëâ. Assume
ùëâ= [ùëõ].
Output: Length ùëòof shortest path from ùë†to ùë°or ‚àûif no
such path exists.
1: Let ùê∑be length-ùëõarray.
2: Set ùê∑[ùë†] = 0 and ùê∑[ùëñ] = ‚àûfor all ùëñ‚àà[ùëõ] ‚ßµ{ùë†}.
3: Initialize queue ùëÑto contain ùë†.
4: while ùëÑnon empty do
5:
Pop ùë£from ùëÑ
6:
if ùë£= ùë°then
7:
return ùê∑[ùë£]
8:
end if
9:
for ùë¢neighbor of ùë£with ùê∑[ùë¢] = ‚àûdo
10:
Set ùê∑[ùë¢] ‚Üêùê∑[ùë£] + 1
11:
Add ùë¢to ùëÑ.
12:
end for
13: end while
14: return ‚àû
Since we only add to the queue vertices ùë§with ùê∑[ùë§] = ‚àû(and
then immediately set ùê∑[ùë§] to an actual number), we never push to
the queue a vertex more than once, and hence the algorithm makes at
most ùëõ‚Äúpush‚Äù and ‚Äúpop‚Äù operations. For each vertex ùë£, the number
of times we run the inner loop is equal to the degree of ùë£and hence
the total running time is proportional to the sum of all degrees which
equals twice the number ùëöof edges. Algorithm 12.2 returns the cor-
rect answer since the vertices are added to the queue in the order of
their distance from ùë†, and hence we will reach ùë°after we have explored
all the vertices that are closer to ùë†than ùë°.
R
Remark 12.3 ‚Äî On data structures. If you‚Äôve ever taken
an algorithms course, you have probably encountered
many data structures such as lists, arrays, queues,
stacks, heaps, search trees, hash tables and many
more. Data structures are extremely important in com-
puter science, and each one of those offers different
tradeoffs between overhead in storage, operations
supported, cost in time for each operation, and more.
For example, if we store ùëõitems in a list, we will need
a linear (i.e., ùëÇ(ùëõ) time) scan to retrieve an element,
while we achieve the same operation in ùëÇ(1) time if
we used a hash table. However, when we only care
about polynomial-time algorithms, such factors of
ùëÇ(ùëõ) in the running time will not make much differ-


--- Page 402 ---

402
introduction to theoretical computer science
Figure 12.2: A knight‚Äôs tour can be thought of as a
maximally long path on the graph corresponding to
a chessboard where we put an edge between any two
squares that can be reached by one step via a legal
knight move.
ence. Similarly, if we don‚Äôt care about the difference
between ùëÇ(ùëõ) and ùëÇ(ùëõ2), then it doesn‚Äôt matter if we
represent graphs as adjacency lists or adjacency matri-
ces. Hence we will often describe our algorithms at a
very high level, without specifying the particular data
structures that are used to implement them. How-
ever, it will always be clear that there exists some data
structure that is sufficient for our purposes.
12.1.2 Finding the longest path in a graph
The longest path problem is the task of finding the length of the longest
simple (i.e., non intersecting) path between a given pair of vertices
ùë†and ùë°in a given graph ùê∫. If the graph is a road network, then the
longest path might seem less motivated than the shortest path (unless
you are the kind of person that always prefers the ‚Äúscenic route‚Äù).
But graphs can and are used to model a variety of phenomena, and in
many such cases finding the longest path (and some of its variants)
can be very useful. In particular, finding the longest path is a gener-
alization of the famous Hamiltonian path problem which asks for a
maximally long simple path (i.e., path that visits all ùëõvertices once)
between ùë†and ùë°, as well as the notorious traveling salesman problem
(TSP) of finding (in a weighted graph) a path visiting all vertices of
cost at most ùë§. TSP is a classical optimization problem, with appli-
cations ranging from planning and logistics to DNA sequencing and
astronomy.
Surprisingly, while we can find the shortest path in ùëÇ(ùëö) time,
there is no known algorithm for the longest path problem that signif-
icantly improves on the trivial ‚Äúexhaustive search‚Äù or ‚Äúbrute force‚Äù
algorithm that enumerates all the exponentially many possibilities
for such paths. Specifically, the best known algorithms for the longest
path problem take ùëÇ(ùëêùëõ) time for some constant ùëê> 1. (At the mo-
ment the best record is ùëê‚àº1.65 or so; even obtaining an ùëÇ(2ùëõ) time
bound is not that simple, see Exercise 12.1.)
12.1.3 Finding the minimum cut in a graph
Given a graph ùê∫= (ùëâ, ùê∏), a cut of ùê∫is a subset ùëÜ‚äÜùëâsuch that ùëÜ
is neither empty nor is it all of ùëâ. The edges cut by ùëÜare those edges
where one of their endpoints is in ùëÜand the other is in ùëÜ= ùëâ‚ßµùëÜ. We
denote this set of edges by ùê∏(ùëÜ, ùëÜ). If ùë†, ùë°‚ààùëâare a pair of vertices
then an ùë†, ùë°cut is a cut such that ùë†‚ààùëÜand ùë°‚ààùëÜ(see Fig. 12.3).
The minimum ùë†, ùë°cut problem is the task of finding, given ùë†and ùë°, the
minimum number ùëòsuch that there is an ùë†, ùë°cut cutting ùëòedges (the
problem is also sometimes phrased as finding the set that achieves
this minimum; it turns out that algorithms to compute the number
often yield the set as well). Formally, we define MINCUT ‚à∂{0, 1}‚àó‚Üí


--- Page 403 ---

efficient computation: an informal introduction
403
Figure 12.3: A cut in a graph ùê∫= (ùëâ, ùê∏) is simply a
subset ùëÜof its vertices. The edges that are cut by ùëÜ
are all those whose one endpoint is in ùëÜand the other
one is in ùëÜ= ùëâ‚ßµùëÜ. The cut edges are colored red in
this figure.
{0, 1}‚àóto be the function that on input a string representing a triple
(ùê∫= (ùëâ, ùê∏), ùë†, ùë°) of a graph and two vertices, outputs the minimum
number ùëòsuch that there exists a set ùëÜ‚äÜùëâwith ùë†‚ààùëÜ, ùë°‚àâùëÜand
|ùê∏(ùëÜ, ùëÜ)| = ùëò.
Computing minimum ùë†, ùë°cuts is useful in many applications since
minimum cuts often correspond to bottlenecks. For example, in a com-
munication or railroad network the minimum cut between ùë†and ùë°
corresponds to the smallest number of edges that, if dropped, will
disconnect ùë†from ùë°. (This was actually the original motivation for this
problem; see Section 12.6.) Similar applications arise in scheduling
and planning. In the setting of image segmentation, one can define a
graph whose vertices are pixels and whose edges correspond to neigh-
boring pixels of distinct colors. If we want to separate the foreground
from the background then we can pick (or guess) a foreground pixel ùë†
and background pixel ùë°and ask for a minimum cut between them.
The naive algorithm for computing MINCUT will check all 2ùëõpos-
sible subsets of an ùëõ-vertex graph, but it turns out we can do much
better than that. As we‚Äôve seen in this book time and again, there is
more than one algorithm to compute the same function, and some
of those algorithms might be more efficient than others. Luckily the
minimum cut problem is one of those cases. In particular, as we will
see in the next section, there are algorithms that compute MINCUT in
time which is polynomial in the number of vertices.
12.1.4 Min-Cut Max-Flow and Linear programming
We can obtain a polynomial-time algorithm for computing MINCUT
using the Max-Flow Min-Cut Theorem. This theorem says that the
minimum cut between ùë†and ùë°equals the maximum amount of flow
we can send from ùë†to ùë°, if every edge has unit capacity. Specifically,
imagine that every edge of the graph corresponded to a pipe that
could carry one unit of fluid per one unit of time (say 1 liter of water
per second). The maximum ùë†, ùë°flow is the maximum units of water
that we could transfer from ùë†to ùë°over these pipes. If there is an ùë†, ùë°
cut of ùëòedges, then the maximum flow is at most ùëò. The reason is
that such a cut ùëÜacts as a ‚Äúbottleneck‚Äù since at most ùëòunits can flow
from ùëÜto its complement at any given unit of time. This means that
the maximum ùë†, ùë°flow is always at most the value of the minimum
ùë†, ùë°cut. The surprising and non-trivial content of the Max-Flow Min-
Cut Theorem is that the maximum flow is also at least the value of the
minimum cut, and hence computing the cut is the same as computing
the flow.
The Max-Flow Min-Cut Theorem reduces the task of computing a
minimum cut to the task of computing a maximum flow. However, this
still does not show how to compute such a flow. The Ford-Fulkerson


--- Page 404 ---

404
introduction to theoretical computer science
Algorithm is a direct way to compute a flow using incremental im-
provements. But computing flows in polynomial time is also a special
case of a much more general tool known as linear programming.
A flow on a graph ùê∫of ùëöedges can be modeled as a vector ùë•‚àà‚Ñùùëö
where for every edge ùëí, ùë•ùëícorresponds to the amount of water per
time-unit that flows on ùëí. We think of an edge ùëías an ordered pair
(ùë¢, ùë£) (we can choose the order arbitrarily) and let ùë•ùëíbe the amount
of flow that goes from ùë¢to ùë£. (If the flow is in the other direction then
we make ùë•ùëínegative.) Since every edge has capacity one, we know
that ‚àí1 ‚â§ùë•ùëí‚â§1 for every edge ùëí. A valid flow has the property that
the amount of water leaving the source ùë†is the same as the amount
entering the sink ùë°, and that for every other vertex ùë£, the amount of
water entering and leaving ùë£is the same.
Mathematically, we can write these conditions as follows:
‚àë
ùëí‚àãùë†
ùë•ùëí+ ‚àë
ùëí‚àãùë°
ùë•ùëí= 0
‚àë
ùëí‚àãùë£
ùë•ùëí= 0
‚àÄùë£‚ààùëâ‚ßµ{ùë†,ùë°}
‚àí1 ‚â§ùë•ùëí‚â§1
‚àÄùëí‚ààùê∏
(12.1)
where for every vertex ùë£, summing over ùëí‚àãùë£means summing over all
the edges that touch ùë£.
The maximum flow problem can be thought of as the task of max-
imizing ‚àëùëí‚àãùë†ùë•ùëíover all the vectors ùë•‚àà‚Ñùùëöthat satisfy the above
conditions (12.1). Maximizing a linear function ‚Ñì(ùë•) over the set of
ùë•‚àà‚Ñùùëöthat satisfy certain linear equalities and inequalities is known
as linear programming. Luckily, there are polynomial-time algorithms
for solving linear programming, and hence we can solve the maxi-
mum flow (and so, equivalently, minimum cut) problem in polyno-
mial time. In fact, there are much better algorithms for maximum-
flow/minimum-cut, even for weighted directed graphs, with currently
the record standing at ùëÇ(min{ùëö10/7, ùëö‚àöùëõ}) time.
Solved Exercise 12.1 ‚Äî Global minimum cut. Given a graph ùê∫= (ùëâ, ùê∏),
define the global minimum cut of ùê∫to be the minimum over all ùëÜ‚äÜùëâ
with ùëÜ‚â†‚àÖand ùëÜ‚â†ùëâof the number of edges cut by ùëÜ. Prove that
there is a polynomial-time algorithm to compute the global minimum
cut of a graph.
‚ñ†
Solution:
By the above we know that there is a polynomial-time algorithm
ùê¥that on input (ùê∫, ùë†, ùë°) finds the minimum ùë†, ùë°cut in the graph


--- Page 405 ---

efficient computation: an informal introduction
405
Figure 12.4: In a convex function ùëì(left figure), for
every ùë•and ùë¶and ùëù‚àà[0, 1] it holds that ùëì(ùëùùë•+ (1 ‚àí
ùëù)ùë¶) ‚â§ùëù‚ãÖùëì(ùë•)+(1‚àíùëù)‚ãÖùëì(ùë¶). In particular this means
that every local minimum of ùëìis also a global minimum.
In contrast in a non convex function there can be many
local minima.
Figure 12.5: In the high dimensional case, if ùëìis a
convex function (left figure) the global minimum
is the only local minimum, and we can find it by
a local-search algorithm which can be thought of
as dropping a marble and letting it ‚Äúslide down‚Äù
until it reaches the global minimum. In contrast, a
non-convex function (right figure) might have an
exponential number of local minima in which any
local-search algorithm could get stuck.
ùê∫. Using ùê¥, we can obtain an algorithm ùêµthat on input a graph ùê∫
computes the global minimum cut as follows:
1. For every distinct pair ùë†, ùë°
‚àà
ùëâ, Algorithms ùêµsets ùëòùë†,ùë°
‚Üê
ùê¥(ùê∫, ùë†, ùë°).
2. ùêµreturns the minimum of ùëòùë†,ùë°over all distinct pairs ùë†, ùë°
The running time of ùêµwill be ùëÇ(ùëõ2) times the running time of ùê¥
and hence polynomial time. Moreover, if the the global minimum
cut is ùëÜ, then when ùêµreaches an iteration with ùë†‚ààùëÜand ùë°‚àâùëÜit
will obtain the value of this cut, and hence the value output by ùêµ
will be the value of the global minimum cut.
The above is our first example of a reduction in the context of
polynomial-time algorithms. Namely, we reduced the task of com-
puting the global minimum cut to the task of computing minimum
ùë†, ùë°cuts.
‚ñ†
12.1.5 Finding the maximum cut in a graph
The maximum cut problem is the task of finding, given an input graph
ùê∫= (ùëâ, ùê∏), the subset ùëÜ‚äÜùëâthat maximizes the number of edges
cut by ùëÜ. (We can also define an ùë†, ùë°-cut variant of the maximum cut
like we did for minimum cut; the two variants have similar complexity
but the global maximum cut is more common in the literature.) Like
its cousin the minimum cut problem, the maximum cut problem is
also very well motivated. For example, maximum cut arises in VLSI
design, and also has some surprising relation to analyzing the Ising
model in statistical physics.
Surprisingly, while (as we‚Äôve seen) there is a polynomial-time al-
gorithm for the minimum cut problem, there is no known algorithm
solving maximum cut much faster than the trivial ‚Äúbrute force‚Äù algo-
rithm that tries all 2ùëõpossibilities for the set ùëÜ.
12.1.6 A note on convexity
There is an underlying reason for the sometimes radical difference
between the difficulty of maximizing and minimizing a function over
a domain. If ùê∑‚äÜ‚Ñùùëõ, then a function ùëì‚à∂ùê∑‚ÜíùëÖis convex if for every
ùë•, ùë¶‚ààùê∑and ùëù‚àà[0, 1] ùëì(ùëùùë•+ (1 ‚àíùëù)ùë¶) ‚â§ùëùùëì(ùë•) + (1 ‚àíùëù)ùëì(ùë¶). That
is, ùëìapplied to the ùëù-weighted midpoint between ùë•and ùë¶is smaller
than the ùëù-weighted average value of ùëì. If ùê∑itself is convex (which
means that if ùë•, ùë¶are in ùê∑then so is the line segment between them),
then this means that if ùë•is a local minimum of ùëìthen it is also a global
minimum. The reason is that if ùëì(ùë¶) < ùëì(ùë•) then every point ùëß=
ùëùùë•+ (1 ‚àíùëù)ùë¶on the line segment between ùë•and ùë¶will satisfy ùëì(ùëß) ‚â§
ùëùùëì(ùë•) + (1 ‚àíùëù)ùëì(ùë¶) < ùëì(ùë•) and hence in particular ùë•cannot be a local


--- Page 406 ---

406
introduction to theoretical computer science
minimum. Intuitively, local minima of functions are much easier to
find than global ones: after all, any ‚Äúlocal search‚Äù algorithm that keeps
finding a nearby point on which the value is lower, will eventually
arrive at a local minima. One example of such a local search algorithm
is gradient descent which takes a sequence of small steps, each one in
the direction that would reduce the value by the most amount based
on the current derivative.
Indeed, under certain technical conditions, we can often efficiently
find the minimum of convex functions over a convex domain, and
this is the reason why problems such as minimum cut and shortest
path are easy to solve. On the other hand, maximizing a convex func-
tion over a convex domain (or equivalently, minimizing a concave
function) can often be a hard computational task. A linear function
is both convex and concave, which is the reason that both the maxi-
mization and minimization problems for linear functions can be done
efficiently.
The minimum cut problem is not a priori a convex minimization
task, because the set of potential cuts is discrete and not continuous.
However, it turns out that we can embed it in a continuous and con-
vex set via the (linear) maximum flow problem. The ‚Äúmax flow min
cut‚Äù theorem ensures that this embedding is ‚Äútight‚Äù in the sense that
the minimum ‚Äúfractional cut‚Äù that we obtain through the maximum-
flow linear program will be the same as the true minimum cut. Un-
fortunately, we don‚Äôt know of such a tight embedding in the setting of
the maximum cut problem.
Convexity arises time and again in the context of efficient computa-
tion. For example, one of the basic tasks in machine learning is empir-
ical risk minimization. This is the task of finding a classifier for a given
set of training examples. That is, the input is a list of labeled examples
(ùë•0, ùë¶0), ‚Ä¶ , (ùë•ùëö‚àí1, ùë¶ùëö‚àí1), where each ùë•ùëñ‚àà{0, 1}ùëõand ùë¶ùëñ‚àà{0, 1},
and the goal is to find a classifier ‚Ñé‚à∂{0, 1}ùëõ‚Üí{0, 1} (or sometimes
‚Ñé‚à∂{0, 1}ùëõ‚Üí‚Ñù) that minimizes the number of errors. More generally,
we want to find ‚Ñéthat minimizes
ùëö‚àí1
‚àë
ùëñ=0
ùêø(ùë¶ùëñ, ‚Ñé(ùë•ùëñ))
(12.2)
where ùêøis some loss function measuring how far is the predicted la-
bel ‚Ñé(ùë•ùëñ) from the true label ùë¶ùëñ. When ùêøis the square loss function
ùêø(ùë¶, ùë¶‚Ä≤) = (ùë¶‚àíùë¶‚Ä≤)2 and ‚Ñéis a linear function, empirical risk mini-
mization corresponds to the well-known convex minimization task of
linear regression. In other cases, when the task is non convex, there can
be many global or local minima. That said, even if we don‚Äôt find the
global (or even a local) minima, this continuous embedding can still
help us. In particular, when running a local improvement algorithm


--- Page 407 ---

efficient computation: an informal introduction
407
such as Gradient Descent, we might still find a function ‚Ñéthat is ‚Äúuse-
ful‚Äù in the sense of having a small error on future examples from the
same distribution.
12.2 BEYOND GRAPHS
Not all computational problems arise from graphs. We now list some
other examples of computational problems that are of great interest.
12.2.1 SAT
A propositional formula ùúëinvolves ùëõvariables ùë•1, ‚Ä¶ , ùë•ùëõand the logical
operators AND (‚àß), OR (‚à®), and NOT (¬¨, also denoted as ‚ãÖ). We say
that such a formula is in conjunctive normal form (CNF for short) if it is
an AND of ORs of variables or their negations (we call a term of the
form ùë•ùëñor ùë•ùëña literal). For example, this is a CNF formula
(ùë•7 ‚à®ùë•22 ‚à®ùë•15) ‚àß(ùë•37 ‚à®ùë•22) ‚àß(ùë•55 ‚à®ùë•7)
(12.3)
The satisfiability problem is the task of determining, given a CNF
formula ùúë, whether or not there exists a satisfying assignment for ùúë. A
satisfying assignment for ùúëis a string ùë•‚àà{0, 1}ùëõsuch that ùúëevalu-
ates to True if we assign its variables the values of ùë•. The SAT problem
might seem as an abstract question of interest only in logic but in fact
SAT is of huge interest in industrial optimization, with applications
including manufacturing planning, circuit synthesis, software verifica-
tion, air-traffic control, scheduling sports tournaments, and more.
2SAT.
We say that a formula is a ùëò-CNF it is an AND of ORs where
each OR involves exactly ùëòliterals. The ùëò-SAT problem is the restric-
tion of the satisfiability problem for the case that the input formula is
a ùëò-CNF. In particular, the 2SAT problem is to find out, given a 2-CNF
formula ùúë, whether there is an assignment ùë•‚àà{0, 1}ùëõthat satisfies
ùúë, in the sense that it makes it evaluate to 1 or ‚ÄúTrue‚Äù. The trivial,
brute-force, algorithm for 2SAT will enumerate all the 2ùëõassignments
ùë•‚àà{0, 1}ùëõbut fortunately we can do much better. The key is that
we can think of every constraint of the form ‚Ñìùëñ‚à®‚Ñìùëó(where ‚Ñìùëñ, ‚Ñìùëóare
literals, corresponding to variables or their negations) as an implication
‚Ñìùëñ‚áí‚Ñìùëó, since it corresponds to the constraints that if the literal ‚Ñì‚Ä≤
ùëñ= ‚Ñìùëñ
is true then it must be the case that ‚Ñìùëóis true as well. Hence we can
think of ùúëas a directed graph between the 2ùëõliterals, with an edge
from ‚Ñìùëñto ‚Ñìùëócorresponding to an implication from the former to the
latter. It can be shown that ùúëis unsatisfiable if and only if there is a
variable ùë•ùëñsuch that there is a directed path from ùë•ùëñto ùë•ùëñas well as
a directed path from ùë•ùëñto ùë•ùëñ(see Exercise 12.2). This reduces 2SAT
to the (efficiently solvable) problem of determining connectivity in
directed graphs.


--- Page 408 ---

408
introduction to theoretical computer science
3SAT.
The 3SAT problem is the task of determining satisfiability
for 3CNFs. One might think that changing from two to three would
not make that much of a difference for complexity. One would be
wrong. Despite much effort, we do not know of a significantly better
than brute force algorithm for 3SAT (the best known algorithms take
roughly 1.3ùëõsteps).
Interestingly, a similar issue arises time and again in computation,
where the difference between two and three often corresponds to
the difference between tractable and intractable. We do not fully un-
derstand the reasons for this phenomenon, though the notion of NP
completeness we will see later does offer a partial explanation. It may
be related to the fact that optimizing a polynomial often amounts to
equations on its derivative. The derivative of a quadratic polynomial is
linear, while the derivative of a cubic is quadratic, and, as we will see,
the difference between solving linear and quadratic equations can be
quite profound.
12.2.2 Solving linear equations
One of the most useful problems that people have been solving time
and again is solving ùëõlinear equations in ùëõvariables. That is, solve
equations of the form
ùëé0,0ùë•0 + ùëé0,1ùë•1
+ ‚ãØ
+ ùëé0,ùëõ‚àí1ùë•ùëõ‚àí1
= ùëè0
ùëé1,0ùë•0 + ùëé1,1ùë•1
+ ‚ãØ
+ ùëé1,ùëõ‚àí1ùë•ùëõ‚àí1
= ùëè1
‚ãÆ+ ‚ãÆ
+ ‚ãÆ
+ ‚ãÆ
=‚ãÆ
ùëéùëõ‚àí1,0ùë•0 + ùëéùëõ‚àí1,1ùë•1
+ ‚ãØ
+ ùëéùëõ‚àí1,ùëõ‚àí1ùë•ùëõ‚àí1
= ùëèùëõ‚àí1
(12.4)
where {ùëéùëñ,ùëó}ùëñ,ùëó‚àà[ùëõ] and {ùëèùëñ}ùëñ‚àà[ùëõ] are real (or rational) numbers. More
compactly, we can write this as the equations ùê¥ùë•= ùëèwhere ùê¥is an
ùëõ√ó ùëõmatrix, and we think of ùë•, ùëèare column vectors in ‚Ñùùëõ.
The standard Gaussian elimination algorithm can be used to solve
such equations in polynomial time (i.e., determine if they have a so-
lution, and if so, to find it). As we discussed above, if we are willing
to allow some loss in precision, we even have algorithms that handle
linear inequalities, also known as linear programming. In contrast, if
we insist on integer solutions, the task of solving for linear equalities
or inequalities is known as integer programming, and the best known
algorithms are exponential time in the worst case.
R
Remark 12.4 ‚Äî Bit complexity of numbers. Whenever we
discuss problems whose inputs correspond to num-
bers, the input length corresponds to how many bits
are needed to describe the number (or, as is equiv-
alent up to a constant factor, the number of digits


--- Page 409 ---

efficient computation: an informal introduction
409
in base 10, 16 or any other constant). The difference
between the length of the input and the magnitude
of the number itself can be of course quite profound.
For example, most people would agree that there is
a huge difference between having a billion (i.e. 109)
dollars and having nine dollars. Similarly there is a
huge difference between an algorithm that takes ùëõ
steps on an ùëõ-bit number and an algorithm that takes
2ùëõsteps.
One example is the problem (discussed below) of
finding the prime factors of a given integer ùëÅ. The
natural algorithm is to search for such a factor by try-
ing all numbers from 1 to ùëÅ, but that would take ùëÅ
steps which is exponential in the input length, which
is the number of bits needed to describe ùëÅ. (The run-
ning time of this algorithm can be easily improved to
roughly
‚àö
ùëÅ, but this is still exponential (i.e., 2ùëõ/2) in
the number ùëõof bits to describe ùëÅ.) It is an important
and long open question whether there is such an algo-
rithm that runs in time polynomial in the input length
(i.e., polynomial in log ùëÅ).
12.2.3 Solving quadratic equations
Suppose that we want to solve not just linear but also equations in-
volving quadratic terms of the form ùëéùëñ,ùëó,ùëòùë•ùëóùë•ùëò. That is, suppose that
we are given a set of quadratic polynomials ùëù1, ‚Ä¶ , ùëùùëöand consider
the equations {ùëùùëñ(ùë•) = 0}. To avoid issues with bit representations,
we will always assume that the equations contain the constraints
{ùë•2
ùëñ‚àíùë•ùëñ= 0}ùëñ‚àà[ùëõ]. Since only 0 and 1 satisfy the equation ùëé2 ‚àíùëé= 0,
this assumption means that we can restrict attention to solutions in
{0, 1}ùëõ. Solving quadratic equations in several variables is a classical
and extremely well motivated problem. This is the generalization of
the classical case of single-variable quadratic equations that gener-
ations of high school students grapple with. It also generalizes the
quadratic assignment problem, introduced in the 1950‚Äôs as a way to
optimize assignment of economic activities. Once again, we do not
know a much better algorithm for this problem than the one that enu-
merates over all the 2ùëõpossibilities.
12.3 MORE ADVANCED EXAMPLES
We now list a few more examples of interesting problems that are a
little more advanced but are of significant interest in areas such as
physics, economics, number theory, and cryptography.
12.3.1 Determinant of a matrix
The determinant of a ùëõ√ó ùëõmatrix ùê¥, denoted by det(ùê¥), is an ex-
tremely important quantity in linear algebra. For example, it is known


--- Page 410 ---

410
introduction to theoretical computer science
that det(ùê¥) ‚â†0 if and only if ùê¥is nonsingular, which means that it
has an inverse ùê¥‚àí1, and hence we can always uniquely solve equations
of the form ùê¥ùë•= ùëèwhere ùë•and ùëèare ùëõ-dimensional vectors. More
generally, the determinant can be thought of as a quantitative measure
as to what extent ùê¥is far from being singular. If the rows of ùê¥are ‚Äúal-
most‚Äù linearly dependent (for example, if the third row is very close
to being a linear combination of the first two rows) then the determi-
nant will be small, while if they are far from it (for example, if they are
are orthogonal to one another, then the determinant will be large). In
particular, for every matrix ùê¥, the absolute value of the determinant
of ùê¥is at most the product of the norms (i.e., square root of sum of
squares of entries) of the rows, with equality if and only if the rows
are orthogonal to one another.
The determinant can be defined in several ways. One way to define
the determinant of an ùëõ√ó ùëõmatrix ùê¥is:
det(ùê¥) = ‚àë
ùúã‚ààùëÜùëõ
sign(ùúã) ‚àè
ùëñ‚àà[ùëõ]
ùê¥ùëñ,ùúã(ùëñ)
(12.5)
where ùëÜùëõis the set of all permutations from [ùëõ] to [ùëõ] and the sign of
a permutation ùúãis equal to ‚àí1 raised to the power of the number of
inversions in ùúã(pairs ùëñ, ùëósuch that ùëñ> ùëóbut ùúã(ùëñ) < ùúã(ùëó)).
This definition suggests that computing det(ùê¥) might require
summing over |ùëÜùëõ| terms which would take exponential time since
|ùëÜùëõ| = ùëõ! > 2ùëõ. However, there are other ways to compute the de-
terminant. For example, it is known that det is the only function that
satisfies the following conditions:
1. det(AB) = det(ùê¥)det(ùêµ) for every square matrices ùê¥, ùêµ.
2. For every ùëõ√ó ùëõtriangular matrix ùëáwith diagonal entries
ùëë0, ‚Ä¶ , ùëëùëõ‚àí1, det(ùëá) = ‚àè
ùëõ
ùëñ=0 ùëëùëñ. In particular det(ùêº) = 1 where ùêºis
the identity matrix. (A triangular matrix is one in which either all
entries below the diagonal, or all entries above the diagonal, are
zero.)
3. det(ùëÜ) = ‚àí1 where ùëÜis a ‚Äúswap matrix‚Äù that corresponds to
swapping two rows or two columns of ùêº. That is, there are two
coordinates ùëé, ùëèsuch that for every ùëñ, ùëó, ùëÜùëñ,ùëó=
‚éß
{
{
‚é®
{
{
‚é©
1
ùëñ= ùëó, ùëñ‚àâ{ùëé, ùëè}
1
{ùëñ, ùëó} = {ùëé, ùëè}
0
otherwise
.
Using these rules and the Gaussian elimination algorithm, it is
possible to tell whether ùê¥is singular or not, and in the latter case, de-
compose ùê¥as a product of a polynomial number of swap matrices
and triangular matrices. (Indeed one can verify that the row opera-
tions in Gaussian elimination corresponds to either multiplying by a


--- Page 411 ---

efficient computation: an informal introduction
411
swap matrix or by a triangular matrix.) Hence we can compute the
determinant for an ùëõ√ó ùëõmatrix using a polynomial time of arithmetic
operations.
12.3.2 Permanent of a matrix
Given an ùëõ√ó ùëõmatrix ùê¥, the permanent of ùê¥is defined as
perm(ùê¥) = ‚àë
ùúã‚ààùëÜùëõ
‚àè
ùëñ‚àà[ùëõ]
ùê¥ùëñ,ùúã(ùëñ) .
(12.6)
That is, perm(ùê¥) is defined analogously to the determinant in (12.5)
except that we drop the term sign(ùúã). The permanent of a matrix is a
natural quantity, and has been studied in several contexts including
combinatorics and graph theory. It also arises in physics where it can
be used to describe the quantum state of multiple Boson particles (see
here and here).
Permanent modulo 2.
If the entries of ùê¥are integers, then we can de-
fine the Boolean function ùëùùëíùëüùëö2 which outputs on input a matrix ùê¥
the result of the permanent of ùê¥modulo 2. It turns out that we can
compute ùëùùëíùëüùëö2(ùê¥) in polynomial time. The key is that modulo 2, ‚àíùë•
and +ùë•are the same quantity and hence, since the only difference
between (12.5) and (12.6) is that some terms are multiplied by ‚àí1,
det(ùê¥) mod 2 = perm(ùê¥) mod 2 for every ùê¥.
Permanent modulo 3.
Emboldened by our good fortune above, we
might hope to be able to compute the permanent modulo any prime ùëù
and perhaps in full generality. Alas, we have no such luck. In a similar
‚Äútwo to three‚Äù type of a phenomenon, we do not know of a much
better than brute force algorithm to even compute the permanent
modulo 3.
12.3.3 Finding a zero-sum equilibrium
A zero sum game is a game between two players where the payoff for
one is the same as the penalty for the other. That is, whatever the first
player gains, the second player loses. As much as we want to avoid
them, zero sum games do arise in life, and the one good thing about
them is that at least we can compute the optimal strategy.
A zero sum game can be specified by an ùëõ√ó ùëõmatrix ùê¥, where if
player 1 chooses action ùëñand player 2 chooses action ùëóthen player one
gets ùê¥ùëñ,ùëóand player 2 loses the same amount. The famous Min Max
Theorem by John von Neumann states that if we allow probabilistic or
‚Äúmixed‚Äù strategies (where a player does not choose a single action but
rather a distribution over actions) then it does not matter who plays
first and the end result will be the same. Mathematically the min max
theorem is that if we let Œîùëõbe the set of probability distributions over


--- Page 412 ---

412
introduction to theoretical computer science
[ùëõ] (i.e., non-negative columns vectors in ‚Ñùùëõwhose entries sum to 1)
then
max
ùëù‚àà‚àÜùëõ
min
ùëû‚àà‚àÜùëõ
ùëù‚ä§ùê¥ùëû= min
ùëû‚àà‚àÜùëõ
max
ùëù‚àà‚àÜùëõ
ùëù‚ä§ùê¥ùëû
(12.7)
The min-max theorem turns out to be a corollary of linear pro-
gramming duality, and indeed the value of (12.7) can be computed
efficiently by a linear program.
12.3.4 Finding a Nash equilibrium
Fortunately, not all real-world games are zero sum, and we do have
more general games, where the payoff of one player does not neces-
sarily equal the loss of the other. John Nash won the Nobel prize for
showing that there is a notion of equilibrium for such games as well.
In many economic texts it is taken as an article of faith that when
actual agents are involved in such a game then they reach a Nash
equilibrium. However, unlike zero sum games, we do not know of
an efficient algorithm for finding a Nash equilibrium given the de-
scription of a general (non zero sum) game. In particular this means
that, despite economists‚Äô intuitions, there are games for which natural
strategies will take an exponential number of steps to converge to an
equilibrium.
12.3.5 Primality testing
Another classical computational problem, that has been of interest
since the ancient Greeks, is to determine whether a given number
ùëÅis prime or composite. Clearly we can do so by trying to divide it
with all the numbers in 2, ‚Ä¶ , ùëÅ‚àí1, but this would take at least ùëÅ
steps which is exponential in its bit complexity ùëõ= log ùëÅ. We can
reduce these ùëÅsteps to
‚àö
ùëÅby observing that if ùëÅis a composite of
the form ùëÅ= PQ then either ùëÉor ùëÑis smaller than
‚àö
ùëÅ. But this is
still quite terrible. If ùëÅis a 1024 bit integer,
‚àö
ùëÅis about 2512, and so
running this algorithm on such an input would take much more than
the lifetime of the universe.
Luckily, it turns out we can do radically better. In the 1970‚Äôs, Ra-
bin and Miller gave probabilistic algorithms to determine whether a
given number ùëÅis prime or composite in time ùëùùëúùëôùë¶(ùëõ) for ùëõ= log ùëÅ.
We will discuss the probabilistic model of computation later in this
course. In 2002, Agrawal, Kayal, and Saxena found a deterministic
ùëùùëúùëôùë¶(ùëõ) time algorithm for this problem. This is surely a development
that mathematicians from Archimedes till Gauss would have found
exciting.


--- Page 413 ---

efficient computation: an informal introduction
413
Figure 12.6: The current computational status of
several interesting problems. For all of them we either
know a polynomial-time algorithm or the known
algorithms require at least 2ùëõùëêfor some ùëê> 0. In
fact for all except the factoring problem, we either
know an ùëÇ(ùëõ3) time algorithm or the best known
algorithm require at least 2‚Ñ¶(ùëõ) time where ùëõis a
natural parameter such that there is a brute force
algorithm taking roughly 2ùëõor ùëõ! time. Whether this
‚Äúcliff‚Äù between the easy and hard problem is a real
phenomenon or a reflection of our ignorance is still an
open question.
12.3.6 Integer factoring
Given that we can efficiently determine whether a number ùëÅis prime
or composite, we could expect that in the latter case we could also ef-
ficiently find the factorization of ùëÅ. Alas, no such algorithm is known.
In a surprising and exciting turn of events, the non existence of such an
algorithm has been used as a basis for encryptions, and indeed it un-
derlies much of the security of the world wide web. We will return to
the factoring problem later in this course. We remark that we do know
much better than brute force algorithms for this problem. While the
brute force algorithms would require 2‚Ñ¶(ùëõ) time to factor an ùëõ-bit inte-
ger, there are known algorithms running in time roughly 2ùëÇ(‚àöùëõ) and
also algorithms that are widely believed (though not fully rigorously
analyzed) to run in time roughly 2ùëÇ(ùëõ1/3). (By ‚Äúroughly‚Äù we mean that
we neglect factors that are polylogarithmic in ùëõ.)
12.4 OUR CURRENT KNOWLEDGE
The difference between an exponential and polynomial time algo-
rithm might seem merely ‚Äúquantitative‚Äù but it is in fact extremely
significant. As we‚Äôve already seen, the brute force exponential time
algorithm runs out of steam very very fast, and as Edmonds says, in
practice there might not be much difference between a problem where
the best algorithm is exponential and a problem that is not solvable
at all. Thus the efficient algorithms we mentioned above are widely
used and power many computer science applications. Moreover, a
polynomial-time algorithm often arises out of significant insight to
the problem at hand, whether it is the ‚Äúmax-flow min-cut‚Äù result, the
solvability of the determinant, or the group theoretic structure that
enables primality testing. Such insight can be useful regardless of its
computational implications.
At the moment we do not know whether the ‚Äúhard‚Äù problems are
truly hard, or whether it is merely because we haven‚Äôt yet found the
right algorithms for them. However, we will now see that there are
problems that do inherently require exponential time. We just don‚Äôt
know if any of the examples above fall into that category.
‚úì
Chapter Recap
‚Ä¢ There are many natural problems that have
polynomial-time algorithms, and other natural
problems that we‚Äôd love to solve, but for which the
best known algorithms are exponential.
‚Ä¢ Often a polynomial time algorithm relies on dis-
covering some hidden structure in the problem, or
finding a surprising equivalent formulation for it.


--- Page 414 ---

414
introduction to theoretical computer science
2 Hint: Use dynamic programming to compute for
every ùë†, ùë°‚àà[ùëõ] and ùëÜ‚äÜ[ùëõ] the value ùëÉ(ùë†, ùë°, ùëÜ)
which equals 1 if there is a simple path from ùë†to ùë°
that uses exactly the vertices in ùëÜ. Do this iteratively
for ùëÜ‚Äôs of growing sizes.
‚Ä¢ There are many interesting problems where there
is an exponential gap between the best known algo-
rithm and the best algorithm that we can rule out.
Closing this gap is one of the main open questions
of theoretical computer science.
12.5 EXERCISES
Exercise 12.1 ‚Äî exponential time algorithm for longest path. The naive algo-
rithm for computing the longest path in a given graph could take
more than ùëõ! steps. Give a ùëùùëúùëôùë¶(ùëõ)2ùëõtime algorithm for the longest
path problem in ùëõvertex graphs.2
‚ñ†
Exercise 12.2 ‚Äî 2SAT algorithm. For every 2CNF ùúë, define the graph ùê∫ùúë
on 2ùëõvertices corresponding to the literals ùë•1, ‚Ä¶ , ùë•ùëõ, ùë•1, ‚Ä¶ , ùë•ùëõ, such
that there is an edge ‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó‚Éó
‚Ñìùëñ‚Ñìùëóiff the constraint ‚Ñìùëñ‚à®‚Ñìùëóis in ùúë. Prove that ùúë
is unsatisfiable if and only if there is some ùëñsuch that there is a path
from ùë•ùëñto ùë•ùëñand from ùë•ùëñto ùë•ùëñin ùê∫ùúë. Show how to use this to solve
2SAT in polynomial time.
‚ñ†
Exercise 12.3 ‚Äî Reductions for showing algorithms. The following fact is
true: there is a polynomial-time algorithm BIP that on input a graph
ùê∫= (ùëâ, ùê∏) outputs 1 if and only if the graph is bipartite: there is a
partition of ùëâto disjoint parts ùëÜand ùëásuch that every edge (ùë¢, ùë£) ‚ààùê∏
satisfies either ùë¢‚ààùëÜand ùë£‚ààùëáor ùë¢‚ààùëáand ùë£‚ààùëÜ. Use this
fact to prove that there is a polynomial-time algorithm to compute
that following function CLIQUEPARTITION that on input a graph
ùê∫= (ùëâ, ùê∏) outputs 1 if and only if there is a partition of ùëâthe graph
into two parts ùëÜand ùëásuch that both ùëÜand ùëáare cliques: for every
pair of distinct vertices ùë¢, ùë£‚ààùëÜ, the edge (ùë¢, ùë£) is in ùê∏and similarly for
every pair of distinct vertices ùë¢, ùë£‚ààùëá, the edge (ùë¢, ùë£) is in ùê∏.
‚ñ†
12.6 BIBLIOGRAPHICAL NOTES
The classic undergraduate introduction to algorithms text is
[Cor+09]. Two texts that are less ‚Äúencyclopedic‚Äù are Kleinberg and
Tardos [KT06], and Dasgupta, Papadimitriou and Vazirani [DPV08].
Jeff Erickson‚Äôs book is an excellent algorithms text that is freely
available online.
The origins of the minimum cut problem date to the Cold War.
Specifically, Ford and Fulkerson discovered their max-flow/min-cut
algorithm in 1955 as a way to find out the minimum amount of train


--- Page 415 ---

efficient computation: an informal introduction
415
tracks that would need to be blown up to disconnect Russia from the
rest of Europe. See the survey [Sch05] for more.
Some algorithms for the longest path problem are given in [Wil09;
Bjo14].
12.7 FURTHER EXPLORATIONS
Some topics related to this chapter that might be accessible to ad-
vanced students include: (to be completed)


--- Page 416 ---



--- Page 417 ---

13
Modeling running time
‚ÄúWhen the measure of the problem-size is reasonable and when the
sizes assume values arbitrarily large, an asymptotic estimate of ‚Ä¶ the or-
der of difficulty of [an] algorithm .. is theoretically important. It cannot
be rigged by making the algorithm artificially difficult for smaller sizes‚Äù,
Jack Edmonds, ‚ÄúPaths, Trees, and Flowers‚Äù, 1963
Max Newman: It is all very well to say that a machine could ‚Ä¶ do this or
that, but ‚Ä¶ what about the time it would take to do it?
Alan Turing: To my mind this time factor is the one question which will
involve all the real technical difficulty.
BBC radio panel on ‚ÄúCan automatic Calculating Machines Be Said to
Think?‚Äù, 1952
In Chapter 12 we saw examples of efficient algorithms, and made
some claims about their running time, but did not give a mathemati-
cally precise definition for this concept. We do so in this chapter, using
the models of Turing machines and RAM machines (or equivalently
NAND-TM and NAND-RAM) we have seen before. The running
time of an algorithm is not a fixed number since any non-trivial algo-
rithm will take longer to run on longer inputs. Thus, what we want
to measure is the dependence between the number of steps the algo-
rithms takes and the length of the input. In particular we care about
the distinction between algorithms that take at most polynomial time
(i.e., ùëÇ(ùëõùëê) time for some constant ùëê) and problems for which every
algorithm requires at least exponential time (i.e., Œ©(2ùëõùëê) for some ùëê). As
mentioned in Edmond‚Äôs quote in Chapter 12, the difference between
these two can sometimes be as important as the difference between
being computable and uncomputable.
This chapter: A non-mathy overview
In this chapter we formally define what it means for a func-
tion to be computable in a certain number of steps. As dis-
cussed in Chapter 12, running time is not a number, rather
Compiled on 8.26.2020 18:10
Learning Objectives:
‚Ä¢ Formally modeling running time, and in
particular notions such as ùëÇ(ùëõ) or ùëÇ(ùëõ3) time
algorithms.
‚Ä¢ The classes P and EXP modelling polynomial
and exponential time respectively.
‚Ä¢ The time hierarchy theorem, that in particular
says that for every ùëò‚â•1 there are functions
we can compute in ùëÇ(ùëõùëò+1) time but can not
compute in ùëÇ(ùëõùëò) time.
‚Ä¢ The class P/poly of non uniform computation
and the result that P ‚äÜP/poly


--- Page 418 ---

418
introduction to theoretical computer science
Figure 13.1: Overview of the results of this chapter.
what we care about is the scaling behevaiour of the number
of steps as the input size grows. We can use either Turing
machines or RAM machines to give such a formal definition
- it turns out that this doesn‚Äôt make a difference at the reso-
lution we care about. We make several important definitions
and prove some important theorems in this chapter. We will
define the main time complexity classes we use in this book,
and also show the Time Hierarchy Theorem which states that
given more resources (more time steps per input size) we can
compute more functions.
To put this in more ‚Äúmathy‚Äù language, in this chapter we define
what it means for a function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}‚àóto be computable
in time ùëá(ùëõ) steps, where ùëáis some function mapping the length ùëõ
of the input to the number of computation steps allowed. Using this
definition we will do the following (see also Fig. 13.1):
‚Ä¢ We define the class P of Boolean functions that can be computed
in polynomial time and the class EXP of functions that can be com-
puted in exponential time. Note that P ‚äÜEXP if we can compute a
function in polynomial time, we can certainly compute it in expo-
nential time.
‚Ä¢ We show that the times to compute a function using a Turing Ma-
chine and using a RAM machine (or NAND-RAM program) are
polynomially related. In particular this means that the classes P and
EXP are identical regardless of whether they are defined using
Turing Machines or RAM machines / NAND-RAM programs.


--- Page 419 ---

modeling running time
419
‚Ä¢ We give an efficient universal NAND-RAM program and use this to
establish the time hierarchy theorem that in particular implies that P is
a strict subset of EXP.
‚Ä¢ We relate the notions defined here to the non uniform models of
Boolean circuits and NAND-CIRC programs defined in Chapter 3.
We define P/poly to be the class of functions that can be computed
by a sequence of polynomial-sized circuits. We prove that P ‚äÜP/poly
and that P/poly contains uncomputable functions.
13.1 FORMALLY DEFINING RUNNING TIME
Our models of computation such Turing Machines, NAND-TM and
NAND-RAM programs and others all operate by executing a se-
quence of instructions on an input one step at a time. We can define
the running time of an algorithm ùëÄin one of these models by measur-
ing the number of steps ùëÄtakes on input ùë•as a function of the length
|ùë•| of the input. We start by defining running time with respect to Tur-
ing Machines:
Definition 13.1 ‚Äî Running time (Turing Machines). Let ùëá‚à∂‚Ñï‚Üí‚Ñïbe some
function mapping natural numbers to natural numbers. We say
that a function ùêπ
‚à∂{0, 1}‚àó‚Üí{0, 1}‚àóis computable in ùëá(ùëõ) Turing-
Machine time (TM-time for short) if there exists a Turing Machine ùëÄ
such that for every sufficiently large ùëõand every ùë•‚àà{0, 1}ùëõ, when
given input ùë•, the machine ùëÄhalts after executing at most ùëá(ùëõ)
steps and outputs ùêπ(ùë•).
We define TIMETM(ùëá(ùëõ)) to be the set of Boolean functions
(functions mapping {0, 1}‚àóto {0, 1}) that are computable in ùëá(ùëõ)
TM time.
ÔÉ´Big Idea 17 For a function ùêπ
‚à∂
{0, 1}‚àó
‚Üí
{0, 1} and ùëá
‚à∂
‚Ñï‚Üí‚Ñï, we can formally define what it means for ùêπto be computable
in time at most ùëá(ùëõ) where ùëõis the size of the input.
P
Definition 13.1 is not very complicated but is one of
the most important definitions of this book. As usual,
TIMETM(ùëá(ùëõ)) is a class of functions, not of machines. If
ùëÄis a Turing Machine then a statement such as ‚ÄúùëÄ
is a member of TIMETM(ùëõ2)‚Äù does not make sense.
The concept of TM-time as defined here is sometimes
known as ‚Äúsingle-tape Turing machine time‚Äù in the
literature, since some texts consider Turing machines
with more than one working tape.


--- Page 420 ---

420
introduction to theoretical computer science
Figure 13.2: Comparing ùëá(ùëõ) = 10ùëõ3 with ùëá‚Ä≤(ùëõ) =
2ùëõ(on the right figure the Y axis is in log scale).
Since for every large enough ùëõ, ùëá‚Ä≤(ùëõ) ‚â•ùëá(ùëõ),
TIMETM(ùëá(ùëõ)) ‚äÜTIMETM(ùëá‚Ä≤(ùëõ)).
The relaxation of considering only ‚Äúsufficiently large‚Äù ùëõ‚Äôs is not
very important but it is convenient since it allows us to avoid dealing
explicitly with un-interesting ‚Äúedge cases‚Äù. We will mostly anyway be
interested in determining running time only up to constant and even
polynomial factors.
While the notion of being computable within a certain running time
can be defined for every function, the class TIMETM(ùëá(ùëõ)) is a class
of Boolean functions that have a single bit of output. This choice is not
very important, but is made for simplicity and convenience later on.
In fact, every non-Boolean function has a computationally equivalent
Boolean variant, see Exercise 13.3.
Solved Exercise 13.1 ‚Äî Example of time bounds. Prove that TIMETM(10‚ãÖùëõ3) ‚äÜ
TIMETM(2ùëõ).
‚ñ†
Solution:
The proof is illustrated in Fig. 13.2. Suppose that ùêπ‚ààTIMETM(10‚ãÖ
ùëõ3) and hence there some number ùëÅ0 and a machine ùëÄsuch that
for every ùëõ
>
ùëÅ0, and ùë•
‚àà
{0, 1}‚àó, ùëÄ(ùë•) outputs ùêπ(ùë•) within at
most 10 ‚ãÖùëõ3 steps. Since 10 ‚ãÖùëõ3
=
ùëú(2ùëõ), there is some number
ùëÅ1 such that for every ùëõ
>
ùëÅ1, 10 ‚ãÖùëõ3
<
2ùëõ. Hence for every
ùëõ> max{ùëÅ0, ùëÅ1}, ùëÄ(ùë•) will output ùêπ(ùë•) within at most 2ùëõsteps,
just demonstrating that ùêπ‚ààTIMETM(2ùëõ).
‚ñ†
13.1.1 Polynomial and Exponential Time
Unlike the notion of computability, the exact running time can be a
function of the model we use. However, it turns out that if we only
care about ‚Äúcoarse enough‚Äù resolution (as will most often be the case)
then the choice of the model, whether Turing Machines, RAM ma-
chines, NAND-TM/NAND-RAM programs, or C/Python programs,
does not matter. This is known as the extended Church-Turing Thesis.
Specifically we will mostly care about the difference between polyno-
mial and exponential time.
The two main time complexity classes we will be interested in are
the following:
‚Ä¢ Polynomial time: A function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1} is computable in
polynomial time if it is in the class P = ‚à™ùëê‚àà{1,2,3,‚Ä¶}TIMETM(ùëõùëê). That
is, ùêπ‚ààP if there is an algorithm to compute ùêπthat runs in time at
most polynomial (i.e., at most ùëõùëêfor some constant ùëê) in the length
of the input.
‚Ä¢ Exponential time: A function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1} is computable in
exponential time if it is in the class EXP = ‚à™ùëê‚àà{1,2,3,‚Ä¶}TIMETM(2ùëõùëê).


--- Page 421 ---

modeling running time
421
That is, ùêπ‚ààEXP if there is an algorithm to compute ùêπthat runs in
time at most exponential (i.e., at most 2ùëõùëêfor some constant ùëê) in the
length of the input.
In other words, these are defined as follows:
Definition 13.2 ‚Äî P and EXP. Let ùêπ
‚à∂
{0, 1}‚àó
‚Üí
{0, 1}. We say that
ùêπ‚ààP if there is a polynomial ùëù‚à∂‚Ñï‚Üí‚Ñùand a Turing Machine
ùëÄsuch that for every ùë•
‚àà
{0, 1}‚àó, when given input ùë•, the Turing
machine halts within at most ùëù(|ùë•|) steps and outputs ùêπ(ùë•).
We say that ùêπ
‚ààEXP if there is a polynomial ùëù‚à∂‚Ñï‚Üí‚Ñùand
a Turing Machine ùëÄsuch that for every ùë•
‚àà
{0, 1}‚àó, when given
input ùë•, ùëÄhalts within at most 2ùëù(|ùë•|) steps and outputs ùêπ(ùë•).
P
Please take the time to make sure you understand
these definitions. In particular, sometimes students
think of the class EXP as corresponding to functions
that are not in P. However, this is not the case. If ùêπis
in EXP then it can be computed in exponential time.
This does not mean that it cannot be computed in
polynomial time as well.
Solved Exercise 13.2 ‚Äî Differerent definitions of P. Prove that P as defined in
Definition 13.2 is equal to ‚à™ùëê‚àà{1,2,3,‚Ä¶}TIMETM(ùëõùëê)
‚ñ†
Solution:
To show these two sets are equal we need to show that P
‚äÜ
‚à™ùëê‚àà{1,2,3,‚Ä¶}TIMETM(ùëõùëê) and ‚à™ùëê‚àà{1,2,3,‚Ä¶}TIMETM(ùëõùëê)
‚äÜ
P. We start
with the former inclusion. Suppose that ùêπ‚ààP. Then there is some
polynomial ùëù
‚à∂
‚Ñï
‚Üí
‚Ñùand a Turing machine ùëÄsuch that ùëÄ
computes ùêπand ùëÄhalts on every input ùë•within at most ùëù(|ùë•|)
steps. We can write the polynomial ùëù
‚à∂
‚Ñï
‚Üí
‚Ñùin the form
ùëù(ùëõ)
=
‚àë
ùëë
ùëñ=0 ùëéùëñùëõùëñwhere ùëé0, ‚Ä¶ , ùëéùëë
‚àà
‚Ñù, and we assume that ùëéùëë
is nonzero (or otherwise we just let ùëëcorrespond to the largest
number such that ùëéùëëis nonzero). The degree if ùëùthe number ùëë.
Since ùëõùëë
=
ùëú(ùëõùëë+1), no matter what is the coefficient ùëéùëë, for large
enough ùëõ, ùëù(ùëõ)
<
ùëõùëë+1 which means that the Turing machine ùëÄ
will halt on inputs of length ùëõwithin fewer than ùëõùëë+1 steps, and
hence ùêπ‚ààTIMETM(ùëõùëë+1) ‚äÜ‚à™ùëê‚àà{1,2,3,‚Ä¶}TIMETM(ùëõùëê).
For the second inclusion, suppose that ùêπ‚àà‚à™ùëê‚àà{1,2,3,‚Ä¶}TIMETM(ùëõùëê).
Then there is some positive ùëê‚àà‚Ñïsuch that ùêπ‚ààTIMETM(ùëõùëê) which
means that there is a Turing Machine ùëÄand some number ùëÅ0 such


--- Page 422 ---

422
introduction to theoretical computer science
Figure 13.3: Some examples of problems that are
known to be in P and problems that are known to
be in EXP but not known whether or not they are
in P. Since both P and EXP are classes of Boolean
functions, in this figure we always refer to the Boolean
(i.e., Yes/No) variant of the problems.
that ùëÄcomputes ùêπand for every ùëõ
>
ùëÅ0, ùëÄhalts on length ùëõ
inputs within at most ùëõùëêsteps. Let ùëá0 be the maximum number
of steps that ùëÄtakes on inputs of length at most ùëÅ0. Then if we
define the polynomial ùëù(ùëõ) = ùëõùëê+ ùëá0 then we see that ùëÄhalts on
every input ùë•within at most ùëù(|ùë•|) steps and hence the existence of
ùëÄdemonstrates that ùêπ‚ààP.
‚ñ†
Since exponential time is much larger than polynomial time, P ‚äÜ
EXP. All of the problems we listed in Chapter 12 are in EXP, but as
we‚Äôve seen, for some of them there are much better algorithms that
demonstrate that they are in fact in the smaller class P.
P
EXP (but not known to be in P)
Shortest path
Longest Path
Min cut
Max cut
2SAT
3SAT
Linear eqs
Quad. eqs
Zerosum
Nash
Determinant
Permanent
Primality
Factoring
Table : A table of the examples from Chapter 12. All these problems
are in EXP but only the ones on the left column are currently known to
be in P as well (i.e., they have a polynomial-time algorithm). See also
Fig. 13.3.
R
Remark 13.3 ‚Äî Boolean versions of problems. Many
of the problems defined in Chapter 12 correspond to
non Boolean functions (functions with more than one
bit of output) while P and EXP are sets of Boolean
functions. However, for every non-Boolean function
ùêπwe can always define a computationally-equivalent
Boolean function ùê∫by letting ùê∫(ùë•, ùëñ) be the ùëñ-th bit
of ùêπ(ùë•) (see Exercise 13.3). Hence the table above,
as well as Fig. 13.3, refer to the computationally-
equivalent Boolean variants of these problems.
13.2 MODELING RUNNING TIME USING RAM MACHINES / NAND-
RAM
Turing Machines are a clean theoretical model of computation, but
do not closely correspond to real-world computing architectures. The
discrepancy between Turing Machines and actual computers does
not matter much when we consider the question of which functions


--- Page 423 ---

modeling running time
423
are computable, but can make a difference in the context of efficiency.
Even a basic staple of undergraduate algorithms such as ‚Äúmerge sort‚Äù
cannot be implemented on a Turing Machine in ùëÇ(ùëõlog ùëõ) time (see
Section 13.8). RAM machines (or equivalently, NAND-RAM programs)
match more closely actual computing architecture and what we mean
when we say ùëÇ(ùëõ) or ùëÇ(ùëõlog ùëõ) algorithms in algorithms courses
or whiteboard coding interviews. We can define running time with
respect to NAND-RAM programs just as we did for Turing Machines.
Definition 13.4 ‚Äî Running time (RAM). Let ùëá
‚à∂
‚Ñï
‚Üí
‚Ñïbe some func-
tion mapping natural numbers to natural numbers. We say that
a function ùêπ
‚à∂
{0, 1}‚àó
‚Üí
{0, 1}‚àóis computable in ùëá(ùëõ) RAM time
(RAM-time for short) if there exists a NAND-RAM program ùëÉsuch
that for every sufficiently large ùëõand every ùë•‚àà{0, 1}ùëõ, when given
input ùë•, the program ùëÉhalts after executing at most ùëá(ùëõ) lines and
outputs ùêπ(ùë•).
We define TIMERAM(ùëá(ùëõ)) to be the set of Boolean functions
(functions mapping {0, 1}‚àóto {0, 1}) that are computable in ùëá(ùëõ)
RAM time.
Because NAND-RAM programs correspond more closely to our
natural notions of running time, we will use NAND-RAM as our
‚Äúdefault‚Äù model of running time, and hence use TIME(ùëá(ùëõ)) (without
any subscript) to denote TIMERAM(ùëá(ùëõ)). However, it turns out that
as long as we only care about the difference between exponential and
polynomial time, this does not make much difference. The reason is
that Turing Machines can simulate NAND-RAM programs with at
most a polynomial overhead (see also Fig. 13.4):
Theorem 13.5 ‚Äî Relating RAM and Turing machines. Let ùëá‚à∂‚Ñï‚Üí‚Ñïbe a
function such that ùëá(ùëõ) ‚â•ùëõfor every ùëõand the map ùëõ‚Ü¶ùëá(ùëõ) can
be computed by a Turing machine in time ùëÇ(ùëá(ùëõ)). Then
TIMETM(ùëá(ùëõ)) ‚äÜTIMERAM(10 ‚ãÖùëá(ùëõ)) ‚äÜTIMETM(ùëá(ùëõ)4) .
(13.1)
P
The technical details of Theorem 13.5, such as the con-
dition that ùëõ
‚Ü¶
ùëá(ùëõ) is computable in ùëÇ(ùëá(ùëõ)) time
or the constants 10 and 4 in (13.1) (which are not tight
and can be improved), are not very important. In par-
ticular, all non pathological time bound functions we
encounter in practice such as ùëá(ùëõ) = ùëõ, ùëá(ùëõ) = ùëõlog ùëõ,
ùëá(ùëõ)
=
2ùëõetc. will satisfy the conditions of Theo-
rem 13.5, see also Remark 13.6.


--- Page 424 ---

424
introduction to theoretical computer science
Figure 13.4: The proof of Theorem 13.5 shows that
we can simulate ùëásteps of a Turing Machine with ùëá
steps of a NAND-RAM program, and can simulate
ùëásteps of a NAND-RAM program with ùëú(ùëá4)
steps of a Turing Machine. Hence TIMETM(ùëá(ùëõ)) ‚äÜ
TIMERAM(10 ‚ãÖùëá(ùëõ)) ‚äÜTIMETM(ùëá(ùëõ)4).
The main message of the theorem is Turing Machines
and RAM machines are ‚Äúroughly equivalent‚Äù in the
sense that one can simulate the other with polyno-
mial overhead. Similarly, while the proof involves
some technical details, it‚Äôs not very deep or hard, and
merely follows the simulation of RAM machines with
Turing Machines we saw in Theorem 8.1 with more
careful ‚Äúbook keeping‚Äù.
For example, by instantiating Theorem 13.5 with ùëá(ùëõ) = ùëõùëéand
using the fact that 10ùëõùëé= ùëú(ùëõùëé+1), we see that TIMETM(ùëõùëé) ‚äÜ
TIMERAM(ùëõùëé+1) ‚äÜTIMETM(ùëõ4ùëé+4) which means that (by Solved Ex-
ercise 13.2)
P = ‚à™ùëé=1,2,‚Ä¶TIMETM(ùëõùëé) = ‚à™ùëé=1,2,‚Ä¶TIMERAM(ùëõùëé) .
(13.2)
That is, we could have equally well defined P as the class of functions
computable by NAND-RAM programs (instead of Turing Machines)
that run in time polynomial in the length of the input. Similarly, by
instantiating Theorem 13.5 with ùëá(ùëõ) = 2ùëõùëéwe see that the class EXP
can also be defined as the set of functions computable by NAND-RAM
programs in time at most 2ùëù(ùëõ) where ùëùis some polynomial. Similar
equivalence results are known for many models including cellular
automata, C/Python/Javascript programs, parallel computers, and a
great many other models, which justifies the choice of P as capturing
a technology-independent notion of tractability. (See Section 13.3
for more discussion of this issue.) This equivalence between Turing
machines and NAND-RAM (as well as other models) allows us to
pick our favorite model depending on the task at hand (i.e., ‚Äúhave our
cake and eat it too‚Äù) even when we study questions of efficiency, as
long as we only care about the gap between polynomial and exponential
time. When we want to design an algorithm, we can use the extra
power and convenience afforded by NAND-RAM. When we want
to analyze a program or prove a negative result, we can restrict our
attention to Turing machines.
ÔÉ´Big Idea 18 All ‚Äúreasonable‚Äù computational models are equiv-
alent if we only care about the distinction between polynomial and
exponential.
The adjective ‚Äúreasonable‚Äù above refers to all scalable computa-
tional models that have been implemented, with the possible excep-
tion of quantum computers, see Section 13.3 and Chapter 23.
Proof Idea:
The direction TIMETM(ùëá(ùëõ)) ‚äÜTIMERAM(10 ‚ãÖùëá(ùëõ)) is not hard to
show, since a NAND-RAM program ùëÉcan simulate a Turing Machine


--- Page 425 ---

modeling running time
425
ùëÄwith constant overhead by storing the transition table of ùëÄin
an array (as is done in the proof of Theorem 9.1). Simulating every
step of the Turing machine can be done in a constant number ùëêof
steps of RAM, and it can be shown this constant ùëêis smaller than
10. Thus the heart of the theorem is to prove that TIMERAM(ùëá(ùëõ)) ‚äÜ
TIMETM(ùëá(ùëõ)4). This proof closely follows the proof of Theorem 8.1,
where we have shown that every function ùêπthat is computable by
a NAND-RAM program ùëÉis computable by a Turing Machine (or
equivalently a NAND-TM program) ùëÄ. To prove Theorem 13.5, we
follow the exact same proof but just check that the overhead of the
simulation of ùëÉby ùëÄis polynomial. The proof has many details, but
is not deep. It is therefore much more important that you understand
the statement of this theorem than its proof.
‚ãÜ
Proof of Theorem 13.5. We only focus on the non-trivial direction
TIMERAM(ùëá(ùëõ)) ‚äÜTIMETM(ùëá(ùëõ)4). Let ùêπ‚ààTIMERAM(ùëá(ùëõ)). ùêπcan
be computed in time ùëá(ùëõ) by some NAND-RAM program ùëÉand we
need to show that it can also be computed in time ùëá(ùëõ)4 by a Turing
Machine ùëÄ. This will follow from showing that ùêπcan be computed
in time ùëá(ùëõ)4 by a NAND-TM program, since for every NAND-TM
program ùëÑthere is a Turing Machine ùëÄsimulating it such that each
iteration of ùëÑcorresponds to a single step of ùëÄ.
As mentioned above, we follow the proof of Theorem 8.1 (simula-
tion of NAND-RAM programs using NAND-TM programs) and use
the exact same simulation, but with a more careful accounting of the
number of steps that the simulation costs. Recall, that the simulation
of NAND-RAM works by ‚Äúpeeling off‚Äù features of NAND-RAM one
by one, until we are left with NAND-TM.
We will not provide the full details but will present the main ideas
used in showing that every feature of NAND-RAM can be simulated
by NAND-TM with at most a polynomial overhead:
1. Recall that every NAND-RAM variable or array element can con-
tain an integer between 0 and ùëáwhere ùëáis the number of lines that
have been executed so far. Therefore if ùëÉis a NAND-RAM pro-
gram that computes ùêπin ùëá(ùëõ) time, then on inputs of length ùëõ, all
integers used by ùëÉare of magnitude at most ùëá(ùëõ). This means that
the largest value i can ever reach is at most ùëá(ùëõ) and so each one of
ùëÉ‚Äôs variables can be thought of as an array of at most ùëá(ùëõ) indices,
each of which holds a natural number of magnitude at most ùëá(ùëõ).
We let ‚Ñì= ‚åàlog ùëá(ùëõ)‚åâbe the number of bits needed to encode such
numbers. (We can start off the simulation by computing ùëá(ùëõ) and
‚Ñì.)


--- Page 426 ---

426
introduction to theoretical computer science
2. We can encode a NAND-RAM array of length ‚â§ùëá(ùëõ) containing
numbers in {0, ‚Ä¶ , ùëá(ùëõ) ‚àí1} as an Boolean (i.e., NAND-TM) array
of ùëá(ùëõ)‚Ñì= ùëÇ(ùëá(ùëõ) log ùëá(ùëõ)) bits, which we can also think of as
a two dimensional array as we did in the proof of Theorem 8.1. We
encode a NAND-RAM scalar containing a number in {0, ‚Ä¶ , ùëá(ùëõ) ‚àí
1} simply by a shorter NAND-TM array of ‚Ñìbits.
3. We can simulate the two dimensional arrays using one-
dimensional arrays of length ùëá(ùëõ)‚Ñì= ùëÇ(ùëá(ùëõ) log ùëá(ùëõ). All the
arithmetic operations on integers use the grade-school algorithms,
that take time that is polynomial in the number ‚Ñìof bits of the
integers, which is ùëùùëúùëôùë¶(log ùëá(ùëõ)) in our case. Hence we can simulate
ùëá(ùëõ) steps of NAND-RAM with ùëÇ(ùëá(ùëõ)ùëùùëúùëôùë¶(log ùëá(ùëõ)) steps of a
model that uses random access memory but only Boolean-valued
one-dimensional arrays.
4. The most expensive step is to translate from random access mem-
ory to the sequential memory model of NAND-TM/Turing Ma-
chines. As we did in the proof of Theorem 8.1 (see Section 8.2), we
can simulate accessing an array Foo at some location encoded in an
array Bar by:
a. Copying Bar to some temporary array Temp
b. Having an array Index which is initially all zeros except 1 at the
first location.
c. Repeating the following until Temp encodes the number 0:
(Number of repetitions is at most ùëá(ùëõ).)
‚Ä¢ Decrease the number encoded temp by 1. (Take number of steps
polynomial in ‚Ñì= ‚åàlog ùëá(ùëõ)‚åâ.)
‚Ä¢ Decrease i until it is equal to 0. (Take ùëÇ(ùëá(ùëõ) steps.)
‚Ä¢ Scan Index until we reach the point in which it equals 1 and
then change this 1 to 0 and go one step further and write 1 in
this location. (Takes ùëÇ(ùëá(ùëõ)) steps.)
d. When we are done we know that if we scan Index until we reach
the point in which Index[i]= 1 then i contains the value that
was encoded by Bar (Takes ùëÇ(ùëá(ùëõ) steps.)
The total cost for each such operation is ùëÇ(ùëá(ùëõ)2+ùëá(ùëõ)ùëùùëúùëôùë¶(log ùëá(ùëõ))) =
ùëÇ(ùëá(ùëõ)2) steps.
In sum, we simulate a single step of NAND-RAM using
ùëÇ(ùëá(ùëõ)2ùëùùëúùëôùë¶(log ùëá(ùëõ))) steps of NAND-TM, and hence the total
simulation time is ùëÇ(ùëá(ùëõ)3ùëùùëúùëôùë¶(log ùëá(ùëõ))) which is smaller than ùëá(ùëõ)4
for sufficiently large ùëõ.
‚ñ†


--- Page 427 ---

modeling running time
427
R
Remark 13.6 ‚Äî Nice time bounds. When considering
general time bounds such we need to make sure to
rule out some ‚Äúpathological‚Äù cases such as functions ùëá
that don‚Äôt give enough time for the algorithm to read
the input, or functions where the time bound itself is
uncomputable. We say that a function ùëá
‚à∂‚Ñï‚Üí‚Ñïis
a nice time bound function (or nice function for short)
if for every ùëõ
‚àà
‚Ñï, ùëá(ùëõ)
‚â•
ùëõ(i.e., ùëáallows enough
time to read the input), for every ùëõ‚Ä≤ ‚â•ùëõ, ùëá(ùëõ‚Ä≤) ‚â•ùëá(ùëõ)
(i.e., ùëáallows more time on longer inputs), and the
map ùêπ(ùë•)
=
1ùëá(|ùë•|) (i.e., mapping a string of length
ùëõto a sequence of ùëá(ùëõ) ones) can be computed by a
NAND-RAM program in ùëÇ(ùëá(ùëõ)) time.
All the ‚Äúnormal‚Äù time complexity bounds we en-
counter in applications such as ùëá(ùëõ)
=
100ùëõ,
ùëá(ùëõ)
=
ùëõ2 log ùëõ,ùëá(ùëõ)
=
2
‚àöùëõ, etc. are ‚Äúnice‚Äù.
Hence from now on we will only care about the
class TIME(ùëá(ùëõ)) when ùëáis a ‚Äúnice‚Äù function. The
computability condition is in particular typically easily
satisfied. For example, for arithmetic functions such
as ùëá(ùëõ)
=
ùëõ3, we can typically compute the binary
representation of ùëá(ùëõ) in time polynomial in the num-
ber of bits of ùëá(ùëõ) and hence poly-logarithmic in ùëá(ùëõ).
Hence the time to write the string 1ùëá(ùëõ) in such cases
will be ùëá(ùëõ) + ùëùùëúùëôùë¶(log ùëá(ùëõ)) = ùëÇ(ùëá(ùëõ)).
13.3 EXTENDED CHURCH-TURING THESIS (DISCUSSION)
Theorem 13.5 shows that the computational models of Turing Machines
and RAM Machines / NAND-RAM programs are equivalent up to poly-
nomial factors in the running time. Other examples of polynomially
equivalent models include:
‚Ä¢ All standard programming languages, including C/Python/-
JavaScript/Lisp/etc.
‚Ä¢ The ùúÜcalculus (see also Section 13.8).
‚Ä¢ Cellular automata
‚Ä¢ Parallel computers
‚Ä¢ Biological computing devices such as DNA-based computers.
The Extended Church Turing Thesis is the statement that this is true
for all physically realizable computing models. In other words, the
extended Church Turing thesis says that for every scalable computing
device ùê∂(which has a finite description but can be in principle used
to run computation on arbitrarily large inputs), there is some con-
stant ùëésuch that for every function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1} that ùê∂can


--- Page 428 ---

428
introduction to theoretical computer science
compute on ùëõlength inputs using an ùëÜ(ùëõ) amount of physical re-
sources, ùêπis in TIME(ùëÜ(ùëõ)ùëé). This is a strengthening of the (‚Äúplain‚Äù)
Church-Turing Thesis, discussed in Section 8.8, which states that the
set of computable functions is the same for all physically realizable
models, but without requiring the overhead in the simulation between
different models to be at most polynomial.
All the current constructions of scalable computational models and
programming languages conform to the Extended Church-Turing
Thesis, in the sense that they can be simulated with polynomial over-
head by Turing Machines (and hence also by NAND-TM or NAND-
RAM programs). Consequently, the classes P and EXP are robust to
the choice of model, and we can use the programming language of
our choice, or high level descriptions of an algorithm, to determine
whether or not a problem is in P.
Like the Church-Turing thesis itself, the extended Church-Turing
thesis is in the asymptotic setting and does not directly yield an ex-
perimentally testable prediction. However, it can be instantiated with
more concrete bounds on the overhead, yielding experimentally-
testable predictions such as the Physical Extended Church-Turing Thesis
we mentioned in Section 5.6.
In the last hundred+ years of studying and mechanizing com-
putation, no one has yet constructed a scalable computing device
that violates the extended Church Turing Thesis. However, quan-
tum computing, if realized, will pose a serious challenge to the ex-
tended Church-Turing Thesis (see Chapter 23). However, even if
the promises of quantum computing are fully realized, the extended
Church-Turing thesis is ‚Äúmorally‚Äù correct, in the sense that, while we
do need to adapt the thesis to account for the possibility of quantum
computing, its broad outline remains unchanged. We are still able
to model computation mathematically, we can still treat programs
as strings and have a universal program, we still have time hierarchy
and uncomputability results, and there is still no reason to doubt the
(‚Äúplain‚Äù) Church-Turing thesis. Moreover, the prospect of quantum
computing does not seem to make a difference for the time complexity
of many (though not all!) of the concrete problems that we care about.
In particular, as far as we know, out of all the example problems men-
tioned in Chapter 12 the complexity of only one‚Äî integer factoring‚Äî
is affected by modifying our model to include quantum computers as
well.


--- Page 429 ---

modeling running time
429
Figure 13.5: The universal NAND-RAM program
ùëàsimulates an input NAND-RAM program ùëÉ
by storing all of ùëÉ‚Äôs variables inside a single array
Vars of ùëà. If ùëÉhas ùë°variables, then the array Vars
is divided into blocks of length ùë°, where the ùëó-th
coordinate of the ùëñ-th block contains the ùëñ-th element
of the ùëó-th array of ùëÉ. If the ùëó-th variable of ùëÉis
scalar, then we just store its value in the zeroth block
of Vars.
13.4 EFFICIENT UNIVERSAL MACHINE: A NAND-RAM INTER-
PRETER IN NAND-RAM
We have seen in Theorem 9.1 the ‚Äúuniversal Turing Machine‚Äù. Exam-
ining that proof, and combining it with Theorem 13.5 , we can see that
the program ùëàhas a polynomial overhead, in the sense that it can sim-
ulate ùëásteps of a given NAND-TM (or NAND-RAM) program ùëÉon
an input ùë•in ùëÇ(ùëá4) steps. But in fact, by directly simulating NAND-
RAM programs we can do better with only a constant multiplicative
overhead. That is, there is a universal NAND-RAM program ùëàsuch that
for every NAND-RAM program ùëÉ, ùëàsimulates ùëásteps of ùëÉusing
only ùëÇ(ùëá) steps. (The implicit constant in the ùëÇnotation can depend
on the program ùëÉbut does not depend on the length of the input.)
Theorem 13.7 ‚Äî Efficient universality of NAND-RAM. There exists a NAND-
RAM program ùëàsatisfying the following:
1. (ùëàis a universal NAND-RAM program.) For every NAND-RAM
program ùëÉand input ùë•, ùëà(ùëÉ, ùë•)
=
ùëÉ(ùë•) where by ùëà(ùëÉ, ùë•) we
denote the output of ùëàon a string encoding the pair (ùëÉ, ùë•).
2. (ùëàis efficient.) There are some constants ùëé, ùëèsuch that for ev-
ery NAND-RAM program ùëÉ, if ùëÉhalts on input ùë•after most
ùëásteps, then ùëà(ùëÉ, ùë•) halts after at most ùê∂
‚ãÖ
ùëásteps where
ùê∂‚â§ùëé|ùëÉ|ùëè.
P
As in the case of Theorem 13.5, the proof of Theo-
rem 13.7 is not very deep and so it is more important
to understand its statement. Specifically, if you under-
stand how you would go about writing an interpreter
for NAND-RAM using a modern programming lan-
guage such as Python, then you know everything you
need to know about the proof of this theorem.
Proof of Theorem 13.7. To present a universal NAND-RAM program
in full we would need to describe a precise representation scheme,
as well as the full NAND-RAM instructions for the program. While
this can be done, it is more important to focus on the main ideas, and
so we just sketch the proof here. A specification of NAND-RAM is
given in the appendix, and for the purposes of this simulation, we can
simply use the representation of the code NAND-RAM as an ASCII
string.


--- Page 430 ---

430
introduction to theoretical computer science
The program ùëàgets as input a NAND-RAM program ùëÉand an
input ùë•and simulates ùëÉone step at a time. To do so, ùëàdoes the fol-
lowing:
1. ùëàmaintains variables program_counter, and number_steps for the
current line to be executed and the number of steps executed so far.
2. ùëàinitially scans the code of ùëÉto find the number ùë°of unique vari-
able names that ùëÉuses. It will translate each variable name into a
number between 0 and ùë°‚àí1 and use an array Program to store ùëÉ‚Äôs
code where for every line ‚Ñì, Program[‚Ñì] will store the ‚Ñì-th line of ùëÉ
where the variable names have been translated to numbers. (More
concretely, we will use a constant number of arrays to separately
encode the operation used in this line, and the variable names and
indices of the operands.)
3. ùëàmaintains a single array Vars that contains all the values of ùëÉ‚Äôs
variables. We divide Vars into blocks of length ùë°. If ùë†is a num-
ber corresponding to an array variable Foo of ùëÉ, then we store
Foo[0] in Vars[ùë†], we store Foo[1] in Var_values[ùë°+ ùë†], Foo[2]
in Vars[2ùë°+ ùë†] and so on and so forth (see Fig. 13.5). Generally,if
the ùë†-th variable of ùëÉis a scalar variable, then its value will be
stored in location Vars[ùë†]. If it is an array variable then the value of
its ùëñ-th element will be stored in location Vars[ùë°‚ãÖùëñ+ ùë†].
4. To simulate a single step of ùëÉ, the program ùëàrecovers from Pro-
gram the line corresponding to program_counter and executes it.
Since NAND-RAM has a constant number of arithmetic operations,
we can implement the logic of which operation to execute using a
sequence of a constant number of if-then-else‚Äôs. Retrieving from
Vars the values of the operands of each instruction can be done
using a constant number of arithmetic operations.
The setup stages take only a constant (depending on |ùëÉ| but not
on the input ùë•) number of steps. Once we are done with the setup, to
simulate a single step of ùëÉ, we just need to retrieve the corresponding
line and do a constant number of ‚Äúif elses‚Äù and accesses to Vars to
simulate it. Hence the total running time to simulate ùëásteps of the
program ùëÉis at most ùëÇ(ùëá) when suppressing constants that depend
on the program ùëÉ.
‚ñ†
13.4.1 Timed Universal Turing Machine
One corollary of the efficient universal machine is the following.
Given any Turing Machine ùëÄ, input ùë•, and ‚Äústep budget‚Äù ùëá, we can
simulate the execution of ùëÄfor ùëásteps in time that is polynomial in


--- Page 431 ---

modeling running time
431
Figure 13.6: The timed universal Turing Machine takes
as input a Turing machine ùëÄ, an input ùë•, and a time
bound ùëá, and outputs ùëÄ(ùë•) if ùëÄhalts within at
most ùëásteps. Theorem 13.8 states that there is such a
machine that runs in time polynomial in ùëá.
ùëá. Formally, we define a function TIMEDEVAL that takes the three
parameters ùëÄ, ùë•, and the time budget, and outputs ùëÄ(ùë•) if ùëÄhalts
within at most ùëásteps, and outputs 0 otherwise. The timed univer-
sal Turing Machine computes TIMEDEVAL in polynomial time (see
Fig. 13.6). (Since we measure time as a function of the input length,
we define TIMEDEVAL as taking the input ùëárepresented in unary: a
string of ùëáones.)
Theorem 13.8 ‚Äî Timed Universal Turing Machine. Let TIMEDEVAL
‚à∂
{0, 1}‚àó‚Üí{0, 1}‚àóbe the function defined as
TIMEDEVAL(ùëÄ, ùë•, 1ùëá) =
‚éß
{
‚é®
{
‚é©
ùëÄ(ùë•)
ùëÄhalts within ‚â§ùëásteps on ùë•
0
otherwise
.
(13.3)
Then TIMEDEVAL ‚ààP.
Proof. We only sketch the proof since the result follows fairly directly
from Theorem 13.5 and Theorem 13.7. By Theorem 13.5 to show that
TIMEDEVAL ‚ààP, it suffices to give a polynomial-time NAND-RAM
program to compute TIMEDEVAL.
Such a program can be obtained as follows. Given a Turing Ma-
chine ùëÄ, by Theorem 13.5 we can transform it in time polynomial in
its description into a functionally-equivalent NAND-RAM program
ùëÉsuch that the execution of ùëÄon ùëásteps can be simulated by the
execution of ùëÉon ùëê‚ãÖùëásteps. We can then run the universal NAND-
RAM machine of Theorem 13.7 to simulate ùëÉfor ùëê‚ãÖùëásteps, using
ùëÇ(ùëá) time, and output 0 if the execution did not halt within this bud-
get. This shows that TIMEDEVAL can be computed by a NAND-RAM
program in time polynomial in |ùëÄ| and linear in ùëá, which means
TIMEDEVAL ‚ààP.
‚ñ†
13.5 THE TIME HIERARCHY THEOREM
Some functions are uncomputable, but are there functions that can
be computed, but only at an exorbitant cost? For example, is there a
function that can be computed in time 2ùëõ, but can not be computed in
time 20.9ùëõ? It turns out that the answer is Yes:
Theorem 13.9 ‚Äî Time Hierarchy Theorem. For every nice function ùëá
‚à∂
‚Ñï‚Üí‚Ñï, there is a function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1} in TIME(ùëá(ùëõ) log ùëõ)‚ßµ
TIME(ùëá(ùëõ)).
There is nothing special about log ùëõ, and we could have used any
other efficiently computable function that tends to infinity with ùëõ.


--- Page 432 ---

432
introduction to theoretical computer science
ÔÉ´Big Idea 19 If we have more time, we can compute more functions.
R
Remark 13.10 ‚Äî Simpler corollary of the time hierarchy
theorem. The generality of the time hierarchy theorem
can make its proof a little hard to read. It might be
easier to follow the proof if you first try to prove by
yourself the easier statement P ‚ääEXP.
You can do so by showing that the following function
ùêπ
‚à∂{0, 1}‚àó‚à∂‚Üí{0, 1} is in EXP ‚ßµP: for every Turing
Machine ùëÄand input ùë•, ùêπ(ùëÄ, ùë•)
=
1 if and only if
ùëÄhalts on ùë•within at most |ùë•|log |ùë•| steps. One can
show that ùêπ
‚àà
TIME(ùëõùëÇ(log ùëõ))
‚äÜ
EXP using the
universal Turing machine (or the efficient universal
NAND-RAM program of Theorem 13.7). On the other
harnd, we can use similar ideas to those used to show
the uncomputability of HALT in Section 9.3.2 to prove
that ùêπ‚àâP.
Figure 13.7: The Time Hierarchy Theorem (Theo-
rem 13.9) states that all of these classes are distinct.
Proof Idea:
In the proof of Theorem 9.6 (the uncomputability of the Halting
problem), we have shown that the function HALT cannot be com-
puted in any finite time. An examination of the proof shows that it
gives something stronger. Namely, the proof shows that if we fix our
computational budget to be ùëásteps, then not only we can‚Äôt distinguish
between programs that halt and those that do not, but cannot even
distinguish between programs that halt within at most ùëá‚Ä≤ steps and
those that take more than that (where ùëá‚Ä≤ is some number depending
on ùëá). Therefore, the proof of Theorem 13.9 follows the ideas of the


--- Page 433 ---

modeling running time
433
uncomputability of the halting problem, but again with a more careful
accounting of the running time.
‚ãÜ
Proof of Theorem 13.9. Our proof is inspired by the proof of the un-
computability of the halting problem. Specifically, for every function
ùëáas in the theorem‚Äôs statement, we define the Bounded Halting func-
tion HALTùëáas follows. The input to HALTùëáis a pair (ùëÉ, ùë•) such that
|ùëÉ| ‚â§log log |ùë•| encodes some NAND-RAM program. We define
HALTùëá(ùëÉ, ùë•) =
‚éß
{
‚é®
{
‚é©
1,
ùëÉhalts on ùë•within ‚â§100 ‚ãÖùëá(|ùëÉ| + |ùë•|) steps
0,
otherwise
.
(13.4)
(The constant 100 and the function log log ùëõare rather arbitrary, and
are chosen for convenience in this proof.)
Theorem 13.9 is an immediate consequence of the following two
claims:
Claim 1: HALTùëá‚ààTIME(ùëá(ùëõ) ‚ãÖlog ùëõ)
and
Claim 2: HALTùëá‚àâTIME(ùëá(ùëõ)).
Please make sure you understand why indeed the theorem follows
directly from the combination of these two claims. We now turn to
proving them.
Proof of claim 1: We can easily check in linear time whether an
input has the form ùëÉ, ùë•where |ùëÉ| ‚â§log log |ùë•|. Since ùëá(‚ãÖ) is a nice
function, we can evaluate it in ùëÇ(ùëá(ùëõ)) time. Thus, we can compute
HALTùëá(ùëÉ, ùë•) as follows:
1. Compute ùëá0 = ùëá(|ùëÉ| + |ùë•|) in ùëÇ(ùëá0) steps.
2. Use the universal NAND-RAM program of Theorem 13.7 to simu-
late 100‚ãÖùëá0 steps of ùëÉon the input ùë•using at most ùëùùëúùëôùë¶(|ùëÉ|)ùëá0 steps.
(Recall that we use ùëùùëúùëôùë¶(‚Ñì) to denote a quantity that is bounded by
ùëé‚Ñìùëèfor some constants ùëé, ùëè.)
3. If ùëÉhalts within these 100 ‚ãÖùëá0 steps then output 1, else output 0.
The length of the input is ùëõ= |ùëÉ| + |ùë•|. Since |ùë•| ‚â§ùëõand
(log log |ùë•|)ùëè= ùëú(log |ùë•|) for every ùëè, the running time will be
ùëú(ùëá(|ùëÉ| + |ùë•|) log ùëõ) and hence the above algorithm demonstrates that
HALTùëá‚ààTIME(ùëá(ùëõ) ‚ãÖlog ùëõ), completing the proof of Claim 1.
Proof of claim 2: This proof is the heart of Theorem 13.9, and is
very reminiscent of the proof that HALT is not computable. Assume,
for the sake of contradiction, that there is some NAND-RAM program
ùëÉ‚àóthat computes HALTùëá(ùëÉ, ùë•) within ùëá(|ùëÉ| + |ùë•|) steps. We are going


--- Page 434 ---

434
introduction to theoretical computer science
to show a contradiction by creating a program ùëÑand showing that
under our assumptions, if ùëÑruns for less than ùëá(ùëõ) steps when given
(a padded version of) its own code as input then it actually runs for
more than ùëá(ùëõ) steps and vice versa. (It is worth re-reading the last
sentence twice or thrice to make sure you understand this logic. It is
very similar to the direct proof of the uncomputability of the halting
problem where we obtained a contradiction by using an assumed
‚Äúhalting solver‚Äù to construct a program that, given its own code as
input, halts if and only if it does not halt.)
We will define ùëÑ‚àóto be the program that on input a string ùëßdoes
the following:
1. If ùëßdoes not have the form ùëß= ùëÉ1ùëöwhere ùëÉrepresents a NAND-
RAM program and |ùëÉ| < 0.1 log log ùëöthen return 0. (Recall that
1ùëödenotes the string of ùëöones.)
2. Compute ùëè= ùëÉ‚àó(ùëÉ, ùëß) (at a cost of at most ùëá(|ùëÉ| + |ùëß|) steps, under
our assumptions).
3. If ùëè= 1 then ùëÑ‚àógoes into an infinite loop, otherwise it halts.
Let ‚Ñìbe the length description of ùëÑ‚àóas a string, and let ùëöbe larger
than 221000‚Ñì. We will reach a contradiction by splitting into cases ac-
cording to whether or not HALTùëá(ùëÑ‚àó, ùëÑ‚àó1ùëö) equals 0 or 1.
On the one hand, if HALTùëá(ùëÑ‚àó, ùëÑ‚àó1ùëö) = 1, then under our as-
sumption that ùëÉ‚àócomputes HALTùëá, ùëÑ‚àówill go into an infinite loop
on input ùëß= ùëÑ‚àó1ùëö, and hence in particular ùëÑ‚àódoes not halt within
100ùëá(|ùëÑ‚àó| + ùëö) steps on the input ùëß. But this contradicts our assump-
tion that HALTùëá(ùëÑ‚àó, ùëÑ‚àó1ùëö) = 1.
This means that it must hold that HALTùëá(ùëÑ‚àó, ùëÑ‚àó1ùëö) = 0. But
in this case, since we assume ùëÉ‚àócomputes HALTùëá, ùëÑ‚àódoes not do
anything in phase 3 of its computation, and so the only computa-
tion costs come in phases 1 and 2 of the computation. It is not hard
to verify that Phase 1 can be done in linear and in fact less than 5|ùëß|
steps. Phase 2 involves executing ùëÉ‚àó, which under our assumption
requires ùëá(|ùëÑ‚àó| + ùëö) steps. In total we can perform both phases in
less than 10ùëá(|ùëÑ‚àó| + ùëö) in steps, which by definition means that
HALTùëá(ùëÑ‚àó, ùëÑ‚àó1ùëö) = 1, but this is of course a contradiction. This
completes the proof of Claim 2 and hence of Theorem 13.9.
‚ñ†
Solved Exercise 13.3 ‚Äî P vs EXP. Prove that P ‚ääEXP.
‚ñ†
Solution:


--- Page 435 ---

modeling running time
435
Figure 13.8: Some complexity classes and some of the
functions we know (or conjecture) to be contained in
them.
We show why this statement follows from the time hierarchy
theorem, but it can be an instructive exercise to prove it directly,
see Remark 13.10. We need to show that there exists ùêπ‚ààEXP ‚ßµP.
Let ùëá(ùëõ)
=
ùëõlog ùëõand ùëá‚Ä≤(ùëõ)
=
ùëõlog ùëõ/2. Both are nice functions.
Since ùëá(ùëõ)/ùëá‚Ä≤(ùëõ)
=
ùúî(log ùëõ), by Theorem 13.9 there exists some
ùêπin TIME(ùëá‚Ä≤(ùëõ))
‚ää
TIME(ùëá(ùëõ)). Since for sufficiently large ùëõ,
2ùëõ> ùëõlog ùëõ, ùêπ‚ààTIME(2ùëõ) ‚äÜEXP. On the other hand, ùêπ‚àâP.
Indeed, suppose otherwise that there was a constant ùëê
>
0 and
a Turing Machine computing ùêπon ùëõ-length input in at most ùëõùëê
steps for all sufficiently large ùëõ. Then since for ùëõlarge enough
ùëõùëê
<
ùëõlog ùëõ/2, it would have followed that ùêπ
‚àà
TIME(ùëõlog ùëõ/2)
contradicting our choice of ùêπ.
‚ñ†
The time hierarchy theorem tells us that there are functions we can
compute in ùëÇ(ùëõ2) time but not ùëÇ(ùëõ), in 2ùëõtime, but not 2
‚àöùëõ, etc.. In
particular there are most definitely functions that we can compute in
time 2ùëõbut not ùëÇ(ùëõ). We have seen that we have no shortage of natu-
ral functions for which the best known algorithm requires roughly 2ùëõ
time, and that many people have invested significant effort in trying
to improve that. However, unlike in the finite vs. infinite case, for all
of the examples above at the moment we do not know how to rule
out even an ùëÇ(ùëõ) time algorithm. We will however see that there is a
single unproven conjecture that would imply such a result for most of
these problems.
The time hierarchy theorem relies on the existence of an efficient
universal NAND-RAM program, as proven in Theorem 13.7. For
other models such as Turing Machines we have similar time hierarchy
results showing that there are functions computable in time ùëá(ùëõ) and
not in time ùëá(ùëõ)/ùëì(ùëõ) where ùëì(ùëõ) corresponds to the overhead in the
corresponding universal machine.
13.6 NON UNIFORM COMPUTATION
We have now seen two measures of ‚Äúcomputation cost‚Äù for functions.
In Section 4.6 we defined the complexity of computing finite functions
using circuits / straightline programs. Specifically, for a finite function
ùëî‚à∂{0, 1}ùëõ‚Üí{0, 1} and number ùëá‚àà‚Ñï, ùëî‚ààSIZE(ùëá) if there is a circuit
of at most ùëáNAND gates (or equivalently a ùëá-line NAND-CIRC
program) that computes ùëî. To relate this to the classes TIME(ùëá(ùëõ))
defined in this chapter we first need to extend the class SIZE(ùëá(ùëõ))
from finite functions to functions with unbounded input length.
Definition 13.11 ‚Äî Non uniform computation. Let ùêπ
‚à∂
{0, 1}‚àó
‚Üí
{0, 1}
and ùëá
‚à∂‚Ñï‚Üí‚Ñïbe a nice time bound. For every ùëõ‚àà‚Ñï, define


--- Page 436 ---

436
introduction to theoretical computer science
Figure 13.9: We can think of an infinite function
ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1} as a collection of finite functions
ùêπ0, ùêπ1, ùêπ2, ‚Ä¶ where ùêπ‚Üæùëõ‚à∂{0, 1}ùëõ‚Üí{0, 1} is the
restriction of ùêπto inputs of length ùëõ. We say ùêπis in
P/poly if for every ùëõ, the function ùêπ‚Üæùëõis computable
by a polynomial size NAND-CIRC program, or
equivalently, a polynomial sized Boolean circuit.
ùêπ‚Üæùëõ‚à∂{0, 1}ùëõ‚Üí{0, 1} to be the restriction of ùêπto inputs of size ùëõ.
That is, ùêπ‚Üæùëõis the function mapping {0, 1}ùëõto {0, 1} such that for
every ùë•‚àà{0, 1}ùëõ, ùêπ‚Üæùëõ(ùë•) = ùêπ(ùë•).
We say that ùêπis non-uniformly computable in at most ùëá(ùëõ) size, de-
noted by ùêπ
‚ààSIZE(ùëá(ùëõ)) if there exists a sequence (ùê∂0, ùê∂1, ùê∂2, ‚Ä¶)
of NAND circuits such that:
‚Ä¢ For every ùëõ‚àà‚Ñï, ùê∂ùëõcomputes the function ùêπ‚Üæùëõ
‚Ä¢ For every sufficiently large ùëõ, ùê∂ùëõhas at most ùëá(ùëõ) gates.
The non uniform analog to the class P is the class P/poly defined as
P/poly = ‚à™ùëê‚àà‚ÑïSIZE(ùëõùëê) .
(13.5)
There is a big difference between non uniform computation and uni-
form complexity classes such as TIME(ùëá(ùëõ)) or P. The condition
ùêπ‚ààP means that there is a single Turing machine ùëÄthat computes
ùêπon all inputs in polynomial time. The condition ùêπ‚ààP/poly only
means that for every input length ùëõthere can be a different circuit ùê∂ùëõ
that computes ùêπusing polynomially many gates on inputs of these
lengths. As we will see, ùêπ‚ààP/poly does not necessarily imply that
ùêπ‚ààP. However, the other direction is true:
Theorem 13.12 ‚Äî Nonuniform computation contains uniform computa-
tion. There is some ùëé
‚àà
‚Ñïs.t. for every nice ùëá
‚à∂
‚Ñï
‚Üí
‚Ñïand
ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1},
TIME(ùëá(ùëõ)) ‚äÜSIZE(ùëá(ùëõ)ùëé) .
(13.6)
In particular, Theorem 13.12 shows that for every ùëê, TIME(ùëõùëê) ‚äÜ
SIZE(ùëõùëêùëé) and hence P ‚äÜP/poly.
Proof Idea:
The idea behind the proof is to ‚Äúunroll the loop‚Äù. Specifically, we
will use the programming language variants of non-uniform and uni-
form computation: namely NAND-CIRC and NAND-TM. The main
difference between the two is that NAND-TM has loops. However, for
every fixed ùëõ, if we know that a NAND-TM program runs in at most
ùëá(ùëõ) steps, then we can replace its loop by simply ‚Äúcopying and past-
ing‚Äù its code ùëá(ùëõ) times, similar to how in Python we can replace code
such as
for i in range(4):
print(i)
with the ‚Äúloop free‚Äù code


--- Page 437 ---

modeling running time
437
print(0)
print(1)
print(2)
print(3)
To make this idea into an actual proof we need to tackle one tech-
nical difficulty, and this is to ensure that the NAND-TM program is
oblivious in the sense that the value of the index variable i in the ùëó-th
iteration of the loop will depend only on ùëóand not on the contents of
the input. We make a digression to do just that in Section 13.6.1 and
then complete the proof of Theorem 13.12.
‚ãÜ
13.6.1 Oblivious NAND-TM programs
Our approach for proving Theorem 13.12 involves ‚Äúunrolling the
loop‚Äù. For example, consider the following NAND-TM to compute the
XOR function on inputs of arbitrary length:
temp_0 = NAND(X[0],X[0])
Y_nonblank[0] = NAND(X[0],temp_0)
temp_2 = NAND(X[i],Y[0])
temp_3 = NAND(X[i],temp_2)
temp_4 = NAND(Y[0],temp_2)
Y[0] = NAND(temp_3,temp_4)
MODANDJUMP(X_nonblank[i],X_nonblank[i])
Setting (as an example) ùëõ= 3, we can attempt to translate this
NAND-TM program into a NAND-CIRC program for computing
XOR3 ‚à∂{0, 1}3 ‚Üí{0, 1} by simply ‚Äúcopying and pasting‚Äù the loop
three times (dropping the MODANDJMP line):
temp_0 = NAND(X[0],X[0])
Y_nonblank[0] = NAND(X[0],temp_0)
temp_2 = NAND(X[i],Y[0])
temp_3 = NAND(X[i],temp_2)
temp_4 = NAND(Y[0],temp_2)
Y[0] = NAND(temp_3,temp_4)
temp_0 = NAND(X[0],X[0])
Y_nonblank[0] = NAND(X[0],temp_0)
temp_2 = NAND(X[i],Y[0])
temp_3 = NAND(X[i],temp_2)
temp_4 = NAND(Y[0],temp_2)
Y[0] = NAND(temp_3,temp_4)
temp_0 = NAND(X[0],X[0])
Y_nonblank[0] = NAND(X[0],temp_0)


--- Page 438 ---

438
introduction to theoretical computer science
Figure 13.10: A NAND circuit for XOR3 obtained by
‚Äúunrolling the loop‚Äù of the NAND-TM program for
computing XOR three times.
temp_2 = NAND(X[i],Y[0])
temp_3 = NAND(X[i],temp_2)
temp_4 = NAND(Y[0],temp_2)
Y[0] = NAND(temp_3,temp_4)
However, the above is still not a valid NAND-CIRC program since
it contains references to the special variable i. To make it into a valid
NAND-CIRC program, we replace references to i in the first iteration
with 0, references in the second iteration with 1, and references in the
third iteration with 2. (We also create a variable zero and use it for the
first time any variable is instantiated, as well as remove assignments to
non-output variables that are never used later on.) The resulting pro-
gram is a standard ‚Äúloop free and index free‚Äù NAND-CIRC program
that computes XOR3 (see also Fig. 13.10):
temp_0 = NAND(X[0],X[0])
one = NAND(X[0],temp_0)
zero = NAND(one,one)
temp_2 = NAND(X[0],zero)
temp_3 = NAND(X[0],temp_2)
temp_4 = NAND(zero,temp_2)
Y[0] = NAND(temp_3,temp_4)
temp_2 = NAND(X[1],Y[0])
temp_3 = NAND(X[1],temp_2)
temp_4 = NAND(Y[0],temp_2)
Y[0] = NAND(temp_3,temp_4)
temp_2 = NAND(X[2],Y[0])
temp_3 = NAND(X[2],temp_2)
temp_4 = NAND(Y[0],temp_2)
Y[0] = NAND(temp_3,temp_4)
Key to this transformation was the fact that in our original NAND-
TM program for XOR, regardless of whether the input is 011, 100, or
any other string, the index variable i is guaranteed to equal 0 in the
first iteration, 1 in the second iteration, 2 in the third iteration, and so
on and so forth. The particular sequence 0, 1, 2, ‚Ä¶ is immaterial: the
crucial property is that the NAND-TM program for XOR is oblivious
in the sense that the value of the index i in the ùëó-th iteration depends
only on ùëóand does not depend on the particular choice of the input.
Luckily, it is possible to transform every NAND-TM program into
a functionally equivalent oblivious program with at most quadratic
overhead. (Similarly we can transform any Turing machine into a
functionally equivalent oblivious Turing machine, see Exercise 13.6.)


--- Page 439 ---

modeling running time
439
Figure 13.11: We simulate a ùëá(ùëõ)-time NAND-TM
program ùëÉ‚Ä≤ with an oblivious NAND-TM program ùëÉ
by adding special arrays Atstart and Atend to mark
positions 0 and ùëá‚àí1 respectively. The program ùëÉ
will simply ‚Äúsweep‚Äù its arrays from right to left and
back again. If the original program ùëÉ‚Ä≤ would have
moved i in a different direction then we wait ùëÇ(ùëá)
steps until we reach the same point back again, and so
ùëÉruns in ùëÇ(ùëá(ùëõ)2) time.
Theorem 13.13 ‚Äî Making NAND-TM oblivious. Let ùëá
‚à∂‚Ñï‚Üí‚Ñïbe a nice
function and let ùêπ
‚àà
TIMETM(ùëá(ùëõ)). Then there is a NAND-TM
program ùëÉthat computes ùêπin ùëÇ(ùëá(ùëõ)2) steps and satisfying the
following. For every ùëõ‚àà‚Ñïthere is a sequence ùëñ0, ùëñ1, ‚Ä¶ , ùëñùëö‚àí1 such
that for every ùë•
‚àà
{0, 1}ùëõ, if ùëÉis executed on input ùë•then in the
ùëó-th iteration the variable i is equal to ùëñùëó.
In other words, Theorem 13.13 implies that if we can compute ùêπin
ùëá(ùëõ) steps, then we can compute it in ùëÇ(ùëá(ùëõ)2) steps with a program
ùëÉin which the position of i in the ùëó-th iteration depends only on ùëó
and the length of the input, and not on the contents of the input. Such
a program can be easily translated into a NAND-CIRC program of
ùëÇ(ùëá(ùëõ)2) lines by ‚Äúunrolling the loop‚Äù.
Proof Idea:
We can translate any NAND-TM program ùëÉ‚Ä≤ into an oblivious
program ùëÉby making ùëÉ‚Äúsweep‚Äù its arrays. That is, the index i in
ùëÉwill always move all the way from position 0 to position ùëá(ùëõ) ‚àí1
and back again. We can then simulate the program ùëÉ‚Ä≤ with at most
ùëá(ùëõ) overhead: if ùëÉ‚Ä≤ wants to move i left when we are in a rightward
sweep then we simply wait the at most 2ùëá(ùëõ) steps until the next time
we are back in the same position while sweeping to the left.
‚ãÜ
Proof of Theorem 13.13. Let ùëÉ‚Ä≤ be a NAND-TM program computing ùêπ
in ùëá(ùëõ) steps. We construct an oblivious NAND-TM program ùëÉfor
computing ùêπas follows (see also Fig. 13.11).
1. On input ùë•, ùëÉwill compute ùëá= ùëá(|ùë•|) and set up arrays Atstart
and Atend satisfying Atstart[0]= 1 and Atstart[ùëñ]= 0 for ùëñ> 0
and Atend[ùëá‚àí1]= 1 and Atend[i]= 0 for all ùëñ‚â†ùëá‚àí1. We can do
this because ùëáis a nice function. Note that since this computation
does not depend on ùë•but only on its length, it is oblivious.
2. ùëÉwill also have a special array Marker initialized to all zeroes.
3. The index variable of ùëÉwill change direction of movement to
the right whenever Atstart[i]= 1 and to the left whenever
Atend[i]= 1.
4. The program ùëÉsimulates the execution of ùëÉ‚Ä≤. However, if the
MODANDJMP instruction in ùëÉ‚Ä≤ attempts to move to the right when ùëÉ
is moving left (or vice versa) then ùëÉwill set Marker[i] to 1 and
enter into a special ‚Äúwaiting mode‚Äù. In this mode ùëÉwill wait until
the next time in which Marker[i]= 1 (at the next sweep) at which
points ùëÉzeroes Marker[i] and continues with the simulation. In


--- Page 440 ---

440
introduction to theoretical computer science
Figure 13.12: The function UNROLL takes as input a
Turing Machine ùëÄ, an input length parameter ùëõ, a
step budget parameter ùëá, and outputs a circuit ùê∂of
size ùëùùëúùëôùë¶(ùëá) that takes ùëõbits of inputs and outputs
ùëÄ(ùë•) if ùëÄhalts on ùë•within at most ùëásteps.
the worst case this will take 2ùëá(ùëõ) steps (if ùëÉhas to go all the way
from one end to the other and back again.)
5. We also modify ùëÉto ensure it ends the computation after simu-
lating exactly ùëá(ùëõ) steps of ùëÉ‚Ä≤, adding ‚Äúdummy steps‚Äù if ùëÉ‚Ä≤ ends
early.
We see that ùëÉsimulates the execution of ùëÉ‚Ä≤ with an overhead of
ùëÇ(ùëá(ùëõ)) steps of ùëÉper one step of ùëÉ‚Ä≤, hence completing the proof.
‚ñ†
Theorem 13.13 implies Theorem 13.12. Indeed, if ùëÉis a ùëò-line obliv-
ious NAND-TM program computing ùêπin time ùëá(ùëõ) then for every ùëõ
we can obtain a NAND-CIRC program of (ùëò‚àí1) ‚ãÖùëá(ùëõ) lines by simply
making ùëá(ùëõ) copies of ùëÉ(dropping the final MODANDJMP line). In the
ùëó-th copy we replace all references of the form Foo[i] to foo_ùëñùëówhere
ùëñùëóis the value of i in the ùëó-th iteration.
13.6.2 ‚ÄúUnrolling the loop‚Äù: algorithmic transformation of Turing Machines
to circuits
The proof of Theorem 13.12 is algorithmic, in the sense that the proof
yields a polynomial-time algorithm that given a Turing Machine ùëÄ
and parameters ùëáand ùëõ, produces a circuit of ùëÇ(ùëá2) gates that agrees
with ùëÄon all inputs ùë•‚àà{0, 1}ùëõ(as long as ùëÄruns for less than ùëá
steps these inputs.) We record this fact in the following theorem, since
it will be useful for us later on:
Theorem 13.14 ‚Äî Turing-machine to circuit compiler. There is algorithm
UNROLL such that for every Turing Machine ùëÄand numbers ùëõ, ùëá,
UNROLL(ùëÄ, 1ùëá, 1ùëõ) runs for ùëùùëúùëôùë¶(|ùëÄ|, ùëá, ùëõ) steps and outputs a
NAND circuit ùê∂with ùëõinputs, ùëÇ(ùëá2) gates, and one output, such
that
ùê∂(ùë•) =
‚éß
{
‚é®
{
‚é©
ùë¶
ùëÄhalts in ‚â§ùëásteps and outputs ùë¶
0
otherwise
.
(13.7)
Proof. We only sketch the proof since it follows by directly translat-
ing the proof of Theorem 13.12 into an algorithm together with the
simulation of Turing machines by NAND-TM programs (see also
Fig. 13.13). Specifically, UNROLL does the following:
1. Transform the Turing Machine ùëÄinto an equivalent NAND-TM
program ùëÉ.
2. Transform the NAND-TM program ùëÉinto an equivalent oblivious
program ùëÉ‚Ä≤ following the proof of Theorem 13.13. The program ùëÉ‚Ä≤
takes ùëá‚Ä≤ = ùëÇ(ùëá2) steps to simulate ùëásteps of ùëÉ.


--- Page 441 ---

modeling running time
441
3. ‚ÄúUnroll the loop‚Äù of ùëÉ‚Ä≤ by obtaining a NAND-CIRC program of
ùëÇ(ùëá‚Ä≤) lines (or equivalently a NAND circuit with ùëÇ(ùëá2) gates)
corresponding to the execution of ùëá‚Ä≤ iterations of ùëÉ‚Ä≤.
‚ñ†
Figure 13.13: We can transform a Turing Machine ùëÄ,
input length parameter ùëõ, and time bound ùëáinto an
ùëÇ(ùëá2) sized NAND circuit that agrees with ùëÄon all
inputs ùë•‚àà{0, 1}ùëõon which ùëÄhalts in at most ùëá
steps. The transformation is obtained by first using
the equivalence of Turing Machines and NAND-
TM programs ùëÉ, then turning ùëÉinto an equivalent
oblivious NAND-TM program ùëÉ‚Ä≤ via Theorem 13.13,
then ‚Äúunrolling‚Äù ùëÇ(ùëá2) iterations of the loop of
ùëÉ‚Ä≤ to obtain an ùëÇ(ùëá2) line NAND-CIRC program
that agrees with ùëÉ‚Ä≤ on length ùëõinputs, and finally
translating this program into an equivalent circuit.
ÔÉ´Big Idea 20 By ‚Äúunrolling the loop‚Äù we can transform an al-
gorithm that takes ùëá(ùëõ) steps to compute ùêπinto a circuit that uses
ùëùùëúùëôùë¶(ùëá(ùëõ)) gates to compute the restriction of ùêπto {0, 1}ùëõ.
P
Reviewing the transformations described in Fig. 13.13,
as well as solving the following two exercises is a great
way to get more comfort with non-uniform complexity
and in particular with P/poly and its relation to P.
Solved Exercise 13.4 ‚Äî Alternative characterization of P. Prove that for every
ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}, ùêπ‚ààP if and only if there is a polynomial-
time Turing Machine ùëÄsuch that for every ùëõ‚àà‚Ñï, ùëÄ(1ùëõ) outputs a
description of an ùëõinput circuit ùê∂ùëõthat computes the restriction ùêπ‚Üæùëõ
of ùêπto inputs in {0, 1}ùëõ.
‚ñ†
Solution:
We start with the ‚Äúif‚Äù direction. Suppose that there is a polynomial-
time Turing Machine ùëÄthat on input 1ùëõoutputs a circuit ùê∂ùëõthat


--- Page 442 ---

442
introduction to theoretical computer science
computes ùêπ‚Üæùëõ. Then the following is a polynomial-time Turing
Machine ùëÄ‚Ä≤ to compute ùêπ. On input ùë•‚àà{0, 1}‚àó, ùëÄ‚Ä≤ will:
1. Let ùëõ= |ùë•| and compute ùê∂ùëõ= ùëÄ(1ùëõ).
2. Return the evaluation of ùê∂ùëõon ùë•.
Since we can evaluate a Boolean circuit on an input in poly-
nomial time, ùëÄ‚Ä≤ runs in polynomial time and computes ùêπ(ùë•) on
every input ùë•.
For the ‚Äúonly if‚Äù direction, if ùëÄ‚Ä≤ is a Turing Machine that com-
putes ùêπin polynomial-time, then (applying the equivalence of Tur-
ing Machines and NAND-TM as well as Theorem 13.13) there is
also an oblivious NAND-TM program ùëÉthat computes ùêπin time
ùëù(ùëõ) for some polynomial ùëù. We can now define ùëÄto be the Turing
Machine that on input 1ùëõoutputs the NAND circuit obtained by
‚Äúunrolling the loop‚Äù of ùëÉfor ùëù(ùëõ) iterations. The resulting NAND
circuit computes ùêπ‚Üæùëõand has ùëÇ(ùëù(ùëõ)) gates. It can also be trans-
formed to a Boolean circuit with ùëÇ(ùëù(ùëõ)) AND/OR/NOT gates.
‚ñ†
Solved Exercise 13.5 ‚Äî P/poly characterization by advice. Let ùêπ‚à∂{0, 1}‚àó‚Üí
{0, 1}. Then ùêπ‚ààP/poly if and only if there exists a polynomial ùëù‚à∂‚Ñï‚Üí
‚Ñï, a polynomial-time Turing Machine ùëÄand a sequence {ùëéùëõ}ùëõ‚àà‚Ñïof
strings, such that for every ùëõ‚àà‚Ñï:
‚Ä¢ |ùëéùëõ| ‚â§ùëù(ùëõ)
‚Ä¢ For every ùë•‚àà{0, 1}ùëõ, ùëÄ(ùëéùëõ, ùë•) = ùêπ(ùë•).
‚ñ†
Solution:
We only sketch the proof. For the ‚Äúonly if‚Äù direction, if ùêπ
‚àà
P/poly then we can use for ùëéùëõsimply the description of the cor-
responding circuit ùê∂ùëõand for ùëÄthe program that computes in
polynomial time the evaluation of a circuit on its input.
For the ‚Äúif‚Äù direction, we can use the same ‚Äúunrolling the loop‚Äù
technique of Theorem 13.12 to show that if ùëÉis a polynomial-time
NAND-TM program, then for every ùëõ‚àà‚Ñï, the map ùë•‚Ü¶ùëÉ(ùëéùëõ, ùë•)
can be computed by a polynomial size NAND-CIRC program ùëÑùëõ.
‚ñ†
13.6.3 Can uniform algorithms simulate non uniform ones?
Theorem 13.12 shows that every function in TIME(ùëá(ùëõ)) is in
SIZE(ùëùùëúùëôùë¶(ùëá(ùëõ))). One can ask if there is an inverse relation. Suppose
that ùêπis such that ùêπ‚Üæùëõhas a ‚Äúshort‚Äù NAND-CIRC program for every
ùëõ. Can we say that it must be in TIME(ùëá(ùëõ)) for some ‚Äúsmall‚Äù ùëá? The


--- Page 443 ---

modeling running time
443
answer is an emphatic no. Not only is P/poly not contained in P, in fact
P/poly contains functions that are uncomputable!
Theorem 13.15 ‚Äî P/poly contains uncomputable functions. There exists an
uncomputable function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1} such that ùêπ‚ààP/poly.
Proof Idea:
Since P/poly corresponds to non uniform computation, a function
ùêπis in P/poly if for every ùëõ‚àà‚Ñï, the restriction ùêπ‚Üæùëõto inputs of length
ùëõhas a small circuit/program, even if the circuits for different values
of ùëõare completely different from one another. In particular, if ùêπhas
the property that for every equal-length inputs ùë•and ùë•‚Ä≤, ùêπ(ùë•) =
ùêπ(ùë•‚Ä≤) then this means that ùêπ‚Üæùëõis either the constant function zero
or the constant function one for every ùëõ‚àà‚Ñï. Since the constant
function has a (very!) small circuit, such a function ùêπwill always
be in P/poly (indeed even in smaller classes). Yet by a reduction from
the Halting problem, we can obtain a function with this property that
is uncomputable.
‚ãÜ
Proof of Theorem 13.15. Consider the following ‚Äúunary halting func-
tion‚Äù UH ‚à∂{0, 1}‚àó‚Üí{0, 1} defined as follows. We let ùëÜ‚à∂‚Ñï‚Üí{0, 1}‚àó
be the function that on input ùëõ‚àà‚Ñï, outputs the string that corre-
sponds to the binary representation of the number ùëõwithout the most
significant 1 digit. Note that ùëÜis onto. For every ùë•‚àà{0, 1}, we de-
fine UH(ùë•) = HALTONZERO(ùëÜ(|ùë•|)). That is, if ùëõis the length of ùë•,
then UH(ùë•) = 1 if and only if the string ùëÜ(ùëõ) encodes a NAND-TM
program that halts on the input 0.
UH is uncomputable, since otherwise we could compute
HALTONZERO by transforming the input program ùëÉinto the integer
ùëõsuch that ùëÉ= ùëÜ(ùëõ) and then running UH(1ùëõ) (i.e., UH on the string
of ùëõones). On the other hand, for every ùëõ, UHùëõ(ùë•) is either equal
to 0 for all inputs ùë•or equal to 1 on all inputs ùë•, and hence can be
computed by a NAND-CIRC program of a constant number of lines.
‚ñ†
The issue here is of course uniformity. For a function ùêπ‚à∂{0, 1}‚àó‚Üí
{0, 1}, if ùêπis in TIME(ùëá(ùëõ)) then we have a single algorithm that
can compute ùêπ‚Üæùëõfor every ùëõ. On the other hand, ùêπ‚Üæùëõmight be in
SIZE(ùëá(ùëõ)) for every ùëõusing a completely different algorithm for ev-
ery input length. For this reason we typically use P/poly not as a model
of efficient computation but rather as a way to model inefficient compu-
tation. For example, in cryptography people often define an encryp-
tion scheme to be secure if breaking it for a key of length ùëõrequires


--- Page 444 ---

444
introduction to theoretical computer science
more than a polynomial number of NAND lines. Since P ‚äÜP/poly,
this in particular precludes a polynomial time algorithm for doing so,
but there are technical reasons why working in a non uniform model
makes more sense in cryptography. It also allows to talk about se-
curity in non asymptotic terms such as a scheme having ‚Äú128 bits of
security‚Äù.
While it can sometimes be a real issue, in many natural settings the
difference between uniform and non-uniform computation does not
seem so important. In particular, in all the examples of problems not
known to be in P we discussed before: longest path, 3SAT, factoring,
etc., these problems are also not known to be in P/poly either. Thus,
for ‚Äúnatural‚Äù functions, if you pretend that TIME(ùëá(ùëõ)) is roughly the
same as SIZE(ùëá(ùëõ)), you will be right more often than wrong.
Figure 13.14: Relations between P, EXP, and P/poly. It
is known that P ‚äÜEXP, P ‚äÜP/poly and that P/poly
contains uncomputable functions (which in particular
are outside of EXP). It is not known whether or not
EXP ‚äÜP/poly though it is believed that EXP ‚äàP/poly.
13.6.4 Uniform vs. Nonuniform computation: A recap
To summarize, the two models of computation we have described so
far are:
‚Ä¢ Uniform models: Turing machines, NAND-TM programs, RAM ma-
chines, NAND-RAM programs, C/JavaScript/Python, etc. These mod-
els include loops and unbounded memory hence a single program
can compute a function with unbounded input length.
‚Ä¢ Non-uniform models: Boolean Circuits or straightline programs have
no loops and can only compute finite functions. The time to execute
them is exactly the number of lines or gates they contain.
For a function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1} and some nice time bound
ùëá‚à∂‚Ñï‚Üí‚Ñï, we know that:
‚Ä¢ If ùêπis uniformly computable in time ùëá(ùëõ) then there is a sequence
of circuits ùê∂1, ùê∂2, ‚Ä¶ where ùê∂ùëõhas ùëùùëúùëôùë¶(ùëá(ùëõ)) gates and computes
ùêπ‚Üæùëõ(i.e., restriction of ùêπto {0, 1}ùëõ) for every ùëõ.
‚Ä¢ The reverse direction is not necessarily true - there are examples of
functions ùêπ‚à∂{0, 1}ùëõ‚Üí{0, 1} such that ùêπ‚Üæùëõcan be computed by
even a constant size circuit but ùêπis uncomputable.


--- Page 445 ---

modeling running time
445
This means that non uniform complexity is more useful to establish
hardness of a function than its easiness.
‚úì
Chapter Recap
‚Ä¢ We can define the time complexity of a function
using NAND-TM programs, and similarly to the
notion of computability, this appears to capture the
inherent complexity of the function.
‚Ä¢ There are many natural problems that have
polynomial-time algorithms, and other natural
problems that we‚Äôd love to solve, but for which the
best known algorithms are exponential.
‚Ä¢ The definition of polynomial time, and hence the
class P, is robust to the choice of model, whether
it is Turing machines, NAND-TM, NAND-RAM,
modern programming languages, and many other
models.
‚Ä¢ The time hierarchy theorem shows that there are
some problems that can be solved in exponential,
but not in polynomial time. However, we do not
know if that is the case for the natural examples
that we described in this lecture.
‚Ä¢ By ‚Äúunrolling the loop‚Äù we can show that every
function computable in time ùëá(ùëõ) can be computed
by a sequence of NAND-CIRC programs (one for
every input length) each of size at most ùëùùëúùëôùë¶(ùëá(ùëõ))
13.7 EXERCISES
Exercise 13.1 ‚Äî Equivalence of different definitions of P and EXP.. Prove
that the classes P and EXP defined in Definition 13.2 are equal to
‚à™ùëê‚àà{1,2,3,‚Ä¶}TIME(ùëõùëê) and ‚à™ùëê‚àà{1,2,3,‚Ä¶}TIME(2ùëõùëê) respectively. (If
ùëÜ1, ùëÜ2, ùëÜ3, ‚Ä¶ is a collection of sets then the set ùëÜ= ‚à™ùëê‚àà{1,2,3,‚Ä¶}ùëÜùëêis
the set of all elements ùëísuch that there exists some ùëê‚àà{1, 2, 3, ‚Ä¶}
where ùëí‚ààùëÜùëê.)
‚ñ†
Exercise 13.2 ‚Äî Robustness to representation. Theorem 13.5 shows that the
classes P and EXP are robust with respect to variations in the choice
of the computational model. This exercise shows that these classes
are also robust with respect to our choice of the representation of the
input.
Specifically, let ùêπbe a function mapping graphs to {0, 1}, and let
ùêπ‚Ä≤, ùêπ‚Ä≥ ‚à∂{0, 1}‚àó‚Üí{0, 1} be the functions defined as follows. For every
ùë•‚àà{0, 1}‚àó:


--- Page 446 ---

446
introduction to theoretical computer science
1 Hint: This is the Turing Machine analog of Theo-
rem 13.13. We replace one step of the original TM ùëÄ‚Ä≤
computing ùêπwith a ‚Äúsweep‚Äù of the obliviouss TM ùëÄ
in which it goes ùëásteps to the right and then ùëásteps
to the left.
‚Ä¢ ùêπ‚Ä≤(ùë•) = 1 iff ùë•represents a graph ùê∫via the adjacency matrix
representation such that ùêπ(ùê∫) = 1.
‚Ä¢ ùêπ‚Ä≥(ùë•) = 1 iff ùë•represents a graph ùê∫via the adjacency list represen-
tation such that ùêπ(ùê∫) = 1.
Prove that ùêπ‚Ä≤ ‚ààP iff ùêπ‚Ä≥ ‚ààP.
More generally, for every function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}, the answer
to the question of whether ùêπ‚ààP (or whether ùêπ‚ààEXP) is unchanged
by switching representations, as long as transforming one represen-
tation to the other can be done in polynomial time (which essentially
holds for all reasonable representations).
‚ñ†
Exercise 13.3 ‚Äî Boolean functions. For every function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}‚àó,
define ùêµùëúùëúùëô(ùêπ) to be the function mapping {0, 1}‚àóto {0, 1} such that
on input a (string representation of a) triple (ùë•, ùëñ, ùúé) with ùë•‚àà{0, 1}‚àó,
ùëñ‚àà‚Ñïand ùúé‚àà{0, 1},
ùêµùëúùëúùëô(ùêπ)(ùë•, ùëñ, ùúé) =
‚éß
{
{
‚é®
{
{
‚é©
ùêπ(ùë•)ùëñ
ùúé= 0, ùëñ< |ùêπ(ùë•)|
1
ùúé= 1, ùëñ< |ùêπ(ùë•)|
0
otherwise
(13.8)
where ùêπ(ùë•)ùëñis the ùëñ-th bit of the string ùêπ(ùë•).
Prove that ùêπ‚ààP if and only if ùêµùëúùëúùëô(ùêπ) ‚ààP.
‚ñ†
Exercise 13.4 ‚Äî Composition of polynomial time. Prove that if
ùêπ, ùê∫‚à∂{0, 1}‚àó‚Üí{0, 1}‚àóare in P then their composition ùêπ‚àòùê∫,
which is the function ùêªs.t. ùêª(ùë•) = ùêπ(ùê∫(ùë•)), is also in P.
‚ñ†
Exercise 13.5 ‚Äî Non composition of exponential time. Prove that there is some
ùêπ, ùê∫‚à∂{0, 1}‚àó‚Üí{0, 1}‚àós.t. ùêπ, ùê∫‚ààEXP but ùêπ‚àòùê∫is not in EXP.
‚ñ†
Exercise 13.6 ‚Äî Oblivious Turing Machines. We say that a Turing machine ùëÄ
is oblivious if there is some function ùëá‚à∂‚Ñï√ó ‚Ñï‚Üí‚Ñ§such that for every
input ùë•of length ùëõ, and ùë°‚àà‚Ñïit holds that:
‚Ä¢ If ùëÄtakes more than ùë°steps to halt on the input ùë•, then in the ùë°-
th step ùëÄ‚Äôs head will be in the position ùëá(ùëõ, ùë°). (Note that this
position depends only on the length of ùë•and not its contents.)
‚Ä¢ If ùëÄhalts before the ùë°-th step then ùëá(ùëõ, ùë°) = ‚àí1.
Prove that if ùêπ‚ààP then there exists an oblivious Turing machine ùëÄ
that computes ùêπin polynomial time. See footnote for hint.1
‚ñ†


--- Page 447 ---

modeling running time
447
Exercise 13.7 Let EDGE ‚à∂{0, 1}‚àó‚Üí{0, 1} be the function such that on
input a string representing a triple (ùêø, ùëñ, ùëó), where ùêøis the adjacency
list representation of an ùëõvertex graph ùê∫, and ùëñand ùëóare numbers in
[ùëõ], EDGE(ùêø, ùëñ, ùëó) = 1 if the edge {ùëñ, ùëó} is present in the graph. EDGE
outputs 0 on all other inputs.
1. Prove that EDGE ‚ààP.
2. Let PLANARMATRIX ‚à∂{0, 1}‚àó‚Üí{0, 1} be the function that
on input an adjacency matrix ùê¥outputs 1 if and only if the graph
represented by ùê¥is planar (that is, can be drawn on the plane with-
out edges crossing one another). For this question, you can use
without proof the fact that PLANARMATRIX ‚ààP. Prove that
PLANARLIST ‚ààP where PLANARLIST ‚à∂{0, 1}‚àó‚Üí{0, 1} is the
function that on input an adjacency list ùêøoutputs 1 if and only if ùêø
represents a planar graph.
‚ñ†
Exercise 13.8 ‚Äî Evaluate NAND circuits. Let NANDEVAL ‚à∂{0, 1}‚àó‚Üí
{0, 1} be the function such that for every string representing a pair
(ùëÑ, ùë•) where ùëÑis an ùëõ-input 1-output NAND-CIRC (not NAND-TM!)
program and ùë•‚àà{0, 1}ùëõ, NANDEVAL(ùëÑ, ùë•) = ùëÑ(ùë•). On all other
inputs NANDEVAL outputs 0.
Prove that NANDEVAL ‚ààP.
‚ñ†
Exercise 13.9 ‚Äî Find hard function. Let NANDHARD ‚à∂{0, 1}‚àó‚Üí{0, 1}
be the function such that on input a string representing a pair (ùëì, ùë†)
where
‚Ä¢ ùëì‚àà{0, 1}2ùëõfor some ùëõ‚àà‚Ñï
‚Ä¢ ùë†‚àà‚Ñï
NANDHARD(ùëì, ùë†) = 1 if there is no NAND-CIRC program ùëÑ
of at most ùë†lines that computes the function ùêπ‚à∂{0, 1}ùëõ‚Üí{0, 1}
whose truth table is the string ùëì. That is, NANDHARD(ùëì, ùë†) = 1
if for every NAND-CIRC program ùëÑof at most ùë†lines, there exists
some ùë•‚àà{0, 1}ùëõsuch that ùëÑ(ùë•) ‚â†ùëìùë•where ùëìùë•denote the ùë•-the
coordinate of ùëì, using the binary representation to identify {0, 1}ùëõ
with the numbers {0, ‚Ä¶ , 2ùëõ‚àí1}.
1. Prove that NANDHARD ‚ààEXP.
2. (Challenge) Prove that there is an algorithm FINDHARD such
that if ùëõis sufficiently large, then FINDHARD(1ùëõ) runs in time
22ùëÇ(ùëõ) and outputs a string ùëì‚àà{0, 1}2ùëõthat is the truth table of
a function that is not contained in SIZE(2ùëõ/(1000ùëõ)). (In other


--- Page 448 ---

448
introduction to theoretical computer science
2 Hint: Use Item 1, the existence of functions requir-
ing exponentially hard NAND programs, and the fact
that there are only finitely many functions mapping
{0, 1}ùëõto {0, 1}.
words, if ùëìis the string output by FINDHARD(1ùëõ) then if we let
ùêπ‚à∂{0, 1}ùëõ‚Üí{0, 1} be the function such that ùêπ(ùë•) outputs the ùë•-th
coordinate of ùëì, then ùêπ‚àâSIZE(2ùëõ/(1000ùëõ)).2
‚ñ†
Exercise 13.10 Suppose that you are in charge of scheduling courses in
computer science in University X. In University X, computer science
students wake up late, and have to work on their startups in the af-
ternoon, and take long weekends with their investors. So you only
have two possible slots: you can schedule a course either Monday-
Wednesday 11am-1pm or Tuesday-Thursday 11am-1pm.
Let SCHEDULE ‚à∂{0, 1}‚àó‚Üí{0, 1} be the function that takes as input
a list of courses ùêøand a list of conflicts ùê∂(i.e., list of pairs of courses
that cannot share the same time slot) and outputs 1 if and only if there
is a ‚Äúconflict free‚Äù scheduling of the courses in ùêø, where no pair in ùê∂is
scheduled in the same time slot.
More precisely, the list ùêøis a list of strings (ùëê0, ‚Ä¶ , ùëêùëõ‚àí1) and the list
ùê∂is a list of pairs of the form (ùëêùëñ, ùëêùëó). SCHEDULE(ùêø, ùê∂) = 1 if and
only if there exists a partition of ùëê0, ‚Ä¶ , ùëêùëõ‚àí1 into two parts so that there
is no pair (ùëêùëñ, ùëêùëó) ‚ààùê∂such that both ùëêùëñand ùëêùëóare in the same part.
Prove that SCHEDULE ‚ààP. As usual, you do not have to provide
the full code to show that this is the case, and can describe operations
as a high level, as well as appeal to any data structures or other results
mentioned in the book or in lecture. Note that to show that a function
ùêπis in P you need to both (1) present an algorithm ùê¥that computes
ùêπin polynomial time, (2) prove that ùê¥does indeed run in polynomial
time, and does indeed compute the correct answer.
Try to think whether or not your algorithm extends to the case
where there are three possible time slots.
‚ñ†
13.8 BIBLIOGRAPHICAL NOTES
Because we are interested in the maximum number of steps for inputs
of a given length, running-time as we defined it is often known as
worst case complexity. The minimum number of steps (or ‚Äúbest case‚Äù
complexity) to compute a function on length ùëõinputs is typically not
a meaningful quantity since essentially every natural problem will
have some trivially easy instances. However, the average case complexity
(i.e., complexity on a ‚Äútypical‚Äù or ‚Äúrandom‚Äù input) is an interesting
concept which we‚Äôll return to when we discuss cryptography. That
said, worst-case complexity is the most standard and basic of the
complexity measures, and will be our focus in most of this book.
Some lower bounds for single-tape Turing machines are given in
[Maa85].


--- Page 449 ---

modeling running time
449
For defining efficiency in the ùúÜcalculus, one needs to be careful
about the order of application of the reduction steps, which can matter
for computational efficiency, see for example this paper.
The notation P/poly is used for historical reasons. It was introduced
by Karp and Lipton, who considered this class as corresponding to
functions that can be computed by polynomial-time Turing Machines
that are given for any input length ùëõan advice string of length polyno-
mial in ùëõ.


--- Page 450 ---



--- Page 451 ---

14
Polynomial-time reductions
Consider some of the problems we have encountered in Chapter 12:
1. The 3SAT problem: deciding whether a given 3CNF formula has a
satisfying assignment.
2. Finding the longest path in a graph.
3. Finding the maximum cut in a graph.
4. Solving quadratic equations over ùëõvariables ùë•0, ‚Ä¶ , ùë•ùëõ‚àí1 ‚àà‚Ñù.
All of these problems have the following properties:
‚Ä¢ These are important problems, and people have spent significant
effort on trying to find better algorithms for them.
‚Ä¢ Each one of these is a search problem, whereby we search for a
solution that is ‚Äúgood‚Äù in some easy to define sense (e.g., a long
path, a satisfying assignment, etc.).
‚Ä¢ Each of these problems has a trivial exponential time algorithm that
involve enumerating all possible solutions.
‚Ä¢ At the moment, for all these problems the best known algorithm is
not much faster than the trivial one in the worst case.
In this chapter and in Chapter 15 we will see that, despite their
apparent differences, we can relate the computational complexity of
these and many other problems. In fact, it turns out that the prob-
lems above are computationally equivalent, in the sense that solving one
of them immediately implies solving the others. This phenomenon,
known as NP completeness, is one of the surprising discoveries of the-
oretical computer science, and we will see that it has far-reaching
ramifications.
Compiled on 8.26.2020 18:10
Learning Objectives:
‚Ä¢ Introduce the notion of polynomial-time
reductions as a way to relate the complexity of
problems to one another.
‚Ä¢ See several examples of such reductions.
‚Ä¢ 3SAT as a basic starting point for reductions.


--- Page 452 ---

452
introduction to theoretical computer science
This chapter: A non-mathy overview
This chapter introduces the concept of a polynomial time re-
duction which is a central object in computational complexity
and this book in particular. A polynomial-time reduction is a
way to reduce the task of solving one problem to another. The
way we use reductions in complexity is to argue that if the
first problem is hard to solve efficiently, then the second must
also be hard. We see several examples for reductions in this
chapter, and reductions will be the basis for the theory of NP
completeness that we will develop in Chapter 15.
Figure 14.1: In this chapter we show that if the 3SAT
problem cannot be solved in polynomial time, then
neither can the QUADEQ, LONGESTPATH, ISET
and MAXCUT problems. We do this by using the
reduction paradigm showing for example ‚Äúif pigs could
whistle‚Äù (i.e., if we had an efficient algorithm for
QUADEQ) then ‚Äúhorses could fly‚Äù (i.e., we would
have an efficient algorithm for 3SAT.)
In this chapter we will see that for each one of the problems of find-
ing a longest path in a graph, solving quadratic equations, and finding
the maximum cut, if there exists a polynomial-time algorithm for this
problem then there exists a polynomial-time algorithm for the 3SAT
problem as well. In other words, we will reduce the task of solving
3SAT to each one of the above tasks. Another way to interpret these
results is that if there does not exist a polynomial-time algorithm for
3SAT then there does not exist a polynomial-time algorithm for these
other problems as well. In Chapter 15 we will see evidence (though
no proof!) that all of the above problems do not have polynomial-time
algorithms and hence are inherently intractable.
14.1 FORMAL DEFINITIONS OF PROBLEMS
For reasons of technical convenience rather than anything substantial,
we concern ourselves with decision problems (i.e., Yes/No questions) or
in other words Boolean (i.e., one-bit output) functions. We model the


--- Page 453 ---

polynomial-time reductions
453
problems above as functions mapping {0, 1}‚àóto {0, 1} in the following
way:
3SAT.
The 3SAT problem can be phrased as the function 3SAT ‚à∂
{0, 1}‚àó‚Üí{0, 1} that takes as input a 3CNF formula ùúë(i.e., a formula
of the form ùê∂0 ‚àß‚ãØ‚àßùê∂ùëö‚àí1 where each ùê∂ùëñis the OR of three variables
or their negation) and maps ùúëto 1 if there exists some assignment to
the variables of ùúëthat causes it to evalute to true, and to 0 otherwise.
For example
3SAT ("(ùë•0 ‚à®ùë•1 ‚à®ùë•2) ‚àß(ùë•1 ‚à®ùë•2 ‚à®ùë•3) ‚àß(ùë•0 ‚à®ùë•2 ‚à®ùë•3)") = 1
(14.1)
since the assignment ùë•= 1101 satisfies the input formula. In the
above we assume some representation of formulas as strings, and
define the function to output 0 if its input is not a valid representation;
we use the same convention for all the other functions below.
Quadratic equations.
The quadratic equations problem corresponds to the
function QUADEQ ‚à∂{0, 1}‚àó‚Üí{0, 1} that maps a set of quadratic
equations ùê∏to 1 if there is an assignment ùë•that satisfies all equations,
and to 0 otherwise.
Longest path.
The longest path problem corresponds to the function
LONGPATH ‚à∂{0, 1}‚àó‚Üí{0, 1} that maps a graph ùê∫and a number ùëò
to 1 if there is a simple path in ùê∫of length at least ùëò, and maps (ùê∫, ùëò)
to 0 otherwise. The longest path problem is a generalization of the
well-known Hamiltonian Path Problem of determining whether a path
of length ùëõexists in a given ùëõvertex graph.
Maximum cut.
The maximum cut problem corresponds to the function
MAXCUT ‚à∂{0, 1}‚àó‚Üí{0, 1} that maps a graph ùê∫and a number ùëòto
1 if there is a cut in ùê∫that cuts at least ùëòedges, and maps (ùê∫, ùëò) to 0
otherwise.
All of the problems above are in EXP but it is not known whether
or not they are in P. However, we will see in this chapter that if either
QUADEQ , LONGPATH or MAXCUT are in P, then so is 3SAT.
14.2 POLYNOMIAL-TIME REDUCTIONS
Suppose that ùêπ, ùê∫‚à∂{0, 1}‚àó‚Üí{0, 1} are two Boolean functions. A
polynomial-time reduction (or sometimes just ‚Äúreduction‚Äù for short) from
ùêπto ùê∫is a way to show that ùêπis ‚Äúno harder‚Äù than ùê∫, in the sense
that a polynomial-time algorithm for ùê∫implies a polynomial-time
algorithm for ùêπ.


--- Page 454 ---

454
introduction to theoretical computer science
Figure 14.2: If ùêπ‚â§ùëùùê∫then we can transform a
polynomial-time algorithm ùêµthat computes ùê∫
into a polynomial-time algorithm ùê¥that computes
ùêπ. To compute ùêπ(ùë•) we can run the reduction ùëÖ
guaranteed by the fact that ùêπ‚â§ùëùùê∫to obtain ùë¶=
ùêπ(ùë•) and then run our algorithm ùêµfor ùê∫to compute
ùê∫(ùë¶).
Definition 14.1 ‚Äî Polynomial-time reductions. Let ùêπ, ùê∫‚à∂{0, 1}‚àó‚Üí{0, 1}.
We say that ùêπreduces to ùê∫, denoted by ùêπ
‚â§ùëù
ùê∫if there is a
polynomial-time computable ùëÖ
‚à∂
{0, 1}‚àó
‚Üí
{0, 1}‚àósuch that
for every ùë•‚àà{0, 1}‚àó,
ùêπ(ùë•) = ùê∫(ùëÖ(ùë•)) .
(14.2)
We say that ùêπand ùê∫have equivalent complexity if ùêπ‚â§ùëùùê∫and ùê∫‚â§ùëù
ùêπ.
The following exercise justifies our intuition that ùêπ‚â§ùëùùê∫signifies
that ‚Äùùêπis no harder than ùê∫.
Solved Exercise 14.1 ‚Äî Reductions and P. Prove that if ùêπ‚â§ùëùùê∫and ùê∫‚ààP
then ùêπ‚ààP.
‚ñ†
P
As usual, solving this exercise on your own is an excel-
lent way to make sure you understand Definition 14.1.
Solution:
Suppose there was an algorithm ùêµthat compute ùê∫in time ùëù(ùëõ)
where ùëùis its input size. Then, (14.2) directly gives an algorithm
ùê¥to compute ùêπ(see Fig. 14.2). Indeed, on input ùë•
‚àà
{0, 1}‚àó, Al-
gorithm ùê¥will run the polynomial-time reduction ùëÖto obtain
ùë¶
= ùëÖ(ùë•) and then return ùêµ(ùë¶). By (14.2), ùê∫(ùëÖ(ùë•)) = ùêπ(ùë•) and
hence Algorithm ùê¥will indeed compute ùêπ.
We now show that ùê¥runs in polynomial time. By assumption, ùëÖ
can be computed in time ùëû(ùëõ) for some polynomial ùëû. In particular,
this means that |ùë¶| ‚â§ùëû(|ùë•|) (as just writing down ùë¶takes |ùë¶| steps).
Computing ùêµ(ùë¶) will take at most ùëù(|ùë¶|)
‚â§
ùëù(ùëû(|ùë•|)) steps. Thus
the total running time of ùê¥on inputs of length ùëõis at most the time
to compute ùë¶, which is bounded by ùëû(ùëõ), and the time to compute
ùêµ(ùë¶), which is bounded by ùëù(ùëû(ùëõ)), and since the composition of
two polynomials is a polynomial, ùê¥runs in polynomial time.
‚ñ†
ÔÉ´Big Idea 21 A reduction ùêπ
‚â§ùëù
ùê∫shows that ùêπis ‚Äúno harder than
ùê∫‚Äù or equivalently that ùê∫is ‚Äúno easier than ùêπ‚Äù.
14.2.1 Whistling pigs and flying horses
A reduction from ùêπto ùê∫can be used for two purposes:


--- Page 455 ---

polynomial-time reductions
455
‚Ä¢ If we already know an algorithm for ùê∫and ùêπ‚â§ùëùùê∫then we can
use the reduction to obtain an algorithm for ùêπ. This is a widely
used tool in algorithm design. For example in Section 12.1.4 we saw
how the Min-Cut Max-Flow theorem allows to reduce the task of
computing a minimum cut in a graph to the task of computing a
maximum flow in it.
‚Ä¢ If we have proven (or have evidence) that there exists no polynomial-
time algorithm for ùêπand ùêπ‚â§ùëùùê∫then the existence of this reduction
allows us to conclude that there exists no polynomial-time algo-
rithm for ùê∫. This is the ‚Äúif pigs could whistle then horses could
fly‚Äù interpretation we‚Äôve seen in Section 9.4. We show that if there
was an hypothetical efficient algorithm for ùê∫(a ‚Äúwhistling pig‚Äù)
then since ùêπ‚â§ùëùùê∫then there would be an efficient algorithm for
ùêπ(a ‚Äúflying horse‚Äù). In this book we often use reductions for this
second purpose, although the lines between the two is sometimes
blurry (see the bibliographical notes in Section 14.9).
The most crucial difference between the notion in Definition 14.1
and the reductions we saw in the context of uncomputability (e.g.,
in Section 9.4) is that for relating time complexity of problems, we
need the reduction to be computable in polynomial time, as opposed to
merely computable. Definition 14.1 also restricts reductions to have a
very specific format. That is, to show that ùêπ‚â§ùëùùê∫, rather than allow-
ing a general algorithm for ùêπthat uses a ‚Äúmagic box‚Äù that computes
ùê∫, we only allow an algorithm that computes ùêπ(ùë•) by outputting
ùê∫(ùëÖ(ùë•)). This restricted form is convenient for us, but people have
defined and used more general reductions as well (see Section 14.9).
In this chapter we use reductions to relate the computational com-
plexity of the problems mentioned above: 3SAT, Quadratic Equations,
Maximum Cut, and Longest Path, as well as a few others. We will
reduce 3SAT to the latter problems, demonstrating that solving any
one of them efficiently will result in an efficient algorithm for 3SAT.
In Chapter 15 we show the other direction: reducing each one of these
problems to 3SAT in one fell swoop.
Transitivity of reductions.
Since we think of ùêπ‚â§ùëùùê∫as saying that
(as far as polynomial-time computation is concerned) ùêπis ‚Äúeasier or
equal in difficulty to‚Äù ùê∫, we would expect that if ùêπ‚â§ùëùùê∫and ùê∫‚â§ùëùùêª,
then it would hold that ùêπ‚â§ùëùùêª. Indeed this is the case:
Solved Exercise 14.2 ‚Äî Transitivity of polynomial-time reductions. For every
ùêπ, ùê∫, ùêª‚à∂{0, 1}‚àó‚Üí{0, 1}, if ùêπ‚â§ùëùùê∫and ùê∫‚â§ùëùùêªthen ùêπ‚â§ùëùùêª.
‚ñ†


--- Page 456 ---

456
introduction to theoretical computer science
1 If you are familiar with matrix notation you may
note that such equations can be written as ùê¥ùë•= b
where ùê¥is an ùëö√ó ùëõmatrix with entries in 0/1 and
b ‚àà‚Ñïùëö.
Solution:
If ùêπ‚â§ùëùùê∫and ùê∫‚â§ùëùùêªthen there exist polynomial-time com-
putable functions ùëÖ1 and ùëÖ2 mapping {0, 1}‚àóto {0, 1}‚àósuch that
for every ùë•‚àà{0, 1}‚àó, ùêπ(ùë•) = ùê∫(ùëÖ1(ùë•)) and for every ùë¶‚àà{0, 1}‚àó,
ùê∫(ùë¶)
=
ùêª(ùëÖ2(ùë¶)). Combining these two equalities, we see that
for every ùë•‚àà{0, 1}‚àó, ùêπ(ùë•) = ùêª(ùëÖ2(ùëÖ1(ùë•))) and so to show that
ùêπ
‚â§ùëù
ùêª, it is sufficient to show that the map ùë•
‚Ü¶
ùëÖ2(ùëÖ1(ùë•)) is
computable in polynomial time. But if there are some constants ùëê, ùëë
such that ùëÖ1(ùë•) is computable in time |ùë•|ùëêand ùëÖ2(ùë¶) is computable
in time |ùë¶|ùëëthen ùëÖ2(ùëÖ1(ùë•)) is computable in time (|ùë•|ùëê)ùëë
=
|ùë•|ùëêùëë
which is polynomial.
‚ñ†
14.3 REDUCING 3SAT TO ZERO ONE AND QUADRATIC EQUATIONS
We now show our first example of a reduction. The Zero-One Lin-
ear Equations problem corresponds to the function 01EQ ‚à∂{0, 1}‚àó‚Üí
{0, 1} whose input is a collection ùê∏of linear equations in variables
ùë•0, ‚Ä¶ , ùë•ùëõ‚àí1, and the output is 1 iff there is an assignment ùë•‚àà{0, 1}ùëõ
of 0/1 values to the variables that satisfies all the equations. For exam-
ple, if the input ùê∏is a string encoding the set of equations
ùë•0 + ùë•1 + ùë•2 = 2
ùë•0 + ùë•2 = 1
ùë•1 + ùë•2 = 2
(14.3)
then 01EQ(ùê∏) = 1 since the assignment ùë•= 011 satisfies all three
equations. We specifically restrict attention to linear equations in
variables ùë•0, ‚Ä¶ , ùë•ùëõ‚àí1 in which every equation has the form ‚àëùëñ‚ààùëÜùë•ùëñ=
ùëèwhere ùëÜ‚äÜ[ùëõ] and ùëè‚àà‚Ñï.1
If we asked the question of whether there is a solution ùë•‚àà‚Ñùùëõof
real numbers to ùê∏, then this can be solved using the famous Gaussian
elimination algorithm in polynomial time. However, there is no known
efficient algorithm to solve 01EQ. Indeed, such an algorithm would
imply an algorithm for 3SAT as shown by the following theorem:
Theorem 14.2 ‚Äî Hardness of 01ùê∏ùëÑ. 3SAT ‚â§ùëù01EQ
Proof Idea:
A constraint ùë•2 ‚à®ùë•5 ‚à®ùë•7 can be written as ùë•2 + (1 ‚àíùë•5) + ùë•7 ‚â•1.
This is a linear inequality but since the sum on the left-hand side is
at most three, we can also turn it into an equality by adding two new
variables ùë¶, ùëßand writing it as ùë•2 + (1 ‚àíùë•5) + ùë•7 + ùë¶+ ùëß= 3. (We will
use fresh such variables ùë¶, ùëßfor every constraint.) Finally, for every
variable ùë•ùëñwe can add a variable ùë•‚Ä≤
ùëñcorresponding to its negation by


--- Page 457 ---

polynomial-time reductions
457
adding the equation ùë•ùëñ+ùë•‚Ä≤
ùëñ= 1, hence mapping the original constraint
ùë•2 ‚à®ùë•5 ‚à®ùë•7 to ùë•2 + ùë•‚Ä≤
5 + ùë•7 + ùë¶+ ùëß= 3. The main takeaway
technique from this reduction is the idea of adding auxiliary variables
to replace an equation such as ùë•1 + ùë•2 + ùë•3 ‚â•1 that is not quite in the
form we want with the equivalent (for 0/1 valued variables) equation
ùë•1 + ùë•2 + ùë•3 + ùë¢+ ùë£= 3 which is in the form we want.
‚ãÜ
Figure 14.3: Left: Python code implementing the
reduction of 3SAT to 01EQ. Right: Example output of
the reduction. Code is in our repository.
Proof of Theorem 14.2. To prove the theorem we need to:
1. Describe an algorithm ùëÖfor mapping an input ùúëfor 3SAT into an
input ùê∏for 01EQ.
2. Prove that the algorithm runs in polynomial time.
3. Prove that 01EQ(ùëÖ(ùúë)) = 3SAT(ùúë) for every 3CNF formula ùúë.
We now proceed to do just that. Since this is our first reduction, we
will spell out this proof in detail. However it straightforwardly follows
the proof idea.


--- Page 458 ---

458
introduction to theoretical computer science
Algorithm 14.3 ‚Äî 3ùëÜùê¥ùëáto 01ùê∏ùëÑreduction.
Input: 3CNF formula ùúëwith ùëõvariables ùë•0, ‚Ä¶ , ùë•ùëõ‚àí1 and ùëö
clauses.
Output: Set ùê∏of linear equations over 0/1 such that
3ùëÜùê¥ùëá(ùúë) = 1 -iff 01ùê∏ùëÑ(ùê∏) = 1.
1: Let ùê∏‚Äôs variables be ùë•0, ‚Ä¶ , ùë•ùëõ‚àí1, ùë•‚Ä≤
0, ‚Ä¶ , ùë•‚Ä≤
ùëõ‚àí1, ùë¶0, ‚Ä¶ , ùë¶ùëö‚àí1,
ùëß0, ‚Ä¶ , ùëßùëö‚àí1.
2: for ùëñ‚àà[ùëõ] do
3:
add to ùê∏the equation ùë•ùëñ+ ùë•‚Ä≤
ùëñ= 1
4: end for
5: for j‚àà[m] do
6:
Let ùëó-th clause be ùë§0 ‚à®ùë§1 ‚à®ùë§2 where ùë§0, ùë§1, ùë§2 are
literals.
7:
for ùëé‚àà[3] do
8:
if ùë§ùëéis variable ùë•ùëñthen
9:
set ùë°ùëé‚Üêùë•ùëñ
10:
end if
11:
if ùë§ùëéis negation ¬¨ùë•ùëñthen
12:
set ùë°ùëé‚Üêùë•‚Ä≤
ùëñ
13:
end if
14:
end for
15:
Add to ùê∏the equation ùë°0 + ùë°1 + ùë°2 + ùë¶ùëó+ ùëßùëó= 3.
16: end for
17: return ùê∏
The reduction is described in Algorithm 14.3, see also Fig. 14.3. If
the input formula has ùëõvariable and ùëösteps, Algorithm 14.3 creates a
set ùê∏of ùëõ+ùëöequations over 2ùëõ+2ùëövariables. Algorithm 14.3 makes
an initial loop of ùëõsteps (each taking constant time) and then another
loop of ùëösteps (each taking constant time) to create the equations,
and hence it runs in polynomial time.
Let ùëÖbe the function computed by Algorithm 14.3. The heart of
the proof is to show that for every 3CNF ùúë, 01EQ(ùëÖ(ùúë)) = 3SAT(ùúë).
We split the proof into two parts. The first part, traditionally known
as the completeness property, is to show that if 3SAT(ùúë) = 1 then
ùëÇ1EQ(ùëÖ(ùúë)) = 1. The second part, traditionally known as the sound-
ness property, is to show that if 01EQ(ùëÖ(ùúë)) = 1 then 3SAT(ùúë) = 1.
(The names ‚Äúcompleteness‚Äù and ‚Äúsoundness‚Äù derive viewing a so-
lution to ùëÖ(ùúë) as a ‚Äúproof‚Äù that ùúëis satisfiable, in which case these
conditions corresponds to completeness and soundness as defined
in Section 11.1.1. However, if you find the names confusing you can
simply think of completeness as the ‚Äú1-instance maps to 1-instance‚Äù


--- Page 459 ---

polynomial-time reductions
459
property and soundness as the ‚Äú0-instance maps to 0-instance‚Äù prop-
erty.)
We complete the proof by showing both parts:
‚Ä¢ Completeness: Suppose that 3SAT(ùúë) = 1, which means that
there is an assignment ùë•‚àà{0, 1}ùëõthat satisfies ùúë. If we use the
assignment ùë•0, ‚Ä¶ , ùë•ùëõ‚àí1 and 1 ‚àíùë•0, ‚Ä¶ , 1 ‚àíùë•ùëõ‚àí1 for the first 2ùëõ
variables of ùê∏= ùëÖ(ùúë) then we will satisfy all equations of the form
ùë•ùëñ+ ùë•‚Ä≤
ùëñ= 1. Moreover, for every ùëó‚àà[ùëõ], if ùë°0 + ùë°1 + ùë°2 + ùë¶ùëó+ ùëßùëó=
3(‚àó) is the equation arising from the ùëóth clause of ùúë(with ùë°0, ùë°1, ùë°2
being variables of the form ùë•ùëñor ùë•‚Ä≤
ùëñdepending on the literals of the
clause) then our assignment to the first 2ùëõvariables ensures that
ùë°0 + ùë°1 + ùë°2 ‚â•1 (since ùë•satisfied ùúë) and hence we can assign values
to ùë¶ùëóand ùëßùëóthat will ensure that the equation (‚àó) is satisfied. Hence
in this case ùê∏= ùëÖ(ùúë) is satisfied, meaning that 01EQ(ùëÖ(ùúë)) = 1.
‚Ä¢ Soundness: Suppose that 01EQ(ùëÖ(ùúë)) = 1, which means that the
set of equations ùê∏= ùëÖ(ùúë) has a satisfying assignment ùë•0, ‚Ä¶ , ùë•ùëõ‚àí1,
ùë•‚Ä≤
0, ‚Ä¶ , ùë•‚Ä≤
ùëõ‚àí1, ùë¶0, ‚Ä¶ , ùë¶ùëö‚àí1, ùëß0, ‚Ä¶ , ùëßùëö‚àí1. Then, since the equations
contain the condition ùë•ùëñ+ ùë•‚Ä≤
ùëñ= 1, for every ùëñ‚àà[ùëõ], ùë•‚Ä≤
ùëñis the negation
of ùë•ùëñ, and morover, for every ùëó‚àà[ùëö], if ùê∂has the form ùë§0 ‚à®ùë§1 ‚à®ùë§2
is the ùëó-th clause of ùê∂, then the corresponding assignment ùë•will
ensure that ùë§0 + ùë§1 + ùë§2 ‚â•1, implying that ùê∂is satisfied. Hence in
this case 3SAT(ùúë) = 1.
‚ñ†
14.3.1 Quadratic equations
Now that we reduced 3SAT to 01EQ, we can use this to reduce 3SAT
to the quadratic equations problem. This is the function QUADEQ in
which the input is a list of ùëõ-variate polynomials ùëù0, ‚Ä¶ , ùëùùëö‚àí1 ‚à∂‚Ñùùëõ‚Üí‚Ñù
that are all of degree at most two (i.e., they are quadratic) and with
integer coefficients. (The latter condition is for convenience and can
be achieved by scaling.) We define QUADEQ(ùëù0, ‚Ä¶ , ùëùùëö‚àí1) to equal 1
if and only if there is a solution ùë•‚àà‚Ñùùëõto the equations ùëù0(ùë•) = 0,
ùëù1(ùë•) = 0, ‚Ä¶, ùëùùëö‚àí1(ùë•) = 0.
For example, the following is a set of quadratic equations over the
variables ùë•0, ùë•1, ùë•2:
ùë•2
0 ‚àíùë•0 = 0
ùë•2
1 ‚àíùë•1 = 0
ùë•2
2 ‚àíùë•2 = 0
1 ‚àíùë•0 ‚àíùë•1 + ùë•0ùë•1 = 0
(14.4)
You can verify that ùë•‚àà‚Ñù3 satisfies this set of equations if and only if
ùë•‚àà{0, 1}3 and ùë•0 ‚à®ùë•1 = 1.


--- Page 460 ---

460
introduction to theoretical computer science
Theorem 14.4 ‚Äî Hardness of quadratic equations.
3SAT ‚â§ùëùQUADEQ
(14.5)
Proof Idea:
Using the transitivity of reductions (Solved Exercise 14.2), it is
enough to show that 01EQ ‚â§ùëùQUADEQ, but this follows since we
can phrase the equation ùë•ùëñ‚àà{0, 1} as the quadratic constraint ùë•2
ùëñ‚àí
ùë•ùëñ= 0. The takeaway technique of this reduction is that we can use
nonlinearity to force continuous variables (e.g., variables taking values
in ‚Ñù) to be discrete (e.g., take values in {0, 1}).
‚ãÜ
Proof of Theorem 14.4. By Theorem 14.2 and Solved Exercise 14.2,
it is sufficient to prove that 01EQ ‚â§ùëùQUADEQ. Let ùê∏be an in-
stance of 01EQ with variables ùë•0, ‚Ä¶ , ùë•ùëö‚àí1. We map ùê∏to the set of
quadratic equations ùê∏‚Ä≤ that is obtained by taking the linear equations
in ùê∏and adding to them the ùëõquadratic equations ùë•2
ùëñ‚àíùë•ùëñ= 0 for
all ùëñ‚àà[ùëõ]. (See Algorithm 14.5.) The map ùê∏‚Ü¶ùê∏‚Ä≤ can be com-
puted in polynomial time. We claim that 01EQ(ùê∏) = 1 if and only
if QUADEQ(ùê∏‚Ä≤) = 1. Indeed, the only difference between the two
instances is that:
‚Ä¢ In the 01EQ instance ùê∏, the equations are over variables ùë•0, ‚Ä¶ , ùë•ùëõ‚àí1
in {0, 1}.
‚Ä¢ In the QUADEQ instance ùê∏‚Ä≤, the equations are over variables
ùë•0, ‚Ä¶ , ùë•ùëõ‚àí1 ‚àà‚Ñùbut we have the extra constraints ùë•2
ùëñ‚àíùë•ùëñ= 0
for all ùëñ‚àà[ùëõ].
Since for every ùëé‚àà‚Ñù, ùëé2 ‚àíùëé= 0 if and only if ùëé‚àà{0, 1}, the two
sets of equations are equivalent and 01EQ(ùê∏) = QUADEQ(ùê∏‚Ä≤) which
is what we wanted to prove.
‚ñ†


--- Page 461 ---

polynomial-time reductions
461
Algorithm 14.5 ‚Äî 3ùëÜùê¥ùëáto 01ùê∏ùëÑreduction.
Input: Set ùê∏of linear equations over ùëõvariables ùë•0, ‚Ä¶ , ùë•ùëõ‚àí1.
Output: Set ùê∏‚Ä≤ of quadratic eqations ovar ùëövariables
ùë§0, ‚Ä¶ , ùë§ùëö‚àí1 such that there is an 0/1 assignment
ùë•‚àà{0, 1}ùëõ
1: satisfying the equations of ùê∏-iff there is an assignment
ùë§‚àà‚Ñùùëösatisfying the equations of ùê∏‚Ä≤.
2: That is, ùëÇ1ùê∏ùëÑ(ùê∏) = ùëÑùëàùê¥ùê∑ùê∏ùëÑ(ùê∏‚Ä≤).
3: Let ùëö‚Üêùëõ.
4: Variables of ùê∏‚Ä≤ are set to be same variable ùë•0, ‚Ä¶ , ùë•ùëõ‚àí1
as ùê∏.
5: for every equation ùëí‚ààùê∏do
6:
Add ùëíto ùê∏‚Ä≤
7: end for
8: for ùëñ‚àà[ùëõ] do
9:
Add to ùê∏‚Ä≤ the equation ùë•2
ùëñ‚àíùë•ùëñ= 0.
10: end for
11: return ùê∏‚Ä≤
14.4 THE INDEPENDENT SET PROBLEM
For a graph ùê∫= (ùëâ, ùê∏), an independent set (also known as a stable
set) is a subset ùëÜ‚äÜùëâsuch that there are no edges with both end-
points in ùëÜ(in other words, ùê∏(ùëÜ, ùëÜ) = ‚àÖ). Every ‚Äúsingleton‚Äù (set
consisting of a single vertex) is trivially an independent set, but find-
ing larger independent sets can be challenging. The maximum indepen-
dent set problem (henceforth simply ‚Äúindependent set‚Äù) is the task of
finding the largest independent set in the graph. The independent set
problem is naturally related to scheduling problems: if we put an edge
between two conflicting tasks, then an independent set corresponds
to a set of tasks that can all be scheduled together without conflicts.
The independent set problem has been studied in a variety of settings,
including for example in the case of algorithms for finding structure in
protein-protein interaction graphs.
As mentioned in Section 14.1, we think of the independent set prob-
lem as the function ISET ‚à∂{0, 1}‚àó‚Üí{0, 1} that on input a graph ùê∫
and a number ùëòoutputs 1 if and only if the graph ùê∫contains an in-
dependent set of size at least ùëò. We now reduce 3SAT to Independent
set.
Theorem 14.6 ‚Äî Hardness of Independent Set. 3SAT ‚â§ùëùISET.
Proof Idea:


--- Page 462 ---

462
introduction to theoretical computer science
Figure 14.4: An example of the reduction of 3SAT
to ISET for the case the original input formula is
ùúë= (ùë•0 ‚à®ùë•1 ‚à®ùë•2)‚àß(ùë•0 ‚à®ùë•1 ‚à®ùë•2)‚àß(ùë•1 ‚à®ùë•2 ‚à®ùë•3). We
map each clause of ùúëto a triangle of three vertices,
each tagged above with ‚Äúùë•ùëñ= 0‚Äù or ‚Äúùë•ùëñ= 1‚Äù
depending on the value of ùë•ùëñthat would satisfy the
particular literal. We put an edge between every two
literals that are conflicting (i.e., tagged with ‚Äúùë•ùëñ= 0‚Äù
and ‚Äúùë•ùëñ= 1‚Äù respectively).
The idea is that finding a satisfying assignment to a 3SAT formula
corresponds to satisfying many local constraints without creating
any conflicts. One can think of ‚Äúùë•17 = 0‚Äù and ‚Äúùë•17 = 1‚Äù as two
conflicting events, and of the constraints ùë•17 ‚à®ùë•5 ‚à®ùë•9 as creating
a conflict between the events ‚Äúùë•17 = 0‚Äù, ‚Äúùë•5 = 1‚Äù and ‚Äúùë•9 = 0‚Äù,
saying that these three cannot simultaneously co-occur. Using these
ideas, we can we can think of solving a 3SAT problem as trying to
schedule non conflicting events, though the devil is, as usual, in the
details. The takeaway technique here is to map each clause of the
original formula into a gadget which is a small subgraph (or more
generally ‚Äúsubinstance‚Äù) satisfying some convenient properties. We
will see these ‚Äúgadgets‚Äù used time and again in the construction of
polynomial-time reductions.
‚ãÜ
Algorithm 14.7 ‚Äî 3ùëÜùê¥ùëáto ùêºùëÜreduction.
Input: 3ùëÜùê¥ùëáformula ùúëwith ùëõvariables and ùëöclauses.
Output: Graph ùê∫
=
(ùëâ, ùê∏) and number ùëò, such that ùê∫
has an independent set of size ùëò-iff ùúëhas a satisfying
assignment.
1: That is, 3ùëÜùê¥ùëá(ùúë) = ùêºùëÜùê∏ùëá(ùê∫, ùëò),
2: Initialize ùëâ‚Üê‚àÖ, ùê∏‚Üê‚àÖ
3: for every clause ùê∂= ùë¶‚à®ùë¶‚Ä≤ ‚à®ùë¶‚Ä≥ of ùúëdo
4:
Add three vertices (ùê∂, ùë¶), (ùê∂, ùë¶‚Ä≤), (ùê∂, ùë¶‚Ä≥) to ùëâ
5:
Add edges {(ùê∂, ùë¶), (ùê∂, ùë¶‚Ä≤)}, {(ùê∂, ùë¶‚Ä≤), (ùê∂, ùë¶‚Ä≥)},
{(ùê∂, ùë¶‚Ä≥), (ùê∂, ùë¶)} to ùê∏.
6: end for
7: for every distinct clauses ùê∂, ùê∂‚Ä≤ in ùúëdo
8:
for every ùëñ‚àà[ùëõ] do
9:
if ùê∂contains literal ùë•ùëñand ùê∂‚Ä≤ contains literal ùë•ùëñ
then
10:
Add edge {(ùê∂, ùë•ùëñ), (ùê∂, ùë•ùëñ)} to ùê∏
11:
end if
12:
end for
13: end for
14: return ùê∫= (ùëâ, ùê∏)
Proof of Theorem 14.6. Given a 3SAT formula ùúëon ùëõvariables and
with ùëöclauses, we will create a graph ùê∫with 3ùëövertices as follows.
(See Algorithm 14.7, see also Fig. 14.4 for an example and Fig. 14.5 for
Python code.)
‚Ä¢ A clause ùê∂in ùúëhas the form ùê∂= ùë¶‚à®ùë¶‚Ä≤ ‚à®ùë¶‚Ä≥ where ùë¶, ùë¶‚Ä≤, ùë¶‚Ä≥ are
literals (variables or their negation). For each such clause ùê∂, we will


--- Page 463 ---

polynomial-time reductions
463
add three vertices to ùê∫, and label them (ùê∂, ùë¶), (ùê∂, ùë¶‚Ä≤), and (ùê∂, ùë¶‚Ä≥)
respectively. We will also add the three edges between all pairs of
these vertices, so they form a triangle. Since there are ùëöclauses in ùúë,
the graph ùê∫will have 3ùëövertices.
‚Ä¢ In addition to the above edges, we also add an edge between every
pair vertices of the form (ùê∂, ùë¶) and (ùê∂‚Ä≤, ùë¶‚Ä≤) where ùë¶and ùë¶‚Ä≤ are con-
flicting literals. That is, we add an edge between (ùê∂, ùë¶) and (ùê∂‚Ä≤, ùë¶‚Ä≤)
if there is an ùëñsuch that ùë¶= ùë•ùëñand ùë¶‚Ä≤ = ùë•ùëñor vice versa.
The algorithm constructing of ùê∫based on ùúëtakes polynomial time
since it involves two loops, the first taking ùëÇ(ùëö) steps and the second
taking ùëÇ(ùëö2ùëõ) steps (see Algorithm 14.7). Hence to prove the theo-
rem we need to show that ùúëis satisfiable if and only if ùê∫contains an
independent set of ùëövertices. We now show both directions of this
equivalence:
Part 1: Completeness. The ‚Äúcompleteness‚Äù direction is to show that
if ùúëhas a satisfying assignment ùë•‚àó, then ùê∫has an independent set ùëÜ‚àó
of ùëövertices. Let us now show this.
Indeed, suppose that ùúëhas a satisfying assignment ùë•‚àó‚àà{0, 1}ùëõ.
Then for every clause ùê∂= ùë¶‚à®ùë¶‚Ä≤ ‚à®ùë¶‚Ä≥ of ùúë, one of the literals ùë¶, ùë¶‚Ä≤, ùë¶‚Ä≥
must evaluate to true under the assignment ùë•‚àó(as otherwise it would
not satisfy ùúë). We let ùëÜbe a set of ùëövertices that is obtained by choos-
ing for every clause ùê∂one vertex of the form (ùê∂, ùë¶) such that ùë¶eval-
uates to true under ùë•‚àó. (If there is more than one such vertex for the
same ùê∂, we arbitrarily choose one of them.)
We claim that ùëÜis an independent set. Indeed, suppose otherwise
that there was a pair of vertices (ùê∂, ùë¶) and (ùê∂‚Ä≤, ùë¶‚Ä≤) in ùëÜthat have an
edge between them. Since we picked one vertex out of each triangle
corresponding to a clause, it must be that ùê∂‚â†ùê∂‚Ä≤. Hence the only
way that there is an edge between (ùê∂, ùë¶) and (ùê∂, ùë¶‚Ä≤) is if ùë¶and ùë¶‚Ä≤ are
conflicting literals (i.e. ùë¶= ùë•ùëñand ùë¶‚Ä≤ = ùë•ùëñfor some ùëñ). But then they
can‚Äôt both evaluate to true under the assignment ùë•‚àó, which contradicts
the way we constructed the set ùëÜ. This completes the proof of the
completeness condition.
Part 2: Soundness. The ‚Äúsoundness‚Äù direction is to show that if
ùê∫has an independent set ùëÜ‚àóof ùëövertices, then ùúëhas a satisfying
assignment ùë•‚àó‚àà{0, 1}ùëõ. Let us now show this.
Indeed, suppose that ùê∫has an independent set ùëÜ‚àówith ùëövertices.
We will define an assignment ùë•‚àó‚àà{0, 1}ùëõfor the variables of ùúëas
follows. For every ùëñ‚àà[ùëõ], we set ùë•‚àó
ùëñaccording to the following rules:
‚Ä¢ If ùëÜ‚àócontains a vertex of the form (ùê∂, ùë•ùëñ) then we set ùë•‚àó
ùëñ= 1.
‚Ä¢ If ùëÜ‚àócontains a vertex of the form (ùê∂, ùë•ùëñ) then we set ùë•‚àó
ùëñ= 0.


--- Page 464 ---

464
introduction to theoretical computer science
‚Ä¢ If ùëÜ‚àódoes not contain a vertex of either of these forms, then it does
not matter which value we give to ùë•‚àó
ùëñ, but for concreteness we‚Äôll set
ùë•‚àó
ùëñ= 0.
The first observation is that ùë•‚àóis indeed well defined, in the sense
that the rules above do not conflict with one another, and ask to set ùë•‚àó
ùëñ
to be both 0 and 1. This follows from the fact that ùëÜ‚àóis an independent
set and hence if it contains a vertex of the form (ùê∂, ùë•ùëñ) then it cannot
contain a vertex of the form (ùê∂‚Ä≤, ùë•ùëñ).
We now claim that ùë•‚àóis a satisfying assignment for ùúë. Indeed, since
ùëÜ‚àóis an independent set, it cannot have more than one vertex inside
each one of the ùëötriangles (ùê∂, ùë¶), (ùê∂, ùë¶‚Ä≤), (ùê∂, ùë¶‚Ä≥) corresponding to a
clause of ùúë. Hence since |ùëÜ‚àó| = ùëö, it must have exactly one vertex in
each such triangle. For every clause ùê∂of ùúë, if (ùê∂, ùë¶) is the vertex in
ùëÜ‚àóin the triangle corresponding to ùê∂, then by the way we defined ùë•‚àó,
the literal ùë¶must evaluate to true, which means that ùë•‚àósatisfies this
clause. Therefore ùë•‚àósatisfies all clauses of ùúë, which is the definition of
a satisfying assignment.
This completes the proof of Theorem 14.6
‚ñ†
Figure 14.5: The reduction of 3SAT to Independent Set.
On the righthand side is Python code that implements
this reduction. On the lefthand side is a sample
output of the reduction. We use black for the ‚Äútriangle
edges‚Äù and red for the ‚Äúconflict edges‚Äù. Note that the
satisfying assignment ùë•‚àó= 0110 corresponds to the
independent set (0, ¬¨ùë•3), (1, ¬¨ùë•0), (2, ùë•2).
14.5 SOME EXERCISES AND ANATOMY OF A REDUCTION.
Reductions can be confusing and working out exercises is a great way
to gain more comfort with them. Here is one such example. As usual,
I recommend you try it out yourself before looking at the solution.
Solved Exercise 14.3 ‚Äî Vertex cover. A vertex cover in a graph ùê∫= (ùëâ, ùê∏)
is a subset ùëÜ‚äÜùëâof vertices such that every edge touches at least one
vertex of ùëÜ(see ??). The vertex cover problem is the task to determine,
given a graph ùê∫and a number ùëò, whether there exists a vertex cover
in the graph with at most ùëòvertices. Formally, this is the function
VC ‚à∂{0, 1}‚àó‚Üí{0, 1} such that for every ùê∫= (ùëâ, ùê∏) and ùëò‚àà‚Ñï,


--- Page 465 ---

polynomial-time reductions
465
Figure 14.6: A vertex cover in a graph is a subset of
vertices that touches all edges. In this 7-vertex graph,
the 3 filled vertices are a vertex cover.
VC(ùê∫, ùëò) = 1 if and only if there exists a vertex cover ùëÜ‚äÜùëâsuch that
|ùëÜ| ‚â§ùëò.
Prove that 3SAT ‚â§ùëùVC.
‚ñ†
Solution:
The key observation is that if ùëÜ
‚äÜ
ùëâis a vertex cover that
touches all vertices, then there is no edge ùëísuch that both ùë†‚Äôs end-
points are in the set ùëÜ
=
ùëâ‚ßµùëÜ, and vice versa. In other words,
ùëÜis a vertex cover if and only if ùëÜis an independent set. Since
the size of ùëÜis |ùëâ|
‚àí
|ùëÜ|, we see that the polynomial-time map
ùëÖ(ùê∫, ùëò)
=
(ùê∫, ùëõ‚àíùëò) (where ùëõis the number of vertices of ùê∫)
satisfies that VC(ùëÖ(ùê∫, ùëò))
=
ISET(ùê∫, ùëò) which means that it is a
reduction from independent set to vertex cover.
‚ñ†
Solved Exercise 14.4 ‚Äî Clique is equivalent to independent set. The maximum
clique problem corresponds to the function CLIQUE ‚à∂{0, 1}‚àó‚Üí{0, 1}
such that for a graph ùê∫and a number ùëò, CLIQUE(ùê∫, ùëò) = 1 iff there
is a ùëÜsubset of ùëòvertices such that for every distinct ùë¢, ùë£‚ààùëÜ, the edge
ùë¢, ùë£is in ùê∫. Such a set is known as a clique.
Prove that CLIQUE ‚â§ùëùISET and ISET ‚â§ùëùCLIQUE.
‚ñ†
Solution:
If ùê∫
=
(ùëâ, ùê∏) is a graph, we denote by ùê∫its complement which
is the graph on the same vertices ùëâand such that for every distinct
ùë¢, ùë£
‚àà
ùëâ, the edge {ùë¢, ùë£} is present in ùê∫if and only if this edge is
not present in ùê∫.
This means that for every set ùëÜ, ùëÜis an independent set in ùê∫if
and only if ùëÜis a clique in ùê∫. Therefore for every ùëò, ISET(ùê∫, ùëò)
=
CLIQUE(ùê∫, ùëò). Since the map ùê∫‚Ü¶ùê∫can be computed efficiently,
this yields a reduction ISET ‚â§ùëùCLIQUE. Moreover, since ùê∫= ùê∫
this yields a reduction in the other direction as well.
‚ñ†
14.5.1 Dominating set
In the two examples above, the reduction was almost ‚Äútrivial‚Äù: the
reduction from independent set to vertex cover merely changes the
number ùëòto ùëõ‚àíùëò, and the reduction from independent set to clique
flips edges to non-edges and vice versa. The following exercise re-
quires a somewhat more interesting reduction.
Solved Exercise 14.5 ‚Äî Dominating set. A dominating set in a graph ùê∫=
(ùëâ, ùê∏) is a subset ùëÜ‚äÜùëâof vertices such that for every ùë¢‚ààùëâ‚ßµùëÜis a
neighbor in ùê∫of some ùë†‚ààùëÜ(see Fig. 14.7). The dominating set problem


--- Page 466 ---

466
introduction to theoretical computer science
Figure 14.7: A dominating set is a subset ùëÜof vertices
such that every vertex in the graph is either in ùëÜor a
neighbor of ùëÜ. The figure above are two copies of the
same graph. The red vertices on the left are a vertex
cover that is not a dominating set. The blue vertices
on the right are a dominating set that is not a vertex
cover.
is the task, given a graph ùê∫= (ùëâ, ùê∏) and number ùëò, of determining
whether there exists a dominating set ùëÜ‚äÜùëâwith |ùëÜ| ‚â§ùëò. Formally,
this is the function DS ‚à∂{0, 1}‚àó‚Üí{0, 1} such that DS(ùê∫, ùëò) = 1 iff
there is a dominating set in ùê∫of at most ùëòvertices.
Prove that ISET ‚â§ùëùDS.
‚ñ†
Solution:
Since we know that ISET ‚â§ùëùVC, using transitivity, it is enough
to show that VC
‚â§ùëù
DS. As Fig. 14.7 shows, a dominating set is
not the same thing as a vertex cover. However, we can still relate
the two problems. The idea is to map a graph ùê∫into a graph ùêª
such that a vertex cover in ùê∫would translate into a dominating set
in ùêªand vice versa. We do so by including in ùêªall the vertices
and edges of ùê∫, but for every edge {ùë¢, ùë£} of ùê∫we also add to ùêªa
new vertex ùë§ùë¢,ùë£and connect it to both ùë¢and ùë£. Let ‚Ñìbe the number
of isolated vertices in ùê∫. The idea behind the proof is that we can
transform a vertex cover ùëÜof ùëòvertices in ùê∫into a dominating set
of ùëò
+
‚Ñìvertices in ùêªby adding to ùëÜall the isolated vertices, and
moreover we can transform every ùëò+ ‚Ñìsized dominating set in ùêª
into a vertex cover in ùê∫. We now give the details.
Description of the algorithm. Given an instance (ùê∫, ùëò) for the
vertex cover problem, we will map ùê∫into an instance (ùêª, ùëò‚Ä≤) for
the dominating set problem as follows (see Fig. 14.8 for Python
implementation):
Algorithm 14.8 ‚Äî ùëâùê∂to ùê∑ùëÜreduction.
Input: Graph ùê∫= (ùëâ, ùê∏) and number ùëò.
Output: Graph ùêª= (ùëâ‚Ä≤, ùê∏‚Ä≤) and number ùëò‚Ä≤, such that
ùê∫has a vertex cover of size ùëò-iff ùêªhas a dominating
set of size ùëò‚Ä≤
1: That is, ùê∑ùëÜ(ùêª, ùëò‚Ä≤) = ùêºùëÜùê∏ùëá(ùê∫, ùëò),
2: Initialize ùëâ‚Ä≤ ‚Üêùëâ, ùê∏‚Ä≤ ‚Üêùê∏
3: for every edge {ùë¢, ùë£} ‚ààùê∏do
4:
Add vertex ùë§ùë¢,ùë£to ùëâ‚Ä≤
5:
Add edges {ùë¢, ùë§ùë¢,ùë£}, {ùë£, ùë§ùë¢,ùë£} to ùê∏‚Ä≤.
6: end for
7: Let ‚Ñì‚Üênumber of isolated vertices in ùê∫
8: return (ùêª= (ùëâ‚Ä≤, ùê∏‚Ä≤) , ùëò+ ‚Ñì)
Algorithm 14.8 runs in polynomial time, since the loop takes
ùëÇ(ùëö) steps where ùëöis the number of edges, with each step can be
implemented in constant or at most linear time (depending on the


--- Page 467 ---

polynomial-time reductions
467
representation of the graph ùêª). Counting the number of isolated
vertices in an ùëõvertex graph ùê∫can be done in time ùëÇ(ùëõ2) if ùê∫is
represented in the adjacency matrix representation and ùëÇ(ùëõ) time
if it is represented in the adjacency list representation. Regardless
the algorithm runs in polynomial time.
To complete the proof we need to prove that for every ùê∫, ùëò, if
ùêª, ùëò‚Ä≤ is the output of Algorithm 14.8 on input (ùê∫, ùëò), then
DS(ùêª, ùëò‚Ä≤)
=
VC(ùê∫, ùëò). We split the proof into two parts. The
completeness part is that if VC(ùê∫, ùëò) = 1 then DS(ùêª, ùëò‚Ä≤) = 1. The
soundness part is that if DS(ùêª, ùëò‚Ä≤) = 1 then VC(ùê∫, ùëò) = 1.
Completeness. Suppose that VC(ùê∫, ùëò) = 1. Then there is a ver-
tex cover ùëÜ
‚äÜ
ùëâof at most ùëòvertices. Let ùêºbe the set of isolated
vertices in ùê∫and ‚Ñìbe their number. Then |ùëÜ‚à™ùêº| ‚â§ùëò+ ‚Ñì. We claim
that ùëÜ‚à™ùêºis a dominating set in ùêª‚Ä≤. Indeed for every vertex ùë£of ùêª‚Ä≤
there are three cases:
‚Ä¢ Case 1: ùë£is an isolated vertex of ùê∫. In this case ùë£is in ùëÜ‚à™ùêº.
‚Ä¢ Case 2: ùë£is a non-isolated vertex of ùê∫and hence there is an edge
{ùë¢, ùë£} of ùê∫for some ùë¢. In this case since ùëÜis a vertex cover, one
of ùë¢, ùë£has to be in ùëÜ, and hence either ùë£or a neighbor of ùë£has to
be in ùëÜ‚äÜùëÜ‚à™ùêº.
‚Ä¢ Case 3: ùë£is of the form ùë§ùë¢,ùë¢‚Ä≤ for some two neighbors ùë¢, ùë¢‚Ä≤ in ùê∫.
But then since ùëÜis a vertex cover, one of ùë¢, ùë¢‚Ä≤ has to be in ùëÜand
hence ùëÜcontains a neighbor of ùë£.
We conclude that ùëÜ
‚à™
ùêºis a dominating set of size at most
ùëò‚Ä≤ = ùëò+ ‚Ñìin ùêª‚Ä≤ and hence under the assumption that VC(ùê∫.ùëò) = 1,
DS(ùêª‚Ä≤, ùëò‚Ä≤) = 1.
Soundness. Suppose that DS(ùêª, ùëò‚Ä≤) = 1. Then there is a domi-
nating set ùê∑of size at most ùëò‚Ä≤ = ùëò+ ‚Ñìin ùêª. For every edge {ùë¢, ùë£} in
the graph ùê∫, if ùê∑contains the vertex ùë§ùë¢,ùë£then we remove this ver-
tex and add ùë¢in its place. The only two neighbors of ùë§ùë¢,ùë£are ùë¢and
ùë£, and since ùë¢is a neighbor of both ùë§ùë¢,ùë£and of ùë£, replacing ùë§ùë¢,ùë£
with ùë£maintains the property that it is a dominating set. Moreover,
this change cannot increase the size of ùê∑. Thus following this mod-
ification, we can assume that ùê∑is a dominating set of at most ùëò+ ‚Ñì
vertices that does not contain any vertices of the form ùë§ùë¢,ùë£.
Let ùêºbe the set of isolated vertices in ùê∫. These vertices are also
isolated in ùêªand hence must be included in ùê∑(an isolated ver-
tex must be in any dominating set, since it has no neighbors). We
let ùëÜ
=
ùê∑‚ßµùêº. Then |ùëÜ|
‚â§
ùêº. We claim that ùëÜis a vertex cover
in ùê∫. Indeed, for every edge {ùë¢, ùë£} of ùê∫, either the vertex ùë§ùë¢,ùë£or


--- Page 468 ---

468
introduction to theoretical computer science
one of its neighbors must be in ùëÜby the dominating set property.
But since we ensured ùëÜdoesn‚Äôt contain any of the vertices of the
form ùë§ùë¢,ùë£, it must be the case that either ùë¢or ùë£is in ùëÜ. This shows
that ùëÜis a vertex cover of ùê∫of size at most ùëò, hence proving that
VC(ùê∫, ùëò) = 1.
‚ñ†
A corollary of Algorithm 14.8 and the other reduction we have seen
so far is that if DS ‚ààP (i.e., dominating set has a polynomial-time al-
gorithm) then 3SAT ‚ààP (i.e., 3SAT has a polynomial-time algorithm).
By the contra-positive, if 3SAT does not have a polynomial-time algo-
rithm then neither does dominating set.
Figure 14.8: Python implementation of the reduction
from vertex cover to dominating set, together with an
example of an input graph and the resulting output
graph. This reduction allows to transform a hypothet-
ical polynomial-time algorithm for dominating set (a
‚Äúwhistling pig‚Äù) into a hypothetical polynomial-time
algorithm for vertex-cover (a ‚Äúflying horse‚Äù).
14.5.2 Anatomy of a reduction
Figure 14.9: The four components of a reduction,
illustrated for the particular reduction of vertex cover
to dominating set. A reduction from problem ùêπto
problem ùê∫is an algorithm that maps an input ùë•for ùêπ
into an input ùëÖ(ùë•) for ùê∫. To show that the reduction
is correct we need to show the properties of efficiency:
algorithm ùëÖruns in polynomial time, completeness:
if ùêπ(ùë•) = 1 then ùê∫(ùëÖ(ùë•)) = 1, and soundness: if
ùêπ(ùëÖ(ùë•)) = 1 then ùê∫(ùë•) = 1.
The reduction of Solved Exercise 14.5 gives a good illustration of
the anatomy of a reduction. A reduction consists of four parts:
‚Ä¢ Algorithm description: This is the description of how the algorithm
maps an input into the output. For example, in Solved Exercise 14.5
this is the description of how we map an instance (ùê∫, ùëò) of the
vertex cover problem into an instance (ùêª, ùëò‚Ä≤) of the dominating set
problem.


--- Page 469 ---

polynomial-time reductions
469
‚Ä¢ Algorithm analysis: It is not enough to describe how the algorithm
works but we need to also explain why it works. In particular we
need to provide an analysis explaining why the reduction is both
efficient (i.e., runs in polynomial time) and correct (satisfies that
ùê∫(ùëÖ(ùë•)) = ùêπ(ùë•) for every ùë•). Specifically, the components of
analysis of a reduction ùëÖinclude:
‚Äì Efficiency: We need to show that ùëÖruns in polynomial time. In
most reductions we encounter this part is straightforward, as the
reductions we typically use involve a constant number of nested
loops, each involving a constant number of operations. For ex-
ample, the reduction of Solved Exercise 14.5 just enumerates over
the edges and vertices of the input graph.
‚Äì Completeness: In a reduction ùëÖdemonstrating ùêπ‚â§ùëùùê∫, the
completeness condition is the condition that for every ùë•‚àà{0, 1}‚àó,
if ùêπ(ùë•) = 1 then ùê∫(ùëÖ(ùë•)) = 1. Typically we construct the
reduction to ensure that this holds, by giving a way to map a
‚Äúcertificate/solution‚Äù certifying that ùêπ(ùë•) = 1 into a solution
certifying that ùê∫(ùëÖ(ùë•)) = 1. For example, in Solved Exercise 14.5
we constructed the graph ùêªsuch that for every vertex cover ùëÜ
in ùê∫, the set ùëÜ‚à™ùêº(where ùêºis the isolated vertices) would be a
dominating set in ùêª.
‚Äì Soundness: This is the condition that if ùêπ(ùë•) = 0 then
ùê∫(ùëÖ(ùë•)) = 0 or (taking the contrapositive) if ùê∫(ùëÖ(ùë•)) = 1 then
ùêπ(ùë•) = 1. This is sometimes straightforward but can often be
harder to show than the completeness condition, and in more
advanced reductions (such as the reduction SAT ‚â§ùëùISET
of Theorem 14.6) demonstrating soundness is the main part
of the analysis. For example, in Solved Exercise 14.5 to show
soundness we needed to show that for every dominating set ùê∑in
the graph ùêª, there exists a vertex cover ùëÜof size at most |ùê∑| ‚àí‚Ñì
in the graph ùê∫(where ‚Ñìis the number of isolated vertices).
This was challenging since the dominating set ùê∑might not be
necessarily the one we ‚Äúhad in mind‚Äù. In particular, in the proof
above we needed to modify ùê∑to ensure that it does not contain
vertices of the form ùë§ùë¢,ùë£, and it was important to show that this
modification still maintains the property that ùê∑is a dominating
set, and also does not make it bigger.
Whenever you need to provide a reduction, you should make sure
that your description has all these components. While it is sometimes
tempting to weave together the description of the reduction and its
analysis, it is usually clearer if you separate the two, and also break
down the analysis to its three components of efficiency, completeness,
and soundness.


--- Page 470 ---

470
introduction to theoretical computer science
14.6 REDUCING INDEPENDENT SET TO MAXIMUM CUT
We now show that the independent set problem reduces to the maxi-
mum cut (or ‚Äúmax cut‚Äù) problem, modeled as the function MAXCUT
that on input a pair (ùê∫, ùëò) outputs 1 iff ùê∫contains a cut of at least ùëò
edges. Since both are graph problems, a reduction from independent
set to max cut maps one graph into the other, but as we will see the
output graph does not have to have the same vertices or edges as the
input graph.
Theorem 14.9 ‚Äî Hardness of Max Cut. ISET ‚â§ùëùMAXCUT
Proof Idea:
We will map a graph ùê∫into a graph ùêªsuch that a large indepen-
dent set in ùê∫becomes a partition cutting many edges in ùêª. We can
think of a cut in ùêªas coloring each vertex either ‚Äúblue‚Äù or ‚Äúred‚Äù. We
will add a special ‚Äúsource‚Äù vertex ùë†‚àó, connect it to all other vertices,
and assume without loss of generality that it is colored blue. Hence
the more vertices we color red, the more edges from ùë†‚àówe cut. Now,
for every edge ùë¢, ùë£in the original graph ùê∫we will add a special ‚Äúgad-
get‚Äù which will be a small subgraph that involves ùë¢,ùë£, the source ùë†‚àó,
and two other additional vertices. We design the gadget in a way so
that if the red vertices are not an independent set in ùê∫then the cor-
responding cut in ùêªwill be ‚Äúpenalized‚Äù in the sense that it would
not cut as many edges. Once we set for ourselves this objective, it is
not hard to find a gadget that achieves it‚àísee the proof below. Once
again the takeaway technique is to use (this time a slightly more
clever) gadget.
‚ãÜ
Figure 14.10: In the reduction of ISET to MAXCUT
we map an ùëõ-vertex ùëö-edge graph ùê∫into the
ùëõ+ 2ùëö+ 1 vertex and ùëõ+ 5ùëöedge graph ùêªas
follows. The graph ùêªcontains a special ‚Äúsource‚Äù
vertex ùë†‚àó,ùëõvertices ùë£0, ‚Ä¶ , ùë£ùëõ‚àí1, and 2ùëöver-
tices ùëí0
0, ùëí1
0, ‚Ä¶ , ùëí0
ùëö‚àí1, ùëí1
ùëö‚àí1 with each pair cor-
responding to an edge of ùê∫. We put an edge be-
tween ùë†‚àóand ùë£ùëñfor every ùëñ‚àà[ùëõ], and if the ùë°-th
edge of ùê∫was (ùë£ùëñ, ùë£ùëó) then we add the five edges
(ùë†‚àó, ùëí0
ùë°), (ùë†‚àó, ùëí1
ùë°), (ùë£ùëñ, ùëí0
ùë°), (ùë£ùëó, ùëí1
ùë°), (ùëí0
ùë°, ùëí1
ùë°). The intent
is that if cut at most one of ùë£ùëñ, ùë£ùëófrom ùë†‚àóthen we‚Äôll
be able to cut 4 out of these five edges, while if we
cut both ùë£ùëñand ùë£ùëófrom ùë†‚àóthen we‚Äôll be able to cut at
most three of them.
Proof of Theorem 14.9. We will transform a graph ùê∫of ùëõvertices and ùëö
edges into a graph ùêªof ùëõ+ 1 + 2ùëövertices and ùëõ+ 5ùëöedges in the
following way (see also Fig. 14.10). The graph ùêªcontains all vertices


--- Page 471 ---

polynomial-time reductions
471
Figure 14.11: In the reduction of independent set
to max cut, for every ùë°‚àà[ùëö], we have a ‚Äúgadget‚Äù
corresponding to the ùë°-th edge ùëí= {ùë£ùëñ, ùë£ùëó} in the
original graph. If we think of the side of the cut
containing the special source vertex ùë†‚àóas ‚Äúwhite‚Äù and
the other side as ‚Äúblue‚Äù, then the leftmost and center
figures show that if ùë£ùëñand ùë£ùëóare not both blue then
we can cut four edges from the gadget. In contrast,
by enumerating all possibilities one can verify that
if both ùë¢and ùë£are blue, then no matter how we
color the intermediate vertices ùëí0
ùë°, ùëí1
ùë°, we will cut at
most three edges from the gadget. The figure above
contains only the gadget edges and ignores the edges
connecting ùë†‚àóto the vertices ùë£0, ‚Ä¶ , ùë£ùëõ‚àí1.
of ùê∫(though not the edges between them!) and in addition ùêªalso
has:
* A special vertex ùë†‚àóthat is connected to all the vertices of ùê∫
* For every edge ùëí= {ùë¢, ùë£} ‚ààùê∏(ùê∫), two vertices ùëí0, ùëí1 such that ùëí0
is connected to ùë¢and ùëí1 is connected to ùë£, and moreover we add the
edges {ùëí0, ùëí1}, {ùëí0, ùë†‚àó}, {ùëí1, ùë†‚àó} to ùêª.
Theorem 14.9 will follow by showing that ùê∫contains an indepen-
dent set of size at least ùëòif and only if ùêªhas a cut cutting at least
ùëò+ 4ùëöedges. We now prove both directions of this equivalence:
Part 1: Completeness. If ùêºis an independent ùëò-sized set in ùê∫, then
we can define ùëÜto be a cut in ùêªof the following form: we let ùëÜcon-
tain all the vertices of ùêºand for every edge ùëí= {ùë¢, ùë£} ‚ààùê∏(ùê∫), if ùë¢‚ààùêº
and ùë£‚àâùêºthen we add ùëí1 to ùëÜ; if ùë¢‚àâùêºand ùë£‚ààùêºthen we add ùëí0 to
ùëÜ; and if ùë¢‚àâùêºand ùë£‚àâùêºthen we add both ùëí0 and ùëí1 to ùëÜ. (We don‚Äôt
need to worry about the case that both ùë¢and ùë£are in ùêºsince it is an
independent set.) We can verify that in all cases the number of edges
from ùëÜto its complement in the gadget corresponding to ùëíwill be four
(see Fig. 14.11). Since ùë†‚àóis not in ùëÜ, we also have ùëòedges from ùë†‚àóto ùêº,
for a total of ùëò+ 4ùëöedges.
Part 2: Soundness. Suppose that ùëÜis a cut in ùêªthat cuts at least
ùê∂= ùëò+ 4ùëöedges. We can assume that ùë†‚àóis not in ùëÜ(otherwise we
can ‚Äúflip‚Äù ùëÜto its complement ùëÜ, since this does not change the size
of the cut). Now let ùêºbe the set of vertices in ùëÜthat correspond to the
original vertices of ùê∫. If ùêºwas an independent set of size ùëòthen we
would be done. This might not always be the case but we will see that
if ùêºis not an independent set then it‚Äôs also larger than ùëò. Specifically,
we define ùëöùëñùëõ= |ùê∏(ùêº, ùêº)| be the set of edges in ùê∫that are contained
in ùêºand let ùëöùëúùë¢ùë°= ùëö‚àíùëöùëñùëõ(i.e., if ùêºis an independent set then
ùëöùëñùëõ= 0 and ùëöùëúùë¢ùë°= ùëö). By the properties of our gadget we know
that for every edge {ùë¢, ùë£} of ùê∫, we can cut at most three edges when
both ùë¢and ùë£are in ùëÜ, and at most four edges otherwise. Hence the
number ùê∂of edges cut by ùëÜsatisfies ùê∂‚â§|ùêº| + 3ùëöùëñùëõ+ 4ùëöùëúùë¢ùë°=
|ùêº| + 3ùëöùëñùëõ+ 4(ùëö‚àíùëöùëñùëõ) = |ùêº| + 4ùëö‚àíùëöùëñùëõ. Since ùê∂= ùëò+ 4ùëöwe
get that |ùêº| ‚àíùëöùëñùëõ‚â•ùëò. Now we can transform ùêºinto an independent
set ùêº‚Ä≤ by going over every one of the ùëöùëñùëõedges that are inside ùêºand
removing one of the endpoints of the edge from it. The resulting set ùêº‚Ä≤
is an independent set in the graph ùê∫of size |ùêº| ‚àíùëöùëñùëõ‚â•ùëòand so this
concludes the proof of the soundness condition.
‚ñ†


--- Page 472 ---

472
introduction to theoretical computer science
Figure 14.12: The reduction of independent set to
max cut. On the righthand side is Python code
implementing the reduction. On the lefthand side is
an example output of the reduction where we apply
it to the independent set instance that is obtained by
running the reduction of Theorem 14.6 on the 3CNF
formula (ùë•0 ‚à®ùë•3 ‚à®ùë•2)‚àß(ùë•0 ‚à®ùë•1 ‚à®ùë•2)‚àß(ùë•1 ‚à®ùë•2 ‚à®ùë•3).
Figure 14.13: We can transform a 3SAT formula ùúëinto
a graph ùê∫such that the longest path in the graph ùê∫
would correspond to a satisfying assignment in ùúë. In
this graph, the black colored part corresponds to the
variables of ùúëand the blue colored part corresponds
to the vertices. A sufficiently long path would have to
first ‚Äúsnake‚Äù through the black part, for each variable
choosing either the ‚Äúupper path‚Äù (corresponding
to assigning it the value True) or the ‚Äúlower path‚Äù
(corresponding to assigning it the value False). Then
to achieve maximum length the path would traverse
through the blue part, where to go between two
vertices corresponding to a clause such as ùë•17 ‚à®ùë•32 ‚à®
ùë•57, the corresponding vertices would have to have
been not traversed before.
Figure 14.14: The graph above with the longest path
marked on it, the part of the path corresponding to
variables is in green and part corresponding to the
clauses is in pink.
14.7 REDUCING 3SAT TO LONGEST PATH
Note:
This section is still a little messy; feel free to skip it or just read
it without going into the proof details. The proof appears in Section
7.5 in Sipser‚Äôs book.
One of the most basic algorithms in Computer Science is Dijkstra‚Äôs
algorithm to find the shortest path between two vertices. We now show
that in contrast, an efficient algorithm for the longest path problem
would imply a polynomial-time algorithm for 3SAT.
Theorem 14.10 ‚Äî Hardness of longest path.
3SAT ‚â§ùëùLONGPATH
(14.6)
Proof Idea:
To prove Theorem 14.10 need to show how to transform a 3CNF
formula ùúëinto a graph ùê∫and two vertices ùë†, ùë°such that ùê∫has a path
of length at least ùëòif and only if ùúëis satisfiable. The idea of the reduc-
tion is sketched in Fig. 14.13 and Fig. 14.14. We will construct a graph
that contains a potentially long ‚Äúsnaking path‚Äù that corresponds to
all variables in the formula. We will add a ‚Äúgadget‚Äù corresponding
to each clause of ùúëin a way that we would only be able to use the
gadgets if we have a satisfying assignment.
‚ãÜ
def TSAT2LONGPATH(œÜ):
"""Reduce 3SAT to LONGPATH"""
def var(v): # return variable and True/False depending
if positive or negated
‚Ü™
return int(v[2:]),False if v[0]=="¬¨" else
int(v[1:]),True
‚Ü™
n = numvars(œÜ)
clauses = getclauses(œÜ)
m = len(clauses)
G =Graph()


--- Page 473 ---

polynomial-time reductions
473
G.edge("start","start_0")
for i in range(n): # add 2 length-m paths per variable
G.edge(f"start_{i}",f"v_{i}_{0}_T")
G.edge(f"start_{i}",f"v_{i}_{0}_F")
for j in range(m-1):
G.edge(f"v_{i}_{j}_T",f"v_{i}_{j+1}_T")
G.edge(f"v_{i}_{j}_F",f"v_{i}_{j+1}_F")
G.edge(f"v_{i}_{m-1}_T",f"end_{i}")
G.edge(f"v_{i}_{m-1}_F",f"end_{i}")
if i<n-1:
G.edge(f"end_{i}",f"start_{i+1}")
G.edge(f"end_{n-1}","start_clauses")
for j,C in enumerate(clauses): # add gadget for each
clause
‚Ü™
for v in enumerate(C):
i,sign = var(v[1])
s = "F" if sign else "T"
G.edge(f"C_{j}_in",f"v_{i}_{j}_{s}")
G.edge(f"v_{i}_{j}_{s}",f"C_{j}_out")
if j<m-1:
G.edge(f"C_{j}_out",f"C_{j+1}_in")
G.edge("start_clauses","C_0_in")
G.edge(f"C_{m-1}_out","end")
return G, 1+n*(m+1)+1+2*m+1
Proof of Theorem 14.10. We build a graph ùê∫that ‚Äúsnakes‚Äù from ùë†to ùë°as
follows. After ùë†we add a sequence of ùëõlong loops. Each loop has an
‚Äúupper path‚Äù and a ‚Äúlower path‚Äù. A simple path cannot take both the
upper path and the lower path, and so it will need to take exactly one
of them to reach ùë†from ùë°.
Our intention is that a path in the graph will correspond to an as-
signment ùë•‚àà{0, 1}ùëõin the sense that taking the upper path in the ùëñùë°‚Ñé
loop corresponds to assigning ùë•ùëñ= 1 and taking the lower path cor-
responds to assigning ùë•ùëñ= 0. When we are done snaking through all
the ùëõloops corresponding to the variables to reach ùë°we need to pass
through ùëö‚Äúobstacles‚Äù: for each clause ùëówe will have a small gad-
get consisting of a pair of vertices ùë†ùëó, ùë°ùëóthat have three paths between
them. For example, if the ùëóùë°‚Ñéclause had the form ùë•17 ‚à®ùë•55 ‚à®ùë•72 then
one path would go through a vertex in the lower loop corresponding
to ùë•17, one path would go through a vertex in the upper loop corre-
sponding to ùë•55 and the third would go through the lower loop cor-
responding to ùë•72. We see that if we went in the first stage according
to a satisfying assignment then we will be able to find a free vertex to
travel from ùë†ùëóto ùë°ùëó. We link ùë°1 to ùë†2, ùë°2 to ùë†3, etc and link ùë°ùëöto ùë°. Thus


--- Page 474 ---

474
introduction to theoretical computer science
Figure 14.15: The result of applying the reduction of
3SAT to LONGPATH to the formula (ùë•0 ‚à®¬¨ùë•3 ‚à®ùë•2) ‚àß
(¬¨ùë•0 ‚à®ùë•1 ‚à®¬¨ùë•2) ‚àß(ùë•1 ‚à®ùë•2 ‚à®¬¨ùë•3).
a satisfying assignment would correspond to a path from ùë†to ùë°that
goes through one path in each loop corresponding to the variables,
and one path in each loop corresponding to the clauses. We can make
the loop corresponding to the variables long enough so that we must
take the entire path in each loop in order to have a fighting chance of
getting a path as long as the one corresponds to a satisfying assign-
ment. But if we do that, then the only way if we are able to reach ùë°is
if the paths we took corresponded to a satisfying assignment, since
otherwise we will have one clause ùëówhere we cannot reach ùë°ùëófrom ùë†ùëó
without using a vertex we already used before.
‚ñ†
14.7.1 Summary of relations
We have shown that there are a number of functions ùêπfor which we
can prove a statement of the form ‚ÄúIf ùêπ‚ààP then 3SAT ‚ààP‚Äù. Hence
coming up with a polynomial-time algorithm for even one of these
problems will entail a polynomial-time algorithm for 3SAT (see for
example Fig. 14.16). In Chapter 15 we will show the inverse direction
(‚ÄúIf 3SAT ‚ààP then ùêπ‚ààP‚Äù) for these functions, hence allowing us to
conclude that they have equivalent complexity to 3SAT.
Figure 14.16: So far we have shown that P ‚äÜEXP
and that several problems we care about such as
3SAT and MAXCUT are in EXP but it is not known
whether or not they are in EXP. However, since
3SAT ‚â§ùëùMAXCUT we can rule out the possiblity
that MAXCUT ‚ààP but 3SAT ‚àâP. The relation of
P/poly to the class EXP is not known. We know that
EXP does not contain P/poly since the latter even
contains uncomputable functions, but we do not
know whether ot not EXP ‚äÜP/poly (though it is
believed that this is not the case and in particular that
both 3SAT and MAXCUT are not in P/poly).
‚úì
Chapter Recap
‚Ä¢ The computational complexity of many seemingly
unrelated computational problems can be related
to one another through the use of reductions.
‚Ä¢ If ùêπ
‚â§ùëù
ùê∫then a polynomial-time algorithm
for ùê∫can be transformed into a polynomial-time
algorithm for ùêπ.
‚Ä¢ Equivalently, if ùêπ
‚â§ùëù
ùê∫and ùêπdoes not have a
polynomial-time algorithm then neither does ùê∫.
‚Ä¢ We‚Äôve developed many techniques to show that
3SAT ‚â§ùëùùêπfor interesting functions ùêπ. Sometimes
we can do so by using transitivity of reductions: if
3SAT ‚â§ùëùùê∫and ùê∫‚â§ùëùùêπthen 3SAT ‚â§ùëùùêπ.


--- Page 475 ---

polynomial-time reductions
475
14.8 EXERCISES
14.9 BIBLIOGRAPHICAL NOTES
Several notions of reductions are defined in the literature. The notion
defined in Definition 14.1 is often known as a mapping reduction, many
to one reduction or a Karp reduction.
The maximal (as opposed to maximum) independent set is the task
of finding a ‚Äúlocal maximum‚Äù of an independent set: an independent
set ùëÜsuch that one cannot add a vertex to it without losing the in-
dependence property (such a set is known as a vertex cover). Unlike
finding a maximum independent set, finding a maximal independent
set can be done efficiently by a greedy algorithm, but this local maxi-
mum can be much smaller than the global maximum.
Reduction of independent set to max cut taken from these notes.
Image of Hamiltonian Path through Dodecahedron by Christoph
Sommer.
We have mentioned that the line between reductions used for algo-
rithm design and showing hardness is sometimes blurry. An excellent
example for this is the area of SAT Solvers (see [Gom+08]). In this
field people use algorithms for SAT (that take exponential time in the
worst case but often are much faster on many instances in practice)
together with reductions of the form ùêπ‚â§ùëùSAT to derive algorithms
for other functions ùêπof interest.


--- Page 476 ---



--- Page 477 ---

15
NP, NP completeness, and the Cook-Levin Theorem
‚ÄúIn this paper we give theorems that suggest, but do not imply, that these
problems, as well as many others, will remain intractable perpetually‚Äù, Richard
Karp, 1972
‚ÄúSad to say, but it will be many more years, if ever before we really understand
the Mystical Power of Twoness‚Ä¶ 2-SAT is easy, 3-SAT is hard, 2-dimensional
matching is easy, 3-dimensional matching is hard. Why? oh, Why?‚Äù Eugene
Lawler
So far we have shown that 3SAT is no harder than Quadratic Equa-
tions, Independent Set, Maximum Cut, and Longest Path. But to show
that these problems are computationally equivalent we need to give re-
ductions in the other direction, reducing each one of these problems to
3SAT as well. It turns out we can reduce all three problems to 3SAT in
one fell swoop.
In fact, this result extends far beyond these particular problems. All
of the problems we discussed in Chapter 14, and a great many other
problems, share the same commonality: they are all search problems,
where the goal is to decide, given an instance ùë•, whether there exists
a solution ùë¶that satisfies some condition that can be verified in poly-
nomial time. For example, in 3SAT, the instance is a formula and the
solution is an assignment to the variable; in Max-Cut the instance is a
graph and the solution is a cut in the graph; and so on and so forth. It
turns out that every such search problem can be reduced to 3SAT.
15.1 THE CLASS NP
To make the above precise, we will make the following mathematical
definition. We define the class NP to contain all Boolean functions that
correspond to a search problem of the form above. That is, a Boolean
function ùêπis in NP if ùêπhas the form that on input a string ùë•, ùêπ(ùë•) = 1
if and only if there exists a ‚Äúsolution‚Äù string ùë§such that the pair (ùë•, ùë§)
satisfies some polynomial-time checkable condition. Formally, NP is
defined as follows:
Compiled on 8.26.2020 18:10
Learning Objectives:
‚Ä¢ Introduce the class NP capturing a great many
important computational problems
‚Ä¢ NP-completeness: evidence that a problem
might be intractable.
‚Ä¢ The P vs NP problem.


--- Page 478 ---

478
introduction to theoretical computer science
Figure 15.1: Overview of the results of this chapter.
We define NP to contain all decision problems for
which a solution can be efficiently verified. The main
result of this chapter is the Cook Levin Theorem (The-
orem 15.6) which states that 3SAT has a polynomial-
time algorithm if and only if every problem in NP
has a polynomial-time algorithm. Another way to
state this theorem is that 3SAT is NP complete. We
will prove the Cook-Levin theorem by defining the
two intermediate problems NANDSAT and 3NAND,
proving that NANDSAT is NP complete, and then
proving that NANDSAT ‚â§ùëù3NAND ‚â§ùëù3SAT.
Figure 15.2: The class NP corresponds to problems
where solutions can be efficiently verified. That is, this
is the class of functions ùêπsuch that ùêπ(ùë•) = 1 if there
is a ‚Äúsolution‚Äù ùë§of length polynomial in |ùë•| that can
be verified by a polynomial-time algorithm ùëâ.
Definition 15.1 ‚Äî NP. We say that ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1} is in NP if there
exists some integer ùëé> 0 and ùëâ‚à∂{0, 1}‚àó‚Üí{0, 1} such that ùëâ‚ààP
and for every ùë•‚àà{0, 1}ùëõ,
ùêπ(ùë•) = 1 ‚áî‚àÉùë§‚àà{0,1}ùëõùëés.t. ùëâ(ùë•ùë§) = 1 .
(15.1)
In other words, for ùêπto be in NP, there needs to exist some
polynomial-time computable verification function ùëâ, such that if
ùêπ(ùë•) = 1 then there must exist ùë§(of length polynomial in |ùë•|) such
that ùëâ(ùë•ùë§) = 1, and if ùêπ(ùë•) = 0 then for every such ùë§, ùëâ(ùë•ùë§) = 0.
Since the existence of this string ùë§certifies that ùêπ(ùë•) = 1, ùë§is often
referred to as a certificate, witness, or proof that ùêπ(ùë•) = 1.
See also Fig. 15.2 for an illustration of Definition 15.1. The name
NP stands for ‚Äúnondeterministic polynomial time‚Äù and is used for
historical reasons; see the bibiographical notes. The string ùë§in (15.1)
is sometimes known as a solution, certificate, or witness for the instance
ùë•.
Solved Exercise 15.1 ‚Äî Alternative definition of NP. Show that the condition
that |ùë§| = |ùë•|ùëéin Definition 15.1 can be replaced by the condition
that |ùë§| ‚â§ùëù(|ùë•|) for some polynomial ùëù. That is, prove that for every
ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}, ùêπ‚ààNP if and only if there is a polynomial-
time Turing machine ùëâand a polynomial ùëù‚à∂‚Ñï‚Üí‚Ñïsuch that for
every ùë•‚àà{0, 1}‚àóùêπ(ùë•) = 1 if and only if there exists ùë§‚àà{0, 1}‚àówith
|ùë§| ‚â§ùëù(|ùë•|) such that ùëâ(ùë•, ùë§) = 1.
‚ñ†


--- Page 479 ---

np, np completeness, and the cook-levin theorem
479
Solution:
The ‚Äúonly if‚Äù direction (namely that if ùêπ
‚àà
NP then there is an
algorithm ùëâand a polynomial ùëùas above) follows immediately
from Definition 15.1 by letting ùëù(ùëõ)
=
ùëõùëé. For the ‚Äúif‚Äù direc-
tion, the idea is that if a string ùë§is of size at most ùëù(ùëõ) for degree
ùëëpolynomial ùëù, then there is some ùëõ0 such that for all ùëõ
>
ùëõ0,
|ùë§|
<
ùëõùëë+1. Hence we can encode ùë§by a string of exactly length
ùëõùëë+1 by padding it with 1 and an appropriate number of zeroes.
Hence if there is an algorithm ùëâand polynomial ùëùas above, then
we can define an algorithm ùëâ‚Ä≤ that does the following on input
ùë•, ùë§‚Ä≤ with |ùë•| = ùëõand |ùë§‚Ä≤| = ùëõùëé:
‚Ä¢ If ùëõ
‚â§
ùëõ0 then ùëâ‚Ä≤(ùë•, ùë§‚Ä≤) ignores ùë§‚Ä≤ and enumerates over all ùë§
of length at most ùëù(ùëõ) and outputs 1 if there exists ùë§such that
ùëâ(ùë•, ùë§) = 1. (Since ùëõ< ùëõ0, this only takes a constant number of
steps.)
‚Ä¢ If ùëõ
>
ùëõ0 then ùëâ‚Ä≤(ùë•, ùë§‚Ä≤) ‚Äústrips out‚Äù the padding by dropping
all the rightmost zeroes from ùë§until it reaches out the first 1
(which it drops as well) and obtains a string ùë§. If |ùë§|
‚â§
ùëù(ùëõ)
then ùëâ‚Ä≤ outputs ùëâ(ùë•, ùë§).
Since ùëâruns in polynomial time, ùëâ‚Ä≤ runs in polynomial time
as well, and by definition for every ùë•, there exists ùë§‚Ä≤
‚àà
{0, 1}|ùë•|ùëé
such that ùëâ‚Ä≤(ùë•ùë§‚Ä≤) = 1 if and only if there exists ùë§‚àà{0, 1}‚àówith
|ùë§| ‚â§ùëù(|ùë•|) such that ùëâ(ùë•ùë§) = 1.
‚ñ†
The definition of NP means that for every ùêπ‚ààNP and string
ùë•‚àà{0, 1}‚àó, ùêπ(ùë•) = 1 if and only if there is a short and efficiently
verifiable proof of this fact. That is, we can think of the function ùëâin
Definition 15.1 as a verifier algorithm, similar to what we‚Äôve seen in
Section 11.1. The verifier checks whether a given string ùë§‚àà{0, 1}‚àóis a
valid proof for the statement ‚Äúùêπ(ùë•) = 1‚Äù. Essentially all proof systems
considered in mathematics involve line-by-line checks that can be car-
ried out in polynomial time. Thus the heart of NP is asking for state-
ments that have short (i.e., polynomial in the size of the statements)
proof. Indeed, as we will see in Chapter 16, Kurt G√∂del phrased the
question of whether NP = P as asking whether ‚Äúthe mental work of
a mathematician [in proving theorems] could be completely replaced
by a machine‚Äù.
R
Remark 15.2 ‚Äî NP not (necessarily) closed under com-
plement. Definition 15.1 is asymmetric in the sense that


--- Page 480 ---

480
introduction to theoretical computer science
there is a difference between an output of 1 and an
output of 0. You should make sure you understand
why this definition does not guarantee that if ùêπ‚ààNP
then the function 1 ‚àíùêπ(i.e., the map ùë•‚Ü¶1 ‚àíùêπ(ùë•)) is
in NP as well.
In fact, it is believed that there do exist functions ùêπ
such that ùêπ‚ààNP but 1 ‚àíùêπ‚àâNP. For example, as
shown below, 3SAT ‚ààNP, but the function 3SAT that
on input a 3CNF formula ùúëoutputs 1 if and only if ùúë
is not satisfiable is not known (nor believed) to be in
NP. This is in contrast to the class P which does satisfy
that if ùêπ‚ààP then 1 ‚àíùêπis in P as well.
15.1.1 Examples of functions in NP
We now present some examples of functions that are in the class NP.
We start with the canonical example of the 3SAT function.
‚ñ†Example 15.3 ‚Äî 3ùëÜùê¥ùëá
‚àà
NP. 3SAT is in NP since for every ‚Ñì-
variable formula ùúë, 3SAT(ùúë)
=
1 if and only if there exists a
satisfying assignment ùë•
‚àà
{0, 1}‚Ñìsuch that ùúë(ùë•)
=
1, and we
can check this condition in polynomial time.
The above reasoning explains why 3SAT is in NP, but since this
is our first example, we will now belabor the point and expand out
in full formality the precise representation of the witness ùë§and the
algorithm ùëâthat demonstrate that 3SAT is in NP. Since demon-
strating that functions are in NP is fairly straightforward, in future
cases we will not use as much detail, and the reader can also feel
free to skip the rest of this example.
Using Solved Exercise 15.1, it is OK if witness is of size at most
polynomial in the input length ùëõ, rather than of precisely size ùëõùëé
for some integer ùëé
>
0. Specifically, we can represent a 3CNF
formula ùúëwith ùëòvariables and ùëöclauses as a string of length
ùëõ
=
ùëÇ(ùëölog ùëò), since every one of the ùëöclauses involves three
variables and their negation, and the identity of each variable can
be represented using ‚åàlog2 ùëò‚åâ. We assume that every variable par-
ticipates in some clause (as otherwise it can be ignored) and hence
that ùëö‚â•ùëò, which in particular means that the input length ùëõis at
least as large as ùëöand ùëò.
We can represent an assignment to the ùëòvariables using a ùëò-
length string ùë§. The following algorithm checks whether a given ùë§
satisfies the formula ùúë:


--- Page 481 ---

np, np completeness, and the cook-levin theorem
481
Algorithm 15.4 ‚Äî Verifier for 3ùëÜùê¥ùëá.
Input: 3CNF formula ùúëon ùëòvariables and with ùëö
clauses, string ùë§‚àà{0, 1}ùëò
Output: 1 iff ùë§satisfies ùúë
1: for ùëó‚àà[ùëö] do
2:
Let ‚Ñì1 ‚à®‚Ñì2 ‚à®‚Ñìùëóbe the ùëó-th clause of ùúë
3:
if ùë§violates all three literals then
4:
return 0
5:
end if
6: end for
7: return 1
Algorithm 15.4 takes ùëÇ(ùëö) time to enumerate over all clauses,
and will return 1 if and only if ùë¶satisfies all the clauses.
Here are some more examples for problems in NP. For each one
of these problems we merely sketch how the witness is represented
and why it is efficiently checkable, but working out the details can be a
good way to get more comfortable with Definition 15.1:
‚Ä¢ QUADEQ is in NP since for every ‚Ñì-variable instance of quadratic
equations ùê∏, QUADEQ(ùê∏) = 1 if and only if there exists an assign-
ment ùë•‚àà{0, 1}‚Ñìthat satisfies ùê∏. We can check the condition that
ùë•satisfies ùê∏in polynomial time by enumerating over all the equa-
tions in ùê∏, and for each such equation ùëí, plug in the values of ùë•and
verify that ùëíis satisfied.
‚Ä¢ ISET is in NP since for every graph ùê∫and integer ùëò, ISET(ùê∫, ùëò) =
1 if and only if there exists a set ùëÜof ùëòvertices that contains no
pair of neighbors in ùê∫. We can check the condition that ùëÜis an
independent set of size ‚â•ùëòin polynomial time by first checking
that |ùëÜ| ‚â•ùëòand then enumerating over all edges {ùë¢, ùë£} in ùê∫, and
for each such edge verify that either ùë¢‚àâùëÜor ùë£‚àâùëÜ.
‚Ä¢ LONGPATH is in NP since for every graph ùê∫and integer ùëò,
LONGPATH(ùê∫, ùëò) = 1 if and only if there exists a simple path ùëÉ
in ùê∫that is of length at least ùëò. We can check the condition that ùëÉ
is a simple path of length ùëòin polynomial time by checking that it
has the form (ùë£0, ùë£1, ‚Ä¶ , ùë£ùëò) where each ùë£ùëñis a vertex in ùê∫, no ùë£ùëñis
repeated, and for every ùëñ‚àà[ùëò], the edge {ùë£ùëñ, ùë£ùëñ+1} is present in the
graph.
‚Ä¢ MAXCUT is in NP since for every graph ùê∫and integer ùëò,
MAXCUT(ùê∫, ùëò) = 1 if and only if there exists a cut (ùëÜ, ùëÜ) in ùê∫that
cuts at least ùëòedges. We can check that condition that (ùëÜ, ùëÜ) is a
cut of value at least ùëòin polynomial time by checking that ùëÜis a


--- Page 482 ---

482
introduction to theoretical computer science
subset of ùê∫‚Äôs vertices and enumerating over all the edges {ùë¢, ùë£} of
ùê∫, counting those edges such that ùë¢‚ààùëÜand ùë£‚àâùëÜor vice versa.
15.1.2 Basic facts about NP
The definition of NP is one of the most important definitions of this
book, and is worth while taking the time to digest and internalize. The
following solved exercises establish some basic properties of this class.
As usual, I highly recommend that you try to work out the solutions
yourself.
Solved Exercise 15.2 ‚Äî Verifying is no harder than solving. Prove that P ‚äÜNP.
‚ñ†
Solution:
Suppose that ùêπ‚ààP. Define the following function ùëâ: ùëâ(ùë•0ùëõ) =
1 iff ùëõ= |ùë•| and ùêπ(ùë•) = 1. (ùëâoutputs 0 on all other inputs.) Since
ùêπ‚ààP we can clearly compute ùëâin polynomial time as well.
Let ùë•‚àà{0, 1}ùëõbe some string. If ùêπ(ùë•) = 1 then ùëâ(ùë•0ùëõ) = 1. On
the other hand, if ùêπ(ùë•) = 0 then for every ùë§‚àà{0, 1}ùëõ, ùëâ(ùë•ùë§) = 0.
Therefore, setting ùëé= ùëè= 1, we see that ùëâsatisfies (15.1), and es-
tablishes that ùêπ‚ààNP.
‚ñ†
R
Remark 15.5 ‚Äî NP does not mean non-polynomial!.
People sometimes think that NP stands for ‚Äúnon poly-
nomial time‚Äù. As Solved Exercise 15.2 shows, this is
far from the truth, and in fact every polynomial-time
computable function is in NP as well.
If ùêπis in NP it certainly does not mean that ùêπis hard
to compute (though it does not, as far as we know,
necessarily mean that it‚Äôs easy to compute either).
Rather, it means that ùêπis easy to verify, in the technical
sense of Definition 15.1.
Solved Exercise 15.3 ‚Äî NP is in exponential time. Prove that NP ‚äÜEXP.
‚ñ†
Solution:
Suppose that ùêπ
‚àà
NP and let ùëâbe the polynomial-time com-
putable function that satisfies (15.1) and ùëéthe corresponding
constant. Then given every ùë•
‚àà
{0, 1}ùëõ, we can check whether
ùêπ(ùë•)
=
1 in time ùëùùëúùëôùë¶(ùëõ) ‚ãÖ2ùëõùëé
=
ùëú(2ùëõùëé+1) by enumerating over
all the 2ùëõùëéstrings ùë§‚àà{0, 1}ùëõùëéand checking whether ùëâ(ùë•ùë§) = 1,
in which case we return 1. If ùëâ(ùë•ùë§)
=
0 for every such ùë§then we
return 0. By construction, the algorithm above will run in time at


--- Page 483 ---

np, np completeness, and the cook-levin theorem
483
most exponential in its input length and by the definition of NP it
will return ùêπ(ùë•) for every ùë•.
‚ñ†
Solved Exercise 15.2 and Solved Exercise 15.3 together imply that
P ‚äÜNP ‚äÜEXP .
(15.2)
The time hierarchy theorem (Theorem 13.9) implies that P ‚ääEXP
and hence at least one of the two inclusions P ‚äÜNP or NP ‚äÜEXP
is strict. It is believed that both of them are in fact strict inclusions.
That is, it is believed that there are functions in NP that cannot be
computed in polynomial time (this is the P ‚â†NP conjecture) and
that there are functions ùêπin EXP for which we cannot even effi-
ciently certify that ùêπ(ùë•) = 1 for a given input ùë•. One function ùêπ
that is believed to lie in EXP ‚ßµNP is the function 3SAT defined as
3SAT(ùúë) = 1 ‚àí3SAT(ùúë) for every 3CNF formula ùúë. The conjecture
that 3SAT ‚àâNP is known as the ‚ÄúNP ‚â†co ‚àíNP‚Äù conjecture. It
implies the P ‚â†NP conjecture (see Exercise 15.2).
We have previously informally equated the notion of ùêπ‚â§ùëùùê∫with
ùêπbeing ‚Äúno harder than ùê∫‚Äù and in particular have seen in Solved
Exercise 14.1 that if ùê∫‚ààP and ùêπ‚â§ùëùùê∫, then ùêπ‚ààP as well. The
following exercise shows that if ùêπ‚â§ùëùùê∫then it is also ‚Äúno harder to
verify‚Äù than ùê∫. That is, regardless of whether or not it is in P, if ùê∫has
the property that solutions to it can be efficiently verified, then so does
ùêπ.
Solved Exercise 15.4 ‚Äî Reductions and NP. Let ùêπ, ùê∫‚à∂{0, 1}‚àó‚Üí{0, 1}.
Show that if ùêπ‚â§ùëùùê∫and ùê∫‚ààNP then ùêπ‚ààNP.
‚ñ†
Solution:
Suppose that ùê∫is in NP and in particular there exists ùëéand ùëâ
‚àà
P such that for every ùë¶‚àà{0, 1}‚àó, ùê∫(ùë¶) = 1 ‚áî‚àÉùë§‚àà{0,1}|ùë¶|ùëéùëâ(ùë¶ùë§) = 1.
Suppose also that ùêπ
‚â§ùëù
ùê∫and so in particular there is a ùëõùëè-
time computable function ùëÖsuch that ùêπ(ùë•)
=
ùê∫(ùëÖ(ùë•)) for all
ùë•
‚àà
{0, 1}‚àó. Define ùëâ‚Ä≤ to be a Turing Machine that on input a pair
(ùë•, ùë§) computes ùë¶
= ùëÖ(ùë•) and returns 1 if and only if |ùë§| = |ùë¶|ùëé
and ùëâ(ùë¶ùë§)
=
1. Then ùëâ‚Ä≤ runs in polynomial time, and for every
ùë•‚àà{0, 1}‚àó, ùêπ(ùë•) = 1 iff there exists ùë§of size |ùëÖ(ùë•)|ùëéwhich is at
most polynomial in |ùë•| such that ùëâ‚Ä≤(ùë•, ùë§) = 1, hence demonstrating
that ùêπ‚ààNP.
‚ñ†


--- Page 484 ---

484
introduction to theoretical computer science
15.2 FROM NP TO 3SAT: THE COOK-LEVIN THEOREM
We have seen several examples of problems for which we do not know
if their best algorithm is polynomial or exponential, but we can show
that they are in NP. That is, we don‚Äôt know if they are easy to solve, but
we do know that it is easy to verify a given solution. There are many,
many, many, more examples of interesting functions we would like to
compute that are easily shown to be in NP. What is quite amazing is
that if we can solve 3SAT then we can solve all of them!
The following is one of the most fundamental theorems in Com-
puter Science:
Theorem 15.6 ‚Äî Cook-Levin Theorem. For every ùêπ‚ààNP, ùêπ‚â§ùëù3SAT.
We will soon show the proof of Theorem 15.6, but note that it im-
mediately implies that QUADEQ, LONGPATH, and MAXCUT all
reduce to 3SAT. Combining it with the reductions we‚Äôve seen in Chap-
ter 14, it implies that all these problems are equivalent! For example,
to reduce QUADEQ to LONGPATH, we can first reduce QUADEQ to
3SAT using Theorem 15.6 and use the reduction we‚Äôve seen in Theo-
rem 14.10 from 3SAT to LONGPATH. That is, since QUADEQ ‚ààNP,
Theorem 15.6 implies that QUADEQ ‚â§ùëù3SAT, and Theorem 14.10
implies that 3SAT ‚â§ùëùLONGPATH, which by the transitivity of reduc-
tions (Solved Exercise 14.2) means that QUADEQ ‚â§ùëùLONGPATH.
Similarly, since LONGPATH ‚ààNP, we can use Theorem 15.6 and
Theorem 14.4 to show that LONGPATH ‚â§ùëù3SAT ‚â§ùëùQUADEQ,
concluding that LONGPATH and QUADEQ are computationally
equivalent.
There is of course nothing special about QUADEQ and LONGPATH
here: by combining (15.6) with the reductions we saw, we see that just
like 3SAT, every ùêπ‚ààNP reduces to LONGPATH, and the same is true
for QUADEQ and MAXCUT. All these problems are in some sense
‚Äúthe hardest in NP‚Äù since an efficient algorithm for any one of them
would imply an efficient algorithm for all the problems in NP. This
motivates the following definition:
Definition 15.7 ‚Äî NP-hardness and NP-completeness. Let ùê∫‚à∂{0, 1}‚àó
‚Üí
{0, 1}. We say that ùê∫is NP hard if for every ùêπ‚ààNP, ùêπ‚â§ùëùùê∫.
We say that ùê∫is NP complete if ùê∫is NP hard and ùê∫‚ààNP.
The Cook-Levin Theorem (Theorem 15.6) can be rephrased as
saying that 3SAT is NP hard, and since it is also in NP, this means that
3SAT is NP complete. Together with the reductions of Chapter 14,
Theorem 15.6 shows that despite their superficial differences, 3SAT,
quadratic equations, longest path, independent set, and maximum


--- Page 485 ---

np, np completeness, and the cook-levin theorem
485
Figure 15.3: The world if P ‚â†NP (left) and P = NP
(right). In the former case the set of NP-complete
problems is disjoint from P and Ladner‚Äôs theorem
shows that there exist problems that are neither in
P nor are NP-complete. (There are remarkably few
natural candidates for such problems, with some
prominent examples being decision variants of
problems such as integer factoring, lattice shortest
vector, and finding Nash equilibria.) In the latter case
that P = NP the notion of NP-completeness loses its
meaning, as essentially all functions in P (save for the
trivial constant zero and constant one functions) are
NP-complete.
Figure 15.4: A rough illustration of the (conjectured)
status of problems in exponential time. Darker colors
correspond to higher running time, and the circle in
the middle is the problems in P. NP is a (conjectured
to be proper) superclass of P and the NP-complete
problems (or NPC for short) are the ‚Äúhardest‚Äù prob-
lems in NP, in the sense that a solution for one of
them implies a solution for all other problems in NP.
It is conjectured that all the NP-complete problems
require at least exp(ùëõùúñ) time to solve for a constant
ùúñ> 0, and many require exp(‚Ñ¶(ùëõ)) time. The per-
manent is not believed to be contained in NP though
it is NP-hard, which means that a polynomial-time
algorithm for it implies that P = NP.
cut, are all NP-complete. Many thousands of additional problems
have been shown to be NP-complete, arising from all the sciences,
mathematics, economics, engineering and many other fields. (For a
few examples, see this Wikipedia page and this website.)
ÔÉ´Big Idea 22 If a single NP-complete has a polynomial-time algo-
rithm, then there is such an algorithm for every decision problem that
corresponds to the existence of an efficiently-verifiable solution.
15.2.1 What does this mean?
As we‚Äôve seen in Solved Exercise 15.2, P ‚äÜNP. The most famous con-
jecture in Computer Science is that this containment is strict. That is,
it is widely conjectured that P ‚â†NP. One way to refute the conjec-
ture that P ‚â†NP is to give a polynomial-time algorithm for even a
single one of the NP-complete problems such as 3SAT, Max Cut, or
the thousands of others that have been studied in all fields of human
endeavors. The fact that these problems have been studied by so many
people, and yet not a single polynomial-time algorithm for any of
them has been found, supports that conjecture that indeed P ‚â†NP. In
fact, for many of these problems (including all the ones we mentioned
above), we don‚Äôt even know of a 2ùëú(ùëõ)-time algorithm! However, to the
frustration of computer scientists, we have not yet been able to prove
that P ‚â†NP or even rule out the existence of an ùëÇ(ùëõ)-time algorithm
for 3SAT. Resolving whether or not P = NP is known as the P vs NP
problem. A million-dollar prize has been offered for the solution of
this problem, a popular book has been written, and every year a new
paper comes out claiming a proof of P = NP or P ‚â†NP, only to wither
under scrutiny.
One of the mysteries of computation is that people have observed a
certain empirical ‚Äúzero-one law‚Äù or ‚Äúdichotomy‚Äù in the computational
complexity of natural problems, in the sense that many natural prob-
lems are either in P (often in TIME(ùëÇ(ùëõ)) or TIME(ùëÇ(ùëõ2))), or they
are are NP hard. This is related to the fact that for most natural prob-
lems, the best known algorithm is either exponential or polynomial,
with not too many examples where the best running time is some
strange intermediate complexity such as 22‚àölog ùëõ. However, it is be-
lieved that there exist problems in NP that are neither in P nor are NP-
complete, and in fact a result known as ‚ÄúLadner‚Äôs Theorem‚Äù shows
that if P ‚â†NP then this is indeed the case (see also Exercise 15.1 and
Fig. 15.3).


--- Page 486 ---

486
introduction to theoretical computer science
15.2.2 The Cook-Levin Theorem: Proof outline
We will now prove the Cook-Levin Theorem, which is the underpin-
ning to a great web of reductions from 3SAT to thousands of problems
across many great fields. Some problems that have been shown to be
NP-complete include: minimum-energy protein folding, minimum
surface-area foam configuration, map coloring, optimal Nash equi-
librium, quantum state entanglement, minimum supersequence of
a genome, minimum codeword problem, shortest vector in a lattice,
minimum genus knots, positive Diophantine equations, integer pro-
gramming, and many many more. The worst-case complexity of all
these problems is (up to polynomial factors) equivalent to that of
3SAT, and through the Cook-Levin Theorem, to all problems in NP.
To prove Theorem 15.6 we need to show that ùêπ‚â§ùëù3SAT for every
ùêπ‚ààNP. We will do so in three stages. We define two intermediate
problems: NANDSAT and 3NAND. We will shortly show the def-
initions of these two problems, but Theorem 15.6 will follow from
combining the following three results:
1. NANDSAT is NP hard (Lemma 15.8).
2. NANDSAT ‚â§ùëù3NAND (Lemma 15.9).
3. 3NAND ‚â§ùëù3SAT (Lemma 15.10).
By the transitivity of reductions, it will follow that for every ùêπ‚àà
NP,
ùêπ‚â§ùëùNANDSAT ‚â§ùëù3NAND ‚â§ùëù3SAT
(15.3)
hence establishing Theorem 15.6.
We will prove these three results Lemma 15.8, Lemma 15.9 and
Lemma 15.10 one by one, providing the requisite definitions as we go
along.
15.3 THE NANDSAT PROBLEM, AND WHY IT IS NP HARD
The function NANDSAT ‚à∂{0, 1}‚àó‚Üí{0, 1} is defined as follows:
‚Ä¢ The input to NANDSAT is a string ùëÑrepresenting a NAND-CIRC
program (or equivalently, a circuit with NAND gates).
‚Ä¢ The output of NANDSAT on input ùëÑis 1 if and only if there exists a
string ùë§‚àà{0, 1}ùëõ(where ùëõis the number of inputs to ùëÑ) such that
ùëÑ(ùë§) = 1.
Solved Exercise 15.5 ‚Äî ùëÅùê¥ùëÅùê∑ùëÜùê¥ùëá‚ààNP. Prove that NANDSAT ‚ààNP.
‚ñ†


--- Page 487 ---

np, np completeness, and the cook-levin theorem
487
Solution:
We have seen that the circuit (or straightline program) evalua-
tion problem can be computed in polynomial time. Specifically,
given a NAND-CIRC program ùëÑof ùë†lines and ùëõinputs, and
ùë§
‚àà
{0, 1}ùëõ, we can evaluate ùëÑon the input ùë§in time which is
polynomial in ùë†and hence verify whether or not ùëÑ(ùë§) = 1.
‚ñ†
We now prove that NANDSAT is NP hard.
Lemma 15.8 NANDSAT is NP hard.
Proof Idea:
The proof closely follows the proof that P ‚äÜP/poly (Theorem 13.12
, see also Section 13.6.2). Specifically, if ùêπ‚ààNP then there is a poly-
nomial time Turing machine ùëÄand positive integer ùëésuch that for
every ùë•‚àà{0, 1}ùëõ, ùêπ(ùë•) = 1 iff there is some ùë§‚àà{0, 1}ùëõùëésuch that
ùëÄ(ùë•ùë§) = 1. The proof that P ‚äÜP/poly gave us way (via ‚Äúunrolling the
loop‚Äù) to come up in polynomial time with a Boolean circuit ùê∂on ùëõùëé
inputs that computes the function ùë§‚Ü¶ùëÄ(ùë•ùë§). We can then translate
ùê∂into an equivalent NAND circuit (or NAND-CIRC program) ùëÑ. We
see that there is a string ùë§‚àà{0, 1}ùëõùëésuch that ùëÑ(ùë§) = 1 if and only if
there is such ùë§satisfying ùëÄ(ùë•ùë§) = 1 which (by definition) happens if
and only if ùêπ(ùë•) = 1. Hence the translation of ùë•into the circuit ùëÑis a
reduction showing ùêπ‚â§ùëùNANDSAT.
‚ãÜ
P
The proof is a little bit technical but ultimately follows
quite directly from the definition of NP, as well as the
ability to ‚Äúunroll the loop‚Äù of NAND-TM programs as
discussed in Section 13.6.2. If you find it confusing, try
to pause here and think how you would implement
in your favorite programming language the function
unroll which on input a NAND-TM program ùëÉ
and numbers ùëá, ùëõoutputs an ùëõ-input NAND-CIRC
program ùëÑof ùëÇ(|ùëá|) lines such that for every input
ùëß‚àà{0, 1}ùëõ, if ùëÉhalts on ùëßwithin at most ùëásteps and
outputs ùë¶, then ùëÑ(ùëß) = ùë¶.
Proof of Lemma 15.8. Let ùêπ‚ààNP. To prove Lemma 15.8 we need to
give a polynomial-time computable function that will map every ùë•‚àó‚àà
{0, 1}‚àóto a NAND-CIRC program ùëÑsuch that ùêπ(ùë•) = NANDSAT(ùëÑ).
Let ùë•‚àó‚àà{0, 1}‚àóbe such a string and let ùëõ= |ùë•‚àó| be its length. By
Definition 15.1 there exists ùëâ‚ààP and positive ùëé‚Ñïsuch that ùêπ(ùë•‚àó) = 1
if and only if there exists ùë§‚àà{0, 1}ùëõùëésatisfying ùëâ(ùë•‚àóùë§) = 1.


--- Page 488 ---

488
introduction to theoretical computer science
Let ùëö= ùëõùëé. Since ùëâ‚ààP there is some NAND-TM program ùëÉ‚àóthat
computes ùëâon inputs of the form ùë•ùë§with ùë•‚àà{0, 1}ùëõand ùë§‚àà{0, 1}ùëö
in at most (ùëõ+ ùëö)
ùëêtime for some constant ùëê. Using our ‚Äúunrolling
the loop NAND-TM to NAND compiler‚Äù of Theorem 13.14, we can
obtain a NAND-CIRC program ùëÑ‚Ä≤ that has ùëõ+ ùëöinputs and at most
ùëÇ((ùëõ+ ùëö)2ùëê) lines such that ùëÑ‚Ä≤(ùë•ùë§) = ùëÉ‚àó(ùë•ùë§) for every ùë•‚àà{0, 1}ùëõ
and ùë§‚àà{0, 1}ùëö.
We can then use a simple ‚Äúhardwiring‚Äù technique, reminiscent of
Remark 9.11 to map ùëÑ‚Ä≤ into a circuit/NAND-CIRC program ùëÑon ùëö
inputs such that ùëÑ(ùë§) = ùëÑ‚Ä≤(ùë•‚àóùë§) for every ùë§‚àà{0, 1}ùëö.
CLAIM: There is a polynomial-time algorithm that on input a
NAND-CIRC program ùëÑ‚Ä≤ on ùëõ+ ùëöinputs and ùë•‚àó‚àà{0, 1}ùëõ, outputs
a NAND-CIRC program ùëÑsuch that for every ùë§‚àà{0, 1}ùëõ, ùëÑ(ùë§) =
ùëÑ‚Ä≤(ùë•‚àóùë§).
PROOF OF CLAIM: We can do so by adding a few lines to ensure
that the variables zero and one are 0 and 1 respectively, and then
simply replacing any reference in ùëÑ‚Ä≤ to an input ùë•ùëñwith ùëñ‚àà[ùëõ] the
corresponding value based on ùë•‚àó
ùëñ. See Fig. 15.5 for an implementation
of this reduction in Python.
Our final reduction maps an input ùë•‚àó, into the NAND-CIRC pro-
gram ùëÑobtained above. By the above discussion, this reduction runs
in polynomial time. Since we know that ùêπ(ùë•‚àó) = 1 if and only if there
exists ùë§‚àà{0, 1}ùëösuch that ùëÉ‚àó(ùë•‚àóùë§) = 1, this means that ùêπ(ùë•‚àó) = 1 if
and only if NANDSAT(ùëÑ) = 1, which is what we wanted to prove.
‚ñ†
Figure 15.5: Given an ùëá-line NAND-CIRC program
ùëÑthat has ùëõ+ ùëöinputs and some ùë•‚àó‚àà{0, 1}ùëõ,
we can transform ùëÑinto a ùëá+ 3 line NAND-CIRC
program ùëÑ‚Ä≤ that computes the map ùë§‚Ü¶ùëÑ(ùë•‚àóùë§)
for ùë§‚àà{0, 1}ùëöby simply adding code to compute
the zero and one constants, replacing all references to
X[ùëñ] with either zero or one depending on the value
of ùë•‚àó
ùëñ, and then replacing the remaining references
to X[ùëó] with X[ùëó‚àíùëõ]. Above is Python code that
implements this transformation, as well as an example
of its execution on a simple program.
15.4 THE 3NAND PROBLEM
The 3NAND problem is defined as follows:
‚Ä¢ The input is a logical formula Œ® on a set of variables ùëß0, ‚Ä¶ , ùëßùëü‚àí1
which is an AND of constraints of the form ùëßùëñ= NAND(ùëßùëó, ùëßùëò).


--- Page 489 ---

np, np completeness, and the cook-levin theorem
489
‚Ä¢ The output is 1 if and only if there is an input ùëß‚àà{0, 1}ùëüthat
satisfies all of the constraints.
For example, the following is a 3NAND formula with 5 variables
and 3 constraints:
Œ® = (ùëß3 = NAND(ùëß0, ùëß2))‚àß(ùëß1 = NAND(ùëß0, ùëß2))‚àß(ùëß4 = NAND(ùëß3, ùëß1)) .
(15.4)
In this case 3NAND(Œ®) = 1 since the assignment ùëß= 01010 satisfies
it. Given a 3NAND formula Œ® on ùëüvariables and an assignment ùëß‚àà
{0, 1}ùëü, we can check in polynomial time whether Œ®(ùëß) = 1, and hence
3NAND ‚ààNP. We now prove that 3NAND is NP hard:
Lemma 15.9 NANDSAT ‚â§ùëù3NAND.
Proof Idea:
To prove Lemma 15.9 we need to give a polynomial-time map from
every NAND-CIRC program ùëÑto a 3NAND formula Œ® such that there
exists ùë§such that ùëÑ(ùë§) = 1 if and only if there exists ùëßsatisfying Œ®.
For every line ùëñof ùëÑ, we define a corresponding variable ùëßùëñof Œ®. If
the line ùëñhas the form foo = NAND(bar,blah) then we will add the
clause ùëßùëñ= NAND(ùëßùëó, ùëßùëò) where ùëóand ùëòare the last lines in which bar
and blah were written to. We will also set variables corresponding
to the input variables, as well as add a clause to ensure that the final
output is 1. The resulting reduction can be implemented in about a
dozen lines of Python, see Fig. 15.6.
‚ãÜ
Figure 15.6: Python code to reduce an instance ùëÑof
NANDSAT to an instance Œ® of 3NAND. In the exam-
ple above we transform the NAND-CIRC program
xor5 which has 5 input variables and 16 lines, into
a 3NAND formula Œ® that has 24 variables and 20
clauses. Since xor5 outputs 1 on the input 1, 0, 0, 1, 1,
there exists an assignment ùëß‚àà{0, 1}24 to Œ® such that
(ùëß0, ùëß1, ùëß2, ùëß3, ùëß4) = (1, 0, 0, 1, 1) and Œ® evaluates to
true on ùëß.


--- Page 490 ---

490
introduction to theoretical computer science
Proof of Lemma 15.9. To prove Lemma 15.9 we need to give a reduction
from NANDSAT to 3NAND. Let ùëÑbe a NAND-CIRC program with
ùëõinputs, one output, and ùëölines. We can assume without loss of
generality that ùëÑcontains the variables one and zero as usual.
We map ùëÑto a 3NAND formula Œ® as follows:
‚Ä¢ Œ® has ùëö+ ùëõvariables ùëß0, ‚Ä¶ , ùëßùëö+ùëõ‚àí1.
‚Ä¢ The first ùëõvariables ùëß0, ‚Ä¶ , ùëßùëõ‚àí1 will corresponds to the inputs of ùëÑ.
The next ùëövariables ùëßùëõ, ‚Ä¶ , ùëßùëõ+ùëö‚àí1 will correspond to the ùëölines
of ùëÑ.
‚Ä¢ For every ‚Ñì‚àà{ùëõ, ùëõ+ 1, ‚Ä¶ , ùëõ+ ùëö}, if the ‚Ñì‚àíùëõ-th line of the program
ùëÑis foo = NAND(bar,blah) then we add to Œ® the constraint ùëß‚Ñì=
NAND(ùëßùëó, ùëßùëò) where ùëó‚àíùëõand ùëò‚àíùëõcorrespond to the last lines
in which the variables bar and blah (respectively) were written to.
If one or both of bar and blah was not written to before then we
use ùëß‚Ñì0 instead of the corresponding value ùëßùëóor ùëßùëòin the constraint,
where ‚Ñì0 ‚àíùëõis the line in which zero is assigned a value. If one or
both of bar and blah is an input variable X[i] then we use ùëßùëñin the
constraint.
‚Ä¢ Let ‚Ñì‚àóbe the last line in which the output y_0 is assigned a value.
Then we add the constraint ùëß‚Ñì‚àó= NAND(ùëß‚Ñì0, ùëß‚Ñì0) where ‚Ñì0 ‚àíùëõis as
above the last line in which zero is assigned a value. Note that this
is effectively the constraint ùëß‚Ñì‚àó= NAND(0, 0) = 1.
To complete the proof we need to show that there exists ùë§‚àà{0, 1}ùëõ
s.t. ùëÑ(ùë§) = 1 if and only if there exists ùëß‚àà{0, 1}ùëõ+ùëöthat satisfies all
constraints in Œ®. We now show both sides of this equivalence.
Part I: Completeness. Suppose that there is ùë§‚àà{0, 1}ùëõs.t. ùëÑ(ùë§) =
1. Let ùëß‚àà{0, 1}ùëõ+ùëöbe defined as follows: for ùëñ‚àà[ùëõ], ùëßùëñ= ùë§ùëñand
for ùëñ‚àà{ùëõ, ùëõ+ 1, ‚Ä¶ , ùëõ+ ùëö} ùëßùëñequals the value that is assigned in
the (ùëñ‚àíùëõ)-th line of ùëÑwhen executed on ùë§. Then by construction
ùëßsatisfies all of the constraints of Œ® (including the constraint that
ùëß‚Ñì‚àó= NAND(0, 0) = 1 since ùëÑ(ùë§) = 1.)
Part II: Soundness. Suppose that there exists ùëß‚àà{0, 1}ùëõ+ùëösatisfy-
ing Œ®. Soundness will follow by showing that ùëÑ(ùëß0, ‚Ä¶ , ùëßùëõ‚àí1) = 1 (and
hence in particular there exists ùë§‚àà{0, 1}ùëõ, namely ùë§= ùëß0 ‚ãØùëßùëõ‚àí1,
such that ùëÑ(ùë§) = 1). To do this we will prove the following claim
(‚àó): for every ‚Ñì‚àà[ùëö], ùëß‚Ñì+ùëõequals the value assigned in the ‚Ñì-th step
of the execution of the program ùëÑon ùëß0, ‚Ä¶ , ùëßùëõ‚àí1. Note that because ùëß
satisfies the constraints of Œ®, (‚àó) is sufficient to prove the soundness
condition since these constraints imply that the last value assigned to
the variable y_0 in the execution of ùëÑon ùëß0 ‚ãØùë§ùëõ‚àí1 is equal to 1. To
prove (‚àó) suppose, towards a contradiction, that it is false, and let ‚Ñìbe


--- Page 491 ---

np, np completeness, and the cook-levin theorem
491
Figure 15.7: A 3NAND instance that is obtained by
taking a NAND-TM program for computing the
AND function, unrolling it to obtain a NANDSAT
instance, and then composing it with the reduction of
Lemma 15.9.
the smallest number such that ùëß‚Ñì+ùëõis not equal to the value assigned
in the ‚Ñì-th step of the execution of ùëÑon ùëß0, ‚Ä¶ , ùëßùëõ‚àí1. But since ùëßsat-
isfies the constraints of Œ®, we get that ùëß‚Ñì+ùëõ= NAND(ùëßùëñ, ùëßùëó) where
(by the assumption above that ‚Ñìis smallest with this property) these
values do correspond to the values last assigned to the variables on the
righthand side of the assignment operator in the ‚Ñì-th line of the pro-
gram. But this means that the value assigned in the ‚Ñì-th step is indeed
simply the NAND of ùëßùëñand ùëßùëó, contradicting our assumption on the
choice of ‚Ñì.
‚ñ†
15.5 FROM 3NAND TO 3SAT
The final step in the proof of Theorem 15.6 is the following:
Lemma 15.10 3NAND ‚â§ùëù3SAT.
Proof Idea:
To prove Lemma 15.10 we need to map a 3NAND formula ùúëinto
a 3SAT formula ùúìsuch that ùúëis satisfiable if and only if ùúìis. The
idea is that we can transform every NAND constraint of the form
ùëé= NAND(ùëè, ùëê) into the AND of ORs involving the variables ùëé, ùëè, ùëê
and their negations, where each of the ORs contains at most three
terms. The construction is fairly straightforward, and the details are
given below.
‚ãÜ
P
It is a good exercise for you to try to find a 3CNF for-
mula ùúâon three variables ùëé, ùëè, ùëêsuch that ùúâ(ùëé, ùëè, ùëê) is
true if and only if ùëé= NAND(ùëè, ùëê). Once you do so, try
to see why this implies a reduction from 3NAND to
3SAT, and hence completes the proof of Lemma 15.10
Figure 15.8: Code and example output for the reduc-
tion given in Lemma 15.10 of 3NAND to 3SAT.


--- Page 492 ---

492
introduction to theoretical computer science
1 The resulting formula will have some of the OR‚Äôs
involving only two variables. If we wanted to insist on
each formula involving three distinct variables we can
always add a ‚Äúdummy variable‚Äù ùëßùëõ+ùëöand include it
in all the OR‚Äôs involving only two variables, and add a
constraint requiring this dummy variable to be zero.
Proof of Lemma 15.10. The constraint
ùëßùëñ= NAND(ùëßùëó, ùëßùëò)
(15.5)
is satisfied if ùëßùëñ= 1 whenever (ùëßùëó, ùëßùëò) ‚â†(1, 1). By going through all
cases, we can verify that (15.5) is equivalent to the constraint
(ùëßùëñ‚à®ùëßùëó‚à®ùëßùëò) ‚àß(ùëßùëñ‚à®ùëßùëó) ‚àß(ùëßùëñ‚à®ùëßùëò) .
(15.6)
Indeed if ùëßùëó= ùëßùëò= 1 then the first constraint of Eq. (15.6) is only
true if ùëßùëñ= 0. On the other hand, if either of ùëßùëóor ùëßùëòequals 0 then un-
less ùëßùëñ= 1 either the second or third constraints will fail. This means
that, given any 3NAND formula ùúëover ùëõvariables ùëß0, ‚Ä¶ , ùëßùëõ‚àí1, we can
obtain a 3SAT formula ùúìover the same variables by replacing every
3NAND constraint of ùúëwith three 3OR constraints as in Eq. (15.6).1
Because of the equivalence of (15.5) and (15.6), the formula ùúìsat-
isfies that ùúì(ùëß0, ‚Ä¶ , ùëßùëõ‚àí1) = ùúë(ùëß0, ‚Ä¶ , ùëßùëõ‚àí1) for every assignment
ùëß0, ‚Ä¶ , ùëßùëõ‚àí1 ‚àà{0, 1}ùëõto the variables. In particular ùúìis satisfiable if
and only if ùúëis, thus completing the proof.
‚ñ†
Figure 15.9: An instance of the independent set problem
obtained by applying the reductions NANDSAT ‚â§ùëù
3NAND ‚â§ùëù3SAT ‚â§ùëùISAT starting with the xor5
NAND-CIRC program.
15.6 WRAPPING UP
We have shown that for every function ùêπin NP, ùêπ‚â§ùëùNANDSAT ‚â§ùëù
3NAND ‚â§ùëù3SAT, and so 3SAT is NP-hard. Since in Chapter 14 we
saw that 3SAT ‚â§ùëùQUADEQ, 3SAT ‚â§ùëùISET, 3SAT ‚â§ùëùMAXCUT
and 3SAT ‚â§ùëùLONGPATH, all these problems are NP-hard as well.
Finally, since all the aforementioned problems are in NP, they are
all in fact NP-complete and have equivalent complexity. There are
thousands of other natural problems that are NP-complete as well.
Finding a polynomial-time algorithm for any one of them will imply a
polynomial-time algorithm for all of them.


--- Page 493 ---

np, np completeness, and the cook-levin theorem
493
Figure 15.10: We believe that P ‚â†NP and all NP
complete problems lie outside of P, but we cannot
rule out the possiblity that P = NP. However, we
can rule out the possiblity that some NP-complete
problems are in P and other do not, since we know
that if even one NP-complete problem is in P then
P = NP. The relation between P/poly and NP is
not known though it can be shown that if one NP-
complete problem is in P/poly then NP ‚äÜP/poly.
2 Hint: Use the function ùêπthat on input a formula ùúë
and a string of the form 1ùë°, outputs 1 if and only if ùúë
is satisfiable and ùë°= |ùúë|log |ùúë|.
3 Hint: Prove and then use the fact that P is closed
under complement.
‚úì
Chapter Recap
‚Ä¢ Many of the problems for which we don‚Äôt know
polynomial-time algorithms are NP-complete,
which means that finding a polynomial-time algo-
rithm for one of them would imply a polynomial-
time algorithm for all of them.
‚Ä¢ It is conjectured that NP
‚â†
P which means that
we believe that polynomial-time algorithms for
these problems are not merely unknown but are
nonexistent.
‚Ä¢ While an NP-hardness result means for example
that a full-fledged ‚Äútextbook‚Äù solution to a problem
such as MAX-CUT that is as clean and general as
the algorithm for MIN-CUT probably does not
exist, it does not mean that we need to give up
whenever we see a MAX-CUT instance. Later in
this course we will discuss several strategies to deal
with NP-hardness, including average-case complexity
and approximation algorithms.
15.7 EXERCISES
Exercise 15.1 ‚Äî Poor man‚Äôs Ladner‚Äôs Theorem. Prove that if there is no
ùëõùëÇ(log2 ùëõ) time algorithm for 3SAT then there is some ùêπ‚ààNP such
that ùêπ‚àâP and ùêπis not NP complete.2
‚ñ†
Exercise 15.2 ‚Äî NP ‚â†co ‚àíNP ‚áíNP ‚â†P. Let 3SAT be the function
that on input a 3CNF formula ùúëreturn 1 ‚àí3SAT(ùúë). Prove that if
3SAT ‚àâNP then P ‚â†NP. See footnote for hint.3
‚ñ†
Exercise 15.3 Define WSAT to be the following function: the input is a
CNF formula ùúëwhere each clause is the OR of one to three variables
(without negations), and a number ùëò‚àà‚Ñï. For example, the following
formula can be used for a valid input to WSAT: ùúë= (ùë•5 ‚à®ùë•2 ‚à®ùë•1) ‚àß
(ùë•1 ‚à®ùë•3 ‚à®ùë•0) ‚àß(ùë•2 ‚à®ùë•4 ‚à®ùë•0). The output WSAT(ùúë, ùëò) = 1 if and
only if there exists a satisfying assignment to ùúëin which exactly ùëò
of the variables get the value 1. For example for the formula above


--- Page 494 ---

494
introduction to theoretical computer science
WSAT(ùúë, 2) = 1 since the assignment (1, 1, 0, 0, 0, 0) satisfies all the
clauses. However WSAT(ùúë, 1) = 0 since there is no single variable
appearing in all clauses.
Prove that WSAT is NP-complete.
‚ñ†
Exercise 15.4 In the employee recruiting problem we are given a list of
potential employees, each of which has some subset of ùëöpotential
skills, and a number ùëò. We need to assemble a team of ùëòemployees
such that for every skill there would be one member of the team with
this skill.
For example, if Alice has the skills ‚ÄúC programming‚Äù, ‚ÄúNAND
programming‚Äù and ‚ÄúSolving Differential Equations‚Äù, Bob has the
skills ‚ÄúC programming‚Äù and ‚ÄúSolving Differential Equations‚Äù, and
Charlie has the skills ‚ÄúNAND programming‚Äù and ‚ÄúCoffee Brewing‚Äù,
then if we want a team of two people that covers all the four skills, we
would hire Alice and Charlie.
Define the function EMP s.t. on input the skills ùêøof all potential
employees (in the form of a sequence ùêøof ùëõlists ùêø1, ‚Ä¶ , ùêøùëõ, each
containing distinct numbers between 0 and ùëö), and a number ùëò,
EMP(ùêø, ùëò) = 1 if and only if there is a subset ùëÜof ùëòpotential em-
ployees such that for every skill ùëóin [ùëö], there is an employee in ùëÜthat
has the skill ùëó.
Prove that EMP is NP complete.
‚ñ†
Exercise 15.5 ‚Äî Balanced max cut. Prove that the ‚Äúbalanced variant‚Äù of
the maximum cut problem is NP-complete, where this is defined as
BMC ‚à∂{0, 1}‚àó‚Üí{0, 1} where for every graph ùê∫= (ùëâ, ùê∏) and ùëò‚àà‚Ñï,
BMC(ùê∫, ùëò) = 1 if and only if there exists a cut ùëÜin ùê∫cutting at least ùëò
edges such that |ùëÜ| = |ùëâ|/2.
‚ñ†
Exercise 15.6 ‚Äî Regular expression intersection. Let MANYREGS be the fol-
lowing function: On input a list of regular expressions ùëíùë•ùëù0, ‚Ä¶ , expùëö
(represented as strings in some standard way), output 1 if and only if
there is a single string ùë•‚àà{0, 1}‚àóthat matches all of them. Prove that
MANYREGS is NP-hard.
‚ñ†
15.8 BIBLIOGRAPHICAL NOTES
Aaronson‚Äôs 120 page survey [Aar16] is a beautiful and extensive ex-
position to the P vs NP problem, its importance and status. See also
as well as Chapter 3 in Wigderson‚Äôs excellent book [Wig19]. Johnson
[Joh12] gives a survey of the historical development of the theory of
NP completeness. The following web page keeps a catalog of failed


--- Page 495 ---

np, np completeness, and the cook-levin theorem
495
attempts at settling P vs NP. At the time of this writing, it lists about
110 papers claiming to resolve the question, of which about 60 claim to
prove that P = NP and about 50 claim to prove that P ‚â†NP.
Eugene Lawler‚Äôs quote on the ‚Äúmystical power of twoness‚Äù was
taken from the wonderful book ‚ÄúThe Nature of Computation‚Äù by
Moore and Mertens. See also this memorial essay on Lawler by
Lenstra.


--- Page 496 ---



--- Page 497 ---

1 Paul Erd≈ës (1913-1996) was one of the most prolific
mathematicians of all times. Though he was an athe-
ist, Erd≈ës often referred to ‚ÄúThe Book‚Äù in which God
keeps the most elegant proof of each mathematical
theorem.
2 The ùëò-th Ramsey number, denoted as ùëÖ(ùëò, ùëò), is the
smallest number ùëõsuch that for every graph ùê∫on ùëõ
vertices, both ùê∫and its complement contain a ùëò-sized
independent set. If P = NP then we can compute
ùëÖ(ùëò, ùëò) in time polynomial in 2ùëò, while otherwise it
can potentially take closer to 222ùëòsteps.
16
What if P equals NP?
‚ÄúYou don‚Äôt have to believe in God, but you should believe in The Book.‚Äù, Paul
Erd≈ës, 1985.1
‚ÄúNo more half measures, Walter‚Äù, Mike Ehrmantraut in ‚ÄúBreaking Bad‚Äù,
2010.
‚ÄúThe evidence in favor of [P ‚â†NP] and [ its algebraic counterpart ] is so
overwhelming, and the consequences of their failure are so grotesque, that their
status may perhaps be compared to that of physical laws rather than that of
ordinary mathematical conjectures.‚Äù, Volker Strassen, laudation for Leslie
Valiant, 1986.
‚ÄúSuppose aliens invade the earth and threaten to obliterate it in a year‚Äôs time
unless human beings can find the [fifth Ramsey number]. We could marshal
the world‚Äôs best minds and fastest computers, and within a year we could prob-
ably calculate the value. If the aliens demanded the [sixth Ramsey number],
however, we would have no choice but to launch a preemptive attack.‚Äù, Paul
Erd≈ës, as quoted by Graham and Spencer, 1990.2
We have mentioned that the question of whether P = NP, which
is equivalent to whether there is a polynomial-time algorithm for
3SAT, is the great open question of Computer Science. But why is it so
important? In this chapter, we will try to figure out the implications of
such an algorithm.
First, let us get one qualm out of the way. Sometimes people say,
‚ÄúWhat if P = NP but the best algorithm for 3SAT takes ùëõ1000 time?‚Äù Well,
ùëõ1000 is much larger than, say, 20.001‚àöùëõfor any input smaller than 250,
as large as a harddrive as you will encounter, and so another way to
phrase this question is to say ‚Äúwhat if the complexity of 3SAT is ex-
ponential for all inputs that we will ever encounter, but then grows
much smaller than that?‚Äù To me this sounds like the computer science
equivalent of asking, ‚Äúwhat if the laws of physics change completely
once they are out of the range of our telescopes?‚Äù. Sure, this is a valid
possibility, but wondering about it does not sound like the most pro-
ductive use of our time.
Compiled on 8.26.2020 18:10
Learning Objectives:
‚Ä¢ Explore the consequences of P = NP
‚Ä¢ Search-to-decision reduction: transform
algorithms that solve decision version to
search version for NP-complete problems.
‚Ä¢ Optimization and learning problems
‚Ä¢ Quantifier elimination and solving problems
in the polynomial hierarchy.
‚Ä¢ What is the evidence for P = NP vs P ‚â†NP?


--- Page 498 ---

498
introduction to theoretical computer science
So, as the saying goes, we‚Äôll keep an open mind, but not so open
that our brains fall out, and assume from now on that:
‚Ä¢ There is a mathematical god,
and
‚Ä¢ She does not ‚Äúbeat around the bush‚Äô ‚Äô or take ‚Äúhalf measures‚Äù.
What we mean by this is that we will consider two extreme scenar-
ios:
‚Ä¢ 3SAT is very easy: 3SAT has an ùëÇ(ùëõ) or ùëÇ(ùëõ2) time algorithm with
a not too huge constant (say smaller than 106.)
‚Ä¢ 3SAT is very hard: 3SAT is exponentially hard and cannot be
solved faster than 2ùúñùëõfor some not too tiny ùúñ> 0 (say at least
10‚àí6). We can even make the stronger assumption that for every
sufficiently large ùëõ, the restriction of 3SAT to inputs of length ùëõ
cannot be computed by a circuit of fewer than 2ùúñùëõgates.
At the time of writing, the fastest known algorithm for 3SAT re-
quires more than 20.35ùëõto solve ùëõvariable formulas, while we do not
even know how to rule out the possibility that we can compute 3SAT
using 10ùëõgates. To put it in perspective, for the case ùëõ= 1000 our
lower and upper bounds for the computational costs are apart by
a factor of about 10100. As far as we know, it could be the case that
1000-variable 3SAT can be solved in a millisecond on a first-generation
iPhone, and it can also be the case that such instances require more
than the age of the universe to solve on the world‚Äôs fastest supercom-
puter.
So far, most of our evidence points to the latter possibility of 3SAT
being exponentially hard, but we have not ruled out the former possi-
bility either. In this chapter we will explore some of the consequences
of the ‚Äú3SAT easy‚Äù scenario.
16.1 SEARCH-TO-DECISION REDUCTION
A priori, having a fast algorithm for 3SAT might not seem so impres-
sive. Sure, such an algorithm allows us to decide the satisfiability of
not just 3CNF formulas but also of quadratic equations, as well as find
out whether there is a long path in a graph, and solve many other de-
cision problems. But this is not typically what we want to do. It‚Äôs not
enough to know if a formula is satisfiable: we want to discover the
actual satisfying assignment. Similarly, it‚Äôs not enough to find out if a
graph has a long path: we want to actually find the path.
It turns out that if we can solve these decision problems, we can
solve the corresponding search problems as well:


--- Page 499 ---

what if p equals np?
499
Theorem 16.1 ‚Äî Search vs Decision. Suppose that P
=
NP. Then
for every polynomial-time algorithm ùëâand ùëé, ùëè
‚àà
‚Ñï,there is a
polynomial-time algorithm FINDùëâsuch that for every ùë•‚àà{0, 1}ùëõ,
if there exists ùë¶‚àà{0, 1}ùëéùëõùëèsatisfying ùëâ(ùë•ùë¶) = 1, then FINDùëâ(ùë•)
finds some string ùë¶‚Ä≤ satisfying this condition.
P
To understand what the statement of Theo-
rem 16.1 means, let us look at the special case of
the MAXCUT problem. It is not hard to see that there
is a polynomial-time algorithm VERIFYCUT such that
VERIFYCUT(ùê∫, ùëò, ùëÜ)
=
1 if and only if ùëÜis a subset
of ùê∫‚Äôs vertices that cuts at least ùëòedges. Theorem 16.1
implies that if P = NP then there is a polynomial-time
algorithm FINDCUT that on input ùê∫, ùëòoutputs a set
ùëÜsuch that VERIFYCUT(ùê∫, ùëò, ùëÜ)
=
1 if such a set
exists. This means that if P = NP, by trying all values
of ùëòwe can find in polynomial time a maximum cut
in any given graph. We can use a similar argument to
show that if P = NP then we can find a satisfying as-
signment for every satisfiable 3CNF formula, find the
longest path in a graph, solve integer programming,
and so and so forth.
Proof Idea:
The idea behind the proof of Theorem 16.1 is simple; let us
demonstrate it for the special case of 3SAT. (In fact, this case is not
so ‚Äúspecial‚Äù‚àísince 3SAT is NP-complete, we can reduce the task of
solving the search problem for MAXCUT or any other problem in
NP to the task of solving it for 3SAT.) Suppose that P = NP and we
are given a satisfiable 3CNF formula ùúë, and we now want to find a
satisfying assignment ùë¶for ùúë. Define 3SAT0(ùúë) to output 1 if there is
a satisfying assignment ùë¶for ùúësuch that its first bit is 0, and similarly
define 3SAT1(ùúë) = 1 if there is a satisfying assignment ùë¶with ùë¶0 = 1.
The key observation is that both 3SAT0 and 3SAT1 are in NP, and so if
P = NP then we can compute them in polynomial time as well. Thus
we can use this to find the first bit of the satisfying assignment. We
can continue in this way to recover all the bits.
‚ãÜ
Proof of Theorem 16.1. Let ùëâbe some polynomial time algorithm and
ùëé, ùëè‚àà‚Ñïsome constants. Define the function STARTSWITHùëâas
follows: For every ùë•‚àà{0, 1}‚àóand ùëß‚àà{0, 1}‚àó, STARTSWITHùëâ(ùë•, ùëß) =
1 if and only if there exists some ùë¶‚àà{0, 1}ùëéùëõùëè‚àí|ùëß| (where ùëõ= |ùë•|) such
that ùëâ(ùë•ùëßùë¶) = 1. That is, STARTSWITHùëâ(ùë•, ùëß) outputs 1 if there is
some string ùë§of length ùëé|ùë•|ùëèsuch that ùëâ(ùë•, ùë§) = 1 and the first |ùëß|


--- Page 500 ---

500
introduction to theoretical computer science
bits of ùë§are ùëß0, ‚Ä¶ , ùëß‚Ñì‚àí1. Since, given ùë•, ùë¶, ùëßas above, we can check in
polynomial time if ùëâ(ùë•ùëßùë¶) = 1, the function STARTSWITHùëâis in NP
and hence if P = NP we can compute it in polynomial time.
Now for every such polynomial-time ùëâand ùëé, ùëè‚àà‚Ñï, we can imple-
ment FINDùëâ(ùë•) as follows:
Algorithm 16.2 ‚Äî ùêπùêºùëÅùê∑ùëâ: Search to decision reduction.
Input: ùë•‚àà{0, 1}ùëõ
Output: ùëß‚àà{0, 1}ùëéùëõùëès.t. ùëâ(ùë•ùëß) = 1, if such ùëßexists. Other-
wise output the empty string.
1: Initially ùëß0 = ùëß1 = ‚ãØ= ùëßùëéùëõùëè‚àí1 = 0.
2: for ‚Ñì= 0, ‚Ä¶ , ùëéùëõùëè‚àí1 do
3:
Let ùëè0 ‚ÜêùëÜùëáùê¥ùëÖùëáùëÜùëäùêºùëáùêªùëâ(ùë•ùëß0 ‚ãØùëß‚Ñì‚àí10).
4:
Let ùëè1 ‚ÜêùëÜùëáùê¥ùëÖùëáùëÜùëäùêºùëáùêªùëâ(ùë•ùëß0 ‚ãØùëß‚Ñì‚àí11).
5:
if ùëè0 = ùëè1 = 0 then
6:
return ‚Äù‚Äù
7:
# Can‚Äôt extend ùë•ùëß0 ‚Ä¶ ùëß‚Ñì‚àí1 to input ùëâaccepts
8:
end if
9:
if ùëè0 = 1 then
10:
ùëß‚Ñì‚Üê0
11:
# Can extend ùë•ùëß0 ‚Ä¶ ùë•‚Ñì‚àí1 with 0 to accepting input
12:
else
13:
ùëß‚Ñì‚Üê1
14:
# Can extend ùë•ùëß0 ‚Ä¶ ùë•‚Ñì‚àí1 with 1 to accepting input
15:
end if
16: end for
17: return ùëß0, ‚Ä¶ , ùëßùëéùëõùëè‚àí1
To analyze Algorithm 16.2, note that it makes 2ùëéùëõùëèinvocations to
STARTSWITHùëâand hence if the latter is polynomial-time, then so is
Algorithm 16.2 Now suppose that ùë•is such that there exists some ùë¶
satisfying ùëâ(ùë•ùë¶) = 1. We claim that at every step ‚Ñì= 0, ‚Ä¶ , ùëéùëõùëè‚àí1, we
maintain the invariant that there exists ùë¶‚àà{0, 1}ùëéùëõùëèwhose first ‚Ñìbits
are ùëßs.t. ùëâ(ùë•ùë¶) = 1. Note that this claim implies the theorem, since in
particular it means that for ‚Ñì= ùëéùëõùëè‚àí1, ùëßsatisfies ùëâ(ùë•ùëß) = 1.
We prove the claim by induction. For ‚Ñì= 0, this holds vacuously.
Now for every ‚Ñì> 0, if the call STARTSWITHùëâ(ùë•ùëß0 ‚ãØùëß‚Ñì‚àí10)
returns 1, then we are guaranteed the invariant by definition of
STARTSWITHùëâ. Now under our inductive hypothesis, there is
ùë¶‚Ñì, ‚Ä¶ , ùë¶ùëéùëõùëè‚àí1 such that ùëÉ(ùë•ùëß0, ‚Ä¶ , ùëß‚Ñì‚àí1ùë¶‚Ñì, ‚Ä¶ , ùë¶ùëéùëõùëè‚àí1) = 1. If the call to
STARTSWITHùëâ(ùë•ùëß0 ‚ãØùëß‚Ñì‚àí10) returns 0 then it must be the case that
ùë¶‚Ñì= 1, and hence when we set ùëß‚Ñì= 1 we maintain the invariant.
‚ñ†


--- Page 501 ---

what if p equals np?
501
16.2 OPTIMIZATION
Theorem 16.1 allows us to find solutions for NP problems if P = NP,
but it is not immediately clear that we can find the optimal solution.
For example, suppose that P = NP, and you are given a graph ùê∫. Can
you find the longest simple path in ùê∫in polynomial time?
P
This is actually an excellent question for you to at-
tempt on your own. That is, assuming P
=
NP, give
a polynomial-time algorithm that on input a graph ùê∫,
outputs a maximally long simple path in the graph ùê∫.
The answer is Yes. The idea is simple: if P = NP then we can find
out in polynomial time if an ùëõ-vertex graph ùê∫contains a simple path
of length ùëõ, and moreover, by Theorem 16.1, if ùê∫does contain such a
path, then we can find it. (Can you see why?) If ùê∫does not contain a
simple path of length ùëõ, then we will check if it contains a simple path
of length ùëõ‚àí1, and continue in this way to find the largest ùëòsuch that
ùê∫contains a simple path of length ùëò.
The above reasoning was not specifically tailored to finding paths
in graphs. In fact, it can be vastly generalized to proving the following
result:
Theorem 16.3 ‚Äî Optimization from P = NP. Suppose that P = NP. Then
for every polynomial-time computable function ùëì
‚à∂{0, 1}‚àó
‚Üí‚Ñï
(identifying ùëì(ùë•) with natural numbers via the binary representa-
tion) there is a polynomial-time algorithm OPT such that on input
ùë•‚àà{0, 1}‚àó,
OPT(ùë•, 1ùëö) =
max
ùë¶‚àà{0,1}ùëöùëì(ùë•, ùë¶) .
(16.1)
Moreover under the same assumption, there is a polynomial-
time algorithm FINDOPT such that for every ùë•‚àà{0, 1}‚àó, FINDOPT(ùë•, 1ùëö)
outputs ùë¶‚àó‚àà{0, 1}‚àósuch that ùëì(ùë•, ùë¶‚àó) = maxùë¶‚àà{0,1}ùëöùëì(ùë•, ùë¶).
P
The statement of Theorem 16.3 is a bit cumbersome.
To understand it, think how it would subsume the
example above of a polynomial time algorithm for
finding the maximum length path in a graph. In
this case the function ùëìwould be the map that on
input a pair ùë•, ùë¶outputs 0 if the pair (ùë•, ùë¶) does not
represent some graph and a simple path inside the
graph respectively; otherwise ùëì(ùë•, ùë¶) would equal
the length of the path ùë¶in the graph ùë•. Since a path
in an ùëõvertex graph can be represented by at most


--- Page 502 ---

502
introduction to theoretical computer science
ùëõlog ùëõbits, for every ùë•representing a graph of ùëõver-
tices, finding maxùë¶‚àà{0,1}ùëõlog ùëõùëì(ùë•, ùë¶) corresponds to
finding the length of the maximum simple path in the
graph corresponding to ùë•, and finding the string ùë¶‚àó
that achieves this maximum corresponds to actually
finding the path.
Proof Idea:
The proof follows by generalizing our ideas from the longest path
example above. Let ùëìbe as in the theorem statement. If P = NP then
for every for every string ùë•‚àà{0, 1}‚àóand number ùëò, we can test in
in ùëùùëúùëôùë¶(|ùë•|, ùëö) time whether there exists ùë¶such that ùëì(ùë•, ùë¶) ‚â•ùëò, or
in other words test whether maxùë¶‚àà{0,1}ùëöùëì(ùë•, ùë¶) ‚â•ùëò. If ùëì(ùë•, ùë¶) is an
integer between 0 and ùëùùëúùëôùë¶(|ùë•| + |ùë¶|) (as is the case in the example of
longest path) then we can just try out all possibilities for ùëòto find the
maximum number ùëòfor which maxùë¶ùëì(ùë•, ùë¶) ‚â•ùëò. Otherwise, we can
use binary search to hone down on the right value. Once we do so, we
can use search-to-decision to actually find the string ùë¶‚àóthat achieves
the maximum.
‚ãÜ
Proof of Theorem 16.3. For every ùëìas in the theorem statement, we can
define the Boolean function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1} as follows.
ùêπ(ùë•, 1ùëö, ùëò) =
‚éß
{
‚é®
{
‚é©
1
‚àÉùë¶‚àà{0,1}ùëöùëì(ùë•, ùë¶) ‚â•ùëò
0
otherwise
(16.2)
Since ùëìis computable in polynomial time, ùêπis in NP, and so under
our assumption that P = NP, ùêπitself can be computed in polynomial
time. Now, for every ùë•and ùëö, we can compute the largest ùëòsuch that
ùêπ(ùë•, 1ùëö, ùëò) = 1 by a binary search. Specifically, we will do this as
follows:
1. We maintain two numbers ùëé, ùëèsuch that we are guaranteed that
ùëé‚â§maxùë¶‚àà{0,1}ùëöùëì(ùë•, ùë¶) < ùëè.
2. Initially we set ùëé= 0 and ùëè= 2ùëá(ùëõ) where ùëá(ùëõ) is the running time
of ùëì. (A function with ùëá(ùëõ) running time can‚Äôt output more than
ùëá(ùëõ) bits and so can‚Äôt output a number larger than 2ùëá(ùëõ).)
3. At each point in time, we compute the midpoint ùëê= ‚åä(ùëé+ ùëè)/2‚åã)
and let ùë¶= ùêπ(1ùëõ, ùëê).
a. If ùë¶= 1 then we set ùëé= ùëêand leave ùëèas it is.
b. If ùë¶= 0 then we set ùëè= ùëêand leave ùëéas it is.
4. We then go back to step 3, until ùëè‚â§ùëé+ 1.


--- Page 503 ---

what if p equals np?
503
Since |ùëè‚àíùëé| shrinks by a factor of 2, within log2 2ùëá(ùëõ) = ùëá(ùëõ)
steps, we will get to the point at which ùëè‚â§ùëé+ 1, and then we can
simply output ùëé. Once we find the maximum value of ùëòsuch that
ùêπ(ùë•, 1ùëö, ùëò) = 1, we can use the search to decision reduction of Theo-
rem 16.1 to obtain the actual value ùë¶‚àó‚àà{0, 1}ùëösuch that ùëì(ùë•, ùë¶‚àó) = ùëò.
‚ñ†
‚ñ†Example 16.4 ‚Äî Integer programming. One application for Theo-
rem 16.3 is in solving optimization problems. For example, the task
of linear programming is to find ùë¶‚àà‚Ñùùëõthat maximizes some linear
objective ‚àë
ùëõ‚àí1
ùëñ=0 ùëêùëñùë¶ùëñsubject to the constraint that ùë¶satisfies linear
inequalities of the form ‚àë
ùëõ‚àí1
ùëñ=0 ùëéùëñùë¶ùëñ
‚â§
ùëê. As we discussed in Sec-
tion 12.1.3, there is a known polynomial-time algorithm for linear
programming. However, if we want to place additional constraints
on ùë¶, such as requiring the coordinates of ùë¶to be integer or 0/1
valued then the best-known algorithms run in exponential time in
the worst case. However, if P
=
NP then Theorem 16.3 tells us
that we would be able to solve all problems of this form in poly-
nomial time. For every string ùë•that describes a set of constraints
and objective, we will define a function ùëìsuch that if ùë¶satisfies
the constraints of ùë•then ùëì(ùë•, ùë¶) is the value of the objective, and
otherwise we set ùëì(ùë•, ùë¶) = ‚àíùëÄwhere ùëÄis some large number. We
can then use Theorem 16.3 to compute the ùë¶that maximizes ùëì(ùë•, ùë¶)
and that will give us the assignment for the variables that satisfies
our constraints and maximizes the objective. (If the computation
results in ùë¶such that ùëì(ùë•, ùë¶) = ‚àíùëÄthen we can double ùëÄand try
again; if the true maximum objective is achieved by some string
ùë¶‚àó, then eventually ùëÄwill be large enough so that ‚àíùëÄwould be
smaller than the objective achieved by ùë¶‚àó, and hence when we run
procedure of Theorem 16.3 we would get a value larger than ‚àíùëÄ.)
R
Remark 16.5 ‚Äî Need for binary search.. In many exam-
ples, such as the case of finding longest path, we don‚Äôt
need to use the binary search step in Theorem 16.3,
and can simply enumerate over all possible values for
ùëòuntil we find the correct one. One example where
we do need to use this binary search step is in the case
of the problem of finding a maximum length path in
a weighted graph. This is the problem where ùê∫is a
weighted graph, and every edge of ùê∫is given a weight
which is a number between 0 and 2ùëò. Theorem 16.3
shows that we can find the maximum-weight simple
path in ùê∫(i.e., simple path maximizing the sum of


--- Page 504 ---

504
introduction to theoretical computer science
3 This is often known as Empirical Risk Minimization.
the weights of its edges) in time polynomial in the
number of vertices and in ùëò.
Beyond just this example there is a vast field of math-
ematical optimization that studies problems of the
same form as in Theorem 16.3. In the context of opti-
mization, ùë•typically denotes a set of constraints over
some variables (that can be Boolean, integer, or real
valued), ùë¶encodes an assignment to these variables,
and ùëì(ùë•, ùë¶) is the value of some objective function that
we want to maximize. Given that we don‚Äôt know
efficient algorithms for NP complete problems, re-
searchers in optimization research study special cases
of functions ùëì(such as linear programming and
semidefinite programming) where it is possible to
optimize the value efficiently. Optimization is widely
used in a great many scientific areas including: ma-
chine learning, engineering, economics and operations
research.
16.2.1 Example: Supervised learning
One classical optimization task is supervised learning. In supervised
learning we are given a list of examples ùë•0, ùë•1, ‚Ä¶ , ùë•ùëö‚àí1 (where we
can think of each ùë•ùëñas a string in {0, 1}ùëõfor some ùëõ) and the la-
bels for them ùë¶0, ‚Ä¶ , ùë¶ùëõ‚àí1 (which we will think of simply bits, i.e.,
ùë¶ùëñ‚àà{0, 1}). For example, we can think of the ùë•ùëñ‚Äôs as images of ei-
ther dogs or cats, for which ùë¶ùëñ= 1 in the former case and ùë¶ùëñ= 0
in the latter case. Our goal is to come up with a hypothesis or predic-
tor ‚Ñé‚à∂{0, 1}ùëõ‚Üí{0, 1} such that if we are given a new example ùë•
that has an (unknown to us) label ùë¶, then with high probability ‚Ñé
will predict the label. That is, with high probability it will hold that
‚Ñé(ùë•) = ùë¶. The idea in supervised learning is to use the Occam‚Äôs Ra-
zor principle: the simplest hypothesis that explains the data is likely
to be correct. There are several ways to model this, but one popular
approach is to pick some fairly simple function ùêª‚à∂{0, 1}ùëò+ùëõ‚Üí{0, 1}.
We think of the first ùëòinputs as the parameters and the last ùëõinputs
as the example data. (For example, we can think of the first ùëòinputs
of ùêªas specifying the weights and connections for some neural net-
work that will then be applied on the latter ùëõinputs.) We can then
phrase the supervised learning problem as finding, given a set of la-
beled examples ùëÜ= {(ùë•0, ùë¶0), ‚Ä¶ , (ùë•ùëö‚àí1, ùë¶ùëö‚àí1)}, the set of parameters
ùúÉ0, ‚Ä¶ , ùúÉùëò‚àí1 ‚àà{0, 1} that minimizes the number of errors made by the
predictor ùë•‚Ü¶ùêª(ùúÉ, ùë•).3
In other words, we can define for every set ùëÜas above the function
ùêπùëÜ‚à∂{0, 1}ùëò‚Üí[ùëö] such that ùêπùëÜ(ùúÉ) = ‚àë(ùë•,ùë¶)‚ààùëÜ|ùêª(ùúÉ, ùë•) ‚àíùë¶|. Now,
finding the value ùúÉthat minimizes ùêπùëÜ(ùúÉ) is equivalent to solving the
supervised learning problem with respect to ùêª. For every polynomial-


--- Page 505 ---

what if p equals np?
505
time computable ùêª‚à∂{0, 1}ùëò+ùëõ‚Üí{0, 1}, the task of minimizing
ùêπùëÜ(ùúÉ) can be ‚Äúmassaged‚Äù to fit the form of Theorem 16.3 and hence if
P = NP, then we can solve the supervised learning problem in great
generality. In fact, this observation extends to essentially any learn-
ing model, and allows for finding the optimal predictors given the
minimum number of examples. (This is in contrast to many current
learning algorithms, which often rely on having access to an extremely
large number of examples‚àífar beyond the minimum needed, and
in particular far beyond the number of examples humans use for the
same tasks.)
16.2.2 Example: Breaking cryptosystems
We will discuss cryptography later in this course, but it turns out that
if P = NP then almost every cryptosystem can be efficiently bro-
ken. One approach is to treat finding an encryption key as an in-
stance of a supervised learning problem. If there is an encryption
scheme that maps a ‚Äúplaintext‚Äù message ùëùand a key ùúÉto a ‚Äúcipher-
text‚Äù ùëê, then given examples of ciphertext/plaintext pairs of the
form (ùëê0, ùëù0), ‚Ä¶ , (ùëêùëö‚àí1, ùëùùëö‚àí1), our goal is to find the key ùúÉsuch that
ùê∏(ùúÉ, ùëùùëñ) = ùëêùëñwhere ùê∏is the encryption algorithm. While you might
think getting such ‚Äúlabeled examples‚Äù is unrealistic, it turns out (as
many amateur home-brew crypto designers learn the hard way) that
this is actually quite common in real-life scenarios, and that it is also
possible to relax the assumption to having more minimal prior infor-
mation about the plaintext (e.g., that it is English text). We defer a
more formal treatment to Chapter 21.
16.3 FINDING MATHEMATICAL PROOFS
In the context of G√∂del‚Äôs Theorem, we discussed the notion of a proof
system (see Section 11.1). Generally speaking, a proof system can be
thought of as an algorithm ùëâ‚à∂{0, 1}‚àó‚Üí{0, 1} (known as the verifier)
such that given a statement ùë•‚àà{0, 1}‚àóand a candidate proof ùë§‚àà{0, 1}‚àó,
ùëâ(ùë•, ùë§) = 1 if and only if ùë§encodes a valid proof for the statement ùë•.
Any type of proof system that is used in mathematics for geometry,
number theory, analysis, etc., is an instance of this form. In fact, stan-
dard mathematical proof systems have an even simpler form where
the proof ùë§encodes a sequence of lines ùë§0, ‚Ä¶ , ùë§ùëö(each of which is
itself a binary string) such that each line ùë§ùëñis either an axiom or fol-
lows from some prior lines through an application of some inference
rule. For example, Peano‚Äôs axioms encode a set of axioms and rules
for the natural numbers, and one can use them to formalize proofs
in number theory. Also, there are some even stronger axiomatic sys-
tems, the most popular one being Zermelo‚ÄìFraenkel with the Axiom
of Choice or ZFC for short. Thus, although mathematicians typically


--- Page 506 ---

506
introduction to theoretical computer science
4 The undecidability of Entscheidungsproblem refers
to the uncomputability of the function that maps a
statement in first order logic to 1 if and only if that
statement has a proof.
write their papers in natural language, proofs of number theorists
can typically be translated to ZFC or similar systems, and so in par-
ticular the existence of an ùëõ-page proof for a statement ùë•implies that
there exists a string ùë§of length ùëùùëúùëôùë¶(ùëõ) (in fact often ùëÇ(ùëõ) or ùëÇ(ùëõ2))
that encodes the proof in such a system. Moreover, because verify-
ing a proof simply involves going over each line and checking that it
does indeed follow from the prior lines, it is fairly easy to do that in
ùëÇ(|ùë§|) or ùëÇ(|ùë§|2) (where as usual |ùë§| denotes the length of the proof
ùë§). This means that for every reasonable proof system ùëâ, the follow-
ing function SHORTPROOFùëâ‚à∂{0, 1}‚àó‚Üí{0, 1} is in NP, where
for every input of the form ùë•1ùëö, SHORTPROOFùëâ(ùë•, 1ùëö) = 1 if and
only if there exists ùë§‚àà{0, 1}‚àówith |ùë§| ‚â§ùëös.t. ùëâ(ùë•ùë§) = 1. That
is, SHORTPROOFùëâ(ùë•, 1ùëö) = 1 if there is a proof (in the system ùëâ)
of length at most ùëöbits that ùë•is true. Thus, if P = NP, then despite
G√∂del‚Äôs Incompleteness Theorems, we can still automate mathematics
in the sense of finding proofs that are not too long for every statement
that has one. (Frankly speaking, if the shortest proof for some state-
ment requires a terabyte, then human mathematicians won‚Äôt ever find
this proof either.) For this reason, G√∂del himself felt that the question
of whether SHORTPROOFùëâhas a polynomial time algorithm is of
great interest. As G√∂del wrote in a letter to John von Neumann in 1956
(before the concept of NP or even ‚Äúpolynomial time‚Äù was formally
defined):
One can obviously easily construct a Turing machine, which for every
formula ùêπin first order predicate logic and every natural number ùëõ, al-
lows one to decide if there is a proof of ùêπof length ùëõ(length = number
of symbols). Let ùúì(ùêπ, ùëõ) be the number of steps the machine requires
for this and let ùúë(ùëõ) = maxùêπùúì(ùêπ, ùëõ). The question is how fast ùúë(ùëõ)
grows for an optimal machine. One can show that ùúë‚â•ùëò‚ãÖùëõ[for some
constant ùëò> 0]. If there really were a machine with ùúë(ùëõ) ‚àºùëò‚ãÖùëõ(or
even ‚àºùëò‚ãÖùëõ2), this would have consequences of the greatest importance.
Namely, it would obviously mean that in spite of the undecidability
of the Entscheidungsproblem,4 the mental work of a mathematician
concerning Yes-or-No questions could be completely replaced by a ma-
chine. After all, one would simply have to choose the natural number
ùëõso large that when the machine does not deliver a result, it makes no
sense to think more about the problem.
For many reasonable proof systems (including the one that G√∂del
referred to), SHORTPROOFùëâis in fact NP-complete, and so G√∂del can
be thought of as the first person to formulate the P vs NP question.
Unfortunately, the letter was only discovered in 1988.


--- Page 507 ---

what if p equals np?
507
5 Since NAND-CIRC programs are equivalent to
Boolean circuits, the search problem corresponding to
(16.6) known as the circuit minimization problem and
is widely studied in Engineering. You can skip ahead
to Section 16.4.1 to see a particularly compelling
application of this.
16.4 QUANTIFIER ELIMINATION (ADVANCED)
If P = NP then we can solve all NP search and optimization problems in
polynomial time. But can we do more? It turns out that the answer is
that Yes we can!
An NP decision problem can be thought of as the task of deciding,
given some string ùë•‚àà{0, 1}‚àóthe truth of a statement of the form
‚àÉùë¶‚àà{0,1}ùëù(|ùë•|)ùëâ(ùë•ùë¶) = 1
(16.3)
for some polynomial-time algorithm ùëâand polynomial ùëù‚à∂‚Ñï‚Üí‚Ñï.
That is, we are trying to determine, given some string ùë•, whether
there exists a string ùë¶such that ùë•and ùë¶satisfy some polynomial-time
checkable condition ùëâ. For example, in the independent set problem,
the string ùë•represents a graph ùê∫and a number ùëò, the string ùë¶repre-
sents some subset ùëÜof ùê∫‚Äôs vertices, and the condition that we check is
whether |ùëÜ| ‚â•ùëòand there is no edge {ùë¢, ùë£} in ùê∫such that both ùë¢‚ààùëÜ
and ùë£‚ààùëÜ.
We can consider more general statements such as checking, given a
string ùë•‚àà{0, 1}‚àó, the truth of a statement of the form
‚àÉùë¶‚àà{0,1}ùëù0(|ùë•|)‚àÄùëß‚àà{0,1}ùëù1(|ùë•|)ùëâ(ùë•ùë¶ùëß) = 1 ,
(16.4)
which in words corresponds to checking, given some string ùë•, whether
there exists a string ùë¶such that for every string ùëß, the triple (ùë•, ùë¶, ùëß) sat-
isfy some polynomial-time checkable condition. We can also consider
more levels of quantifiers such as checking the truth of the statement
‚àÉùë¶‚àà{0,1}ùëù0(|ùë•|)‚àÄùëß‚àà{0,1}ùëù1(|ùë•|)‚àÉùë§‚àà{0,1}ùëù2(|ùë•|)ùëâ(ùë•ùë¶ùëßùë§) = 1
(16.5)
and so on and so forth.
For example, given an ùëõ-input NAND-CIRC program ùëÉ, we might
want to find the smallest NAND-CIRC program ùëÉ‚Ä≤ that computes the
same function as ùëÉ. The question of whether there is such a ùëÉ‚Ä≤ that
can be described by a string of at most ùë†bits can be phrased as
‚àÉùëÉ‚Ä≤‚àà{0,1}ùë†‚àÄùë•‚àà{0,1}ùëõùëÉ(ùë•) = ùëÉ‚Ä≤(ùë•)
(16.6)
which has the form (16.4).5 Another example of a statement involving
ùëélevels of quantifiers would be to check, given a chess position ùë•,
whether there is a strategy that guarantees that White wins within ùëé
steps. For example is ùëé= 3 we would want to check if given the board
position ùë•, there exists a move ùë¶for White such that for every move ùëßfor
Black there exists a move ùë§for White that ends in a a checkmate.
It turns out that if P = NP then we can solve these kinds of prob-
lems as well:


--- Page 508 ---

508
introduction to theoretical computer science
6 For the ease of notation, we assume that all the
strings we quantify over have the same length ùëö=
ùëù(ùëõ), but using simple padding one can show that
this captures the general case of strings of different
polynomial lengths.
Theorem 16.6 ‚Äî Polynomial hierarchy collapse. If P = NP then for every
ùëé‚àà‚Ñï, polynomial ùëù‚à∂‚Ñï‚Üí‚Ñïand polynomial-time algorithm
ùëâ, there is a polynomial-time algorithm SOLVEùëâ,ùëéthat on input
ùë•‚àà{0, 1}ùëõreturns 1 if and only if
‚àÉùë¶0‚àà{0,1}ùëö‚àÄùë¶1‚àà{0,1}ùëö‚ãØùí¨ùë¶ùëé‚àí1‚àà{0,1}ùëöùëâ(ùë•ùë¶0ùë¶1 ‚ãØùë¶ùëé‚àí1) = 1
(16.7)
where ùëö= ùëù(ùëõ) and ùí¨is either ‚àÉor ‚àÄdepending on whether ùëéis
odd or even, respectively. 6
Proof Idea:
To understand the idea behind the proof, consider the special case
where we want to decide, given ùë•‚àà{0, 1}ùëõ, whether for every ùë¶‚àà
{0, 1}ùëõthere exists ùëß‚àà{0, 1}ùëõsuch that ùëâ(ùë•ùë¶ùëß) = 1. Consider the
function ùêπsuch that ùêπ(ùë•ùë¶) = 1 if there exists ùëß‚àà{0, 1}ùëõsuch that
ùëâ(ùë•ùë¶ùëß) = 1. Since ùëâruns in polynomial-time ùêπ‚ààNP and hence if
P = NP, then there is an algorithm ùëâ‚Ä≤ that on input ùë•, ùë¶outputs 1 if
and only if there exists ùëß‚àà{0, 1}ùëõsuch that ùëâ(ùë•ùë¶ùëß) = 1. Now we
can see that the original statement we consider is true if and only if for
every ùë¶‚àà{0, 1}ùëõ, ùëâ‚Ä≤(ùë•ùë¶) = 1, which means it is false if and only if
the following condition (‚àó) holds: there exists some ùë¶‚àà{0, 1}ùëõsuch
that ùëâ‚Ä≤(ùë•ùë¶) = 0. But for every ùë•‚àà{0, 1}ùëõ, the question of whether
the condition (‚àó) is itself in NP (as we assumed ùëâ‚Ä≤ can be computed
in polynomial time) and hence under the assumption that P = NP
we can determine in polynomial time whether the condition (‚àó), and
hence our original statement, is true.
‚ãÜ
Proof of Theorem 16.6. We prove the theorem by induction. We assume
that there is a polynomial-time algorithm SOLVEùëâ,ùëé‚àí1 that can solve
the problem (16.7) for ùëé‚àí1 and use that to solve the problem for ùëé.
For ùëé= 1, SOLVEùëâ,ùëé‚àí1(ùë•) = 1 iff ùëâ(ùë•) = 1 which is a polynomial-time
computation since ùëâruns in polynomial time. For every ùë•, ùë¶0, define
the statement ùúëùë•,ùë¶0 to be the following:
ùúëùë•,ùë¶0 = ‚àÄùë¶1‚àà{0,1}ùëö‚àÉùë¶2‚àà{0,1}ùëö‚ãØùí¨ùë¶ùëé‚àí1‚àà{0,1}ùëöùëâ(ùë•ùë¶0ùë¶1 ‚ãØùë¶ùëé‚àí1) = 1 (16.8)
By the definition of SOLVEùëâ,ùëé, for every ùë•‚àà{0, 1}ùëõ, our goal is
that SOLVEùëâ,ùëé(ùë•) = 1 if and only if there exists ùë¶0 ‚àà{0, 1}ùëösuch that
ùúëùë•,ùë¶0 is true.
The negation of ùúëùë•,ùë¶0 is the statement
ùúëùë•,ùë¶0 = ‚àÉùë¶1‚àà{0,1}ùëö‚àÄùë¶2‚àà{0,1}ùëö‚ãØùí¨ùë¶ùëé‚àí1‚àà{0,1}ùëöùëâ(ùë•ùë¶0ùë¶1 ‚ãØùë¶ùëé‚àí1) = 0 (16.9)


--- Page 509 ---

what if p equals np?
509
7 We do not know whether such loss is inherent.
As far as we can tell, it‚Äôs possible that the quantified
boolean formula problem has a linear-time algorithm.
We will, however, see later in this course that it
satisfies a notion known as PSPACE-hardness that is
even stronger than NP-hardness.
where ùí¨is ‚àÉif ùí¨was ‚àÄand ùí¨is ‚àÄotherwise. (Please stop and verify
that you understand why this is true, this is a generalization of the fact
that if Œ® is some logical condition then the negation of ‚àÉùë¶‚àÄùëßŒ®(ùë¶, ùëß) is
‚àÄùë¶‚àÉùëß¬¨Œ®(ùë¶, ùëß).)
The crucial observation is that ùúëùë•,ùë¶0 is exactly a statement of the
form we consider with ùëé‚àí1 quantifiers instead of ùëé, and hence by
our inductive hypothesis there is some polynomial time algorithm
ùëÜthat on input ùë•ùë¶0 outputs 1 if and only if ùúëùë•,ùë¶0 is true. If we let ùëÜ
be the algorithm that on input ùë•, ùë¶0 outputs 1 ‚àíùëÜ(ùë•ùë¶0) then we see
that ùëÜoutputs 1 if and only if ùúëùë•,ùë¶0 is true. Hence we can rephrase the
original statement (16.7) as follows:
‚àÉùë¶0‚àà{0,1}ùëöùëÜ(ùë•ùë¶0) = 1
(16.10)
but since ùëÜis a polynomial-time algorithm, Eq. (16.10) is clearly a
statement in NP and hence under our assumption that P = NP there is
a polynomial time algorithm that on input ùë•‚àà{0, 1}ùëõ, will determine
if (16.10) is true and so also if the original statement (16.7) is true.
‚ñ†
The algorithm of Theorem 16.6 can also solve the search problem
as well: find the value ùë¶0 that certifies the truth of (16.7). We note
that while this algorithm is in polynomial time, the exponent of this
polynomial blows up quite fast. If the original NANDSAT algorithm
required Œ©(ùëõ2) time, solving ùëélevels of quantifiers would require time
Œ©(ùëõ2ùëé).7
16.4.1 Application: self improving algorithm for 3SAT
Suppose that we found a polynomial-time algorithm ùê¥for 3SAT that
is ‚Äúgood but not great‚Äù. For example, maybe our algorithm runs in
time ùëêùëõ2 for some not too small constant ùëê. However, it‚Äôs possible
that the best possible SAT algorithm is actually much more efficient
than that. Perhaps, as we guessed before, there is a circuit ùê∂‚àóof at
most 106ùëõgates that computes 3SAT on ùëõvariables, and we simply
haven‚Äôt discovered it yet. We can use Theorem 16.6 to ‚Äúbootstrap‚Äù our
original ‚Äúgood but not great‚Äù 3SAT algorithm to discover the optimal
one. The idea is that we can phrase the question of whether there
exists a size ùë†circuit that computes 3SAT for all length ùëõinputs as
follows: there exists a size ‚â§ùë†circuit ùê∂such that for every formula ùúë
described by a string of length at most ùëõ, if ùê∂(ùúë) = 1 then there exists
an assignment ùë•to the variables of ùúëthat satisfies it. One can see that
this is a statement of the form (16.5) and hence if P = NP we can solve
it in polynomial time as well. We can therefore imagine investing huge
computational resources in running ùê¥one time to discover the circuit
ùê∂‚àóand then using ùê∂‚àófor all further computation.


--- Page 510 ---

510
introduction to theoretical computer science
16.5 APPROXIMATING COUNTING PROBLEMS AND POSTERIOR
SAMPLING (ADVANCED, OPTIONAL)
Given a Boolean circuit ùê∂, if P = NP then we can find an input ùë•(if
one exists) such that ùê∂(ùë•) = 1. But what if there is more than one ùë•
like that? Clearly we can‚Äôt efficiently output all such ùë•‚Äôs; there might
be exponentially many. But we can get an arbitrarily good multiplica-
tive approximation (i.e., a 1¬±ùúñfactor for arbitrarily small ùúñ> 0) for the
number of such ùë•‚Äôs, as well as output a (nearly) uniform member of
this set. The details are beyond the scope of this book, but this result is
formally stated in the following theorem (whose proof is omitted).
Theorem 16.7 ‚Äî Approximate counting if P = NP. Let ùëâ‚à∂{0, 1}‚àó‚Üí{0, 1}
be some polynomial-time algorithm, and suppose that P
=
NP.
Then there exists an algorithm COUNTùëâthat on input ùë•, 1ùëö, ùúñ,
runs in time polynomial in |ùë•|, ùëö, 1/ùúñand outputs a number in
[2ùëö+ 1] satisfying
(1‚àíùúñ)COUNTùëâ(ùë•, ùëö, ùúñ) ‚â§‚à£{ùë¶‚àà{0, 1}ùëö‚à∂ùëâ(ùë•ùë¶) = 1}‚à£‚â§(1+ùúñ)COUNTùëâ(ùë•, ùëö, ùúñ) .
(16.11)
In other words, the algorithm COUNTùëâgives an approximation
up to a factor of 1 ¬± ùúñfor the number of witnesses for ùë•with respect
to the verifying algorithm ùëâ. Once again, to understand this theorem
it can be useful to see how it implies that if P = NP then there is a
polynomial-time algorithm that given a graph ùê∫and a number ùëò,
can compute a number ùêæthat is within a 1 ¬± 0.01 factor equal to the
number of simple paths in ùê∫of length ùëò. (That is, ùêæis between 0.99 to
1.01 times the number of such paths.)
Posterior sampling and probabilistic programming.
The algorithm for count-
ing can also be extended to sampling from a given posterior distri-
bution. That is, if ùê∂‚à∂{0, 1}ùëõ‚Üí{0, 1}ùëöis a Boolean circuit and
ùë¶‚àà{0, 1}ùëö, then if P = NP we can sample from (a close approx-
imation of) the distribution of uniform ùë•‚àà{0, 1}ùëõconditioned on
ùê∂(ùë•) = ùë¶. This task is known as posterior sampling and is crucial for
Bayesian data analysis. These days it is known how to achieve pos-
terior sampling only for circuits ùê∂of very special form, and even in
these cases more often than not we do have guarantees on the quality
of the sampling algorithm. The field of making inferences by sampling
from posterior distribution specified by circuits or programs is known
as probabilistic programming.


--- Page 511 ---

what if p equals np?
511
8 One interesting theory is that P = NP and evolution
has already discovered this algorithm, which we are
already using without realizing it. At the moment,
there seems to be very little evidence for such a sce-
nario. In fact, we have some partial results in the
other direction showing that, regardless of whether
P = NP, many types of ‚Äúlocal search‚Äù or ‚Äúevolution-
ary‚Äù algorithms require exponential time to solve
3SAT and other NP-hard problems.
16.6 WHAT DOES ALL OF THIS IMPLY?
So, what will happen if we have a 106ùëõalgorithm for 3SAT? We have
mentioned that NP-hard problems arise in many contexts, and indeed
scientists, engineers, programmers and others routinely encounter
such problems in their daily work. A better 3SAT algorithm will prob-
ably make their lives easier, but that is the wrong place to look for
the most foundational consequences. Indeed, while the invention of
electronic computers did of course make it easier to do calculations
that people were already doing with mechanical devices and pen and
paper, the main applications computers are used for today were not
even imagined before their invention.
An exponentially faster algorithm for all NP problems would be
no less radical an improvement (and indeed, in some sense would
be more) than the computer itself, and it is as hard for us to imagine
what it would imply as it was for Babbage to envision today‚Äôs world.
For starters, such an algorithm would completely change the way we
program computers. Since we could automatically find the ‚Äúbest‚Äù
(in any measure we chose) program that achieves a certain task, we
would not need to define how to achieve a task, but only specify tests
as to what would be a good solution, and could also ensure that a
program satisfies an exponential number of tests without actually
running them.
The possibility that P = NP is often described as ‚Äúautomating
creativity‚Äù. There is something to that analogy, as we often think of
a creative solution as one that is hard to discover but that, once the
‚Äúspark‚Äù hits, is easy to verify. But there is also an element of hubris
to that statement, implying that the most impressive consequence of
such an algorithmic breakthrough will be that computers would suc-
ceed in doing something that humans already do today. Nevertheless,
artificial intelligence, like many other fields, will clearly be greatly
impacted by an efficient 3SAT algorithm. For example, it is clearly
much easier to find a better Chess-playing algorithm when, given any
algorithm ùëÉ, you can find the smallest algorithm ùëÉ‚Ä≤ that plays Chess
better than ùëÉ. Moreover, as we mentioned above, much of machine
learning (and statistical reasoning in general) is about finding ‚Äúsim-
ple‚Äù concepts that explain the observed data, and if NP = P, we could
search for such concepts automatically for any notion of ‚Äúsimplicity‚Äù
we see fit. In fact, we could even ‚Äúskip the middle man‚Äù and do an
automatic search for the learning algorithm with smallest general-
ization error. Ultimately the field of Artificial Intelligence is about
trying to ‚Äúshortcut‚Äù billions of years of evolution to obtain artificial
programs that match (or beat) the performance of natural ones, and a
fast algorithm for NP would provide the ultimate shortcut.8


--- Page 512 ---

512
introduction to theoretical computer science
More generally, a faster algorithm for NP problems would be im-
mensely useful in any field where one is faced with computational or
quantitative problems‚àíwhich is basically all fields of science, math,
and engineering. This will not only help with concrete problems such
as designing a better bridge, or finding a better drug, but also with
addressing basic mysteries such as trying to find scientific theories or
‚Äúlaws of nature‚Äù. In a fascinating talk, physicist Nima Arkani-Hamed
discusses the effort of finding scientific theories in much the same lan-
guage as one would describe solving an NP problem, for which the
solution is easy to verify or seems ‚Äúinevitable‚Äù, once found, but that
requires searching through a huge landscape of possibilities to reach,
and that often can get ‚Äústuck‚Äù at local optima:
‚Äúthe laws of nature have this amazing feeling of inevitability‚Ä¶ which is associ-
ated with local perfection.‚Äù
‚ÄúThe classical picture of the world is the top of a local mountain in the space of
ideas. And you go up to the top and it looks amazing up there and absolutely
incredible. And you learn that there is a taller mountain out there. Find it,
Mount Quantum‚Ä¶. they‚Äôre not smoothly connected ‚Ä¶ you‚Äôve got to make a
jump to go from classical to quantum ‚Ä¶ This also tells you why we have such
major challenges in trying to extend our understanding of physics. We don‚Äôt
have these knobs, and little wheels, and twiddles that we can turn. We have to
learn how to make these jumps. And it is a tall order. And that‚Äôs why things are
difficult.‚Äù
Finding an efficient algorithm for NP amounts to always being able
to search through an exponential space and find not just the ‚Äúlocal‚Äù
mountain, but the tallest peak.
But perhaps more than any computational speedups, a fast algo-
rithm for NP problems would bring about a new type of understanding.
In many of the areas where NP-completeness arises, it is not as much
a barrier for solving computational problems as it is a barrier for ob-
taining ‚Äúclosed-form formulas‚Äù or other types of more constructive
descriptions of the behavior of natural, biological, social and other sys-
tems. A better algorithm for NP, even if it is ‚Äúmerely‚Äù 2
‚àöùëõ-time, seems
to require obtaining a new way to understand these types of systems,
whether it is characterizing Nash equilibria, spin-glass configurations,
entangled quantum states, or any of the other questions where NP is
currently a barrier for analytical understanding. Such new insights
would be very fruitful regardless of their computational utility.
ÔÉ´Big Idea 23 If P = NP, we can efficiently solve a fantastic number
of decision, search, optimization, counting, and sampling problems
from all areas of human endeavors.


--- Page 513 ---

what if p equals np?
513
9 This inefficiency is not necessarily inherent. Later
in this course we may discuss results in program-
checking, interactive proofs, and average-case com-
plexity, that can be used for efficient verification of
proofs of related statements. In contrast, the ineffi-
ciency of verifying failure of all programs could well
be inherent.
16.7 CAN P ‚â†NP BE NEITHER TRUE NOR FALSE?
The Continuum Hypothesis is a conjecture made by Georg Cantor in
1878, positing the non-existence of a certain type of infinite cardinality.
(One way to phrase it is that for every infinite subset ùëÜof the real
numbers ‚Ñù, either there is a one-to-one and onto function ùëì‚à∂ùëÜ‚Üí‚Ñù
or there is a one-to-one and onto function ùëì‚à∂ùëÜ‚Üí‚Ñï.) This was
considered one of the most important open problems in set theory,
and settling its truth or falseness was the first problem put forward by
Hilbert in the 1900 address we mentioned before. However, using the
theories developed by G√∂del and Turing, in 1963 Paul Cohen proved
that both the Continuum Hypothesis and its negation are consistent
with the standard axioms of set theory (i.e., the Zermelo-Fraenkel
axioms + the Axiom of choice, or ‚ÄúZFC‚Äù for short). Formally, what
he proved is that if ZFC is consistent, then so is ZFC when we assume
either the continuum hypothesis or its negation.
Today, many (though not all) mathematicians interpret this result
as saying that the Continuum Hypothesis is neither true nor false, but
rather is an axiomatic choice that we are free to make one way or the
other. Could the same hold for P ‚â†NP?
In short, the answer is No. For example, suppose that we are try-
ing to decide between the ‚Äú3SAT is easy‚Äù conjecture (there is an 106ùëõ
time algorithm for 3SAT) and the ‚Äú3SAT is hard‚Äù conjecture (for ev-
ery ùëõ, any NAND-CIRC program that solves ùëõvariable 3SAT takes
210‚àí6ùëõlines). Then, since for ùëõ= 108, 210‚àí6ùëõ> 106ùëõ, this boils down
to the finite question of deciding whether or not there is a 1013-line
NAND-CIRC program deciding 3SAT on formulas with 108 variables.
If there is such a program then there is a finite proof of its existence,
namely the approximately 1TB file describing the program, and for
which the verification is the (finite in principle though infeasible in
practice) process of checking that it succeeds on all inputs.9 If there
isn‚Äôt such a program, then there is also a finite proof of that, though
any such proof would take longer since we would need to enumer-
ate over all programs as well. Ultimately, since it boils down to a finite
statement about bits and numbers; either the statement or its negation
must follow from the standard axioms of arithmetic in a finite number
of arithmetic steps. Thus, we cannot justify our ignorance in distin-
guishing between the ‚Äú3SAT easy‚Äù and ‚Äú3SAT hard‚Äù cases by claiming
that this might be an inherently ill-defined question. Similar reason-
ing (with different numbers) applies to other variants of the P vs NP
question. We note that in the case that 3SAT is hard, it may well be
that there is no short proof of this fact using the standard axioms, and
this is a question that people have been studying in various restricted
forms of proof systems.


--- Page 514 ---

514
introduction to theoretical computer science
10 Actually, the computational difficulty of problems in
economics such as finding optimal (or any) equilibria
is quite subtle. Some variants of such problems are
NP-hard, while others have a certain ‚Äúintermediate‚Äù
complexity.
11 Talk more about coping with NP hardness. Main
two approaches are heuristics such as SAT solvers that
succeed on some instances, and proxy measures such
as mathematical relaxations that instead of solving
problem ùëã(e.g., an integer program) solve program
ùëã‚Ä≤ (e.g., a linear program) that is related to that.
Maybe give compressed sensing as an example, and
least square minimization as a proxy for maximum
apostoriori probability.
16.8 IS P = NP ‚ÄúIN PRACTICE‚Äù?
The fact that a problem is NP-hard means that we believe there is no
efficient algorithm that solve it in the worst case. It does not, however,
mean that every single instance of the problem is hard. For exam-
ple, if all the clauses in a 3SAT instance ùúëcontain the same variable
ùë•ùëñ(possibly in negated form), then by guessing a value to ùë•ùëñwe can
reduce ùúëto a 2SAT instance which can then be efficiently solved. Gen-
eralizations of this simple idea are used in ‚ÄúSAT solvers‚Äù, which are
algorithms that have solved certain specific interesting SAT formulas
with thousands of variables, despite the fact that we believe SAT to
be exponentially hard in the worst case. Similarly, a lot of problems
arising in economics and machine learning are NP-hard.10 And yet
vendors and customers manage to figure out market-clearing prices
(as economists like to point out, there is milk on the shelves) and mice
succeed in distinguishing cats from dogs. Hence people (and ma-
chines) seem to regularly succeed in solving interesting instances of
NP-hard problems, typically by using some combination of guessing
while making local improvements.
It is also true that there are many interesting instances of NP-hard
problems that we do not currently know how to solve. Across all ap-
plication areas, whether it is scientific computing, optimization, con-
trol or more, people often encounter hard instances of NP problems
on which our current algorithms fail. In fact, as we will see, all of our
digital security infrastructure relies on the fact that some concrete and
easy-to-generate instances of, say, 3SAT (or, equivalently, any other
NP-hard problem) are exponentially hard to solve.
Thus it would be wrong to say that NP is easy ‚Äúin practice‚Äù, nor
would it be correct to take NP-hardness as the ‚Äúfinal word‚Äù on the
complexity of a problem, particularly when we have more informa-
tion about how any given instance is generated. Understanding both
the ‚Äútypical complexity‚Äù of NP problems, as well as the power and
limitations of certain heuristics (such as various local-search based al-
gorithms) is a very active area of research. We will see more on these
topics later in this course.
11
16.9 WHAT IF P ‚â†NP?
So, P = NP would give us all kinds of fantastical outcomes. But we
strongly suspect that P ‚â†NP, and moreover that there is no much-
better-than-brute-force algorithm for 3SAT. If indeed that is the case, is
it all bad news?
One might think that impossibility results, telling you that you
cannot do something, is the kind of cloud that does not have a silver


--- Page 515 ---

what if p equals np?
515
lining. But in fact, as we already alluded to before, it does. A hard
(in a sufficiently strong sense) problem in NP can be used to create a
code that cannot be broken, a task that for thousands of years has been
the dream of not just spies but of many scientists and mathematicians
over the generations. But the complexity viewpoint turned out to
yield much more than simple codes, achieving tasks that people had
previously not even dared to dream of. These include the notion of
public key cryptography, allowing two people to communicate securely
without ever having exchanged a secret key; electronic cash, allowing
private and secure transaction without a central authority; and secure
multiparty computation, enabling parties to compute a joint function on
private inputs without revealing any extra information about it. Also,
as we will see, computational hardness can be used to replace the role
of randomness in many settings.
Furthermore, while it is often convenient to pretend that computa-
tional problems are simply handed to us, and that our job as computer
scientists is to find the most efficient algorithm for them, this is not
how things work in most computing applications. Typically even for-
mulating the problem to solve is a highly non-trivial task. When we
discover that the problem we want to solve is NP-hard, this might be a
useful sign that we used the wrong formulation for it.
Beyond all these, the quest to understand computational hardness
‚àíincluding the discoveries of lower bounds for restricted compu-
tational models, as well as new types of reductions (such as those
arising from ‚Äúprobabilistically checkable proofs‚Äù) ‚àíhas already had
surprising positive applications to problems in algorithm design, as
well as in coding for both communication and storage. This is not
surprising since, as we mentioned before, from group theory to the
theory of relativity, the pursuit of impossibility results has often been
one of the most fruitful enterprises of mankind.
‚úì
Chapter Recap
‚Ä¢ The question of whether P
=
NP is one of the
most important and fascinating questions of com-
puter science and science at large, touching on all
fields of the natural and social sciences, as well as
mathematics and engineering.
‚Ä¢ Our current evidence and understanding supports
the ‚ÄúSAT hard‚Äù scenario that there is no much-
better-than-brute-force algorithm for 3SAT or many
other NP-hard problems.
‚Ä¢ We are very far from proving this, however. Re-
searchers have studied proving lower bounds on
the number of gates to compute explicit functions
in restricted forms of circuits, and have made some


--- Page 516 ---

516
introduction to theoretical computer science
advances in this effort, along the way generating
mathematical tools that have found other uses.
However, we have made essentially no headway
in proving lower bounds for general models of
computation such as Boolean circuits and Turing
machines. Indeed, we currently do not even know
how to rule out the possibility that for every ùëõ‚àà‚Ñï,
SAT restricted to ùëõ-length inputs has a Boolean
circuit of less than 10ùëõgates (even though there
exist ùëõ-input functions that require at least 2ùëõ/(10ùëõ)
gates to compute).
‚Ä¢ Understanding how to cope with this compu-
tational intractability, and even benefit from it,
comprises much of the research in theoretical
computer science.
16.10 EXERCISES
16.11 BIBLIOGRAPHICAL NOTES
As mentioned before, Aaronson‚Äôs survey [Aar16] is a great exposition
of the P vs NP problem. Another recommended survey by Aaronson
is [Aar05] which discusses the question of whether NP complete
problems could be computed by any physical means.
The paper [BU11] discusses some results about problems in the
polynomial hierarchy.


--- Page 517 ---

17
Space bounded computation
PLAN: Example of space bounded algorithms, importance of pre-
serving space. The classes L and PSPACE, space hierarchy theorem,
PSPACE=NPSPACE, constant space = regular languages.
17.1 EXERCISES
17.2 BIBLIOGRAPHICAL NOTES
Compiled on 8.26.2020 18:10


--- Page 518 ---



--- Page 519 ---

IV
RANDOMIZED COMPUTATION


--- Page 520 ---



--- Page 521 ---

18
Probability Theory 101
‚ÄúGod doesn‚Äôt play dice with the universe‚Äù, Albert Einstein
‚ÄúEinstein was doubly wrong ‚Ä¶ not only does God definitely play dice, but He
sometimes confuses us by throwing them where they can‚Äôt be seen.‚Äù, Stephen
Hawking
‚Äú ‚ÄòThe probability of winning a battle has no place in our theory because it
does not belong to any [random experiment]. Probability cannot be applied
to this problem any more than the physical concept of work can be applied to
the ‚Äôwork‚Äô done by an actor reciting his part.‚Äù, Richard Von Mises, 1928
(paraphrased)
‚ÄúI am unable to see why ‚Äòobjectivity‚Äô requires us to interpret every probability
as a frequency in some random experiment; particularly when in most problems
probabilities are frequencies only in an imaginary universe invented just for the
purpose of allowing a frequency interpretation.‚Äù, E.T. Jaynes, 1976
Before we show how to use randomness in algorithms, let us do a
quick review of some basic notions in probability theory. This is not
meant to replace a course on probability theory, and if you have not
seen this material before, I highly recommend you look at additional
resources to get up to speed. Fortunately, we will not need many of
the advanced notions of probability theory, but, as we will see, even
the so-called ‚Äúsimple‚Äù setting of tossing ùëõcoins can lead to very subtle
and interesting issues.
18.1 RANDOM COINS
The nature of randomness and probability is a topic of great philo-
sophical, scientific and mathematical depth. Is there actual random-
ness in the world, or does it proceed in a deterministic clockwork fash-
ion from some initial conditions set at the beginning of time? Does
probability refer to our uncertainty of beliefs, or to the frequency of
occurrences in repeated experiments? How can we define probability
over infinite sets?
Compiled on 8.26.2020 18:10
Learning Objectives:
‚Ä¢ Review the basic notion of probability theory
that we will use.
‚Ä¢ Sample spaces, and in particular the space
{0, 1}ùëõ
‚Ä¢ Events, probabilities of unions and
intersections.
‚Ä¢ Random variables and their expectation,
variance, and standard deviation.
‚Ä¢ Independence and correlation for both events
and random variables.
‚Ä¢ Markov, Chebyshev and Chernoff tail bounds
(bounding the probability that a random
variable will deviate from its expectation).


--- Page 522 ---

522
introduction to theoretical computer science
Figure 18.1: The probabilistic experiment of tossing
three coins corresponds to making 2 √ó 2 √ó 2 = 8
choices, each with equal probability. In this example,
the blue set corresponds to the event ùê¥= {ùë•‚àà
{0, 1}3 | ùë•0 = 0} where the first coin toss is equal
to 0, and the pink set corresponds to the event ùêµ=
{ùë•‚àà{0, 1}3 | ùë•1 = 1} where the second coin toss is
equal to 1 (with their intersection having a purplish
color). As we can see, each of these events contains 4
elements (out of 8 total) and so has probability 1/2.
The intersection of ùê¥and ùêµcontains two elements,
and so the probability that both of these events occur
is 2/8 = 1/4.
Figure 18.2: The event that if we toss three coins
ùë•0, ùë•1, ùë•2 ‚àà{0, 1} then the sum of the ùë•ùëñ‚Äôs is even
has probability 1/2 since it corresponds to exactly 4
out of the 8 possible strings of length 3.
These are all important questions that have been studied and de-
bated by scientists, mathematicians, statisticians and philosophers.
Fortunately, we will not need to deal directly with these questions
here. We will be mostly interested in the setting of tossing ùëõrandom,
unbiased and independent coins. Below we define the basic proba-
bilistic objects of events and random variables when restricted to this
setting. These can be defined for much more general probabilistic ex-
periments or sample spaces, and later on we will briefly discuss how
this can be done. However, the ùëõ-coin case is sufficient for almost
everything we‚Äôll need in this course.
If instead of ‚Äúheads‚Äù and ‚Äútails‚Äù we encode the sides of each coin
by ‚Äúzero‚Äù and ‚Äúone‚Äù, we can encode the result of tossing ùëõcoins as
a string in {0, 1}ùëõ. Each particular outcome ùë•‚àà{0, 1}ùëõis obtained
with probability 2‚àíùëõ. For example, if we toss three coins, then we
obtain each of the 8 outcomes 000, 001, 010, 011, 100, 101, 110, 111
with probability 2‚àí3 = 1/8 (see also Fig. 18.1). We can describe the
experiment of tossing ùëõcoins as choosing a string ùë•uniformly at
random from {0, 1}ùëõ, and hence we‚Äôll use the shorthand ùë•‚àº{0, 1}ùëõ
for ùë•that is chosen according to this experiment.
An event is simply a subset ùê¥of {0, 1}ùëõ. The probability of ùê¥, de-
noted by Prùë•‚àº{0,1}ùëõ[ùê¥] (or Pr[ùê¥] for short, when the sample space is
understood from the context), is the probability that an ùë•chosen uni-
formly at random will be contained in ùê¥. Note that this is the same as
|ùê¥|/2ùëõ(where |ùê¥| as usual denotes the number of elements in the set
ùê¥). For example, the probability that ùë•has an even number of ones
is Pr[ùê¥] where ùê¥= {ùë•‚à∂‚àë
ùëõ‚àí1
ùëñ=0 ùë•ùëñ
= 0 mod 2}. In the case ùëõ= 3,
ùê¥= {000, 011, 101, 110}, and hence Pr[ùê¥] = 4
8 = 1
2 (see Fig. 18.2). It
turns out this is true for every ùëõ:
Lemma 18.1 For every ùëõ> 0,
Pr
ùë•‚àº{0,1}ùëõ[
ùëõ‚àí1
‚àë
ùëñ=0
ùë•ùëñis even ] = 1/2
(18.1)
P
To test your intuition on probability, try to stop here
and prove the lemma on your own.
Proof of Lemma 18.1. We prove the lemma by induction on ùëõ. For the
case ùëõ= 1 it is clear since ùë•= 0 is even and ùë•= 1 is odd, and hence
the probability that ùë•‚àà{0, 1} is even is 1/2. Let ùëõ> 1. We assume
by induction that the lemma is true for ùëõ‚àí1 and we will prove it
for ùëõ. We split the set {0, 1}ùëõinto four disjoint sets ùê∏0, ùê∏1, ùëÇ0, ùëÇ1,
where for ùëè‚àà{0, 1}, ùê∏ùëèis defined as the set of ùë•‚àà{0, 1}ùëõsuch that


--- Page 523 ---

probability theory 101
523
ùë•0 ‚ãØùë•ùëõ‚àí2 has even number of ones and ùë•ùëõ‚àí1 = ùëèand similarly ùëÇùëèis
the set of ùë•‚àà{0, 1}ùëõsuch that ùë•0 ‚ãØùë•ùëõ‚àí2 has odd number of ones and
ùë•ùëõ‚àí1 = ùëè. Since ùê∏0 is obtained by simply extending ùëõ‚àí1-length string
with even number of ones by the digit 0, the size of ùê∏0 is simply the
number of such ùëõ‚àí1-length strings which by the induction hypothesis
is 2ùëõ‚àí1/2 = 2ùëõ‚àí2. The same reasoning applies for ùê∏1, ùëÇ0, and ùëÇ1.
Hence each one of the four sets ùê∏0, ùê∏1, ùëÇ0, ùëÇ1 is of size 2ùëõ‚àí2. Since
ùë•‚àà{0, 1}ùëõhas an even number of ones if and only if ùë•‚ààùê∏0 ‚à™ùëÇ1
(i.e., either the first ùëõ‚àí1 coordinates sum up to an even number and
the final coordinate is 0 or the first ùëõ‚àí1 coordinates sum up to an odd
number and the final coordinate is 1), we get that the probability that
ùë•satisfies this property is
|ùê∏0‚à™ùëÇ1|
2ùëõ
= 2ùëõ‚àí2 + 2ùëõ‚àí2
2ùëõ
= 1
2 ,
(18.2)
using the fact that ùê∏0 and ùëÇ1 are disjoint and hence |ùê∏0 ‚à™ùëÇ1| =
|ùê∏0| + |ùëÇ1|.
‚ñ†
We can also use the intersection (‚à©) and union (‚à™) operators to
talk about the probability of both event ùê¥and event ùêµhappening, or
the probability of event ùê¥or event ùêµhappening. For example, the
probability ùëùthat ùë•has an even number of ones and ùë•0 = 1 is the same
as Pr[ùê¥‚à©ùêµ] where ùê¥= {ùë•‚àà{0, 1}ùëõ‚à∂‚àë
ùëõ‚àí1
ùëñ=0 ùë•ùëñ= 0 mod 2} and
ùêµ= {ùë•‚àà{0, 1}ùëõ‚à∂ùë•0 = 1}. This probability is equal to 1/4 for
ùëõ> 1. (It is a great exercise for you to pause here and verify that you
understand why this is the case.)
Because intersection corresponds to considering the logical AND
of the conditions that two events happen, while union corresponds
to considering the logical OR, we will sometimes use the ‚àßand ‚à®
operators instead of ‚à©and ‚à™, and so write this probability ùëù= Pr[ùê¥‚à©
ùêµ] defined above also as
Pr
ùë•‚àº{0,1}ùëõ[‚àë
ùëñ
ùë•ùëñ= 0
mod 2 ‚àßùë•0 = 1] .
(18.3)
If ùê¥‚äÜ{0, 1}ùëõis an event, then ùê¥= {0, 1}ùëõ‚ßµùê¥corresponds to the
event that ùê¥does not happen. Since |ùê¥| = 2ùëõ‚àí|ùê¥|, we get that
Pr[ùê¥] = |ùê¥|
2ùëõ= 2ùëõ‚àí|ùê¥|
2ùëõ
= 1 ‚àí|ùê¥|
2ùëõ= 1 ‚àíPr[ùê¥]
(18.4)
This makes sense: since ùê¥happens if and only if ùê¥does not happen,
the probability of ùê¥should be one minus the probability of ùê¥.
R


--- Page 524 ---

524
introduction to theoretical computer science
Remark 18.2 ‚Äî Remember the sample space. While the
above definition might seem very simple and almost
trivial, the human mind seems not to have evolved for
probabilistic reasoning, and it is surprising how often
people can get even the simplest settings of probability
wrong. One way to make sure you don‚Äôt get confused
when trying to calculate probability statements is
to always ask yourself the following two questions:
(1) Do I understand what is the sample space that
this probability is taken over?, and (2) Do I under-
stand what is the definition of the event that we are
analyzing?.
For example, suppose that I were to randomize seating
in my course, and then it turned out that students
sitting in row 7 performed better on the final: how
surprising should we find this? If we started out with
the hypothesis that there is something special about
the number 7 and chose it ahead of time, then the
event that we are discussing is the event ùê¥that stu-
dents sitting in number 7 had better performance on
the final, and we might find it surprising. However, if
we first looked at the results and then chose the row
whose average performance is best, then the event
we are discussing is the event ùêµthat there exists some
row where the performance is higher than the over-
all average. ùêµis a superset of ùê¥, and its probability
(even if there is no correlation between sitting and
performance) can be quite significant.
18.1.1 Random variables
Events correspond to Yes/No questions, but often we want to analyze
finer questions. For example, if we make a bet at the roulette wheel,
we don‚Äôt want to just analyze whether we won or lost, but also how
much we‚Äôve gained. A (real valued) random variable is simply a way
to associate a number with the result of a probabilistic experiment.
Formally, a random variable is a function ùëã‚à∂{0, 1}ùëõ‚Üí‚Ñùthat maps
every outcome ùë•‚àà{0, 1}ùëõto an element ùëã(ùë•) ‚àà‚Ñù. For example, the
function SUM ‚à∂{0, 1}ùëõ‚Üí‚Ñùthat maps ùë•to the sum of its coordinates
(i.e., to ‚àë
ùëõ‚àí1
ùëñ=0 ùë•ùëñ) is a random variable.
The expectation of a random variable ùëã, denoted by ùîº[ùëã], is the
average value that this number takes, taken over all draws from the
probabilistic experiment. In other words, the expectation of ùëãis de-
fined as follows:
ùîº[ùëã] =
‚àë
ùë•‚àà{0,1}ùëõ
2‚àíùëõùëã(ùë•) .
(18.5)
If ùëãand ùëåare random variables, then we can define ùëã+ ùëåas
simply the random variable that maps a point ùë•‚àà{0, 1}ùëõto ùëã(ùë•) +
ùëå(ùë•). One basic and very useful property of the expectation is that it
is linear:


--- Page 525 ---

probability theory 101
525
Lemma 18.3 ‚Äî Linearity of expectation.
ùîº[ùëã+ ùëå] = ùîº[ùëã] + ùîº[ùëå]
(18.6)
Proof.
ùîº[ùëã+ ùëå] =
‚àë
ùë•‚àà{0,1}ùëõ
2‚àíùëõ(ùëã(ùë•) + ùëå(ùë•)) =
‚àë
ùë•‚àà{0,1}ùëè
2‚àíùëõùëã(ùë•) +
‚àë
ùë•‚àà{0,1}ùëè
2‚àíùëõùëå(ùë•) =
ùîº[ùëã] + ùîº[ùëå]
(18.7)
‚ñ†
Similarly, ùîº[ùëòùëã] = ùëòùîº[ùëã] for every ùëò‚àà‚Ñù.
Solved Exercise 18.1 ‚Äî Expectation of sum. Let ùëã‚à∂{0, 1}ùëõ‚Üí‚Ñùbe the
random variable that maps ùë•‚àà{0, 1}ùëõto ùë•0 + ùë•1 + ‚Ä¶ + ùë•ùëõ‚àí1. Prove
that ùîº[ùëã] = ùëõ/2.
‚ñ†
Solution:
We can solve this using the linearity of expectation. We can de-
fine random variables ùëã0, ùëã1, ‚Ä¶ , ùëãùëõ‚àí1 such that ùëãùëñ(ùë•) = ùë•ùëñ. Since
each ùë•ùëñequals 1 with probability 1/2 and 0 with probability 1/2,
ùîº[ùëãùëñ] = 1/2. Since ùëã= ‚àë
ùëõ‚àí1
ùëñ=0 ùëãùëñ, by the linearity of expectation
ùîº[ùëã] = ùîº[ùëã0] + ùîº[ùëã1] + ‚ãØ+ ùîº[ùëãùëõ‚àí1] = ùëõ
2 .
(18.8)
‚ñ†
P
If you have not seen discrete probability before, please
go over this argument again until you are sure you
follow it; it is a prototypical simple example of the
type of reasoning we will employ again and again in
this course.
If ùê¥is an event, then 1ùê¥is the random variable such that 1ùê¥(ùë•)
equals 1 if ùë•‚ààùê¥, and 1ùê¥(ùë•) = 0 otherwise. Note that Pr[ùê¥] = ùîº[1ùê¥]
(can you see why?). Using this and the linearity of expectation, we
can show one of the most useful bounds in probability theory:
Lemma 18.4 ‚Äî Union bound. For every two events ùê¥, ùêµ, Pr[ùê¥‚à™ùêµ] ‚â§
Pr[ùê¥] + Pr[ùêµ]
P
Before looking at the proof, try to see why the union
bound makes intuitive sense. We can also prove
it directly from the definition of probabilities and


--- Page 526 ---

526
introduction to theoretical computer science
Figure 18.3: The union bound tells us that the proba-
bility of ùê¥or ùêµhappening is at most the sum of the
individual probabilities. We can see it by noting that
for every two sets |ùê¥‚à™ùêµ| ‚â§|ùê¥| + |ùêµ| (with equality
only if ùê¥and ùêµhave no intersection).
the cardinality of sets, together with the equation
|ùê¥‚à™ùêµ|
‚â§
|ùê¥| + |ùêµ|. Can you see why the latter
equation is true? (See also Fig. 18.3.)
Proof of Lemma 18.4. For every ùë•, the variable 1ùê¥‚à™ùêµ(ùë•) ‚â§1ùê¥(ùë•)+1ùêµ(ùë•).
Hence, Pr[ùê¥‚à™ùêµ] = ùîº[1ùê¥‚à™ùêµ] ‚â§ùîº[1ùê¥+1ùêµ] = ùîº[1ùê¥]+ùîº[1ùêµ] = Pr[ùê¥]+Pr[ùêµ].
‚ñ†
The way we often use this in theoretical computer science is to
argue that, for example, if there is a list of 100 bad events that can hap-
pen, and each one of them happens with probability at most 1/10000,
then with probability at least 1 ‚àí100/10000 = 0.99, no bad event
happens.
18.1.2 Distributions over strings
While most of the time we think of random variables as having
as output a real number, we sometimes consider random vari-
ables whose output is a string. That is, we can think of a map
ùëå‚à∂{0, 1}ùëõ‚Üí{0, 1}‚àóand consider the ‚Äúrandom variable‚Äù ùëåsuch
that for every ùë¶‚àà{0, 1}‚àó, the probability that ùëåoutputs ùë¶is equal
to
1
2ùëõ|{ùë•‚àà{0, 1}ùëõ| ùëå(ùë•) = ùë¶}|. To avoid confusion, we will typically
refer to such string-valued random variables as distributions over
strings. So, a distribution ùëåover strings {0, 1}‚àócan be thought of as
a finite collection of strings ùë¶0, ‚Ä¶ , ùë¶ùëÄ‚àí1 ‚àà{0, 1}‚àóand probabilities
ùëù0, ‚Ä¶ , ùëùùëÄ‚àí1 (which are non-negative numbers summing up to one),
so that Pr[ùëå= ùë¶ùëñ] = ùëùùëñ.
Two distributions ùëåand ùëå‚Ä≤ are identical if they assign the same
probability to every string. For example, consider the following two
functions ùëå, ùëå‚Ä≤ ‚à∂{0, 1}2 ‚Üí{0, 1}2. For every ùë•‚àà{0, 1}2, we define
ùëå(ùë•) = ùë•and ùëå‚Ä≤(ùë•) = ùë•0(ùë•0 ‚äïùë•1) where ‚äïis the XOR operations.
Although these are two different functions, they induce the same
distribution over {0, 1}2 when invoked on a uniform input. The distri-
bution ùëå(ùë•) for ùë•‚àº{0, 1}2 is of course the uniform distribution over
{0, 1}2. On the other hand ùëå‚Ä≤ is simply the map 00 ‚Ü¶00, 01 ‚Ü¶01,
10 ‚Ü¶11, 11 ‚Ü¶10 which is a permutation of ùëå.
18.1.3 More general sample spaces
While throughout most of this book we assume that the underlying
probabilistic experiment corresponds to tossing ùëõindependent coins,
all the claims we make easily generalize to sampling ùë•from a more
general finite or countable set ùëÜ(and not-so-easily generalizes to
uncountable sets ùëÜas well). A probability distribution over a finite set
ùëÜis simply a function ùúá‚à∂ùëÜ‚Üí[0, 1] such that ‚àëùë•‚ààùëÜùúá(ùë•) = 1. We
think of this as the experiment where we obtain every ùë•‚ààùëÜwith


--- Page 527 ---

probability theory 101
527
Figure 18.4: Two events ùê¥and ùêµare independent if
Pr[ùê¥‚à©ùêµ] = Pr[ùê¥] ‚ãÖPr[ùêµ]. In the two figures above,
the empty ùë•√ó ùë•square is the sample space, and ùê¥
and ùêµare two events in this sample space. In the left
figure, ùê¥and ùêµare independent, while in the right
figure they are negatively correlated, since ùêµis less
likely to occur if we condition on ùê¥(and vice versa).
Mathematically, one can see this by noticing that in
the left figure the areas of ùê¥and ùêµrespectively are
ùëé‚ãÖùë•and ùëè‚ãÖùë•, and so their probabilities are ùëé‚ãÖùë•
ùë•2 = ùëé
ùë•
and ùëè‚ãÖùë•
ùë•2 = ùëè
ùë•respectively, while the area of ùê¥‚à©ùêµis
ùëé‚ãÖùëèwhich corresponds to the probability ùëé‚ãÖùëè
ùë•2 . In the
right figure, the area of the triangle ùêµis ùëè‚ãÖùë•
2 which
corresponds to a probability of
ùëè
2ùë•, but the area of
ùê¥‚à©ùêµis ùëè‚Ä≤‚ãÖùëé
2
for some ùëè‚Ä≤ < ùëè. This means that the
probability of ùê¥‚à©ùêµis ùëè‚Ä≤‚ãÖùëé
2ùë•2 <
ùëè
2ùë•‚ãÖùëé
ùë•, or in other words
Pr[ùê¥‚à©ùêµ] < Pr[ùê¥] ‚ãÖPr[ùêµ].
probability ùúá(ùë•), and sometimes denote this as ùë•‚àºùúá. In particular,
tossing ùëõrandom coins corresponds to the probability distribution
ùúá‚à∂{0, 1}ùëõ‚Üí[0, 1] defined as ùúá(ùë•) = 2‚àíùëõfor every ùë•‚àà{0, 1}ùëõ. An
event ùê¥is a subset of ùëÜ, and the probability of ùê¥, which we denote by
Prùúá[ùê¥], is ‚àëùë•‚ààùê¥ùúá(ùë•). A random variable is a function ùëã‚à∂ùëÜ‚Üí‚Ñù, where
the probability that ùëã= ùë¶is equal to ‚àëùë•‚ààùëÜs.t. ùëã(ùë•)=ùë¶ùúá(ùë•).
18.2 CORRELATIONS AND INDEPENDENCE
One of the most delicate but important concepts in probability is the
notion of independence (and the opposing notion of correlations). Subtle
correlations are often behind surprises and errors in probability and
statistical analysis, and several mistaken predictions have been blamed
on miscalculating the correlations between, say, housing prices in
Florida and Arizona, or voter preferences in Ohio and Michigan. See
also Joe Blitzstein‚Äôs aptly named talk ‚ÄúConditioning is the Soul of
Statistics‚Äù. (Another thorny issue is of course the difference between
correlation and causation. Luckily, this is another point we don‚Äôt need to
worry about in our clean setting of tossing ùëõcoins.)
Two events ùê¥and ùêµare independent if the fact that ùê¥happens
makes ùêµneither more nor less likely to happen. For example, if we
think of the experiment of tossing 3 random coins ùë•‚àà{0, 1}3, and we
let ùê¥be the event that ùë•0 = 1 and ùêµthe event that ùë•0 + ùë•1 + ùë•2 ‚â•2,
then if ùê¥happens it is more likely that ùêµhappens, and hence these
events are not independent. On the other hand, if we let ùê∂be the event
that ùë•1 = 1, then because the second coin toss is not affected by the
result of the first one, the events ùê¥and ùê∂are independent.
The formal definition is that events ùê¥and ùêµare independent if
Pr[ùê¥‚à©ùêµ] = Pr[ùê¥] ‚ãÖPr[ùêµ]. If Pr[ùê¥‚à©ùêµ] > Pr[ùê¥] ‚ãÖPr[ùêµ] then we say
that ùê¥and ùêµare positively correlated, while if Pr[ùê¥‚à©ùêµ] < Pr[ùê¥] ‚ãÖPr[ùêµ]
then we say that ùê¥and ùêµare negatively correlated (see Fig. 18.1).
If we consider the above examples on the experiment of choosing
ùë•‚àà{0, 1}3 then we can see that
Pr[ùë•0 = 1] = 1
2
Pr[ùë•0 + ùë•1 + ùë•2 ‚â•2] = Pr[{011, 101, 110, 111}] = 4
8 = 1
2
(18.9)
but
Pr[ùë•0 = 1 ‚àßùë•0 +ùë•1 +ùë•2 ‚â•2] = Pr[{101, 110, 111}] = 3
8 > 1
2 ‚ãÖ1
2 (18.10)
and hence, as we already observed, the events {ùë•0 = 1} and {ùë•0 +
ùë•1 + ùë•2 ‚â•2} are not independent and in fact are positively correlated.
On the other hand, Pr[ùë•0 = 1 ‚àßùë•1 = 1] = Pr[{110, 111}] = 2
8 = 1
2 ‚ãÖ1
2
and hence the events {ùë•0 = 1} and {ùë•1 = 1} are indeed independent.


--- Page 528 ---

528
introduction to theoretical computer science
Figure 18.5: Consider the sample space {0, 1}ùëõand
the events ùê¥, ùêµ, ùê∂, ùê∑, ùê∏corresponding to ùê¥: ùë•0 = 1,
ùêµ: ùë•1 = 1, ùê∂: ùë•0 + ùë•1 + ùë•2 ‚â•2, ùê∑: ùë•0 + ùë•1 + ùë•2 =
0ùëöùëúùëë2 and ùê∑: ùë•0 + ùë•1 = 0ùëöùëúùëë2. We can see that
ùê¥and ùêµare independent, ùê∂is positively correlated
with ùê¥and positively correlated with ùêµ, the three
events ùê¥, ùêµ, ùê∑are mutually independent, and while
every pair out of ùê¥, ùêµ, ùê∏is independent, the three
events ùê¥, ùêµ, ùê∏are not mutually independent since
their intersection has probability 2
8 = 1
4 instead of
1
2 ‚ãÖ1
2 ‚ãÖ1
2 = 1
8 .
R
Remark 18.5 ‚Äî Disjointness vs independence. People
sometimes confuse the notion of disjointness and in-
dependence, but these are actually quite different. Two
events ùê¥and ùêµare disjoint if ùê¥‚à©ùêµ= ‚àÖ, which means
that if ùê¥happens then ùêµdefinitely does not happen.
They are independent if Pr[ùê¥‚à©ùêµ] = Pr[ùê¥] Pr[ùêµ] which
means that knowing that ùê¥happens gives us no infor-
mation about whether ùêµhappened or not. If ùê¥and ùêµ
have nonzero probability, then being disjoint implies
that they are not independent, since in particular it
means that they are negatively correlated.
Conditional probability:
If ùê¥and ùêµare events, and ùê¥happens with
nonzero probability then we define the probability that ùêµhappens
conditioned on ùê¥to be Pr[ùêµ|ùê¥] = Pr[ùê¥‚à©ùêµ]/ Pr[ùê¥]. This corresponds
to calculating the probability that ùêµhappens if we already know
that ùê¥happened. Note that ùê¥and ùêµare independent if and only if
Pr[ùêµ|ùê¥] = Pr[ùêµ].
More than two events:
We can generalize this definition to more than
two events. We say that events ùê¥1, ‚Ä¶ , ùê¥ùëòare mutually independent
if knowing that any set of them occurred or didn‚Äôt occur does not
change the probability that an event outside the set occurs. Formally,
the condition is that for every subset ùêº‚äÜ[ùëò],
Pr[‚àßùëñ‚ààùêºùê¥ùëñ] = ‚àè
ùëñ‚ààùêº
Pr[ùê¥ùëñ].
(18.11)
For example, if ùë•‚àº{0, 1}3, then the events {ùë•0 = 1}, {ùë•1 = 1} and
{ùë•2 = 1} are mutually independent. On the other hand, the events
{ùë•0 = 1}, {ùë•1 = 1} and {ùë•0 + ùë•1 = 0 mod 2} are not mutually
independent, even though every pair of these events is independent
(can you see why? see also Fig. 18.5).
18.2.1 Independent random variables
We say that two random variables ùëã‚à∂{0, 1}ùëõ‚Üí‚Ñùand ùëå‚à∂{0, 1}ùëõ‚Üí‚Ñù
are independent if for every ùë¢, ùë£‚àà‚Ñù, the events {ùëã= ùë¢} and {ùëå= ùë£}
are independent. (We use {ùëã= ùë¢} as shorthand for {ùë•| ùëã(ùë•) = ùë¢}.)
In other words, ùëãand ùëåare independent if Pr[ùëã= ùë¢‚àßùëå= ùë£] =
Pr[ùëã= ùë¢] Pr[ùëå= ùë£] for every ùë¢, ùë£‚àà‚Ñù. For example, if two random
variables depend on the result of tossing different coins then they are
independent:
Lemma 18.6 Suppose that ùëÜ= {ùë†0, ‚Ä¶ , ùë†ùëò‚àí1} and ùëá= {ùë°0, ‚Ä¶ , ùë°ùëö‚àí1} are
disjoint subsets of {0, ‚Ä¶ , ùëõ‚àí1} and let ùëã, ùëå‚à∂{0, 1}ùëõ‚Üí‚Ñùbe random
variables such that ùëã= ùêπ(ùë•ùë†0, ‚Ä¶ , ùë•ùë†ùëò‚àí1) and ùëå= ùê∫(ùë•ùë°0, ‚Ä¶ , ùë•ùë°ùëö‚àí1) for


--- Page 529 ---

probability theory 101
529
some functions ùêπ‚à∂{0, 1}ùëò‚Üí‚Ñùand ùê∫‚à∂{0, 1}ùëö‚Üí‚Ñù. Then ùëãand ùëå
are independent.
P
The notation in the lemma‚Äôs statement is a bit cum-
bersome, but at the end of the day, it simply says that
if ùëãand ùëåare random variables that depend on two
disjoint sets ùëÜand ùëáof coins (for example, ùëãmight
be the sum of the first ùëõ/2 coins, and ùëåmight be the
largest consecutive stretch of zeroes in the second ùëõ/2
coins), then they are independent.
Proof of Lemma 18.6. Let ùëé, ùëè‚àà‚Ñù, and let ùê¥= {ùë•‚àà{0, 1}ùëò‚à∂ùêπ(ùë•) = ùëé}
and ùêµ= {ùë•‚àà{0, 1}ùëö‚à∂ùêπ(ùë•) = ùëè}. Since ùëÜand ùëáare disjoint, we can
reorder the indices so that ùëÜ= {0, ‚Ä¶ , ùëò‚àí1} and ùëá= {ùëò, ‚Ä¶ , ùëò+ ùëö‚àí1}
without affecting any of the probabilities. Hence we can write Pr[ùëã=
ùëé‚àßùëã= ùëè] = |ùê∂|/2ùëõwhere ùê∂= {ùë•0, ‚Ä¶ , ùë•ùëõ‚àí1 ‚à∂(ùë•0, ‚Ä¶ , ùë•ùëò‚àí1) ‚àà
ùê¥‚àß(ùë•ùëò, ‚Ä¶ , ùë•ùëò+ùëö‚àí1) ‚ààùêµ}. Another way to write this using string
concatenation is that ùê∂= {ùë•ùë¶ùëß‚à∂ùë•‚ààùê¥, ùë¶‚ààùêµ, ùëß‚àà{0, 1}ùëõ‚àíùëò‚àíùëö}, and
hence |ùê∂| = |ùê¥||ùêµ|2ùëõ‚àíùëò‚àíùëö, which means that
|ùê∂|
2ùëõ= |ùê¥|
2ùëò
|ùêµ|
2ùëö2ùëõ‚àíùëò‚àíùëö
2ùëõ‚àíùëò‚àíùëö= Pr[ùëã= ùëé] Pr[ùëå= ùëè].
(18.12)
‚ñ†
If ùëãand ùëåare independent random variables then (letting ùëÜùëã, ùëÜùëå
denote the sets of all numbers that have positive probability of being
the output of ùëãand ùëå, respectively):
ùîº[XY] =
‚àë
ùëé‚ààùëÜùëã,ùëè‚ààùëÜùëå
Pr[ùëã= ùëé‚àßùëå= ùëè] ‚ãÖùëéùëè=(1)
‚àë
ùëé‚ààùëÜùëã,ùëè‚ààùëÜùëå
Pr[ùëã= ùëé] Pr[ùëå= ùëè] ‚ãÖùëéùëè=(2)
( ‚àë
ùëé‚ààùëÜùëã
Pr[ùëã= ùëé] ‚ãÖùëé) ( ‚àë
ùëè‚ààùëÜùëå
Pr[ùëå= ùëè] ‚ãÖùëè) =(3)
ùîº[ùëã] ùîº[ùëå]
(18.13)
where the first equality (=(1)) follows from the independence of ùëã
and ùëå, the second equality (=(2)) follows by ‚Äúopening the parenthe-
ses‚Äù of the righthand side, and the third equality (=(3)) follows from
the definition of expectation. (This is not an ‚Äúif and only if‚Äù; see Exer-
cise 18.3.)
Another useful fact is that if ùëãand ùëåare independent random
variables, then so are ùêπ(ùëã) and ùê∫(ùëå) for all functions ùêπ, ùê∫‚à∂‚Ñù‚Üí‚Ñù.
This is intuitively true since learning ùêπ(ùëã) can only provide us with
less information than does learning ùëãitself. Hence, if learning ùëã
does not teach us anything about ùëå(and so also about ùêπ(ùëå)) then


--- Page 530 ---

530
introduction to theoretical computer science
neither will learning ùêπ(ùëã). Indeed, to prove this we can write for
every ùëé, ùëè‚àà‚Ñù:
Pr[ùêπ(ùëã) = ùëé‚àßùê∫(ùëå) = ùëè] =
‚àë
ùë•s.t.ùêπ(ùë•)=ùëé,ùë¶s.t. ùê∫(ùë¶)=ùëè
Pr[ùëã= ùë•‚àßùëå= ùë¶] =
‚àë
ùë•s.t.ùêπ(ùë•)=ùëé,ùë¶s.t. ùê∫(ùë¶)=ùëè
Pr[ùëã= ùë•] Pr[ùëå= ùë¶] =
‚éõ
‚éú
‚éù
‚àë
ùë•s.t.ùêπ(ùë•)=ùëé
Pr[ùëã= ùë•]‚éû
‚éü
‚é†
‚ãÖ‚éõ
‚éú
‚éù
‚àë
ùë¶s.t.ùê∫(ùë¶)=ùëè
Pr[ùëå= ùë¶]‚éû
‚éü
‚é†
=
Pr[ùêπ(ùëã) = ùëé] Pr[ùê∫(ùëå) = ùëè].
(18.14)
18.2.2 Collections of independent random variables
We can extend the notions of independence to more than two random
variables: we say that the random variables ùëã0, ‚Ä¶ , ùëãùëõ‚àí1 are mutually
independent if for every ùëé0, ‚Ä¶ , ùëéùëõ‚àí1 ‚àà‚Ñù,
Pr [ùëã0 = ùëé0 ‚àß‚ãØ‚àßùëãùëõ‚àí1 = ùëéùëõ‚àí1] = Pr[ùëã0 = ùëé0] ‚ãØPr[ùëãùëõ‚àí1 = ùëéùëõ‚àí1].
(18.15)
And similarly, we have that
Lemma 18.7 ‚Äî Expectation of product of independent random variables. If
ùëã0, ‚Ä¶ , ùëãùëõ‚àí1 are mutually independent then
ùîº[
ùëõ‚àí1
‚àè
ùëñ=0
ùëãùëñ] =
ùëõ‚àí1
‚àè
ùëñ=0
ùîº[ùëãùëñ].
(18.16)
Lemma 18.8 ‚Äî Functions preserve independence. If ùëã0, ‚Ä¶ , ùëãùëõ‚àí1 are mu-
tually independent, and ùëå0, ‚Ä¶ , ùëåùëõ‚àí1 are defined as ùëåùëñ= ùêπùëñ(ùëãùëñ) for
some functions ùêπ0, ‚Ä¶ , ùêπùëõ‚àí1 ‚à∂‚Ñù‚Üí‚Ñù, then ùëå0, ‚Ä¶ , ùëåùëõ‚àí1 are mutually
independent as well.
P
We leave proving Lemma 18.7 and Lemma 18.8 as
Exercise 18.6 and Exercise 18.7. It is good idea for you
stop now and do these exercises to make sure you are
comfortable with the notion of independence, as we
will use it heavily later on in this course.
18.3 CONCENTRATION AND TAIL BOUNDS
The name ‚Äúexpectation‚Äù is somewhat misleading. For example, sup-
pose that you and I place a bet on the outcome of 10 coin tosses, where
if they all come out to be 1‚Äôs then I pay you 100,000 dollars and other-


--- Page 531 ---

probability theory 101
531
Figure 18.6: The probabilities that we obtain a partic-
ular sum when we toss ùëõ= 10, 20, 100, 1000 coins
converge quickly to the Gaussian/normal distribu-
tion.
Figure 18.7: Markov‚Äôs Inequality tells us that a non-
negative random variable ùëãcannot be much larger
than its expectation, with high probability. For exam-
ple, if the expectation of ùëãis ùúá, then the probability
that ùëã> 4ùúámust be at most 1/4, as otherwise just
the contribution from this part of the sample space
will be too large.
wise you pay me 10 dollars. If we let ùëã‚à∂{0, 1}10 ‚Üí‚Ñùbe the random
variable denoting your gain, then we see that
ùîº[ùëã] = 2‚àí10 ‚ãÖ100000 ‚àí(1 ‚àí2‚àí10)10 ‚àº90.
(18.17)
But we don‚Äôt really ‚Äúexpect‚Äù the result of this experiment to be for
you to gain 90 dollars. Rather, 99.9% of the time you will pay me 10
dollars, and you will hit the jackpot 0.01% of the times.
However, if we repeat this experiment again and again (with fresh
and hence independent coins), then in the long run we do expect your
average earning to be close to 90 dollars, which is the reason why
casinos can make money in a predictable way even though every
individual bet is random. For example, if we toss ùëõindependent and
unbiased coins, then as ùëõgrows, the number of coins that come up
ones will be more and more concentrated around ùëõ/2 according to the
famous ‚Äúbell curve‚Äù (see Fig. 18.6).
Much of probability theory is concerned with so called concentration
or tail bounds, which are upper bounds on the probability that a ran-
dom variable ùëãdeviates too much from its expectation. The first and
simplest one of them is Markov‚Äôs inequality:
Theorem 18.9 ‚Äî Markov‚Äôs inequality. If ùëãis a non-negative random
variable then for every ùëò> 1, Pr[ùëã‚â•ùëòùîº[ùëã]] ‚â§1/ùëò.
P
Markov‚Äôs Inequality is actually a very natural state-
ment (see also Fig. 18.7). For example, if you know
that the average (not the median!) household income
in the US is 70,000 dollars, then in particular you can
deduce that at most 25 percent of households make
more than 280,000 dollars, since otherwise, even if
the remaining 75 percent had zero income, the top
25 percent alone would cause the average income to
be larger than 70,000 dollars. From this example you
can already see that in many situations, Markov‚Äôs
inequality will not be tight and the probability of devi-
ating from expectation will be much smaller: see the
Chebyshev and Chernoff inequalities below.
Proof of Theorem 18.9. Let ùúá= ùîº[ùëã] and define ùëå= 1ùëã‚â•ùëòùúá. That
is, ùëå(ùë•) = 1 if ùëã(ùë•) ‚â•ùëòùúáand ùëå(ùë•) = 0 otherwise. Note that by
definition, for every ùë•, ùëå(ùë•) ‚â§ùëã/(ùëòùúá). We need to show ùîº[ùëå] ‚â§1/ùëò.
But this follows since ùîº[ùëå] ‚â§ùîº[ùëã/ùëò(ùúá)] = ùîº[ùëã]/(ùëòùúá) = ùúá/(ùëòùúá) = 1/ùëò.
‚ñ†


--- Page 532 ---

532
introduction to theoretical computer science
The averaging principle.
While the expectation of a random variable
ùëãis hardly always the ‚Äútypical value‚Äù, we can show that ùëãis guar-
anteed to achieve a value that is at least its expectation with positive
probability. For example, if the average grade in an exam is 87 points,
at least one student got a grade 87 or more on the exam. This is known
as the averaging principle, and despite its simplicity it is surprisingly
useful.
Lemma 18.10 Let ùëãbe a random variable, then Pr[ùëã‚â•ùîº[ùëã]] > 0.
Proof. Suppose towards the sake of contradiction that Pr[ùëã< ùîº[ùëã]] =
1. Then the random variable ùëå= ùîº[ùëã] ‚àíùëãis always positive. By
linearity of expectation ùîº[ùëå] = ùîº[ùëã] ‚àíùîº[ùëã] = 0. Yet by Markov, a
non-negative random variable ùëåwith ùîº[ùëå] = 0 must equal 0 with
probability 1, since the probability that ùëå> ùëò‚ãÖ0 = 0 is at most 1/ùëòfor
every ùëò> 1. Hence we get a contradiction to the assumption that ùëåis
always positive.
‚ñ†
18.3.1 Chebyshev‚Äôs Inequality
Markov‚Äôs inequality says that a (non-negative) random variable ùëã
can‚Äôt go too crazy and be, say, a million times its expectation, with
significant probability. But ideally we would like to say that with
high probability, ùëãshould be very close to its expectation, e.g., in the
range [0.99ùúá, 1.01ùúá] where ùúá= ùîº[ùëã]. In such a case we say that ùëãis
concentrated, and hence its expectation (i.e., mean) will be close to its
median and other ways of measuring ùëã‚Äôs ‚Äútypical value‚Äù. Chebyshev‚Äôs
inequality can be thought of as saying that ùëãis concentrated if it has a
small standard deviation.
A standard way to measure the deviation of a random variable
from its expectation is by using its standard deviation. For a random
variable ùëã, we define the variance of ùëãas Var[ùëã] = ùîº[(ùëã‚àíùúá)2]
where ùúá= ùîº[ùëã]; i.e., the variance is the average squared distance
of ùëãfrom its expectation. The standard deviation of ùëãis defined as
ùúé[ùëã] = ‚àöVar[ùëã]. (This is well-defined since the variance, being an
average of a square, is always a non-negative number.)
Using Chebyshev‚Äôs inequality, we can control the probability that
a random variable is too many standard deviations away from its
expectation.
Theorem 18.11 ‚Äî Chebyshev‚Äôs inequality. Suppose that ùúá
=
ùîº[ùëã] and
ùúé2 = Var[ùëã]. Then for every ùëò> 0, Pr[|ùëã‚àíùúá| ‚â•ùëòùúé] ‚â§1/ùëò2.
Proof. The proof follows from Markov‚Äôs inequality. We define the
random variable ùëå= (ùëã‚àíùúá)2. Then ùîº[ùëå] = Var[ùëã] = ùúé2, and hence


--- Page 533 ---

probability theory 101
533
Figure 18.8: In the normal distribution or the Bell curve,
the probability of deviating ùëòstandard deviations
from the expectation shrinks exponentially in ùëò2, and
specifically with probability at least 1 ‚àí2ùëí‚àíùëò2/2, a
random variable ùëãof expectation ùúáand standard
deviation ùúésatisfies ùúá‚àíùëòùúé‚â§ùëã‚â§ùúá+ùëòùúé. This figure
gives more precise bounds for ùëò= 1, 2, 3, 4, 5, 6.
(Image credit:Imran Baghirov)
by Markov the probability that ùëå> ùëò2ùúé2 is at most 1/ùëò2. But clearly
(ùëã‚àíùúá)2 ‚â•ùëò2ùúé2 if and only if |ùëã‚àíùúá| ‚â•ùëòùúé.
‚ñ†
One example of how to use Chebyshev‚Äôs inequality is the setting
when ùëã= ùëã1 + ‚ãØ+ ùëãùëõwhere ùëãùëñ‚Äôs are independent and identically
distributed (i.i.d for short) variables with values in [0, 1] where each
has expectation 1/2. Since ùîº[ùëã] = ‚àëùëñùîº[ùëãùëñ] = ùëõ/2, we would like to
say that ùëãis very likely to be in, say, the interval [0.499ùëõ, 0.501ùëõ]. Us-
ing Markov‚Äôs inequality directly will not help us, since it will only tell
us that ùëãis very likely to be at most 100ùëõ(which we already knew,
since it always lies between 0 and ùëõ). However, since ùëã1, ‚Ä¶ , ùëãùëõare
independent,
Var[ùëã1 + ‚ãØ+ ùëãùëõ] = Var[ùëã1] + ‚ãØ+ Var[ùëãùëõ] .
(18.18)
(We leave showing this to the reader as Exercise 18.8.)
For every random variable ùëãùëñin [0, 1], Var[ùëãùëñ] ‚â§1 (if the variable
is always in [0, 1], it can‚Äôt be more than 1 away from its expectation),
and hence (18.18) implies that Var[ùëã] ‚â§ùëõand hence ùúé[ùëã] ‚â§‚àöùëõ.
For large ùëõ, ‚àöùëõ‚â™0.001ùëõ, and in particular if ‚àöùëõ‚â§0.001ùëõ/ùëò, we can
use Chebyshev‚Äôs inequality to bound the probability that ùëãis not in
[0.499ùëõ, 0.501ùëõ] by 1/ùëò2.
18.3.2 The Chernoff bound
Chebyshev‚Äôs inequality already shows a connection between inde-
pendence and concentration, but in many cases we can hope for
a quantitatively much stronger result. If, as in the example above,
ùëã= ùëã1 + ‚Ä¶ + ùëãùëõwhere the ùëãùëñ‚Äôs are bounded i.i.d random variables
of mean 1/2, then as ùëõgrows, the distribution of ùëãwould be roughly
the normal or Gaussian distribution‚àíthat is, distributed according to
the bell curve (see Fig. 18.6 and Fig. 18.8). This distribution has the
property of being very concentrated in the sense that the probability of
deviating ùëòstandard deviations from the mean is not merely 1/ùëò2 as is
guaranteed by Chebyshev, but rather is roughly ùëí‚àíùëò2. Specifically, for
a normal random variable ùëãof expectation ùúáand standard deviation
ùúé, the probability that |ùëã‚àíùúá| ‚â•ùëòùúéis at most 2ùëí‚àíùëò2/2. That is, we have
an exponential decay of the probability of deviation.
The following extremely useful theorem shows that such expo-
nential decay occurs every time we have a sum of independent and
bounded variables. This theorem is known under many names in dif-
ferent communities, though it is mostly called the Chernoff bound in
the computer science literature:


--- Page 534 ---

534
introduction to theoretical computer science
Theorem 18.12 ‚Äî Chernoff/Hoeffding bound. If ùëã0, ‚Ä¶ , ùëãùëõ‚àí1 are i.i.d ran-
dom variables such that ùëãùëñ‚àà[0, 1] and ùîº[ùëãùëñ] = ùëùfor every ùëñ, then
for every ùúñ> 0
Pr[‚à£
ùëõ‚àí1
‚àë
ùëñ=0
ùëãùëñ‚àíùëùùëõ‚à£> ùúñùëõ] ‚â§2 ‚ãÖùëí‚àí2ùúñ2ùëõ.
(18.19)
We omit the proof, which appears in many texts, and uses Markov‚Äôs
inequality on i.i.d random variables ùëå0, ‚Ä¶ , ùëåùëõthat are of the form
ùëåùëñ= ùëíùúÜùëãùëñfor some carefully chosen parameter ùúÜ. See Exercise 18.11
for a proof of the simple (but highly useful and representative) case
where each ùëãùëñis {0, 1} valued and ùëù= 1/2. (See also Exercise 18.12
for a generalization.)
R
Remark 18.13 ‚Äî Slight simplification of Chernoff. Since ùëí
is roughly 2.7 (and in particular larger than 2),
(18.19) would still be true if we replaced its righthand
side with ùëí‚àí2ùúñ2ùëõ+1. For ùëõ
>
1/ùúñ2, the equation will
still be true if we replaced the righthand side with
the simpler ùëí‚àíùúñ2ùëõ. Hence we will sometimes use the
Chernoff bound as stating that for ùëã0, ‚Ä¶ , ùëãùëõ‚àí1 and ùëù
as above, ùëõ> 1/ùúñ2 then
Pr[‚à£
ùëõ‚àí1
‚àë
ùëñ=0
ùëãùëñ‚àíùëùùëõ‚à£> ùúñùëõ] ‚â§ùëí‚àíùúñ2ùëõ.
(18.20)
18.3.3 Application: Supervised learning and empirical risk minimization
Here is a nice application of the Chernoff bound. Consider the task
of supervised learning. You are given a set ùëÜof ùëõsamples of the form
(ùë•0, ùë¶0), ‚Ä¶ , (ùë•ùëõ‚àí1, ùë¶ùëõ‚àí1) drawn from some unknown distribution ùê∑
over pairs (ùë•, ùë¶). For simplicity we will assume that ùë•ùëñ‚àà{0, 1}ùëöand
ùë¶ùëñ‚àà{0, 1}. (We use here the concept of general distribution over the
finite set {0, 1}ùëö+1 as discussed in Section 18.1.3.) The goal is to find
a classifier ‚Ñé‚à∂{0, 1}ùëö‚Üí{0, 1} that will minimize the test error which
is the probability ùêø(‚Ñé) that ‚Ñé(ùë•) ‚â†ùë¶where (ùë•, ùë¶) is drawn from the
distribution ùê∑. That is, ùêø(‚Ñé) = Pr(ùë•,ùë¶)‚àºùê∑[‚Ñé(ùë•) ‚â†ùë¶].
One way to find such a classifier is to consider a collection ùíûof po-
tential classifiers and look at the classifier ‚Ñéin ùíûthat does best on the
training set ùëÜ. The classifier ‚Ñéis known as the empirical risk minimizer
(see also Section 12.1.6) . The Chernoff bound can be used to show
that as long as the number ùëõof samples is sufficiently larger than the
logarithm of |ùíû|, the test error ùêø(‚Ñé) will be close to its training error


--- Page 535 ---

probability theory 101
535
ÃÇùêøùëÜ(‚Ñé), which is defined as the fraction of pairs (ùë•ùëñ, ùë¶ùëñ) ‚ààùëÜthat it fails
to classify. (Equivalently,
ÃÇùêøùëÜ(‚Ñé) = 1
ùëõ‚àëùëñ‚àà[ùëõ] |‚Ñé(ùë•ùëñ) ‚àíùë¶ùëñ|.)
Theorem 18.14 ‚Äî Generalization of ERM. Let ùê∑be any distribution over
pairs (ùë•, ùë¶)
‚àà
{0, 1}ùëö+1 and ùíûbe any set of functions mapping
{0, 1}ùëöto {0, 1}. Then for every ùúñ, ùõø> 0, if ùëõ>
log |ùíû| log(1/ùõø)
ùúñ2
and ùëÜ
is a set of (ùë•0, ùë¶0), ‚Ä¶ , (ùë•ùëõ‚àí1, ùë¶ùëõ‚àí1) samples that are drawn indepen-
dently from ùê∑then
Pr
ùëÜ[‚àÄ‚Ñé‚ààùíû|ùêø(‚Ñé) ‚àí
ÃÇùêøùëÜ(‚Ñé)| ‚â§ùúñ] > 1 ‚àíùõø,
(18.21)
where the probability is taken over the choice of the set of samples
ùëÜ.
In particular if |ùíû| ‚â§2ùëòand ùëõ> ùëòlog(1/ùõø)
ùúñ2
then with probability at
least 1‚àíùõø, the classifier ‚Ñé‚àó‚ààùíûthat minimizes that empirical test er-
ror
ÃÇùêøùëÜ(ùê∂) satisfies ùêø(‚Ñé‚àó) ‚â§
ÃÇùêøùëÜ(‚Ñé‚àó) + ùúñ, and hence its test error is at
most ùúñworse than its training error.
Proof Idea:
The idea is to combine the Chernoff bound with the union bound.
Let ùëò= log |ùíû|. We first use the Chernoff bound to show that for
every fixed ‚Ñé‚ààùíû, if we choose ùëÜat random then the probability that
|ùêø(‚Ñé) ‚àí
ÃÇùêøùëÜ(‚Ñé)| > ùúñwill be smaller than
ùõø
2ùëò. We can then use the union
bound over all the 2ùëòmembers of ùíûto show that this will be the case
for every ‚Ñé.
‚ãÜ
Proof of Theorem 18.14. Set ùëò= log |ùíû| and so ùëõ> ùëòlog(1/ùõø)/ùúñ2. We
start by making the following claim
CLAIM: For every ‚Ñé‚ààùíû, the probability over ùëÜthat |ùêø(‚Ñé) ‚àí
ÃÇùêøùëÜ(‚Ñé)| ‚â•ùúñis smaller than ùõø/2ùëò.
We prove the claim using the Chernoff bound. Specifically, for ev-
ery such ‚Ñé, let us defined a collection of random variables ùëã0, ‚Ä¶ , ùëãùëõ‚àí1
as follows:
ùëãùëñ=
‚éß
{
‚é®
{
‚é©
1
‚Ñé(ùë•ùëñ) ‚â†ùë¶ùëñ
0
otherwise
.
(18.22)
Since the samples (ùë•0, ùë¶0), ‚Ä¶ , (ùë•ùëõ‚àí1, ùë¶ùëõ‚àí1) are drawn independently
from the same distribution ùê∑, the random variables ùëã0, ‚Ä¶ , ùëãùëõ‚àí1 are
independently and identically distributed. Moreover, for every ùëñ,
ùîº[ùëãùëñ] = ùêø(‚Ñé). Hence by the Chernoff bound (see (18.20)), the proba-
bility that | ‚àë
ùëõ
ùëñ=0 ùëãùëñ‚àíùëõ‚ãÖùêø(‚Ñé)| ‚â•ùúñùëõis at most ùëí‚àíùúñ2ùëõ< ùëí‚àíùëòlog(1/ùõø) < ùõø/2ùëò
(using the fact that ùëí> 2). Since
ÃÇùêø(‚Ñé) = 1
ùëõ‚àëùëñ‚àà[ùëõ] ùëãùëñ, this completes
the proof of the claim.


--- Page 536 ---

536
introduction to theoretical computer science
Given the claim, the theorem follows from the union bound. In-
deed, for every ‚Ñé‚ààùíû, define the ‚Äúbad event‚Äù ùêµ‚Ñéto be the event (over
the choice of ùëÜ) that |ùêø(‚Ñé) ‚àí
ÃÇùêøùëÜ(‚Ñé)| > ùúñ. By the claim Pr[ùêµ‚Ñé] < ùõø/2ùëò,
and hence by the union bound the probability that the union of ùêµ‚Ñéfor
all ‚Ñé‚àà‚Ñãhappens is smaller than |ùíû|ùõø/2ùëò= ùõø. If for every ‚Ñé‚ààùíû, ùêµ‚Ñé
does not happen, it means that for every ‚Ñé‚àà‚Ñã, |ùêø(‚Ñé) ‚àí
ÃÇùêøùëÜ(‚Ñé)| ‚â§ùúñ,
and so the probability of the latter event is larger than 1 ‚àíùõøwhich is
what we wanted to prove.
‚ñ†
‚úì
Chapter Recap
‚Ä¢ A basic probabilistic experiment corresponds to
tossing ùëõcoins or choosing ùë•uniformly at random
from {0, 1}ùëõ.
‚Ä¢ Random variables assign a real number to every
result of a coin toss. The expectation of a random
variable ùëãis its average value.
‚Ä¢ There are several concentration results, also known
as tail bounds showing that under certain condi-
tions, random variables deviate significantly from
their expectation only with small probability.
18.4 EXERCISES
Exercise 18.1 Suppose that we toss three independent fair coins ùëé, ùëè, ùëê‚àà
{0, 1}. What is the probability that the XOR of ùëé,ùëè, and ùëêis equal to 1?
What is the probability that the AND of these three values is equal to
1? Are these two events independent?
‚ñ†
Exercise 18.2 Give an example of random variables ùëã, ùëå‚à∂{0, 1}3 ‚Üí‚Ñù
such that ùîº[XY] ‚â†ùîº[ùëã] ùîº[ùëå].
‚ñ†
Exercise 18.3 Give an example of random variables ùëã, ùëå‚à∂{0, 1}3 ‚Üí‚Ñù
such that ùëãand ùëåare not independent but ùîº[XY] = ùîº[ùëã] ùîº[ùëå].
‚ñ†
Exercise 18.4 Let ùëõbe an odd number, and let ùëã‚à∂{0, 1}ùëõ‚Üí‚Ñùbe the
random variable defined as follows: for every ùë•‚àà{0, 1}ùëõ, ùëã(ùë•) = 1 if
‚àëùëñ=0 ùë•ùëñ> ùëõ/2 and ùëã(ùë•) = 0 otherwise. Prove that ùîº[ùëã] = 1/2.
‚ñ†
Exercise 18.5 ‚Äî standard deviation. 1. Give an example for a random
variable ùëãsuch that ùëã‚Äôs standard deviation is equal to ùîº[|ùëã‚àíùîº[ùëã]|]


--- Page 537 ---

probability theory 101
537
1 While you don‚Äôt need this to solve this exercise, this
is the function that maps ùëùto the entropy (as defined
in Exercise 18.9) of the ùëù-biased coin distribution over
{0, 1}, which is the function ùúá‚à∂{0, 1} ‚Üí[0, 1] s.y.
ùúá(0) = 1 ‚àíùëùand ùúá(1) = ùëù.
2 Hint: Use Stirling‚Äôs formula for approximating the
factorial function.
2. Give an example for a random variable ùëãsuch that ùëã‚Äôs standard
deviation is not equal to ùîº[|ùëã‚àíùîº[ùëã]|]
‚ñ†
Exercise 18.6 ‚Äî Product of expectations. Prove Lemma 18.7
‚ñ†
Exercise 18.7 ‚Äî Transformations preserve independence. Prove Lemma 18.8
‚ñ†
Exercise 18.8 ‚Äî Variance of independent random variables. Prove that if
ùëã0, ‚Ä¶ , ùëãùëõ‚àí1 are independent random variables then Var[ùëã0 + ‚ãØ+
ùëãùëõ‚àí1] = ‚àë
ùëõ‚àí1
ùëñ=0 Var[ùëãùëñ].
‚ñ†
Exercise 18.9 ‚Äî Entropy (challenge). Recall the definition of a distribution
ùúáover some finite set ùëÜ. Shannon defined the entropy of a distribution
ùúá, denoted by ùêª(ùúá), to be ‚àëùë•‚ààùëÜùúá(ùë•) log(1/ùúá(ùë•)). The idea is that if ùúá
is a distribution of entropy ùëò, then encoding members of ùúáwill require
ùëòbits, in an amortized sense. In this exercise we justify this definition.
Let ùúábe such that ùêª(ùúá) = ùëò.
1. Prove that for every one to one function ùêπ‚à∂ùëÜ‚Üí{0, 1}‚àó,
ùîºùë•‚àºùúá|ùêπ(ùë•)| ‚â•ùëò.
2. Prove that for every ùúñ, there is some ùëõand a one-to-one function
ùêπ‚à∂ùëÜùëõ‚Üí{0, 1}‚àó, such that ùîºùë•‚àºùúáùëõ|ùêπ(ùë•)| ‚â§ùëõ(ùëò+ ùúñ), where ùë•‚àºùúá
denotes the experiments of choosing ùë•0, ‚Ä¶ , ùë•ùëõ‚àí1 each independently
from ùëÜusing the distribution ùúá.
‚ñ†
Exercise 18.10 ‚Äî Entropy approximation to binomial. Let ùêª(ùëù) = ùëùlog(1/ùëù) +
(1 ‚àíùëù) log(1/(1 ‚àíùëù)).1 Prove that for every ùëù‚àà(0, 1) and ùúñ> 0, if ùëõis
large enough then2
2(ùêª(ùëù)‚àíùúñ)ùëõ( ùëõ
ùëùùëõ) ‚â§2(ùêª(ùëù)+ùúñ)ùëõ
(18.23)
where (ùëõ
ùëò) is the binomial coefficient
ùëõ!
ùëò!(ùëõ‚àíùëò)! which is equal to the
number of ùëò-size subsets of {0, ‚Ä¶ , ùëõ‚àí1}.
‚ñ†
Exercise 18.11 ‚Äî Chernoff using Stirling. 1. Prove that Prùë•‚àº{0,1}ùëõ[‚àëùë•ùëñ=
ùëò] = (ùëõ
ùëò)2‚àíùëõ.
2. Use this and Exercise 18.10 to prove (an approximate version of)
the Chernoff bound for the case that ùëã0, ‚Ä¶ , ùëãùëõ‚àí1 are i.i.d. random
variables over {0, 1} each equaling 0 and 1 with probability 1/2.
That is, prove that for every ùúñ> 0, and ùëã0, ‚Ä¶ , ùëãùëõ‚àí1 as above,
Pr[| ‚àë
ùëõ‚àí1
ùëñ=0 ùëãùëñ‚àíùëõ
2 | > ùúñùëõ] < 20.1‚ãÖùúñ2ùëõ.
‚ñ†


--- Page 538 ---

538
introduction to theoretical computer science
3 Hint: Bound the number of tuples ùëó0, ‚Ä¶ , ùëóùëõ‚àí1 such
that every ùëóùëñis even and ‚àëùëóùëñ= ùëòusing the Binomial
coefficient and the fact that in any such tuple there are
at most ùëò/2 distinct indices.
4 Hint: Set ùëò= 2‚åàùúñ2ùëõ/1000‚åâand then show that if the
event | ‚àëùëåùëñ| ‚â•ùúñùëõhappens then the random variable
(‚àëùëåùëñ)ùëòis a factor of ùúñ‚àíùëòlarger than its expectation.
Exercise 18.12 ‚Äî Poor man‚Äôs Chernoff. Exercise 18.11 establishes the Cher-
noff bound for the case that ùëã0, ‚Ä¶ , ùëãùëõ‚àí1 are i.i.d variables over {0, 1}
with expectation 1/2. In this exercise we use a slightly different
method (bounding the moments of the random variables) to estab-
lish a version of Chernoff where the random variables range over [0, 1]
and their expectation is some number ùëù‚àà[0, 1] that may be different
than 1/2. Let ùëã0, ‚Ä¶ , ùëãùëõ‚àí1 be i.i.d random variables with ùîºùëãùëñ= ùëùand
Pr[0 ‚â§ùëãùëñ‚â§1] = 1. Define ùëåùëñ= ùëãùëñ‚àíùëù.
1. Prove that for every ùëó0, ‚Ä¶ , ùëóùëõ‚àí1 ‚àà‚Ñï, if there exists one ùëñsuch that ùëóùëñ
is odd then ùîº[‚àè
ùëõ‚àí1
ùëñ=0 ùëåùëóùëñ
ùëñ] = 0.
2. Prove that for every ùëò, ùîº[(‚àë
ùëõ‚àí1
ùëñ=0 ùëåùëñ)ùëò] ‚â§(10ùëòùëõ)ùëò/2.3
3. Prove that for every ùúñ> 0, Pr[| ‚àëùëñùëåùëñ| ‚â•ùúñùëõ] ‚â•2‚àíùúñ2ùëõ/(10000 log 1/ùúñ).4
‚ñ†
Exercise 18.13 ‚Äî Sampling. Suppose that a country has 300,000,000 citi-
zens, 52 percent of which prefer the color ‚Äúgreen‚Äù and 48 percent of
which prefer the color ‚Äúorange‚Äù. Suppose we sample ùëõrandom citi-
zens and ask them their favorite color (assume they will answer truth-
fully). What is the smallest value ùëõamong the following choices so
that the probability that the majority of the sample answers ‚Äúgreen‚Äù is
at most 0.05?
a. 1,000
b. 10,000
c. 100,000
d. 1,000,000
‚ñ†
Exercise 18.14 Would the answer to Exercise 18.13 change if the country
had 300,000,000,000 citizens?
‚ñ†
Exercise 18.15 ‚Äî Sampling (2). Under the same assumptions as Exer-
cise 18.13, what is the smallest value ùëõamong the following choices so
that the probability that the majority of the sample answers ‚Äúgreen‚Äù is
at most 2‚àí100?
a. 1,000
b. 10,000
c. 100,000


--- Page 539 ---

probability theory 101
539
d. 1,000,000
e. It is impossible to get such low probability since there are fewer
than 2100 citizens.
‚ñ†
18.5 BIBLIOGRAPHICAL NOTES
There are many sources for more information on discrete probability,
including the texts referenced in Section 1.9. One particularly recom-
mended source for probability is Harvard‚Äôs STAT 110 class, whose
lectures are available on youtube and whose book is available online.
The version of the Chernoff bound that we stated in Theorem 18.12
is sometimes known as Hoeffding‚Äôs Inequality. Other variants of the
Chernoff bound are known as well, but all of them are equally good
for the applications of this book.


--- Page 540 ---



--- Page 541 ---

Figure 19.1: A 1947 entry in the log book of the Har-
vard MARK II computer containing an actual bug that
caused a hardware malfunction. By Courtesy of the
Naval Surface Warfare Center.
1 Some texts also talk about ‚ÄúLas Vegas algorithms‚Äù
that always return the right answer but whose run-
ning time is only polynomial on the average. Since
this Monte Carlo vs Las Vegas terminology is confus-
ing, we will not use these terms anymore, and simply
talk about randomized algorithms.
19
Probabilistic computation
‚Äúin 1946 .. (I asked myself) what are the chances that a Canfield solitaire laid
out with 52 cards will come out successfully? After spending a lot of time
trying to estimate them by pure combinatorial calculations, I wondered whether
a more practical method ‚Ä¶ might not be to lay it our say one hundred times and
simple observe and count‚Äù, Stanislaw Ulam, 1983
‚ÄúThe salient features of our method are that it is probabilistic ‚Ä¶ and with a
controllable miniscule probability of error.‚Äù, Michael Rabin, 1977
In early computer systems, much effort was taken to drive out
randomness and noise. Hardware components were prone to non-
deterministic behavior from a number of causes, whether it was vac-
uum tubes overheating or actual physical bugs causing short circuits
(see Fig. 19.1). This motivated John von Neumann, one of the early
computing pioneers, to write a paper on how to error correct computa-
tion, introducing the notion of redundancy.
So it is quite surprising that randomness turned out not just a hin-
drance but also a resource for computation, enabling us to achieve
tasks much more efficiently than previously known. One of the first
applications involved the very same John von Neumann. While he
was sick in bed and playing cards, Stan Ulam came up with the ob-
servation that calculating statistics of a system could be done much
faster by running several randomized simulations. He mentioned this
idea to von Neumann, who became very excited about it; indeed, it
turned out to be crucial for the neutron transport calculations that
were needed for development of the Atom bomb and later on the hy-
drogen bomb. Because this project was highly classified, Ulam, von
Neumann and their collaborators came up with the codeword ‚ÄúMonte
Carlo‚Äù for this approach (based on the famous casinos where Ulam‚Äôs
uncle gambled). The name stuck, and randomized algorithms are
known as Monte Carlo algorithms to this day.1
In this chapter, we will see some examples of randomized algo-
rithms that use randomness to compute a quantity in a faster or
Compiled on 8.26.2020 18:10
Learning Objectives:
‚Ä¢ See examples of randomized algorithms
‚Ä¢ Get more comfort with analyzing
probabilistic processes and tail bounds
‚Ä¢ Success amplification using tail bounds


--- Page 542 ---

542
introduction to theoretical computer science
simpler way than was known otherwise. We will describe the algo-
rithms in an informal / ‚Äúpseudo-code‚Äù way, rather than as NAND-
TM, NAND-RAM programs or Turing macines. In Chapter 20 we will
discuss how to augment the computational models we say before to
incorporate the ability to ‚Äútoss coins‚Äù.
19.1 FINDING APPROXIMATELY GOOD MAXIMUM CUTS
We start with the following example. Recall the maximum cut problem
of finding, given a graph ùê∫= (ùëâ, ùê∏), the cut that maximizes the num-
ber of edges. This problem is NP-hard, which means that we do not
know of any efficient algorithm that can solve it, but randomization
enables a simple algorithm that can cut at least half of the edges:
Theorem 19.1 ‚Äî Approximating max cut. There is an efficient probabilis-
tic algorithm that on input an ùëõ-vertex ùëö-edge graph ùê∫, outputs a
cut (ùëÜ, ùëÜ) that cuts at least ùëö/2 of the edges of ùê∫in expectation.
Proof Idea:
We simply choose a random cut: we choose a subset ùëÜof vertices by
choosing every vertex ùë£to be a member of ùëÜwith probability 1/2 in-
dependently. It‚Äôs not hard to see that each edge is cut with probability
1/2 and so the expected number of cut edges is ùëö/2.
‚ãÜ
Proof of Theorem 19.1. The algorithm is extremely simple:
Algorithm Random Cut:
Input: Graph ùê∫= (ùëâ, ùê∏) with ùëõvertices and ùëöedges. Denote ùëâ=
{ùë£0, ùë£1, ‚Ä¶ , ùë£ùëõ‚àí1}.
Operation:
1. Pick ùë•uniformly at random in {0, 1}ùëõ.
2. Let ùëÜ‚äÜùëâbe the set {ùë£ùëñ‚à∂ùë•ùëñ= 1 , ùëñ‚àà[ùëõ]} that includes all vertices
corresponding to coordinates of ùë•where ùë•ùëñ= 1.
3. Output the cut (ùëÜ, ùëÜ).
We claim that the expected number of edges cut by the algorithm is
ùëö/2. Indeed, for every edge ùëí‚ààùê∏, let ùëãùëíbe the random variable such
that ùëãùëí(ùë•) = 1 if the edge ùëíis cut by ùë•, and ùëãùëí(ùë•) = 0 otherwise. For
every such edge ùëí= {ùëñ, ùëó}, ùëãùëí(ùë•) = 1 if and only if ùë•ùëñ‚â†ùë•ùëó. Since the
pair (ùë•ùëñ, ùë•ùëó) obtains each of the values 00, 01, 10, 11 with probability
1/4, the probability that ùë•ùëñ‚â†ùë•ùëóis 1/2 and hence ùîº[ùëãùëí] = 1/2. If we let


--- Page 543 ---

probabilistic computation
543
ùëãbe the random variable corresponding to the total number of edges
cut by ùëÜ, then ùëã= ‚àëùëí‚ààùê∏ùëãùëíand hence by linearity of expectation
ùîº[ùëã] = ‚àë
ùëí‚ààùê∏
ùîº[ùëãùëí] = ùëö(1/2) = ùëö/2 .
(19.1)
‚ñ†
Randomized algorithms work in the worst case.
It is tempting of a random-
ized algorithm such as the one of Theorem 19.1 as an algorithm that
works for a ‚Äúrandom input graph‚Äù but it is actually much better than
that. The expectation in this theorem is not taken over the choice of
the graph, but rather only over the random choices of the algorithm. In
particular, for every graph ùê∫, the algorithm is guaranteed to cut half of
the edges of the input graph in expectation. That is,
ÔÉ´Big Idea 24 A randomized algorithm outputs the correct value
with good probability on every possible input.
We will define more formally what ‚Äúgood probability‚Äù means in
Chapter 20 but the crucial point is that this probability is always only
taken over the random choices of the algorithm, while the input is not
chosen at random.
19.1.1 Amplifying the success of randomized algorithms
Theorem 19.1 gives us an algorithm that cuts ùëö/2 edges in expectation.
But, as we saw before, expectation does not immediately imply con-
centration, and so a priori, it may be the case that when we run the
algorithm, most of the time we don‚Äôt get a cut matching the expecta-
tion. Luckily, we can amplify the probability of success by repeating
the process several times and outputting the best cut we find. We
start by arguing that the probability the algorithm above succeeds in
cutting at least ùëö/2 edges is not too tiny.
Lemma 19.2 The probability that a random cut in an ùëöedge graph cuts
at least ùëö/2 edges is at least 1/(2ùëö).
Proof Idea:
To see the idea behind the proof, think of the case that ùëö= 1000. In
this case one can show that we will cut at least 500 edges with proba-
bility at least 0.001 (and so in particular larger than 1/(2ùëö) = 1/2000).
Specifically, if we assume otherwise, then this means that with proba-
bility more than 0.999 the algorithm cuts 499 or fewer edges. But since
we can never cut more than the total of 1000 edges, given this assump-
tion, the highest value the expected number of edges cut is if we cut
exactly 499 edges with probability 0.999 and cut 1000 edges with prob-
ability 0.001. Yet even in this case the expected number of edges will


--- Page 544 ---

544
introduction to theoretical computer science
be 0.999 ‚ãÖ499 + 0.001 ‚ãÖ1000 < 500, which contradicts the fact that we‚Äôve
calculated the expectation to be at least 500 in Theorem 19.1.
‚ãÜ
Proof of Lemma 19.2. Let ùëùbe the probability that we cut at least ùëö/2
edges and suppose, towards a contradiction, that ùëù< 1/(2ùëö). Since
the number of edges cut is an integer, and ùëö/2 is a multiple of 0.5,
by definition of ùëù, with probability 1 ‚àíùëùwe cut at most ùëö/2 ‚àí0.5
edges. Moreover, since we can never cut more than ùëöedges, under
our assumption that ùëù< 1/(2ùëö), we can bound the expected number
of edges cut by
ùëùùëö+ (1 ‚àíùëù)(ùëö/2 ‚àí0.5) ‚â§ùëùùëö+ ùëö/2 ‚àí0.5
(19.2)
But if ùëù< 1/(2ùëö) then ùëùùëö< 0.5 and so the righthand side is smaller
than ùëö/2, which contradicts the fact that (as proven in Theorem 19.1)
the expected number of edges cut is at least ùëö/2.
‚ñ†
19.1.2 Success amplification
Lemma 19.2 shows that our algorithm succeeds at least some of the
time, but we‚Äôd like to succeed almost all of the time. The approach
to do that is to simply repeat our algorithm many times, with fresh
randomness each time, and output the best cut we get in one of these
repetitions. It turns out that with extremely high probability we will
get a cut of size at least ùëö/2. For example, if we repeat this experiment
2000ùëötimes, then (using the inequality (1 ‚àí1/ùëò)ùëò‚â§1/ùëí‚â§1/2) we
can show that the probability that we will never cut at least ùëö/2 edges
is at most
(1 ‚àí1/(2ùëö))2000ùëö‚â§2‚àí1000 .
(19.3)
More generally, the same calculations can be used to show the
following lemma:
Lemma 19.3 There is an algorithm that on input a graph ùê∫= (ùëâ, ùê∏) and
a number ùëò, runs in time polynomial in |ùëâ| and ùëòand outputs a cut
(ùëÜ, ùëÜ) such that
Pr[number of edges cut by (ùëÜ, ùëÜ) ‚â•|ùê∏|/2] ‚â•1 ‚àí2‚àíùëò.
(19.4)
Proof of Lemma 19.3. The algorithm will work as follows:
Algorithm AMPLIFY RANDOM CUT:
Input: Graph ùê∫= (ùëâ, ùê∏) with ùëõvertices and ùëöedges. Denote ùëâ=
{ùë£0, ùë£1, ‚Ä¶ , ùë£ùëõ‚àí1}. Number ùëò> 0.
Operation:


--- Page 545 ---

probabilistic computation
545
1. Repeat the following 200ùëòùëötimes:
a. Pick ùë•uniformly at random in {0, 1}ùëõ.
b. Let ùëÜ‚äÜùëâbe the set {ùë£ùëñ
‚à∂
ùë•ùëñ= 1 , ùëñ‚àà[ùëõ]} that includes all
vertices corresponding to coordinates of ùë•where ùë•ùëñ= 1.
c. If (ùëÜ, ùëÜ) cuts at least ùëö/2 then halt and output (ùëÜ, ùëÜ).
2. Output ‚Äúfailed‚Äù
We leave completing the analysis as an exercise to the reader (see
Exercise 19.1).
‚ñ†
19.1.3 Two-sided amplification
The analysis above relied on the fact that the maximum cut has one
sided error. By this we mean that if we get a cut of size at least ùëö/2
then we know we have succeeded. This is common for randomized
algorithms, but is not the only case. In particular, consider the task of
computing some Boolean function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}. A randomized
algorithm ùê¥for computing ùêπ, given input ùë•, might toss coins and suc-
ceed in outputting ùêπ(ùë•) with probability, say, 0.9. We say that ùê¥has
two sided errors if there is positive probability that ùê¥(ùë•) outputs 1 when
ùêπ(ùë•) = 0, and positive probability that ùê¥(ùë•) outputs 0 when ùêπ(ùë•) = 1.
In such a case, to simplify ùê¥‚Äôs success, we cannot simply repeat it ùëò
times and output 1 if a single one of those repetitions resulted in 1,
nor can we output 0 if a single one of the repetitions resulted in 0. But
we can output the majority value of these repetitions. By the Chernoff
bound (Theorem 18.12), with probability exponentially close to 1 (i.e.,
1 ‚àí2‚Ñ¶(ùëò)), the fraction of the repetitions where ùê¥will output ùêπ(ùë•)
will be at least, say 0.89, and in such cases we will of course output the
correct answer.
The above translates into the following theorem
Theorem 19.4 ‚Äî Two-sided amplification. If ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1} is a func-
tion such that there is a polynomial-time algorithm ùê¥satisfying
Pr[ùê¥(ùë•) = ùêπ(ùë•)] ‚â•0.51
(19.5)
for every ùë•‚àà
{0, 1}‚àó, then there is a polynomial time algorithm ùêµ
satisfying
Pr[ùêµ(ùë•) = ùêπ(ùë•)] ‚â•1 ‚àí2‚àí|ùë•|
(19.6)
for every ùë•‚àà{0, 1}‚àó.
We omit the proof of Theorem 19.4, since we will prove a more
general result later on in Theorem 20.5.


--- Page 546 ---

546
introduction to theoretical computer science
2 This question does have some significance to prac-
tice, since hardware that generates high quality
randomness at speed is nontrivial to construct.
19.1.4 What does this mean?
We have shown a probabilistic algorithm that on any ùëöedge graph
ùê∫, will output a cut of at least ùëö/2 edges with probability at least
1 ‚àí2‚àí1000. Does it mean that we can consider this problem as ‚Äúeasy‚Äù?
Should we be somewhat wary of using a probabilistic algorithm, since
it can sometimes fail?
First of all, it is important to emphasize that this is still a worst case
guarantee. That is, we are not assuming anything about the input
graph: the probability is only due to the internal randomness of the al-
gorithm. While a probabilistic algorithm might not seem as nice as a
deterministic algorithm that is guaranteed to give an output, to get a
sense of what a failure probability of 2‚àí1000 means, note that:
‚Ä¢ The chance of winning the Massachusetts Mega Million lottery is
one over (75)5 ‚ãÖ15 which is roughly 2‚àí35. So 2‚àí1000 corresponds to
winning the lottery about 300 times in a row, at which point you
might not care so much about your algorithm failing.
‚Ä¢ The chance for a U.S. resident to be struck by lightning is about
1/700000, which corresponds to about 2‚àí45 chance that you‚Äôll be
struck by lightning the very second that you‚Äôre reading this sen-
tence (after which again you might not care so much about the
algorithm‚Äôs performance).
‚Ä¢ Since the earth is about 5 billion years old, we can estimate the
chance that an asteroid of the magnitude that caused the dinosaurs‚Äô
extinction will hit us this very second to be about 2‚àí60. It is quite
likely that even a deterministic algorithm will fail if this happens.
So, in practical terms, a probabilistic algorithm is just as good as
a deterministic one. But it is still a theoretically fascinating ques-
tion whether randomized algorithms actually yield more power, or
whether is it the case that for any computational problem that can be
solved by probabilistic algorithm, there is a deterministic algorithm
with nearly the same performance.2 For example, we will see in Ex-
ercise 19.2 that there is in fact a deterministic algorithm that can cut
at least ùëö/2 edges in an ùëö-edge graph. We will discuss this question
in generality in Chapter 20. For now, let us see a couple of examples
where randomization leads to algorithms that are better in some sense
than the known deterministic algorithms.
19.1.5 Solving SAT through randomization
The 3SAT problem is NP hard, and so it is unlikely that it has a poly-
nomial (or even subexponential) time algorithm. But this does not
mean that we can‚Äôt do at least somewhat better than the trivial 2ùëõal-
gorithm for ùëõ-variable 3SAT. The best known worst-case algorithms


--- Page 547 ---

probabilistic computation
547
3 At the time of this writing, the best known ran-
domized algorithms for 3SAT run in time roughly
ùëÇ(1.308ùëõ), and the best known deterministic algo-
rithms run in time ùëÇ(1.3303ùëõ) in the worst case.
for 3SAT are randomized, and are related to the following simple
algorithm, variants of which are also used in practice:
Algorithm WalkSAT:
Input: An ùëõvariable 3CNF formula ùúë.
Parameters: ùëá, ùëÜ‚àà‚Ñï
Operation:
1. Repeat the following ùëásteps:
a. Choose a random assignment ùë•‚àà{0, 1}ùëõand repeat the following
for ùëÜsteps:
1. If ùë•satisfies ùúëthen output ùë•.
2. Otherwise, choose a random clause (‚Ñìùëñ‚à®‚Ñìùëó‚à®‚Ñìùëò) that ùë•does
not satisfy, choose a random literal in ‚Ñìùëñ, ‚Ñìùëó, ‚Ñìùëòand modify ùë•to
satisfy this literal.
2. If all the ùëá‚ãÖùëÜrepetitions above did not result in a satisfying assign-
ment then output Unsatisfiable
The running time of this algorithm is ùëÜ‚ãÖùëá‚ãÖùëùùëúùëôùë¶(ùëõ), and so the
key question is how small we can make ùëÜand ùëáso that the proba-
bility that WalkSAT outputs Unsatisfiable on a satisfiable formula
ùúëis small. It is known that we can do so with ST =
ÃÉùëÇ((4/3)ùëõ) =
ÃÉùëÇ(1.333 ‚Ä¶ùëõ) (see Exercise 19.4 for a weaker result), but we‚Äôll show
below a simpler analysis yielding ST =
ÃÉùëÇ(
‚àö
3
ùëõ) =
ÃÉùëÇ(1.74ùëõ), which is
still much better than the trivial 2ùëõbound.3
Theorem 19.5 ‚Äî WalkSAT simple analysis. If we set ùëá
=
100 ‚ãÖ
‚àö
3
ùëõand
ùëÜ
=
ùëõ/2, then the probability we output Unsatisfiable for a
satisfiable ùúëis at most 1/2.
Proof. Suppose that ùúëis a satisfiable formula and let ùë•‚àóbe a satisfying
assignment for it. For every ùë•‚àà{0, 1}ùëõ, denote by Œî(ùë•, ùë•‚àó) the num-
ber of coordinates that differ between ùë•and ùë•‚àó. The heart of the proof
is the following claim:
Claim I: For every ùë•, ùë•‚àóas above, in every local improvement step,
the value Œî(ùë•, ùë•‚àó) is decreased by one with probability at least 1/3.
Proof of Claim I: Since ùë•‚àóis a satisfying assignment, if ùê∂is a clause
that ùë•does not satisfy, then at least one of the variables involve in ùê∂
must get different values in ùë•and ùë•‚àó. Thus when we change ùë•by one
of the three literals in the clause, we have probability at least 1/3 of
decreasing the distance.
The second claim is that our starting point is not that bad:
Claim 2: With probability at least 1/2 over a random ùë•‚àà{0, 1}ùëõ,
Œî(ùë•, ùë•‚àó) ‚â§ùëõ/2.
Proof of Claim II: Consider the map FLIP ‚à∂{0, 1}ùëõ‚Üí{0, 1}ùëõthat
simply ‚Äúflips‚Äù all the bits of its input from 0 to 1 and vice versa. That


--- Page 548 ---

548
introduction to theoretical computer science
Figure 19.2: For every ùë•‚àó‚àà{0, 1}ùëõ, we can sort all
strings in {0, 1}ùëõaccording to their distance from
ùë•‚àó(top to bottom in the above figure), where we let
ùê¥= {ùë•‚àà{0, 1}ùëõ| ùëëùëñùë†ùë°(ùë•, ùë•‚àó‚â§ùëõ/2} be the ‚Äútop
half‚Äù of strings. If we define FLIP ‚à∂{0, 1}ùëõ‚Üí{0, 1}
to be the map that ‚Äúflips‚Äù the bits of a given string ùë•
then it maps every ùë•‚ààùê¥to an output FLIP(ùë•) ‚ààùê¥
in a one-to-one way, and so it demonstrates that
|ùê¥| ‚â§|ùê¥| which implies that Pr[ùê¥] ‚â•Pr[ùê¥] and hence
Pr[ùê¥] ‚â•1/2.
Figure 19.3: The bipartite matching problem in the
graph ùê∫= (ùêø‚à™ùëÖ, ùê∏) can be reduced to the minimum
ùë†, ùë°cut problem in the graph ùê∫‚Ä≤ obtained by adding
vertices ùë†, ùë°to ùê∫, connecting ùë†with ùêøand connecting
ùë°with ùëÖ.
is, FLIP(ùë•0, ‚Ä¶ , ùë•ùëõ‚àí1) = (1‚àíùë•0, ‚Ä¶ , 1‚àíùë•ùëõ‚àí1). Clearly FLIP is one to one.
Moreover, if ùë•is of distance ùëòto ùë•‚àó, then FLIP(ùë•) is distance ùëõ‚àíùëòto
ùë•‚àó. Now let ùêµbe the ‚Äúbad event‚Äù in which ùë•is of distance > ùëõ/2 from
ùë•‚àó. Then the set ùê¥= FLIP(ùêµ) = {FLIP(ùë•)
‚à∂
ùë•‚àà{0, 1}ùëõ} satisfies
|ùê¥| = |ùêµ| and that if ùë•‚ààùê¥then ùë•is of distance < ùëõ/2 from ùë•‚àó. Since
ùê¥and ùêµare disjoint events, Pr[ùê¥] + Pr[ùêµ] ‚â§1. Since they have the
same cardinality, they have the same probability and so we get that
2 Pr[ùêµ] ‚â§1 or Pr[ùêµ] ‚â§1/2. (See also Fig. 19.2).
Claims I and II imply that each of the ùëáiterations of the outer loop
succeeds with probability at least 1/2 ‚ãÖ
‚àö
3
‚àíùëõ. Indeed, by Claim II,
the original guess ùë•will satisfy Œî(ùë•, ùë•‚àó) ‚â§ùëõ/2 with probability
Pr[Œî(ùë•, ùë•‚àó) ‚â§ùëõ/2] ‚â•1/2. By Claim I, even conditioned on all the
history so far, for each of the ùëÜ= ùëõ/2 steps of the inner loop we have
probability at least ‚â•1/3 of being ‚Äúlucky‚Äù and decreasing the distance
(i.e. the output of Œî) by one. The chance we will be lucky in all ùëõ/2
steps is hence at least (1/3)ùëõ/2 =
‚àö
3
‚àíùëõ.
Since any single iteration of the outer loop succeeds with probabil-
ity at least 1
2 ‚ãÖ
‚àö
3
‚àíùëõ, the probability that we never do so in ùëá= 100
‚àö
3
ùëõ
repetitions is at most (1 ‚àí
1
2
‚àö
3
ùëõ)100‚ãÖ
‚àö
3
ùëõ
‚â§(1/ùëí)50.
‚ñ†
19.1.6 Bipartite matching
The matching problem is one of the canonical optimization problems,
arising in all kinds of applications: matching residents with hospitals,
kidney donors with patients, flights with crews, and many others.
One prototypical variant is bipartite perfect matching. In this problem,
we are given a bipartite graph ùê∫= (ùêø‚à™ùëÖ, ùê∏) which has 2ùëõvertices
partitioned into ùëõ-sized sets ùêøand ùëÖ, where all edges have one end-
point in ùêøand the other in ùëÖ. The goal is to determine whether there
is a perfect matching, a subset ùëÄ‚äÜùê∏of ùëõdisjoint edges. That is, ùëÄ
matches every vertex in ùêøto a unique vertex in ùëÖ.
The bipartite matching problem turns out to have a polynomial-
time algorithm, since we can reduce finding a matching in ùê∫to find-
ing a maximum flow (or equivalently, minimum cut) in a related
graph ùê∫‚Ä≤ (see Fig. 19.3). However, we will see a different probabilistic
algorithm to determine whether a graph contains such a matching.
Let us label ùê∫‚Äôs vertices as ùêø= {‚Ñì0, ‚Ä¶ , ‚Ñìùëõ‚àí1} and ùëÖ= {ùëü0, ‚Ä¶ , ùëüùëõ‚àí1}.
A matching ùëÄcorresponds to a permutation ùúã‚ààùëÜùëõ(i.e., one-to-one
and onto function ùúã‚à∂[ùëõ] ‚Üí[ùëõ]) where for every ùëñ‚àà[ùëõ], we define ùúã(ùëñ)
to be the unique ùëósuch that ùëÄcontains the edge {‚Ñìùëñ, ùëüùëó}. Define an
ùëõ√ó ùëõmatrix ùê¥= ùê¥(ùê∫) where ùê¥ùëñ,ùëó= 1 if and only if the edge {‚Ñìùëñ, ùëüùëó}
is present and ùê¥ùëñ,ùëó= 0 otherwise. The correspondence between
matchings and permutations implies the following claim:


--- Page 549 ---

probabilistic computation
549
4 The sign of a permutation ùúã‚à∂[ùëõ] ‚Üí[ùëõ], denoted by
ùë†ùëñùëîùëõ(ùúã), can be defined in several equivalent ways,
one of which is that ùë†ùëñùëîùëõ(ùúã) = (‚àí1)ùêºùëÅùëâ(ùúã) where
INV(ùëùùëñ) = |{(ùë•, ùë¶) ‚àà[ùëõ] | ùë•< ùë¶‚àßùúã(ùë•) > ùúã(ùë¶)} (i.e.,
INV(ùúã) is the number of pairs of elements that are
inverted by ùúã). The importance of the term ùë†ùëñùëîùëõ(ùúã) is
that it makes ùëÉequal to the determinant of the matrix
(ùë•ùëñ,ùëó) and hence efficiently computable.
Figure 19.4: A degree ùëëcurve in one variable can
have at most ùëëroots. In higher dimensions, a ùëõ-
variate degree-ùëëpolynomial can have an infinite
number roots though the set of roots will be an ùëõ‚àí1
dimensional surface. Over a finite field ùîΩ, an ùëõ-variate
degree ùëëpolynomial has at most ùëë|ùîΩ|ùëõ‚àí1 roots.
Lemma 19.6 ‚Äî Matching polynomial. Define ùëÉ= ùëÉ(ùê∫) to be the polynomial
mapping ‚Ñùùëõ2 to ‚Ñùwhere
ùëÉ(ùë•0,0, ‚Ä¶ , ùë•ùëõ‚àí1,ùëõ‚àí1) = ‚àë
ùúã‚ààùëÜùëõ
(
ùëõ‚àí1
‚àè
ùëñ=0
ùë†ùëñùëîùëõ(ùúã)ùê¥ùëñ,ùúã(ùëñ))
ùëõ‚àí1
‚àè
ùëñ=0
ùë•ùëñ,ùúã(ùëñ)
(19.7)
Then ùê∫has a perfect matching if and only if ùëÉis not identically zero.
That is, ùê∫has a perfect matching if and only if there exists some as-
signment ùë•= (ùë•ùëñ,ùëó)ùëñ,ùëó‚àà[ùëõ] ‚àà‚Ñùùëõ2 such that ùëÉ(ùë•) ‚â†0.4
Proof. If ùê∫has a perfect matching ùëÄ‚àó, then let ùúã‚àóbe the permutation
corresponding to ùëÄand let ùë•‚àó‚àà‚Ñ§ùëõ2 defined as follows: ùë•ùëñ,ùëó= 1 if
ùëó= ùúã(ùëñ) and ùë•ùëñ,ùëó= 0. Note that for every ùúã‚â†ùúã‚àó, ‚àè
ùëõ‚àí1
ùëñ=0 ùë•ùëñ,ùúã(ùëñ) = 0 but
‚àè
ùëõ‚àí1
ùëñ=0 ùë•‚àó
ùëñ,ùúã‚àó(ùëñ) = 1. Hence ùëÉ(ùë•‚àó) will equal ‚àè
ùëõ‚àí1
ùëñ=0 ùê¥ùëñ,ùúã‚àó(ùëñ). But since ùëÄ‚àóis
a perfect matching in ùê∫, ‚àè
ùëõ‚àí1
ùëñ=0 ùê¥ùëñ,ùúã‚àó(ùëñ) = 1.
On the other hand, suppose that ùëÉis not identically zero. By (19.7),
this means that at least one of the terms ‚àè
ùëõ‚àí1
ùëñ=0 ùê¥ùëñ,ùúã(ùëñ) is not equal to
zero. But then this permutation ùúãmust be a perfect matching in ùê∫.
‚ñ†
As we‚Äôve seen before, for every ùë•‚àà‚Ñùùëõ2, we can compute ùëÉ(ùë•)
by simply computing the determinant of the matrix ùê¥(ùë•), which is
obtained by replacing ùê¥ùëñ,ùëówith ùê¥ùëñ,ùëóùë•ùëñ,ùëó. This reduces testing perfect
matching to the zero testing problem for polynomials: given some poly-
nomial ùëÉ(‚ãÖ), test whether ùëÉis identically zero or not. The intuition
behind our randomized algorithm for zero testing is the following:
If a polynomial is not identically zero, then it can‚Äôt have ‚Äútoo many‚Äù roots.
This intuition sort of makes sense. For one variable polynomi-
als, we know that a nonzero linear function has at most one root, a
quadratic function (e.g., a parabola) has at most two roots, and gener-
ally a degree ùëëequation has at most ùëëroots. While in more than one
variable there can be an infinite number of roots (e.g., the polynomial
ùë•0 + ùë¶0 vanishes on the line ùë¶= ‚àíùë•) it is still the case that the set of
roots is very ‚Äúsmall‚Äù compared to the set of all inputs. For example,
the root of a bivariate polynomial form a curve, the roots of a three-
variable polynomial form a surface, and more generally the roots of an
ùëõ-variable polynomial are a space of dimension ùëõ‚àí1.
This intuition leads to the following simple randomized algorithm:
To decide if ùëÉis identically zero, choose a ‚Äúrandom‚Äù input ùë•and check if
ùëÉ(ùë•) ‚â†0.
This makes sense: if there are only ‚Äúfew‚Äù roots, then we expect that
with high probability the random input ùë•is not going to be one of
those roots. However, to transform this into an actual algorithm, we


--- Page 550 ---

550
introduction to theoretical computer science
need to make both the intuition and the notion of a ‚Äúrandom‚Äù input
precise. Choosing a random real number is quite problematic, espe-
cially when you have only a finite number of coins at your disposal,
and so we start by reducing the task to a finite setting. We will use the
following result:
Theorem 19.7 ‚Äî Schwartz‚ÄìZippel lemma. For every integer ùëû, and poly-
nomial ùëÉ
‚à∂
‚Ñùùëõ
‚Üí
‚Ñùwith integer coefficients. If ùëÉhas degree at
most ùëëand is not identically zero, then it has at most ùëëùëûùëõ‚àí1 roots in
the set [ùëû]ùëõ= {(ùë•0, ‚Ä¶ , ùë•ùëõ‚àí1) ‚à∂ùë•ùëñ‚àà{0, ‚Ä¶ , ùëû‚àí1}}.
We omit the (not too complicated) proof of Theorem 19.7. We
remark that it holds not just over the real numbers but over any field
as well. Since the matching polynomial ùëÉof Lemma 19.6 has degree at
most ùëõ, Theorem 19.7 leads directly to a simple algorithm for testing if
it is nonzero:
Algorithm Perfect-Matching:
Input: Bipartite graph ùê∫on 2ùëõvertices {‚Ñì0, ‚Ä¶ , ‚Ñìùëõ‚àí1, ùëü0, ‚Ä¶ , ùëüùëõ‚àí1}.
Operation:
1. For every ùëñ, ùëó‚àà[ùëõ], choose ùë•ùëñ,ùëóindependently at random from
[2ùëõ] = {0, ‚Ä¶ 2ùëõ‚àí1}.
2. Compute the determinant of the matrix ùê¥(ùë•) whose (ùëñ, ùëó)ùë°‚Ñéentry
equals ùë•ùëñ,ùëóif the edge {‚Ñìùëñ, ùëüùëó} is present and 0 otherwise.
3. Output no perfect matching if this determinant is zero, and out-
put perfect matching otherwise.
This algorithm can be improved further (e.g., see Exercise 19.5).
While it is not necessarily faster than the cut-based algorithms for per-
fect matching, it does have some advantages. In particular, it is more
amenable for parallelization. (However, it also has the significant dis-
advantage that it does not produce a matching but only states that one
exists.) The Schwartz‚ÄìZippel Lemma, and the associated zero testing
algorithm for polynomials, is widely used across computer science,
including in several settings where we have no known deterministic
algorithm matching their performance.
‚úì
Chapter Recap
‚Ä¢ Using concentration results, we can amplify in
polynomial time the success probability of a prob-
abilistic algorithm from a mere 1/ùëù(ùëõ) to 1 ‚àí2‚àíùëû(ùëõ)
for every polynomials ùëùand ùëû.
‚Ä¢ There are several randomized algorithms that are
better in various senses (e.g., simpler, faster, or
other advantages) than the best known determinis-
tic algorithm for the same problem.


--- Page 551 ---

probabilistic computation
551
5 TODO: add exercise to give a deterministic max cut
algorithm that gives ùëö/2 edges. Talk about greedy
approach.
6 Hint: Think of ùë•‚àà{0, 1}ùëõas choosing ùëònumbers
ùë¶1, ‚Ä¶ , ùë¶ùëò‚àà{0, ‚Ä¶ , 2‚åàlog ùëÄ‚åâ‚àí1}. Output the first such
number that is in {0, ‚Ä¶ , ùëÄ‚àí1}.
19.2 EXERCISES
Exercise 19.1 ‚Äî Amplification for max cut. Prove Lemma 19.3
‚ñ†
Exercise 19.2 ‚Äî Deterministic max cut algorithm. 5
‚ñ†
Exercise 19.3 ‚Äî Simulating distributions using coins. Our model for proba-
bility involves tossing ùëõcoins, but sometimes algorithm require sam-
pling from other distributions, such as selecting a uniform number in
{0, ‚Ä¶ , ùëÄ‚àí1} for some ùëÄ. Fortunately, we can simulate this with an
exponentially small probability of error: prove that for every ùëÄ, if ùëõ>
ùëò‚åàlog ùëÄ‚åâ, then there is a function ùêπ‚à∂{0, 1}ùëõ‚Üí{0, ‚Ä¶ , ùëÄ‚àí1} ‚à™{‚ä•}
such that (1) The probability that ùêπ(ùë•) = ‚ä•is at most 2‚àíùëòand (2) the
distribution of ùêπ(ùë•) conditioned on ùêπ(ùë•) ‚â†‚ä•is equal to the uniform
distribution over {0, ‚Ä¶ , ùëÄ‚àí1}.6
‚ñ†
Exercise 19.4 ‚Äî Better walksat analysis. 1. Prove that for every ùúñ> 0, if
ùëõis large enough then for every ùë•‚àó‚àà{0, 1}ùëõPrùë•‚àº{0,1}ùëõ[Œî(ùë•, ùë•‚àó) ‚â§
ùëõ/3] ‚â§2‚àí(1‚àíùêª(1/3)‚àíùúñ)ùëõwhere ùêª(ùëù) = ùëùlog(1/ùëù)+(1‚àíùëù) log(1/(1‚àíùëù))
is the same function as in Exercise 18.10.
2. Prove that 21‚àíùêª(1/4)+(1/4) log 3 = (3/2).
3. Use the above to prove that for every ùõø> 0 and large enough ùëõ, if
we set ùëá= 1000 ‚ãÖ(3/2 + ùõø)ùëõand ùëÜ= ùëõ/4 in the WalkSAT algorithm
then for every satisfiable 3CNF ùúë, the probability that we output
unsatisfiable is at most 1/2.
‚ñ†
Exercise 19.5 ‚Äî Faster bipartite matching (challenge). (to be completed: im-
prove the matching algorithm by working modulo a prime)
‚ñ†
19.3 BIBLIOGRAPHICAL NOTES
The books of Motwani and Raghavan [MR95] and Mitzenmacher and
Upfal [MU17] are two excellent resources for randomized algorithms.
Some of the history of the discovery of Monte Carlo algorithm is cov-
ered here.
19.4 ACKNOWLEDGEMENTS


--- Page 552 ---



--- Page 553 ---

Figure 20.1: A mechanical coin tosser built for Percy
Diaconis by Harvard technicians Steve Sansone and
Rick Haggerty
20
Modeling randomized computation
‚ÄúAny one who considers arithmetical methods of producing random digits is, of
course, in a state of sin.‚Äù John von Neumann, 1951.
So far we have described randomized algorithms in an informal
way, assuming that an operation such as ‚Äúpick a string ùë•‚àà{0, 1}ùëõ‚Äù
can be done efficiently. We have neglected to address two questions:
1. How do we actually efficiently obtain random strings in the physi-
cal world?
2. What is the mathematical model for randomized computations,
and is it more powerful than deterministic computation?
The first question is of both practical and theoretical importance,
but for now let‚Äôs just say that there are various physical sources of
‚Äúrandom‚Äù or ‚Äúunpredictable‚Äù data. A user‚Äôs mouse movements and
typing pattern, (non solid state) hard drive and network latency,
thermal noise, and radioactive decay have all been used as sources for
randomness (see discussion in Section 20.8). For example, many Intel
chips come with a random number generator built in. One can even
build mechanical coin tossing machines (see Fig. 20.1).
In this chapter we focus on the second question: formally modeling
probabilistic computation and studying its power. We will show that:
1. We can define the class BPP that captures all Boolean functions that
can be computed in polynomial time by a randomized algorithm.
Crucially BPP is still very much a worst case class of computation:
the probability is only over the choice of the random coins of the
algorithm, as opposed to the choice of the input.
2. We can amplify the success probability of randomized algorithms,
and as a result the class BPP would be identical if we changed
the required success probability to any number ùëùthat lies strictly
between 1/2 and 1 (and in fact any number in the range 1/2+1/ùëû(ùëõ)
to 1 ‚àí2‚àíùëû(ùëõ) for any polynomial ùëû(ùëõ)).
Compiled on 8.26.2020 18:10
Learning Objectives:
‚Ä¢ Formal definition of probabilistic polynomial
time: the class BPP.
‚Ä¢ Proof that every function in BPP can be
computed by ùëùùëúùëôùë¶(ùëõ)-sized NAND-CIRC
programs/circuits.
‚Ä¢ Relations between BPP and NP.
‚Ä¢ Pseudorandom generators


--- Page 554 ---

554
introduction to theoretical computer science
3. Though, as is the case for P and NP, there is much we do not know
about the class BPP, we can establish some relations between BPP
and the other complexity classes we saw before. In particular we
will show that P ‚äÜBPP ‚äÜEXP and BPP ‚äÜP/poly.
4. While the relation between BPP and NP is not known, we can show
that if P = NP then BPP = P.
5. We also show that the concept of NP completeness applies equally
well if we use randomized algorithms as our model of ‚Äúefficient
computation‚Äù. That is, if a single NP complete problem has a ran-
domized polynomial-time algorithm, then all of NP can be com-
puted in polynomial-time by randomized algorithms.
6. Finally we will discuss the question of whether BPP = P and show
some of the intriguing evidence that the answer might actually be
‚ÄúYes‚Äù using the concept of pseudorandom generators.
20.1 MODELING RANDOMIZED COMPUTATION
Modeling randomized computation is actually quite easy. We can
add the following operations to any programming language such as
NAND-TM, NAND-RAM, NAND-CIRC etc..:
foo = RAND()
where foo is a variable. The result of applying this operation is that
foo is assigned a random bit in {0, 1}. (Every time the RAND operation
is invoked it returns a fresh independent random bit.) We call the
programming languages that are augmented with this extra operation
RNAND-TM, RNAND-RAM, and RNAND-CIRC respectively.
Similarly, we can easily define randomized Turing machines as
Turing machines in which the transition function ùõøgets as an extra
input (in addition to the current state and symbol read from the tape)
a bit ùëèthat in each step is chosen at random ‚àà{0, 1}. Of course the
function can ignore this bit (and have the same output regardless of
whether ùëè= 0 or ùëè= 1) and hence randomized Turing machines
generalize deterministic Turing machines.
We can use the RAND() operation to define the notion of a function
being computed by a randomized ùëá(ùëõ) time algorithm for every nice
time bound ùëá‚à∂‚Ñï‚Üí‚Ñï, as well as the notion of a finite function being
computed by a size ùëÜrandomized NAND-CIRC program (or, equiv-
alently, a randomized circuit with ùëÜgates that correspond to either
the NAND or coin-tossing operations). However, for simplicity we
will not define randomized computation in full generality, but simply
focus on the class of functions that are computable by randomized
algorithms running in polynomial time, which by historical convention
is known as BPP:


--- Page 555 ---

modeling randomized computation
555
Definition 20.1 ‚Äî The class BPP. Let ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}. We say that
ùêπ
‚àà
BPP if there exist constants ùëé, ùëè
‚àà
‚Ñïand an RNAND-TM
program ùëÉsuch that for every ùë•‚àà{0, 1}‚àó, on input ùë•, the program
ùëÉhalts within at most ùëé|ùë•|ùëèsteps and
Pr[ùëÉ(ùë•) = ùêπ(ùë•)] ‚â•2
3
(20.1)
where this probability is taken over the result of the RAND opera-
tions of ùëÉ.
Note that the probability in (20.1) is taken only over the ran-
dom choices in the execution of ùëÉand not over the choice of the in-
put ùë•. In particular, as discussed in Big Idea 24, BPP is still a worst
case complexity class, in the sense that if ùêπis in BPP then there is a
polynomial-time randomized algorithm that computes ùêπwith proba-
bility at least 2/3 on every possible (and not just random) input.
The same polynomial-overhead simulation of NAND-RAM pro-
grams by NAND-TM programs we saw in Theorem 13.5 extends to
randomized programs as well. Hence the class BPP is the same re-
gardless of whether it is defined via RNAND-TM or RNAND-RAM
programs. Similarly, we could have just as well defined BPP using
randomized Turing machines.
Because of these equivalences, below we will use the name ‚Äúpoly-
nomial time randomized algorithm‚Äù to denote a computation that can be
modeled by a polynomial-time RNAND-TM program, RNAND-RAM
program, or a randomized Turing machine (or any programming lan-
guage that includes a coin tossing operation). Since all these models
are equivalent up to polynomial factors, you can use your favorite
model to capture polynomial-time randomized algorithms without
any loss in generality.
Solved Exercise 20.1 ‚Äî Choosing from a set. Modern programming lan-
guages often involve not just the ability to toss a random coin in {0, 1}
but also to choose an element at random from a set ùëÜ. Show that you
can emulate this primitive using coin tossing. Specifically, show that
there is randomized algorithm ùê¥that on input a set ùëÜof ùëöstrings of
length ùëõ, runs in time ùëùùëúùëôùë¶(ùëõ, ùëö) and outputs either an element ùë•‚ààùëÜ
or ‚Äúfail‚Äù such that
1. Let ùëùbe the probability that ùê¥outputs ‚Äúfail‚Äù, then ùëù< 2‚àíùëõ(a
number small enough that it can be ignored).
2. For every ùë•‚ààùëÜ, the probability that ùê¥outputs ùë•is exactly 1‚àíùëù
ùëö
(and so the output is uniform over ùëÜif we ignore the tiny probabil-
ity of failure)
‚ñ†


--- Page 556 ---

556
introduction to theoretical computer science
Figure 20.2: The two equivalent views of random-
ized algorithms. We can think of such an algorithm
as having access to an internal RAND() operation
that outputs a random independent value in {0, 1}
whenever it is invoked, or we can think of it as a de-
terministic algorithm that in addition to the standard
input ùë•‚àà{0, 1}ùëõobtains an additional auxiliary
input ùëü‚àà{0, 1}ùëöthat is chosen uniformly at random.
Solution:
If the size of ùëÜis a power of two, that is ùëö= 2‚Ñìfor some ‚Ñì‚ààùëÅ,
then we can choose a random element in ùëÜby tossing ‚Ñìcoins to
obtain a string ùë§
‚àà
{0, 1}‚Ñìand then output the ùëñ-th element of ùëÜ
where ùëñis the number whose binary representation is ùë§.
If ùëÜis not a power of two, then our first attempt will be to let
‚Ñì
=
‚åàlog ùëö‚åâand do the same, but then output the ùëñ-th element of
ùëÜif ùëñ
‚àà
[ùëö] and output ‚Äúfail‚Äù otherwise. Conditioned on not out-
putting ‚Äúfail‚Äù, this element is distributed uniformly in ùëÜ. However,
in the worst case, 2‚Ñìcan be almost 2ùëöand so the probability of fail
might be close to half. To reduce the failure probability, we can
repeat the experiment above ùëõtimes. Specifically, we will use the
following algorithm
Algorithm 20.2 ‚Äî Sample from set.
Input: Set ùëÜ= {ùë•0, ‚Ä¶ , ùë•ùëö‚àí1} with ùë•ùëñ‚àà{0, 1}ùëõfor all
ùëñ‚àà[ùëö].
Output: Either ùë•‚ààùëÜor ‚Äùfail‚Äù
1: Let ‚Ñì‚Üê‚åàlog ùëö‚åâ
2: for ùëó= 0, 1, ‚Ä¶ , ùëõ‚àí1 do
3:
Pick ùë§‚àº{0, 1}‚Ñì
4:
Let ùëñ‚àà[2‚Ñì] be number whose binary representa-
tion is ùë§.
5:
if ùëñ< ùëöthen
6:
return ùë•ùëñ
7:
end if
8: end for
9: return ‚Äùfail‚Äù
Conditioned on not failing, the output of Algorithm 20.2 is uni-
formly distributed in ùëÜ. However, since 2‚Ñì
<
2ùëö, the probability
of failure in each iteration is less than 1/2 and so the probability of
failure in all of them is at most (1/2)ùëõ= 2‚àíùëõ.
‚ñ†
20.1.1 An alternative view: random coins as an ‚Äúextra input‚Äù
While we presented randomized computation as adding an extra
‚Äúcoin tossing‚Äù operation to our programs, we can also model this as
being given an additional extra input. That is, we can think of a ran-
domized algorithm ùê¥as a deterministic algorithm ùê¥‚Ä≤ that takes two
inputs ùë•and ùëüwhere the second input ùëüis chosen at random from
{0, 1}ùëöfor some ùëö‚àà‚Ñï(see Fig. 20.2). The equivalence to the Defini-
tion 20.1 is shown in the following theorem:


--- Page 557 ---

modeling randomized computation
557
Theorem 20.3 ‚Äî Alternative characterization of BPP. Let ùêπ
‚à∂
{0, 1}‚àó
‚Üí
{0, 1}. Then ùêπ
‚àà
BPP if and only if there exists ùëé, ùëè
‚àà
‚Ñïand
ùê∫‚à∂{0, 1}‚àó‚Üí{0, 1} such that ùê∫is in P and for every ùë•‚àà{0, 1}‚àó,
Pr
ùëü‚àº{0,1}ùëé|ùë•|ùëè[ùê∫(ùë•ùëü) = ùêπ(ùë•)] ‚â•2
3 .
(20.2)
Proof Idea:
The idea behind the proof is that, as illustrated in Fig. 20.2, we can
simply replace sampling a random coin with reading a bit from the
extra ‚Äúrandom input‚Äù ùëüand vice versa. To prove this rigorously we
need to work through some slightly cumbersome formal notation.
This might be one of those proofs that is easier to work out on your
own than to read.
‚ãÜ
Proof of Theorem 20.3. We start by showing the ‚Äúonly if‚Äù direction.
Let ùêπ‚ààBPP and let ùëÉbe an RNAND-TM program that computes
ùêπas per Definition 20.1, and let ùëé, ùëè‚àà‚Ñïbe such that on every input
of length ùëõ, the program ùëÉhalts within at most ùëéùëõùëèsteps. We will
construct a polynomial-time algorithm that ùëÉ‚Ä≤ such that for every
ùë•‚àà{0, 1}ùëõ, if we set ùëö= ùëéùëõùëè, then
Pr
ùëü‚àº{0,1}ùëö[ùëÉ‚Ä≤(ùë•ùëü) = 1] = Pr[ùëÉ(ùë•) = 1] ,
(20.3)
where the probability in the righthand side is taken over the RAND()
operations in ùëÉ. In particular this means that if we define ùê∫(ùë•ùëü) =
ùëÉ‚Ä≤(ùë•ùëü) then the function ùê∫satisfies the conditions of (20.2).
The algorithm ùëÉ‚Ä≤ will be very simple: it simulates the program ùëÉ,
maintaining a counter ùëñinitialized to 0. Every time that ùëÉmakes a
RAND() operation, the program ùëÉ‚Ä≤ will supply the result from ùëüùëñand
increment ùëñby one. We will never ‚Äúrun out‚Äù of bits, since the running
time of ùëÉis at most ùëéùëõùëèand hence it can make at most this number of
RNAND() calls. The output of ùëÉ‚Ä≤(ùë•ùëü) for a random ùëü‚àº{0, 1}ùëöwill be
distributed identically to the output of ùëÉ(ùë•).
For the other direction, given a function ùê∫‚ààP satisfying the condi-
tion (20.2) and a NAND-TM ùëÉ‚Ä≤ that computes ùê∫in polynomial time,
we can construct an RNAND-TM program ùëÉthat computes ùêπin poly-
nomial time. On input ùë•‚àà{0, 1}ùëõ, the program ùëÉwill simply use the
RNAND() instruction ùëéùëõùëètimes to fill an array R[0] , ‚Ä¶, R[ùëéùëõùëè‚àí1] and
then execute the original program ùëÉ‚Ä≤ on input ùë•ùëüwhere ùëüùëñis the ùëñ-th
element of the array R. Once again, it is clear that if ùëÉ‚Ä≤ runs in polyno-
mial time then so will ùëÉ, and for every input ùë•and ùëü‚àà{0, 1}ùëéùëõùëè, the
output of ùëÉon input ùë•and where the coin tosses outcome is ùëüis equal
to ùëÉ‚Ä≤(ùë•ùëü).


--- Page 558 ---

558
introduction to theoretical computer science
‚ñ†
R
Remark 20.4 ‚Äî Definitions of BPP and NP. The char-
acterization of BPP Theorem 20.3 is reminiscent of
the characterization of NP in Definition 15.1, with
the randomness in the case of BPP playing the role
of the solution in the case of NP. However, there are
important differences between the two:
‚Ä¢ The definition of NP is ‚Äúone sided‚Äù: ùêπ(ùë•)
=
1 if
there exists a solution ùë§such that ùê∫(ùë•ùë§)
=
1 and
ùêπ(ùë•)
=
0 if for every string ùë§of the appropriate
length, ùê∫(ùë•ùë§)
=
0. In contrast, the characteriza-
tion of BPP is symmetric with respect to the cases
ùêπ(ùë•) = 0 and ùêπ(ùë•) = 1.
‚Ä¢ The relation between NP and BPP is not immedi-
ately clear. It is not known whether BPP
‚äÜ
NP,
NP ‚äÜBPP, or these two classes are incomparable.
It is however known (with a non-trivial proof) that
if P = NP then BPP = P (see Theorem 20.11).
‚Ä¢ Most importantly, the definition of NP is ‚Äúinef-
fective,‚Äù since it does not yield a way of actually
finding whether there exists a solution among the
exponentially many possibilities. By contrast, the
definition of BPP gives us a way to compute the
function in practice by simply choosing the second
input at random.
‚ÄùRandom tapes‚Äù.
Theorem 20.3 motivates sometimes considering the
randomness of an RNAND-TM (or RNAND-RAM) program as an
extra input. As such, if ùê¥is a randomized algorithm that on inputs of
length ùëõmakes at most ùëöcoin tosses, we will often use the notation
ùê¥(ùë•; ùëü) (where ùë•‚àà{0, 1}ùëõand ùëü‚àà{0, 1}ùëö) to refer to the result of
executing ùë•when the coin tosses of ùê¥correspond to the coordinates
of ùëü. This second, or ‚Äúauxiliary,‚Äù input is sometimes referred to as
a ‚Äúrandom tape.‚Äù This terminology originates from the model of
randomized Turing machines.
20.1.2 Success amplification of two-sided error algorithms
The number 2/3 might seem arbitrary, but as we‚Äôve seen in Chapter 19
it can be amplified to our liking:
Theorem 20.5 ‚Äî Amplification. Let ùêπ
‚à∂{0, 1}‚àó
‚Üí{0, 1} be a Boolean
function such that there is a polynomial ùëù
‚à∂
‚Ñï
‚Üí
‚Ñïand a
polynomial-time randomized algorithm ùê¥satisfying that for ev-


--- Page 559 ---

modeling randomized computation
559
Figure 20.3: If ùêπ‚ààBPP then there is randomized
polynomial-time algorithm ùëÉwith the following
property: In the case ùêπ(ùë•) = 0 two thirds of the
‚Äúpopulation‚Äù of random choices satisfy ùëÉ(ùë•; ùëü) = 0
and in the case ùêπ(ùë•) = 1 two thirds of the population
satisfy ùëÉ(ùë•; ùëü) = 1. We can think of amplification as
a form of ‚Äúpolling‚Äù of the choices of randomness. By
the Chernoff bound, if we poll a sample of ùëÇ( log(1/ùõø)
ùúñ2
)
random choices ùëü, then with probability at least
1 ‚àíùõø, the fraction of ùëü‚Äôs in the sample satisfying
ùëÉ(ùë•; ùëü) = 1 will give us an estimate of the fraction of
the population within an ùúñmargin of error. This is the
same calculation used by pollsters to determine the
needed sample size in their polls.
ery ùë•‚àà{0, 1}ùëõ,
Pr[ùê¥(ùë•) = ùêπ(ùë•)] ‚â•1
2 +
1
ùëù(ùëõ) .
(20.4)
Then for every polynomial ùëû‚à∂‚Ñï‚Üí‚Ñïthere is a polynomial-time
randomized algorithm ùêµsatisfying for every ùë•‚àà{0, 1}ùëõ,
Pr[ùêµ(ùë•) = ùêπ(ùë•)] ‚â•1 ‚àí2‚àíùëû(ùëõ) .
(20.5)
ÔÉ´Big Idea 25 We can amplify the success of randomized algorithms
to a value that is arbitrarily close to 1.
Proof Idea:
The proof is the same as we‚Äôve seen before in the case of maximum
cut and other examples. We use the Chernoff bound to argue that if
ùê¥computes ùêπwith probability at least 1
2 + ùúñand we run it ùëÇ(ùëò/ùúñ2)
times, each time using fresh and independent random coins, then the
probability that the majority of the answers will not be correct will be
less than 2‚àíùëò. Amplification can be thought of as a ‚Äúpolling‚Äù of the
choices for randomness for the algorithm (see Fig. 20.3).
‚ãÜ
Proof of Theorem 20.5. Let ùê¥be an algorithm satisfying (20.4). Set
ùúñ=
1
ùëù(ùëõ) and ùëò= ùëû(ùëõ) where ùëù, ùëûare the polynomials in the theorem
statement. We can run ùëÉon input ùë•for ùë°= 10ùëò/ùúñ2 times, using fresh
randomness in each execution, and compute the outputs ùë¶0, ‚Ä¶ , ùë¶ùë°‚àí1.
We output the value ùë¶that appeared the largest number of times. Let
ùëãùëñbe the random variable that is equal to 1 if ùë¶ùëñ= ùêπ(ùë•) and equal to
0 otherwise. The random variables ùëã0, ‚Ä¶ , ùëãùë°‚àí1 are i.i.d. and satisfy
ùîº[ùëãùëñ] = Pr[ùëãùëñ= 1] ‚â•1/2 + ùúñ, and hence by linearity of expectation
ùîº[‚àë
ùë°‚àí1
ùëñ=0 ùëãùëñ] ‚â•ùë°(1/2 + ùúñ). For the plurality value to be incorrect, it must
hold that ‚àë
ùë°‚àí1
ùëñ=0 ùëãùëñ‚â§ùë°/2, which means that ‚àë
ùë°‚àí1
ùëñ=0 ùëãùëñis at least ùúñùë°far
from its expectation. Hence by the Chernoff bound (Theorem 18.12),
the probability that the plurality value is not correct is at most 2ùëí‚àíùúñ2ùë°,
which is smaller than 2‚àíùëòfor our choice of ùë°.
‚ñ†
20.2 BPP AND NP COMPLETENESS
Since ‚Äúnoisy processes‚Äù abound in nature, randomized algorithms can
be realized physically, and so it is reasonable to propose BPP rather
than P as our mathematical model for ‚Äúfeasible‚Äù or ‚Äútractable‚Äù com-
putation. One might wonder if this makes all the previous chapters


--- Page 560 ---

560
introduction to theoretical computer science
irrelevant, and in particular if the theory of NP completeness still
applies to probabilistic algorithms. Fortunately, the answer is Yes:
Theorem 20.6 ‚Äî NP hardness and BPP. Suppose that ùêπis NP-hard and
ùêπ‚ààBPP. Then NP ‚äÜBPP.
Before seeing the proof, note that Theorem 20.6 implies that if there
was a randomized polynomial time algorithm for any NP-complete
problem such as 3SAT, ISET etc., then there would be such an algo-
rithm for every problem in NP. Thus, regardless of whether our model
of computation is deterministic or randomized algorithms, NP com-
plete problems retain their status as the ‚Äúhardest problems in NP.‚Äù
Proof Idea:
The idea is to simply run the reduction as usual, and plug it into
the randomized algorithm instead of a deterministic one. It would
be an excellent exercise, and a way to reinforce the definitions of NP-
hardness and randomized algorithms, for you to work out the proof
for yourself. However for the sake of completeness, we include this
proof below.
‚ãÜ
Proof of Theorem 20.6. Suppose that ùêπis NP-hard and ùêπ‚ààBPP.
We will now show that this implies that NP ‚äÜBPP. Let ùê∫‚ààNP.
By the definition of NP-hardness, it follows that ùê∫‚â§ùëùùêπ, or that in
other words there exists a polynomial-time computable function ùëÖ‚à∂
{0, 1}‚àó‚Üí{0, 1}‚àósuch that ùê∫(ùë•) = ùêπ(ùëÖ(ùë•)) for every ùë•‚àà{0, 1}‚àó. Now
if ùêπis in BPP then there is a polynomial-time RNAND-TM program ùëÉ
such that
Pr[ùëÉ(ùë¶) = ùêπ(ùë¶)] ‚â•2/3
(20.6)
for every ùë¶‚àà{0, 1}‚àó(where the probability is taken over the random
coin tosses of ùëÉ). Hence we can get a polynomial-time RNAND-TM
program ùëÉ‚Ä≤ to compute ùê∫by setting ùëÉ‚Ä≤(ùë•) = ùëÉ(ùëÖ(ùë•)). By (20.6)
Pr[ùëÉ‚Ä≤(ùë•) = ùêπ(ùëÖ(ùë•))] ‚â•2/3 and since ùêπ(ùëÖ(ùë•)) = ùê∫(ùë•) this implies that
Pr[ùëÉ‚Ä≤(ùë•) = ùê∫(ùë•)] ‚â•2/3, which proves that ùê∫‚ààBPP.
‚ñ†
Most of the results we‚Äôve seen about NP hardness, including the
search to decision reduction of Theorem 16.1, the decision to optimiza-
tion reduction of Theorem 16.3, and the quantifier elimination result
of Theorem 16.6, all carry over in the same way if we replace P with
BPP as our model of efficient computation. Thus if NP ‚äÜBPP then
we get essentially all of the strange and wonderful consequences of
P = NP. Unsurprisingly, we cannot rule out this possibility. In fact,
unlike P = EXP, which is ruled out by the time hierarchy theorem, we


--- Page 561 ---

modeling randomized computation
561
1 At the time of this writing, the largest ‚Äúnatural‚Äù com-
plexity class which we can‚Äôt rule out being contained
in BPP is the class NEXP, which we did not define
in this course, but corresponds to non deterministic
exponential time. See this paper for a discussion of
this question.
Figure 20.4: Some possibilities for the relations be-
tween BPP and other complexity classes. Most
researchers believe that BPP = P and that these
classes are not powerful enough to solve NP-complete
problems, let alone all problems in EXP. However,
we have not even been able yet to rule out the possi-
bility that randomness is a ‚Äúsilver bullet‚Äù that allows
exponential speedup on all problems, and hence
BPP = EXP. As we‚Äôve already seen, we also can‚Äôt
rule out that P = NP. Interestingly, in the latter case,
P = BPP.
don‚Äôt even know how to rule out the possibility that BPP = EXP! Thus
a priori it‚Äôs possible (though seems highly unlikely) that randomness
is a magical tool that allows us to speed up arbitrary exponential time
computation.1 Nevertheless, as we discuss below, it is believed that
randomization‚Äôs power is much weaker and BPP lies in much more
‚Äúpedestrian‚Äù territory.
20.3 THE POWER OF RANDOMIZATION
A major question is whether randomization can add power to compu-
tation. Mathematically, we can phrase this as the following question:
does BPP = P? Given what we‚Äôve seen so far about the relations of
other complexity classes such as P and NP, or NP and EXP, one might
guess that:
1. We do not know the answer to this question.
2. But we suspect that BPP is different than P.
One would be correct about the former, but wrong about the latter.
As we will see, we do in fact have reasons to believe that BPP = P.
This can be thought of as supporting the extended Church Turing hy-
pothesis that deterministic polynomial-time Turing machines capture
what can be feasibly computed in the physical world.
We now survey some of the relations that are known between
BPP and other complexity classes we have encountered. (See also
Fig. 20.4.)
20.3.1 Solving BPP in exponential time
It is not hard to see that if ùêπis in BPP then it can be computed in
exponential time.
Theorem 20.7 ‚Äî Simulating randomized algorithms in exponential time.
BPP ‚äÜEXP
P
The proof of Theorem 20.7 readily follows by enumer-
ating over all the (exponentially many) choices for the
random coins. We omit the formal proof, as doing it
by yourself is an excellent way to get comfortable with
Definition 20.1.
20.3.2 Simulating randomized algorithms by circuits
We have seen in Theorem 13.12 that if ùêπis in P, then there is a polyno-
mial ùëù‚à∂‚Ñï‚Üí‚Ñïsuch that for every ùëõ, the restriction ùêπ‚Üæùëõof ùêπto inputs


--- Page 562 ---

562
introduction to theoretical computer science
{0, 1}ùëõis in SIZE(ùëù(ùëõ)). (In other words, that P ‚äÜP/poly.) A priori it is
not at all clear that the same holds for a function in BPP, but this does
turn out to be the case.
Figure 20.5: The possible guarantees for a random-
ized algorithm ùê¥computing some function ùêπ. In
the tables above, the columns correspond to differ-
ent inputs and the rows to different choices of the
random tape. A cell at position ùëü, ùë•is colored green
if ùê¥(ùë•; ùëü) = ùêπ(ùë•) (i.e., the algorithm outputs the
correct answer) and red otherwise. The standard BPP
guarantee corresponds to the middle figure, where
for every input ùë•, at least two thirds of the choices
ùëüfor a random tape will result in ùê¥computing the
correct value. That is, every column is colored green
in at least two thirds of its coordinates. In the left
figure we have an ‚Äúaverage case‚Äù guarantee where
the algorithm is only guaranteed to output the correct
answer with probability two thirds over a random
input (i.e., at most one third of the total entries of the
table are colored red, but there could be an all red
column). The right figure corresponds to the ‚Äúoffline
BPP‚Äù case, with probability at least two thirds over
the random choice ùëü, ùëüwill be good for every input.
That is, at least two thirds of the rows are all green.
Theorem 20.8 (BPP ‚äÜP/poly) is proven by amplify-
ing the success of a BPP algorithm until we have the
‚Äúoffline BPP‚Äù guarantee, and then hardwiring the
choice of the randomness ùëüto obtain a nonuniform
deterministic algorithm.
Theorem 20.8 ‚Äî Randomness does not help for non uniform computation.
BPP ‚äÜP/poly.
That is, for every ùêπ‚ààBPP, there exist some ùëé, ùëè‚àà‚Ñïsuch that
for every ùëõ> 0, ùêπ‚Üæùëõ‚ààSIZE(ùëéùëõùëè) where ùêπ‚Üæùëõis the restriction of ùêπto
inputs in {0, 1}ùëõ.
Proof Idea:
The idea behind the proof is that we can first amplify by repetition
the probability of success from 2/3 to 1‚àí0.1‚ãÖ2‚àíùëõ. This will allow us to
show that for every ùëõ‚àà‚Ñïthere exists a single fixed choice of ‚Äúfavorable
coins‚Äù which is a string ùëüof length polynomial in ùëõsuch that if ùëüis
used for the randomness then we output the right answer on all of
the possible 2ùëõinputs. We can then use the standard ‚Äúunravelling the
loop‚Äù technique to transform an RNAND-TM program to an RNAND-
CIRC program, and ‚Äúhardwire‚Äù the favorable choice of random coins
to transform the RNAND-CIRC program into a plain old deterministic
NAND-CIRC program.
‚ãÜ
Proof of Theorem 20.8. Suppose that ùêπ‚ààBPP. Let ùëÉbe a polynomial-
time RNAND-TM program that computes ùêπas per Definition 20.1.
Using Theorem 20.5, we can amplify the success probability of ùëÉto
obtain an RNAND-TM program ùëÉ‚Ä≤ that is at most a factor of ùëÇ(ùëõ)


--- Page 563 ---

modeling randomized computation
563
slower (and hence still polynomial time) such that for every ùë•‚àà
{0, 1}ùëõ
Pr
ùëü‚àº{0,1}ùëö[ùëÉ‚Ä≤(ùë•; ùëü) = ùêπ(ùë•)] ‚â•1 ‚àí0.1 ‚ãÖ2‚àíùëõ,
(20.7)
where ùëöis the number of coin tosses that ùëÉ‚Ä≤ uses on inputs of
length ùëõ. We use the notation ùëÉ‚Ä≤(ùë•; ùëü) to denote the execution of ùëÉ‚Ä≤
on input ùë•and when the result of the coin tosses corresponds to the
string ùëü.
For every ùë•‚àà{0, 1}ùëõ, define the ‚Äúbad‚Äù event ùêµùë•to hold if ùëÉ‚Ä≤(ùë•) ‚â†
ùêπ(ùë•), where the sample space for this event consists of the coins of ùëÉ‚Ä≤.
Then by (20.7), Pr[ùêµùë•] ‚â§0.1 ‚ãÖ2‚àíùëõfor every ùë•‚àà{0, 1}ùëõ. Since there
are 2ùëõmany such ùë•‚Äôs, by the union bound we see that the probability
that the union of the events {ùêµùë•}ùë•‚àà{0,1}ùëõis at most 0.1. This means that
if we choose ùëü‚àº{0, 1}ùëö, then with probability at least 0.9 it will be
the case that for every ùë•‚àà{0, 1}ùëõ, ùêπ(ùë•) = ùëÉ‚Ä≤(ùë•; ùëü). (Indeed, otherwise
the event ùêµùë•would hold for some ùë•.) In particular, because of the
mere fact that the the probability of ‚à™ùë•‚àà{0,1}ùëõùêµùë•is smaller than 1, this
means that there exists a particular ùëü‚àó‚àà{0, 1}ùëösuch that
ùëÉ‚Ä≤(ùë•; ùëü‚àó) = ùêπ(ùë•)
(20.8)
for every ùë•‚àà{0, 1}ùëõ.
Now let us use the standard ‚Äúunravelling the loop‚Äù the technique
and transform ùëÉ‚Ä≤ into a NAND-CIRC program ùëÑof polynomial in
ùëõsize, such that ùëÑ(ùë•ùëü) = ùëÉ‚Ä≤(ùë•; ùëü) for every ùë•‚àà{0, 1}ùëõand ùëü‚àà
{0, 1}ùëö. Then by ‚Äúhardwiring‚Äù the values ùëü‚àó
0, ‚Ä¶ , ùëü‚àó
ùëö‚àí1 in place of the
last ùëöinputs of ùëÑ, we obtain a new NAND-CIRC program ùëÑùëü‚àóthat
satisfies by (20.8) that ùëÑùëü‚àó(ùë•) = ùêπ(ùë•) for every ùë•‚àà{0, 1}ùëõ. This
demonstrates that ùêπ‚Üæùëõhas a polynomial sized NAND-CIRC program,
hence completing the proof of Theorem 20.8.
‚ñ†
20.4 DERANDOMIZATION
The proof of Theorem 20.8 can be summarized as follows: we can
replace a ùëùùëúùëôùë¶(ùëõ)-time algorithm that tosses coins as it runs with an
algorithm that uses a single set of coin tosses ùëü‚àó‚àà{0, 1}ùëùùëúùëôùë¶(ùëõ) which
will be good enough for all inputs of size ùëõ. Another way to say it is
that for the purposes of computing functions, we do not need ‚Äúonline‚Äù
access to random coins and can generate a set of coins ‚Äúoffline‚Äù ahead
of time, before we see the actual input.
But this does not really help us with answering the question of
whether BPP equals P, since we still need to find a way to generate
these ‚Äúoffline‚Äù coins in the first place. To derandomize an RNAND-
TM program we will need to come up with a single deterministic


--- Page 564 ---

564
introduction to theoretical computer science
2 One amusing anecdote is a recent case where scam-
mers managed to predict the imperfect ‚Äúpseudo-
random generator‚Äù used by slot machines to cheat
casinos. Unfortunately we don‚Äôt know the details of
how they did it, since the case was sealed.
algorithm that will work for all input lengths. That is, unlike in the
case of RNAND-CIRC programs, we cannot choose for every input
length ùëõsome string ùëü‚àó‚àà{0, 1}ùëùùëúùëôùë¶(ùëõ) to use as our random coins.
Can we derandomize randomized algorithms, or does randomness
add an inherent extra power for computation? This is a fundamentally
interesting question but is also of practical significance. Ever since
people started to use randomized algorithms during the Manhattan
project, they have been trying to remove the need for randomness and
replace it with numbers that are selected through some deterministic
process. Throughout the years this approach has often been used
successfully, though there have been a number of failures as well.2
A common approach people used over the years was to replace
the random coins of the algorithm by a ‚Äúrandomish looking‚Äù string
that they generated through some arithmetic progress. For example,
one can use the digits of ùúãfor the random tape. Using these type of
methods corresponds to what von Neumann referred to as a ‚Äústate
of sin‚Äù. (Though this is a sin that he himself frequently committed,
as generating true randomness in sufficient quantity was and still is
often too expensive.) The reason that this is considered a ‚Äúsin‚Äù is that
such a procedure will not work in general. For example, it is easy to
modify any probabilistic algorithm ùê¥such as the ones we have seen in
Chapter 19, to an algorithm ùê¥‚Ä≤ that is guaranteed to fail if the random
tape happens to equal the digits of ùúã. This means that the procedure
‚Äúreplace the random tape by the digits of ùúã‚Äù does not yield a general
way to transform a probabilistic algorithm to a deterministic one that
will solve the same problem. Of course, this procedure does not always
fail, but we have no good way to determine when it fails and when
it succeeds. This reasoning is not specific to ùúãand holds for every
deterministically produced string, whether it obtained by ùúã, ùëí, the
Fibonacci series, or anything else.
An algorithm that checks if its random tape is equal to ùúãand then
fails seems to be quite silly, but this is but the ‚Äútip of the iceberg‚Äù for a
very serious issue. Time and again people have learned the hard way
that one needs to be very careful about producing random bits using
deterministic means. As we will see when we discuss cryptography,
many spectacular security failures and break-ins were the result of
using ‚Äúinsufficiently random‚Äù coins.
20.4.1 Pseudorandom generators
So, we can‚Äôt use any single string to ‚Äúderandomize‚Äù a probabilistic
algorithm. It turns out however, that we can use a collection of strings
to do so. Another way to think about it is that rather than trying to
eliminate the need for randomness, we start by focusing on reducing the


--- Page 565 ---

modeling randomized computation
565
Figure 20.6: A pseudorandom generator ùê∫maps
a short string ùë†‚àà{0, 1}‚Ñìinto a long string ùëü‚àà
{0, 1}ùëösuch that an small program/circuit ùëÉcannot
distinguish between the case that it is provided a
random input ùëü‚àº{0, 1}ùëöand the case that it
is provided a ‚Äúpseudorandom‚Äù input of the form
ùëü= ùê∫(ùë†) where ùë†‚àº{0, 1}‚Ñì. The short string ùë†
is sometimes called the seed of the pseudorandom
generator, as it is a small object that can be thought as
yielding a large ‚Äútree of randomness‚Äù.
amount of randomness needed. (Though we will see that if we reduce
the randomness sufficiently, we can eventually get rid of it altogether.)
We make the following definition:
Definition 20.9 ‚Äî Pseudorandom generator. A function ùê∫
‚à∂
{0, 1}‚Ñì
‚Üí
{0, 1}ùëöis a (ùëá, ùúñ)-pseudorandom generator if for every circuit ùê∂with
ùëöinputs, one output, and at most ùëágates,
‚à£
Pr
ùë†‚àº{0,1}‚Ñì[ùê∂(ùê∫(ùë†)) = 1] ‚àí
Pr
ùëü‚àº{0,1}ùëö[ùê∂(ùëü) = 1]‚à£< ùúñ
(20.9)
P
This is a definition that‚Äôs worth reading more than
once, and spending some time to digest it. Note that it
takes several parameters:
‚Ä¢ ùëáis the limit on the number of gates of the circuit
ùê∂that the generator needs to ‚Äúfool‚Äù. The larger ùëá
is, the stronger the generator.
‚Ä¢ ùúñis how close is the output of the pseudorandom
generator to the true uniform distribution over
{0, 1}ùëö. The smaller ùúñis, the stronger the generator.
‚Ä¢ ‚Ñìis the input length and ùëöis the output length.
If ‚Ñì
‚â•
ùëöthen it is trivial to come up with such
a generator: on input ùë†
‚àà
{0, 1}‚Ñì, we can output
ùë†0, ‚Ä¶ , ùë†ùëö‚àí1. In this case Prùë†‚àº{0,1}‚Ñì[ùëÉ(ùê∫(ùë†))
=
1]
will simply equal Prùëü‚àà{0,1}ùëö[ùëÉ(ùëü)
=
1], no matter
how many lines ùëÉhas. So, the smaller ‚Ñìis and the
larger ùëöis, the stronger the generator, and to get
anything non-trivial, we need ùëö> ‚Ñì.
Furthermore note that although our eventual goal is to
fool probabilistic randomized algorithms that take an
unbounded number of inputs, Definition 20.9 refers to
finite and deterministic NAND-CIRC programs.
We can think of a pseudorandom generator as a ‚Äúrandomness
amplifier.‚Äù It takes an input ùë†of ‚Ñìbits chosen at random and ex-
pands these ‚Ñìbits into an output ùëüof ùëö> ‚Ñìpseudorandom bits. If
ùúñis small enough then the pseudorandom bits will ‚Äúlook random‚Äù
to any NAND-CIRC program that is not too big. Still, there are two
questions we haven‚Äôt answered:
‚Ä¢ What reason do we have to believe that pseudorandom generators with
non-trivial parameters exist?
‚Ä¢ Even if they do exist, why would such generators be useful to derandomize
randomized algorithms? After all, Definition 20.9 does not involve


--- Page 566 ---

566
introduction to theoretical computer science
RNAND-TM or RNAND-RAM programs, but rather deterministic
NAND-CIRC programs with no randomness and no loops.
We will now (partially) answer both questions. For the first ques-
tion, let us come clean and confess we do not know how to prove that
interesting pseudorandom generators exist. By interesting we mean
pseudorandom generators that satisfy that ùúñis some small constant
(say ùúñ< 1/3), ùëö> ‚Ñì, and the function ùê∫itself can be computed in
ùëùùëúùëôùë¶(ùëö) time. Nevertheless, Lemma 20.12 (whose statement and proof
is deferred to the end of this chapter) shows that if we only drop the
last condition (polynomial-time computability), then there do in fact
exist pseudorandom generators where ùëöis exponentially larger than ‚Ñì.
P
At this point you might want to skip ahead and look at
the statement of Lemma 20.12. However, since its proof
is somewhat subtle, I recommend you defer reading it
until you‚Äôve finished reading the rest of this chapter.
20.4.2 From existence to constructivity
The fact that there exists a pseudorandom generator does not mean
that there is one that can be efficiently computed. However, it turns
out that we can turn complexity ‚Äúon its head‚Äù and use the assumed
non existence of fast algorithms for problems such as 3SAT to obtain
pseudorandom generators that can then be used to transform random-
ized algorithms into deterministic ones. This is known as the Hardness
vs Randomness paradigm. A number of results along those lines, most
of which are outside the scope of this course, have led researchers to
believe the following conjecture:
Optimal PRG conjecture: There is a polynomial-time computable
function PRG ‚à∂{0, 1}‚àó‚Üí{0, 1} that yields an exponentially secure
pseudorandom generator.
Specifically, there exists a constant ùõø> 0 such that for every ‚Ñìand
ùëö< 2ùõø‚Ñì, if we define ùê∫‚à∂{0, 1}‚Ñì‚Üí{0, 1}ùëöas ùê∫(ùë†)ùëñ= PRG(ùë†, ùëñ) for every
ùë†‚àà{0, 1}‚Ñìand ùëñ‚àà[ùëö], then ùê∫is a (2ùõø‚Ñì, 2‚àíùõø‚Ñì) pseudorandom generator.
P
The ‚Äúoptimal PRG conjecture‚Äù is worth while reading
more than once. What it posits is that we can obtain
(ùëá, ùúñ) pseudorandom generator ùê∫such that every
output bit of ùê∫can be computed in time polynomial
in the length ‚Ñìof the input, where ùëáis exponentially
large in ‚Ñìand ùúñis exponentially small in ‚Ñì. (Note that
we could not hope for the entire output to be com-


--- Page 567 ---

modeling randomized computation
567
3 A pseudorandom generator of the form we posit,
where each output bit can be computed individually
in time polynomial in the seed length, is commonly
known as a pseudorandom function generator. For more
on the many interesting results and connections in the
study of pseudorandomness, see this monograph of Salil
Vadhan.
putable in ‚Ñì, as just writing the output down will take
too long.)
To understand why we call such a pseudorandom
generator ‚Äúoptimal,‚Äù it is a great exercise to convince
yourself that, for example, there does not exist a
(21.1‚Ñì, 2‚àí1.1‚Ñì) pseudorandom generator (in fact, the
number ùõøin the conjecture must be smaller than 1). To
see that we can‚Äôt have ùëá
‚â´2‚Ñì, note that if we allow a
NAND-CIRC program with much more than 2‚Ñìlines
then this NAND-CIRC program could ‚Äúhardwire‚Äù in-
side it all the outputs of ùê∫on all its 2‚Ñìinputs, and use
that to distinguish between a string of the form ùê∫(ùë†)
and a uniformly chosen string in {0, 1}ùëö. To see that
we can‚Äôt have ùúñ‚â™2‚àí‚Ñì, note that by guessing the input
ùë†(which will be successful with probability 2‚àí2‚Ñì), we
can obtain a small (i.e., ùëÇ(‚Ñì) line) NAND-CIRC pro-
gram that achieves a 2‚àí‚Ñìadvantage in distinguishing a
pseudorandom and uniform input. Working out these
details is a highly recommended exercise.
We emphasize again that the optimal PRG conjecture is, as its name
implies, a conjecture, and we still do not know how to prove it. In par-
ticular, it is stronger than the conjecture that P ‚â†NP. But we do have
some evidence for its truth. There is a spectrum of different types of
pseudorandom generators, and there are weaker assumptions than
the optimal PRG conjecture that suffice to prove that BPP = P. In
particular this is known to hold under the assumption that there exists
a function ùêπ‚ààTIME(2ùëÇ(ùëõ)) and ùúñ> 0 such that for every sufficiently
large ùëõ, ùêπ‚Üæùëõis not in SIZE(2ùúñùëõ). The name ‚ÄúOptimal PRG conjecture‚Äù
is non standard. This conjecture is sometimes known in the literature
as the existence of exponentially strong pseudorandom functions.3
20.4.3 Usefulness of pseudorandom generators
We now show that optimal pseudorandom generators are indeed very
useful, by proving the following theorem:
Theorem 20.10 ‚Äî Derandomization of BPP. Suppose that the optimal
PRG conjecture is true. Then BPP = P.
Proof Idea:
The optimal PRG conjecture tells us that we can achieve exponential
expansion of ‚Ñìtruly random coins into as many as 2ùõø‚Ñì‚Äúpseudorandom
coins.‚Äù Looked at from the other direction, it allows us to reduce the
need for randomness by taking an algorithm that uses ùëöcoins and
converting it into an algorithm that only uses ùëÇ(log ùëö) coins. Now an
algorithm of the latter type by can be made fully deterministic by enu-
merating over all the 2ùëÇ(log ùëö) (which is polynomial in ùëö) possibilities
for its random choices.


--- Page 568 ---

568
introduction to theoretical computer science
‚ãÜ
We now proceed with the proof details.
Proof of Theorem 20.10. Let ùêπ‚ààBPP and let ùëÉbe a NAND-TM pro-
gram and ùëé, ùëè, ùëê, ùëëconstants such that for every ùë•‚àà{0, 1}ùëõ, ùëÉ(ùë•)
runs in at most ùëê‚ãÖùëõùëësteps and Prùëü‚àº{0,1}ùëö[ùëÉ(ùë•; ùëü) = ùêπ(ùë•)] ‚â•2/3.
By ‚Äúunrolling the loop‚Äù and hardwiring the input ùë•, we can obtain
for every input ùë•‚àà{0, 1}ùëõa NAND-CIRC program ùëÑùë•of at most,
say, ùëá= 10ùëê‚ãÖùëõùëëlines, that takes ùëöbits of input and such that
ùëÑ(ùëü) = ùëÉ(ùë•; ùëü).
Now suppose that ùê∫‚à∂{0, 1}‚Ñì‚Üí{0, 1} is a (ùëá, 0.1) pseudorandom
generator. Then we could deterministically estimate the probability
ùëù(ùë•) = Prùëü‚àº{0,1}ùëö[ùëÑùë•(ùëü) = 1] up to 0.1 accuracy in time ùëÇ(ùëá‚ãÖ2‚Ñì‚ãÖùëö‚ãÖ
ùëêùëúùë†ùë°(ùê∫)) where ùëêùëúùë†ùë°(ùê∫) is the time that it takes to compute a single
output bit of ùê∫.
The reason is that we know that ÃÉùëù(ùë•) = Prùë†‚àº{0,1}‚Ñì[ùëÑùë•(ùê∫(ùë†)) =
1] will give us such an estimate for ùëù(ùë•), and we can compute the
probability ÃÉùëù(ùë•) by simply trying all 2‚Ñìpossibillites for ùë†. Now, under
the optimal PRG conjecture we can set ùëá= 2ùõø‚Ñìor equivalently ‚Ñì=
1
ùõølog ùëá, and our total computation time is polynomial in 2‚Ñì= ùëá1/ùõø.
Since ùëá‚â§10ùëê‚ãÖùëõùëë, this running time will be polynomial in ùëõ.
This completes the proof, since we are guaranteed that
Prùëü‚àº{0,1}ùëö[ùëÑùë•(ùëü) = ùêπ(ùë•)] ‚â•2/3, and hence estimating the
probability ùëù(ùë•) to within 0.1 accuracy is sufficient to compute ùêπ(ùë•).
‚ñ†
20.5 P = NP AND BPP VS P
Two computational complexity questions that we cannot settle are:
‚Ä¢ Is P = NP? Where we believe the answer is negative.
‚Ä¢ Is BPP = P? Where we believe the answer is positive.
However we can say that the ‚Äúconventional wisdom‚Äù is correct on
at least one of these questions. Namely, if we‚Äôre wrong on the first
count, then we‚Äôll be right on the second one:
Theorem 20.11 ‚Äî Sipser‚ÄìG√°cs Theorem. If P = NP then BPP = P.
P
Before reading the proof, it is instructive to think
why this result is not ‚Äúobvious.‚Äù If P
=
NP then
given any randomized algorithm ùê¥and input ùë•,
we will be able to figure out in polynomial time if
there is a string ùëü
‚àà
{0, 1}ùëöof random coins for ùê¥


--- Page 569 ---

modeling randomized computation
569
Figure 20.7: If ùêπ‚ààBPP then through amplification we
can ensure that there is an algorithm ùê¥to compute
ùêπon ùëõ-length inputs and using ùëöcoins such that
Prùëü‚àà{0,1}ùëö[ùê¥(ùë•ùëü) ‚â†ùêπ(ùë•)] ‚â™1/ùëùùëúùëôùë¶(ùëö). Hence
if ùêπ(ùë•) = 1 then almost all of the 2ùëöchoices for ùëü
will cause ùê¥(ùë•ùëü) to output 1, while if ùêπ(ùë•) = 0 then
ùê¥(ùë•ùëü) = 0 for almost all ùëü‚Äôs. To prove the Sipser‚Äì
G√°cs Theorem we consider several ‚Äúshifts‚Äù of the set
ùëÜ‚äÜ{0, 1}ùëöof the coins ùëüsuch that ùê¥(ùë•ùëü) = 1. If
ùêπ(ùë•) = 1 then we can find a set of ùëòshifts ùë†0, ‚Ä¶ , ùë†ùëò‚àí1
for which ‚à™ùëñ‚àà[ùëò](ùëÜ‚äïùë†ùëñ) = {0, 1}ùëö. If ùêπ(ùë•) = 0 then
for every such set |ùëêùë¢ùëùùëñ‚àà[ùëò]ùëÜùëñ| ‚â§ùëò|ùëÜ| ‚â™2ùëö. We can
phrase the question of whether there is such a set of
shift using a constant number of quantifiers, and so
can solve it in polynomial time if P = NP.
such that ùê¥(ùë•ùëü)
=
1. The problem is that even if
Prùëü‚àà{0,1}ùëö[ùê¥(ùë•ùëü) = ùêπ(ùë•)] ‚â•0.9999, it can still be the
case that even when ùêπ(ùë•)
=
0 there exists a string ùëü
such that ùê¥(ùë•ùëü) = 1.
The proof is rather subtle. It is much more important
that you understand the statement of the theorem than
that you follow all the details of the proof.
Proof Idea:
The construction follows the ‚Äúquantifier elimination‚Äù idea which
we have seen in Theorem 16.6. We will show that for every ùêπ‚ààBPP,
we can reduce the question of some input ùë•satisfies ùêπ(ùë•) = 1 to the
question of whether a formula of the form ‚àÉùë¢‚àà{0,1}ùëö‚àÄùë£‚àà{0,1}ùëòùëÉ(ùë¢, ùë£)
is true, where ùëö, ùëòare polynomial in the length of ùë•and ùëÉis
polynomial-time computable. By Theorem 16.6, if P = NP then we can
decide in polynomial time whether such a formula is true or false.
The idea behind this construction is that using amplification we
can obtain a randomized algorithm ùê¥for computing ùêπusing ùëöcoins
such that for every ùë•‚àà{0, 1}ùëõ, if ùêπ(ùë•) = 0 then the set ùëÜ‚äÜ{0, 1}ùëö
of coins that make ùê¥output 1 is extremely tiny, and if ùêπ(ùë•) = 1 then
it is very large. Now in the case ùêπ(ùë•) = 1, one can show that this
means that there exists a small number ùëòof ‚Äúshifts‚Äù ùë†0, ‚Ä¶ , ùë†ùëò‚àí1 such
that the union of the sets ùëÜ‚äïùë†ùëñ(i.e., sets of the form {ùë†‚äïùë†ùëñ| ùë†‚ààùëÜ})
covers {0, 1}ùëö, while in the case ùêπ(ùë•) = 0 this union will always be of
size at most ùëò|ùëÜ| which is much smaller than 2ùëö. We can express the
condition that there exists ùë†0, ‚Ä¶ , ùë†ùëò‚àí1 such that ‚à™ùëñ‚àà[ùëò](ùëÜ‚äïùë†ùëñ) = {0, 1}ùëö
as a statement with a constant number of quantifiers.
‚ãÜ
Proof of Theorem 20.11. Let ùêπ‚ààBPP. Using Theorem 20.5, there
exists a polynomial-time algorithm ùê¥such that for every ùë•‚àà{0, 1}ùëõ,
Prùë•‚àà{0,1}ùëö[ùê¥(ùë•ùëü) = ùêπ(ùë•)] ‚â•1 ‚àí2‚àíùëõwhere ùëöis polynomial in ùëõ. In
particular (since an exponential dominates a polynomial, and we can
always assume ùëõis sufficiently large), it holds that
Pr
ùë•‚àà{0,1}ùëö[ùê¥(ùë•ùëü) = ùêπ(ùë•)] ‚â•1 ‚àí
1
10ùëö2 .
(20.10)
Let ùë•‚àà{0, 1}ùëõ, and let ùëÜùë•‚äÜ{0, 1}ùëöbe the set {ùëü‚àà{0, 1}ùëö
‚à∂
ùê¥(ùë•ùëü) = 1}. By our assumption, if ùêπ(ùë•) = 0 then |ùëÜùë•| ‚â§
1
10ùëö2 2ùëöand if
ùêπ(ùë•) = 1 then |ùëÜùë•| ‚â•(1 ‚àí
1
10ùëö2 )2ùëö.
For a set ùëÜ‚äÜ{0, 1}ùëöand a string ùë†‚àà{0, 1}ùëö, we define the set
ùëÜ‚äïùë†to be {ùëü‚äïùë†‚à∂ùëü‚ààùëÜ} where ‚äïdenotes the XOR operation. That
is, ùëÜ‚äïùë†is the set ùëÜ‚Äúshifted‚Äù by ùë†. Note that |ùëÜ‚äïùë†| = |ùëÜ|. (Please
make sure that you see why this is true.)
The heart of the proof is the following two claims:


--- Page 570 ---

570
introduction to theoretical computer science
CLAIM I: For every subset ùëÜ‚äÜ{0, 1}ùëö, if |ùëÜ| ‚â§
1
1000ùëö2ùëö, then for
every ùë†0, ‚Ä¶ , ùë†100ùëö‚àí1 ‚àà{0, 1}ùëö, ‚à™ùëñ‚àà[100ùëö](ùëÜ‚äïùë†ùëñ) ‚ää{0, 1}ùëö.
CLAIM II: For every subset ùëÜ‚äÜ{0, 1}ùëö, if |ùëÜ| ‚â•1
22ùëöthen there
exist a set of string ùë†0, ‚Ä¶ , ùë†100ùëö‚àí1 such that ‚à™ùëñ‚àà[100ùëö](ùëÜ‚äïùë†ùëñ) = {0, 1}ùëö.
CLAIM I and CLAIM II together imply the theorem. Indeed, they
mean that under our assumptions, for every ùë•‚àà{0, 1}ùëõ, ùêπ(ùë•) = 1 if
and only if
‚àÉùë†0,‚Ä¶,ùë†100ùëö‚àí1‚àà{0,1}ùëö‚à™ùëñ‚àà[100ùëö] (ùëÜùë•‚äïùë†ùëñ) = {0, 1}ùëö
(20.11)
which we can re-write as
‚àÉùë†0,‚Ä¶,ùë†100ùëö‚àí1‚àà{0,1}ùëö‚àÄùë§‚àà{0,1}ùëö(ùë§‚àà(ùëÜùë•‚äïùë†0)‚à®ùë§‚àà(ùëÜùë•‚äïùë†1)‚à®‚ãØùë§‚àà(ùëÜùë•‚äïùë†100ùëö‚àí1))
(20.12)
or equivalently
‚àÉùë†0,‚Ä¶,ùë†100ùëö‚àí1‚àà{0,1}ùëö‚àÄùë§‚àà{0,1}ùëö(ùê¥(ùë•(ùë§‚äïùë†0)) = 1‚à®ùê¥(ùë•(ùë§‚äïùë†1)) = 1‚à®‚ãØ‚à®ùê¥(ùë•(ùë§‚äïùë†100ùëö‚àí1)) = 1)
(20.13)
which (since ùê¥is computable in polynomial time) is exactly the
type of statement shown in Theorem 16.6 to be decidable in polyno-
mial time if P = NP.
We see that all that is left is to prove CLAIM I and CLAIM II.
CLAIM I follows immediately from the fact that
‚à™ùëñ‚àà[100ùëö‚àí1]|ùëÜùë•‚äïùë†ùëñ| ‚â§
100ùëö‚àí1
‚àë
ùëñ=0
|ùëÜùë•‚äïùë†ùëñ| =
100ùëö‚àí1
‚àë
ùëñ=0
|ùëÜùë•| = 100ùëö|ùëÜùë•| .
(20.14)
To prove CLAIM II, we will use a technique known as the prob-
abilistic method (see the proof of Lemma 20.12 for a more extensive
discussion). Note that this is a completely different use of probability
than in the theorem statement, we just use the methods of probability
to prove an existential statement.
Proof of CLAIM II: Let ùëÜ‚äÜ{0, 1}ùëöwith |ùëÜ| ‚â•0.5 ‚ãÖ2ùëöbe as
in the claim‚Äôs statement. Consider the following probabilistic ex-
periment: we choose 100ùëörandom shifts ùë†0, ‚Ä¶ , ùë†100ùëö‚àí1 indepen-
dently at random in {0, 1}ùëö, and consider the event GOOD that
‚à™ùëñ‚àà[100ùëö](ùëÜ‚äïùë†ùëñ) = {0, 1}ùëö. To prove CLAIM II it is enough to show
that Pr[GOOD] > 0, since that means that in particular there must exist
shifts ùë†0, ‚Ä¶ , ùë†100ùëö‚àí1 that satisfy this condition.
For every ùëß‚àà{0, 1}ùëö, define the event BADùëßto hold if ùëß‚àâ
‚à™ùëñ‚àà[100ùëö‚àí1](ùëÜ‚äïùë†ùëñ). The event GOOD holds if BADùëßfails for every
ùëß‚àà{0, 1}ùëö, and so our goal is to prove that Pr[‚à™ùëß‚àà{0,1}ùëöBADùëß] < 1. By
the union bound, to show this, it is enough to show that Pr[BADùëß] <


--- Page 571 ---

modeling randomized computation
571
4 The condition of independence here is subtle. It
is not the case that all of the 2ùëö√ó 100ùëöevents
{BADùëñ
ùëß}ùëß‚àà{0,1}ùëö,ùëñ‚àà[100ùëö] are mutually independent.
Only for a fixed ùëß‚àà{0, 1}ùëö, the 100ùëöevents of the
form BADùëñ
ùëßare mutually independent.
5 There is a whole (highly recommended) book by
Alon and Spencer devoted to this method.
2‚àíùëöfor every ùëß‚àà{0, 1}ùëö. Define the event BADùëñ
ùëßto hold if ùëß‚àâùëÜ‚äïùë†ùëñ.
Since every shift ùë†ùëñis chosen independently, for every fixed ùëßthe
events BAD0
ùëß, ‚Ä¶ , BAD100ùëö‚àí1
ùëß
are mutually independent,4 and hence
Pr[BADùëß] = Pr[‚à©ùëñ‚àà[100ùëö‚àí1]BADùëñ
ùëß] =
100ùëö‚àí1
‚àè
ùëñ=0
Pr[BADùëñ
ùëß] .
(20.15)
So this means that the result will follow by showing that
Pr[BADùëñ
ùëß] ‚â§1
2 for every ùëß‚àà{0, 1}ùëöand ùëñ‚àà[100ùëö] (as that would
allow to bound the righthand side of (20.15) by 2‚àí100ùëö). In other
words, we need to show that for every ùëß‚àà{0, 1}ùëöand set ùëÜ‚äÜ{0, 1}ùëö
with |ùëÜ| ‚â•1
22ùëö,
Pr
ùë†‚àà{0,1}ùëö[ùëß‚ààùëÜ‚äïùë†] ‚â•1
2 .
(20.16)
To show this, we observe that ùëß‚ààùëÜ‚äïùë†if and only if ùë†‚ààùëÜ‚äïùëß(can
you see why). Hence we can rewrite the probability on the lefthand
side of (20.16) as Prùë†‚àà{0,1}ùëö[ùë†‚ààùëÜ‚äïùëß] which simply equals |ùëÜ‚äïùëß|/2ùëö=
|ùëÜ|/2ùëö‚â•1/2! This concludes the proof of CLAIM I and hence of
Theorem 20.11.
‚ñ†
20.6 NON-CONSTRUCTIVE EXISTENCE OF PSEUDORANDOM GEN-
ERATORS (ADVANCED, OPTIONAL)
We now show that, if we don‚Äôt insist on constructivity of pseudoran-
dom generators, then we can show that there exists pseudorandom
generators with output that exponentially larger in the input length.
Lemma 20.12 ‚Äî Existence of inefficient pseudorandom generators. There is
some absolute constant ùê∂such that for every ùúñ, ùëá, if ‚Ñì> ùê∂(log ùëá+
log(1/ùúñ)) and ùëö‚â§ùëá, then there is an (ùëá, ùúñ) pseudorandom generator
ùê∫‚à∂{0, 1}‚Ñì‚Üí{0, 1}ùëö.
Proof Idea:
The proof uses an extremely useful technique known as the ‚Äúprob-
abilistic method‚Äù which is not too hard mathematically but can be
confusing at first.5 The idea is to give a ‚Äúnon constructive‚Äù proof of
existence of the pseudorandom generator ùê∫by showing that if ùê∫was
chosen at random, then the probability that it would be a valid (ùëá, ùúñ)
pseudorandom generator is positive. In particular this means that
there exists a single ùê∫that is a valid (ùëá, ùúñ) pseudorandom generator.
The probabilistic method is just a proof technique to demonstrate the
existence of such a function. Ultimately, our goal is to show the exis-
tence of a deterministic function ùê∫that satisfies the condition.
‚ãÜ


--- Page 572 ---

572
introduction to theoretical computer science
The above discussion might be rather abstract at this point, but
would become clearer after seeing the proof.
Proof of Lemma 20.12. Let ùúñ, ùëá, ‚Ñì, ùëöbe as in the lemma‚Äôs statement. We
need to show that there exists a function ùê∫‚à∂{0, 1}‚Ñì‚Üí{0, 1}ùëöthat
‚Äúfools‚Äù every ùëáline program ùëÉin the sense of (20.9). We will show
that this follows from the following claim:
Claim I: For every fixed NAND-CIRC program ùëÉ, if we pick ùê∫‚à∂
{0, 1}‚Ñì‚Üí{0, 1}ùëöat random then the probability that (20.9) is violated
is at most 2‚àíùëá2.
Before proving Claim I, let us see why it implies Lemma 20.12. We
can identify a function ùê∫‚à∂{0, 1}‚Ñì‚Üí{0, 1}ùëöwith its ‚Äútruth table‚Äù
or simply the list of evaluations on all its possible 2‚Ñìinputs. Since
each output is an ùëöbit string, we can also think of ùê∫as a string in
{0, 1}ùëö‚ãÖ2‚Ñì. We define ‚Ñ±ùëö
‚Ñìto be the set of all functions from {0, 1}‚Ñìto
{0, 1}ùëö. As discussed above we can identify ‚Ñ±ùëö
‚Ñìwith {0, 1}ùëö‚ãÖ2‚Ñìand
choosing a random function ùê∫‚àº‚Ñ±ùëö
‚Ñìcorresponds to choosing a
random ùëö‚ãÖ2‚Ñì-long bit string.
For every NAND-CIRC program ùëÉlet ùêµùëÉbe the event that, if we
choose ùê∫at random from ‚Ñ±ùëö
‚Ñìthen (20.9) is violated with respect to
the program ùëÉ. It is important to understand what is the sample space
that the event ùêµùëÉis defined over, namely this event depends on the
choice of ùê∫and so ùêµùëÉis a subset of ‚Ñ±ùëö
‚Ñì. An equivalent way to define
the event ùêµùëÉis that it is the subset of all functions mapping {0, 1}‚Ñìto
{0, 1}ùëöthat violate (20.9), or in other words:
ùêµùëÉ=
‚éß
{
‚é®
{
‚é©
ùê∫‚àà‚Ñ±ùëö
‚Ñì‚à£‚à£1
2‚Ñì
‚àë
ùë†‚àà{0,1}‚Ñì
ùëÉ(ùê∫(ùë†)) ‚àí
1
2ùëö
‚àë
ùëü‚àà{0,1}ùëö
ùëÉ(ùëü)‚à£> ùúñ
‚é´
}
‚é¨
}
‚é≠
(20.17)
(We‚Äôve replaced here the probability statements in (20.9) with the
equivalent sums so as to reduce confusion as to what is the sample
space that ùêµùëÉis defined over.)
To understand this proof it is crucial that you pause here and see
how the definition of ùêµùëÉabove corresponds to (20.17). This may well
take re-reading the above text once or twice, but it is a good exercise
at parsing probabilistic statements and learning how to identify the
sample space that these statements correspond to.
Now, we‚Äôve shown in Theorem 5.2 that up to renaming variables
(which makes no difference to program‚Äôs functionality) there are
2ùëÇ(ùëálog ùëá) NAND-CIRC programs of at most ùëálines. Since ùëálog ùëá<
ùëá2 for sufficiently large ùëá, this means that if Claim I is true, then
by the union bound it holds that the probability of the union of
ùêµùëÉover all NAND-CIRC programs of at most ùëálines is at most
2ùëÇ(ùëálog ùëá)2‚àíùëá2 < 0.1 for sufficiently large ùëá. What is important for


--- Page 573 ---

modeling randomized computation
573
us about the number 0.1 is that it is smaller than 1. In particular this
means that there exists a single ùê∫‚àó‚àà‚Ñ±ùëö
‚Ñìsuch that ùê∫‚àódoes not violate
(20.9) with respect to any NAND-CIRC program of at most ùëálines,
but that precisely means that ùê∫‚àóis a (ùëá, ùúñ) pseudorandom generator.
Hence to conclude the proof of Lemma 20.12, it suffices to prove
Claim I. Choosing a random ùê∫‚à∂{0, 1}‚Ñì‚Üí{0, 1}ùëöamounts to choos-
ing ùêø= 2‚Ñìrandom strings ùë¶0, ‚Ä¶ , ùë¶ùêø‚àí1 ‚àà{0, 1}ùëöand letting ùê∫(ùë•) = ùë¶ùë•
(identifying {0, 1}‚Ñìand [ùêø] via the binary representation). This means
that proving the claim amounts to showing that for every fixed func-
tion ùëÉ‚à∂{0, 1}ùëö‚Üí{0, 1}, if ùêø> 2ùê∂(log ùëá+log ùúñ) (which by setting ùê∂> 4,
we can ensure is larger than 10ùëá2/ùúñ2) then the probability that
‚à£1
ùêø
ùêø‚àí1
‚àë
ùëñ=0
ùëÉ(ùë¶ùëñ) ‚àí
Pr
ùë†‚àº{0,1}ùëö[ùëÉ(ùë†) = 1]‚à£> ùúñ
(20.18)
is at most 2‚àíùëá2.
(20.18) follows directly from the Chernoff bound. Indeed, if we
let for every ùëñ‚àà[ùêø] the random variable ùëãùëñdenote ùëÉ(ùë¶ùëñ), then
since ùë¶0, ‚Ä¶ , ùë¶ùêø‚àí1 is chosen independently at random, these are in-
dependently and identically distributed random variables with mean
ùîºùë¶‚àº{0,1}ùëö[ùëÉ(ùë¶)] = Prùë¶‚àº{0,1}ùëö[ùëÉ(ùë¶) = 1] and hence the probability that
they deviate from their expectation by ùúñis at most 2 ‚ãÖ2‚àíùúñ2ùêø/2.
‚ñ†
Figure 20.8: The relation between BPP and the other
complexity classes that we have seen. We know that
P ‚äÜBPP ‚äÜEXP and BPP ‚äÜP/poly but we don‚Äôt
know how BPP compares with NP and can‚Äôt rule out
even BPP = EXP. Most evidence points out to the
possibliity that BPP = P.
‚úì
Chapter Recap
‚Ä¢ We can model randomized algorithms by either
adding a special ‚Äúcoin toss‚Äù operation or assuming
an extra randomly chosen input.
‚Ä¢ The class BPP contains the set of Boolean func-
tions that can be computed by polynomial time
randomized algorithms.
‚Ä¢ BPP is a worst case class of computation: a ran-
domized algorithm to compute a function must
compute it correctly with high probability on every
input.


--- Page 574 ---

574
introduction to theoretical computer science
‚Ä¢ We can amplify the success probability of random-
ized algorithm from any value strictly larger than
1/2 into a success probability that is exponentially
close to 1.
‚Ä¢ We know that P ‚äÜBPP ‚äÜEXP.
‚Ä¢ We also know that BPP ‚äÜP/poly.
‚Ä¢ The relation between BPP and NP is not known,
but we do know that if P = NP then BPP = P.
‚Ä¢ Pseudorandom generators are objects that take
a short random ‚Äúseed‚Äù and expand it to a much
longer output that ‚Äúappears random‚Äù for effi-
cient algorithms. We conjecture that exponentially
strong pseudorandom generators exist. Under this
conjecture, BPP = P.
20.7 EXERCISES
20.8 BIBLIOGRAPHICAL NOTES
In this chapter we ignore the issue of how we actually get random
bits in practice. The output of many physical processes, whether it
is thermal heat, network and hard drive latency, user typing pat-
tern and mouse movements, and more can be thought of as a binary
string sampled from some distribution ùúáthat might have significant
unpredictability (or entropy) but is not necessarily the uniform distri-
bution over {0, 1}ùëõ. Indeed, as this paper shows, even (real-world)
coin tosses do not have exactly the distribution of a uniformly random
string. Therefore, to use the resulting measurements for randomized
algorithms, one typically needs to apply a ‚Äúdistillation‚Äù or random-
ness extraction process to the raw measurements to transform them
to the uniform distribution. Vadhan‚Äôs book [Vad+12] is an excellent
source for more discussion on both randomness extractors and pseu-
dorandom generators.
The name BPP stands for ‚Äúbounded probability polynomial time‚Äù.
This is an historical accident: this class probably should have been
called RP or PP but both names were taken by other classes.
The proof of Theorem 20.8 actually yields more than its statement.
We can use the same ‚Äúunrolling the loop‚Äù arguments we‚Äôve used be-
fore to show that the restriction to {0, 1}ùëõof every function in BPP
is also computable by a polynomial-size RNAND-CIRC program
(i.e., NAND-CIRC program with the RAND operation). Like in the P
vs SIZE(ùëùùëúùëôùë¶(ùëõ)) case, there are also functions outside BPP whose
restrictions can be computed by polynomial-size RNAND-CIRC pro-
grams. Nevertheless the proof of Theorem 20.8 shows that even such
functions can be computed by polynomial sized NAND-CIRC pro-


--- Page 575 ---

modeling randomized computation
575
grams without using the rand operations. This can be phrased as
saying that BPSIZE(ùëá(ùëõ)) ‚äÜSIZE(ùëÇ(ùëõùëá(ùëõ))) (where BPSIZE is
defined in the natural way using RNAND progams). The stronger
version of Theorem 20.8 we mentioned can be phrased as saying that
BPP/poly = P/poly.


--- Page 576 ---



--- Page 577 ---

V
ADVANCED TOPICS


--- Page 578 ---



--- Page 579 ---

21
Cryptography
‚ÄúHuman ingenuity cannot concoct a cipher which human ingenuity cannot
resolve.‚Äù, Edgar Allen Poe, 1841
‚ÄúA good disguise should not reveal the person‚Äôs height‚Äù, Shafi Goldwasser
and Silvio Micali, 1982
‚Äú‚ÄúPerfect Secrecy‚Äù is defined by requiring of a system that after a cryptogram
is intercepted by the enemy the a posteriori probabilities of this cryptogram rep-
resenting various messages be identically the same as the a priori probabilities
of the same messages before the interception. It is shown that perfect secrecy is
possible but requires, if the number of messages is finite, the same number of
possible keys.‚Äù, Claude Shannon, 1945
‚ÄúWe stand today on the brink of a revolution in cryptography.‚Äù, Whitfeld
Diffie and Martin Hellman, 1976
Cryptography - the art or science of ‚Äúsecret writing‚Äù - has been
around for several millennia, and for almost all of that time Edgar
Allan Poe‚Äôs quote above held true. Indeed, the history of cryptography
is littered with the figurative corpses of cryptosystems believed secure
and then broken, and sometimes with the actual corpses of those who
have mistakenly placed their faith in these cryptosystems.
Yet, something changed in the last few decades, which is the ‚Äúrevo-
lution‚Äù alluded to (and to a large extent initiated by) Diffie and Hell-
man‚Äôs 1976 paper quoted above. New cryptosystems have been found
that have not been broken despite being subjected to immense efforts
involving both human ingenuity and computational power on a scale
that completely dwarves the ‚Äúcode breakers‚Äù of Poe‚Äôs time. Even more
amazingly, these cryptosystem are not only seemingly unbreakable,
but they also achieve this under much harsher conditions. Not only do
today‚Äôs attackers have more computational power but they also have
more data to work with. In Poe‚Äôs age, an attacker would be lucky if
they got access to more than a few encryptions of known messages.
These days attackers might have massive amounts of data- terabytes
Compiled on 8.26.2020 18:10
Learning Objectives:
‚Ä¢ Definition of perfect secrecy
‚Ä¢ The one-time pad encryption scheme
‚Ä¢ Necessity of long keys for perfect secrecy
‚Ä¢ Computational secrecy and the derandomized
one-time pad.
‚Ä¢ Public key encryption
‚Ä¢ A taste of advanced topics


--- Page 580 ---

580
introduction to theoretical computer science
Figure 21.1: Snippet from encrypted communication
between queen Mary and Sir Babington
Figure 21.2: XKCD‚Äôs take on the added security of
using uncommon symbols
or more - at their disposal. In fact, with public key encryption, an at-
tacker can generate as many ciphertexts as they wish.
The key to this success has been a clearer understanding of both
how to define security for cryptographic tools and how to relate this
security to concrete computational problems. Cryptography is a vast
and continuously changing topic, but we will touch on some of these
issues in this chapter.
21.1 CLASSICAL CRYPTOSYSTEMS
A great many cryptosystems have been devised and broken through-
out the ages. Let us recount just some of these stories. In 1587, Mary
the queen of Scots, and the heir to the throne of England, wanted to
arrange the assassination of her cousin, queen Elisabeth I of England,
so that she could ascend to the throne and finally escape the house
arrest under which she had been for the last 18 years. As part of this
complicated plot, she sent a coded letter to Sir Anthony Babington.
Mary used what‚Äôs known as a substitution cipher where each letter
is transformed into a different obscure symbol (see Fig. 21.1). At a
first look, such a letter might seem rather inscrutable- a meaningless
sequence of strange symbols. However, after some thought, one might
recognize that these symbols repeat several times and moreover that
different symbols repeat with different frequencies. Now it doesn‚Äôt
take a large leap of faith to assume that perhaps each symbol corre-
sponds to a different letter and the more frequent symbols correspond
to letters that occur in the alphabet with higher frequency. From this
observation, there is a short gap to completely breaking the cipher,
which was in fact done by queen Elisabeth‚Äôs spies who used the de-
coded letters to learn of all the co-conspirators and to convict queen
Mary of treason, a crime for which she was executed. Trusting in su-
perficial security measures (such as using ‚Äúinscrutable‚Äù symbols) is a
trap that users of cryptography have been falling into again and again
over the years. (As in many things, this is the subject of a great XKCD
cartoon, see Fig. 21.2.)
The Vigen√®re cipher is named after Blaise de Vigen√®re who de-
scribed it in a book in 1586 (though it was invented earlier by Bellaso).
The idea is to use a collection of substitution cyphers - if there are ùëõ
different ciphers then the first letter of the plaintext is encoded with
the first cipher, the second with the second cipher, the ùëõùë°‚Ñéwith the
ùëõùë°‚Ñécipher, and then the ùëõ+ 1ùë†ùë°letter is again encoded with the first
cipher. The key is usually a word or a phrase of ùëõletters, and the
ùëñùë°‚Ñésubstitution cipher is obtained by shifting each letter ùëòùëñpositions
in the alphabet. This ‚Äúflattens‚Äù the frequencies and makes it much
harder to do frequency analysis, which is why this cipher was consid-
ered ‚Äúunbreakable‚Äù for 300+ years and got the nickname ‚Äúle chiffre


--- Page 581 ---

cryptography
581
Figure 21.3: Confederate Cipher Disk for implement-
ing the Vigen√®re cipher
Figure 21.4: Confederate encryption of the message
‚ÄúGen‚Äôl Pemberton: You can expect no help from this
side of the river. Let Gen‚Äôl Johnston know, if possible,
when you can attack the same point on the enemy‚Äôs
lines. Inform me also and I will endeavor to make a
diversion. I have sent some caps. I subjoin a despatch
from General Johnston.‚Äù
Figure 21.5: In the Enigma mechanical cipher the secret
key would be the settings of the rotors and internal
wires. As the operator types up their message, the
encrypted version appeared in the display area above,
and the internal state of the cipher was updated (and
so typing the same letter twice would generally result
in two different letters output). Decrypting follows
the same process: if the sender and receiver are using
the same key then typing the ciphertext would result
in the plaintext appearing in the display.
1 Here is a nice exercise: compute (up to an order
of magnitude) the probability that a 50-letter long
message composed of random letters will end up not
containing the letter ‚ÄúL‚Äù.
ind√©chiffrable‚Äù (‚Äúthe unbreakable cipher‚Äù). Nevertheless, Charles
Babbage cracked the Vigen√®re cipher in 1854 (though he did not pub-
lish it). In 1863 Friedrich Kasiski broke the cipher and published the
result. The idea is that once you guess the length of the cipher, you
can reduce the task to breaking a simple substitution cipher which can
be done via frequency analysis (can you see why?). Confederate gen-
erals used Vigen√®re regularly during the civil war, and their messages
were routinely cryptanalzed by Union officers.
The Enigma cipher was a mechanical cipher (looking like a type-
writer, see Fig. 21.5) where each letter typed would get mapped into a
different letter depending on the (rather complicated) key and current
state of the machine which had several rotors that rotated at different
paces. An identically wired machine at the other end could be used
to decrypt. Just as many ciphers in history, this has also been believed
by the Germans to be ‚Äúimpossible to break‚Äù and even quite late in the
war they refused to believe it was broken despite mounting evidence
to that effect. (In fact, some German generals refused to believe it
was broken even after the war.) Breaking Enigma was an heroic effort
which was initiated by the Poles and then completed by the British at
Bletchley Park, with Alan Turing (of the Turing machines) playing a
key role. As part of this effort the Brits built arguably the world‚Äôs first
large scale mechanical computation devices (though they looked more
similar to washing machines than to iPhones). They were also helped
along the way by some quirks and errors of the German operators. For
example, the fact that their messages ended with ‚ÄúHeil Hitler‚Äù turned
out to be quite useful.
Here is one entertaining anecdote: the Enigma machine would
never map a letter to itself. In March 1941, Mavis Batey, a cryptana-
lyst at Bletchley Park received a very long message that she tried to
decrypt. She then noticed a curious property‚Äî the message did not
contain the letter ‚ÄúL‚Äù.1 She realized that the probability that no ‚ÄúL‚Äù‚Äôs
appeared in the message is too small for this to happen by chance.
Hence she surmised that the original message must have been com-
posed only of L‚Äôs. That is, it must have been the case that the operator,
perhaps to test the machine, have simply sent out a message where he
repeatedly pressed the letter ‚ÄúL‚Äù. This observation helped her decode
the next message, which helped inform of a planned Italian attack and
secure a resounding British victory in what became known as ‚Äúthe
Battle of Cape Matapan‚Äù. Mavis also helped break another Enigma
machine. Using the information she provided, the Brits were able
to feed the Germans with the false information that the main allied
invasion would take place in Pas de Calais rather than on Normandy.
In the words of General Eisenhower, the intelligence from Bletchley
park was of ‚Äúpriceless value‚Äù. It made a huge difference for the Allied


--- Page 582 ---

582
introduction to theoretical computer science
Figure 21.6: A private-key encryption scheme is a
pair of algorithms ùê∏, ùê∑such that for every key
ùëò‚àà{0, 1}ùëõand plaintext ùë•‚àà{0, 1}ùêø(ùëõ), ùë¶= ùê∏ùëò(ùë•)
is a ciphertext of length ùê∂(ùëõ). The encryption scheme
is valid if for every such ùë¶, ùê∑ùëò(ùë¶) = ùë•. That is, the
decryption of an encryption of ùë•is ùë•, as long as both
encryption and decryption use the same key.
war effort, thereby shortening World War II and saving millions of
lives. See also this interview with Sir Harry Hinsley.
21.2 DEFINING ENCRYPTION
Many of the troubles that cryptosystem designers faced over history
(and still face!) can be attributed to not properly defining or under-
standing what are the goals they want to achieve in the first place. Let
us focus on the setting of private key encryption. (This is also known as
‚Äúsymmetric encryption‚Äù; for thousands of years, ‚Äúprivate key encryp-
tion‚Äù was synonymous with encryption and only in the 1970‚Äôs was
the concept of public key encryption invented, see Definition 21.11.) A
sender (traditionally called ‚ÄúAlice‚Äù) wants to send a message (known
also as a plaintext) ùë•‚àà{0, 1}‚àóto a receiver (traditionally called ‚ÄúBob‚Äù).
They would like their message to be kept secret from an adversary
who listens in or ‚Äúeavesdrops‚Äù on the communication channel (and is
traditionally called ‚ÄúEve‚Äù).
Alice and Bob share a secret key ùëò‚àà{0, 1}‚àó. (While the letter ùëò
is often used elsewhere in the book to denote a natural number, in
this chapter we use it to denote the string corresponding to a secret
key.) Alice uses the key ùëòto ‚Äúscramble‚Äù or encrypt the plaintext ùë•into
a ciphertext ùë¶, and Bob uses the key ùëòto ‚Äúunscramble‚Äù or decrypt the
ciphertext ùë¶back into the plaintext ùë•. This motivates the following
definition which attempts to capture what it means for an encryption
scheme to be valid or ‚Äúmake sense‚Äù, regardless of whether or not it is
secure:
Definition 21.1 ‚Äî Valid encryption scheme. Let ùêø‚à∂‚Ñï‚Üí‚Ñïand ùê∂‚à∂‚Ñï‚Üí‚Ñï
be two functions mapping natural numbers to natural numbers.
A pair of polynomial-time computable functions (ùê∏, ùê∑) map-
ping strings to strings is a valid private key encryption scheme (or
encryption scheme for short) with plaintext length function ùêø(‚ãÖ) and
ciphertext length function ùê∂(‚ãÖ) if for every ùëõ‚àà‚Ñï, ùëò‚àà{0, 1}ùëõand
ùë•‚àà{0, 1}ùêø(ùëõ), |ùê∏ùëò(ùë•)| = ùê∂(ùëõ) and
ùê∑(ùëò, ùê∏(ùëò, ùë•)) = ùë•.
(21.1)
We will often write the first input (i.e., the key) to the encryp-
tion and decryption as a subscript and so can write (21.1) also as
ùê∑ùëò(ùê∏ùëò(ùë•)) = ùë•.
Solved Exercise 21.1 ‚Äî Lengths of ciphertext and plaintext. Prove that for
every valid encryption scheme (ùê∏, ùê∑) with functions ùêø, ùê∂. ùê∂(ùëõ) ‚â•
ùêø(ùëõ) for every ùëõ.
‚ñ†


--- Page 583 ---

cryptography
583
2 The actual quote is ‚ÄúIl faut qu‚Äôil n‚Äôexige pas le
secret, et qu‚Äôil puisse sans inconv√©nient tomber entre
les mains de l‚Äôennemi‚Äù loosely translated as ‚ÄúThe
system must not require secrecy and can be stolen by
the enemy without causing trouble‚Äù. According to
Steve Bellovin the NSA version is ‚Äúassume that the
first copy of any device we make is shipped to the
Kremlin‚Äù.
Solution:
For every fixed key ùëò‚àà{0, 1}ùëõ, the equation (21.1) implies that
the map ùë¶‚Ü¶ùê∑ùëò(ùë¶) inverts the map ùë•‚Ü¶ùê∏ùëò(ùë•), which in partic-
ular means that the map ùë•
‚Ü¶
ùê∏ùëò(ùë•) must be one to one. Hence
its codomain must be at least as large as its domain, and since its
domain is {0, 1}ùêø(ùëõ) and its codomain is {0, 1}ùê∂(ùëõ) it follows that
ùê∂(ùëõ) ‚â•ùêø(ùëõ).
‚ñ†
Since the ciphertext length is always at least the plaintext length
(and in most applications it is not much longer than that), we typi-
cally focus on the plaintext length as the quantity to optimize in an
encryption scheme. The larger ùêø(ùëõ) is, the better the scheme, since it
means we need a shorter secret key to protect messages of the same
length.
21.3 DEFINING SECURITY OF ENCRYPTION
Definition 21.1 says nothing about the security of ùê∏and ùê∑, and even
allows the trivial encryption scheme that ignores the key altogether
and sets ùê∏ùëò(ùë•) = ùë•for every ùë•. Defining security is not a trivial matter.
P
You would appreciate the subtleties of defining secu-
rity of encryption more if at this point you take a five
minute break from reading, and try (possibly with a
partner) to brainstorm on how you would mathemat-
ically define the notion that an encryption scheme is
secure, in the sense that it protects the secrecy of the
plaintext ùë•.
Throughout history, many attacks on cryptosystems were rooted
in the cryptosystem designers‚Äô reliance on ‚Äúsecurity through
obscurity‚Äù‚Äî trusting that the fact their methods are not known to
their enemy will protect them from being broken. This is a faulty
assumption - if you reuse a method again and again (even with a
different key each time) then eventually your adversaries will figure
out what you are doing. And if Alice and Bob meet frequently in a
secure location to decide on a new method, they might as well take
the opportunity to exchange their secrets. These considerations led
Auguste Kerckhoffs in 1883 to state the following principle:
A cryptosystem should be secure even if everything about the system, except the
key, is public knowledge.2
Why is it OK to assume the key is secret and not the algorithm?
Because we can always choose a fresh key. But of course that won‚Äôt


--- Page 584 ---

584
introduction to theoretical computer science
help us much if our key is ‚Äú1234‚Äù or ‚Äúpassw0rd!‚Äù. In fact, if you use
any deterministic algorithm to choose the key then eventually your
adversary will figure this out. Therefore for security we must choose
the key at random and can restate Kerckhoffs‚Äôs principle as follows:
There is no secrecy without randomness
This is such a crucial point that is worth repeating:
ÔÉ´Big Idea 26 There is no secrecy without randomness.
At the heart of every cryptographic scheme there is a secret key,
and the secret key is always chosen at random. A corollary of that
is that to understand cryptography, you need to know probability
theory.
R
Remark 21.2 ‚Äî Randomness in the real world. Choos-
ing the secrets for cryptography requires generating
randomness, which is often done by measuring some
‚Äúunpredictable‚Äù or ‚Äúhigh entropy‚Äù data, and then
applying hash functions to the result to ‚Äúextract‚Äù a
uniformly random string. Great care must be taken in
doing this, and randomness generators often turn out
to be the Achilles heel of secure systems.
In 2006 a programmer removed a line of code from the
procedure to generate entropy in OpenSSL package
distributed by Debian since it caused a warning in
some automatic verification code. As a result for two
years (until this was discovered) all the randomness
generated by this procedure used only the process
ID as an ‚Äúunpredictable‚Äù source. This means that all
communication done by users in that period is fairly
easily breakable (and in particular, if some entities
recorded that communication they could break it also
retroactively). See XKCD‚Äôs take on that incident.
In 2012 two separate teams of researchers scanned a
large number of RSA keys on the web and found out
that about 4 percent of them are easy to break. The
main issue were devices such as routers, internet-
connected printers and such. These devices sometimes
run variants of Linux- a desktop operating system-
but without a hard drive, mouse or keyboard, they
don‚Äôt have access to many of the entropy sources that
desktop have. Coupled with some good old fashioned
ignorance of cryptography and software bugs, this led
to many keys that are downright trivial to break, see
this blog post and this web page for more details.
Since randomness is so crucial to security, breaking
the procedure to generate randomness can lead to a
complete break of the system that uses this random-
ness. Indeed, the Snowden documents, combined with


--- Page 585 ---

cryptography
585
observations of Shumow and Ferguson, strongly sug-
gest that the NSA has deliberately inserted a trapdoor
in one of the pseudorandom generators published by
the National Institute of Standards and Technologies
(NIST). Fortunately, this generator wasn‚Äôt widely
adapted but apparently the NSA did pay 10 million
dollars to RSA security so the latter would make this
generator their default option in their products.
21.4 PERFECT SECRECY
If you think about encryption scheme security for a while, you might
come up with the following principle for defining security: ‚ÄúAn
encryption scheme is secure if it is not possible to recover the key ùëòfrom
ùê∏ùëò(ùë•)‚Äù. However, a moment‚Äôs thought shows that the key is not really
what we‚Äôre trying to protect. After all, the whole point of an encryp-
tion is to protect the confidentiality of the plaintext ùë•. So, we can try to
define that ‚Äúan encryption scheme is secure if it is not possible to recover the
plaintext ùë•from ùê∏ùëò(ùë•)‚Äù. Yet it is not clear what this means either. Sup-
pose that an encryption scheme reveals the first 10 bits of the plaintext
ùë•. It might still not be possible to recover ùë•completely, but on an in-
tuitive level, this seems like it would be extremely unwise to use such
an encryption scheme in practice. Indeed, often even partial information
about the plaintext is enough for the adversary to achieve its goals.
The above thinking led Shannon in 1945 to formalize the notion of
perfect secrecy, which is that an encryption reveals absolutely nothing
about the message. There are several equivalent ways to define it, but
perhaps the cleanest one is the following:
Definition 21.3 ‚Äî Perfect secrecy. A valid encryption scheme (ùê∏, ùê∑)
with plaintext length ùêø(‚ãÖ) is perfectly secret if for every ùëõ
‚àà
‚Ñïand
plaintexts ùë•, ùë•‚Ä≤ ‚àà{0, 1}ùêø(ùëõ), the following two distributions ùëåand
ùëå‚Ä≤ over {0, 1}‚àóare identical:
‚Ä¢ ùëåis obtained by sampling ùëò‚àº{0, 1}ùëõand outputting ùê∏ùëò(ùë•).
‚Ä¢ ùëå‚Ä≤ is obtained by sampling ùëò‚àº{0, 1}ùëõand outputting ùê∏ùëò(ùë•‚Ä≤).
P
This definition might take more than one reading
to parse. Try to think of how this condition would
correspond to your intuitive notion of ‚Äúlearning no
information‚Äù about ùë•from observing ùê∏ùëò(ùë•), and to
Shannon‚Äôs quote in the beginning of this chapter.


--- Page 586 ---

586
introduction to theoretical computer science
Figure 21.7: For any key length ùëõ, we can visualize an
encryption scheme (ùê∏, ùê∑) as a graph with a vertex
for every one of the 2ùêø(ùëõ) possible plaintexts and for
every one of the ciphertexts in {0, 1}‚àóof the form
ùê∏ùëò(ùë•) for ùëò‚àà{0, 1}ùëõand ùë•‚àà{0, 1}ùêø(ùëõ). For every
plaintext ùë•and key ùëò, we add an edge labeled ùëò
between ùë•and ùê∏ùëò(ùë•). By the validity condition, if we
pick any fixed key ùëò, the map ùë•‚Ü¶ùê∏ùëò(ùë•) must be
one-to-one. The condition of perfect secrecy simply
corresponds to requiring that every two plaintexts
ùë•and ùë•‚Ä≤ have exactly the same set of neighbors (or
multi-set, if there are parallel edges).
In particular, suppose that you knew ahead of time
that Alice sent either an encryption of ùë•or an en-
cryption of ùë•‚Ä≤. Would you learn anything new from
observing the encryption of the message that Alice
actually sent? It may help you to look at Fig. 21.7.
21.4.1 Example: Perfect secrecy in the battlefield
To understand Definition 21.3, suppose that Alice sends only one of
two possible messages: ‚Äúattack‚Äù or ‚Äúretreat‚Äù, which we denote by ùë•0
and ùë•1 respectively, and that she sends each one of those messages
with probability 1/2. Let us put ourselves in the shoes of Eve, the
eavesdropping adversary. A priori we would have guessed that Alice
sent either ùë•0 or ùë•1 with probability 1/2. Now we observe ùë¶= ùê∏ùëò(ùë•ùëñ)
where ùëòis a uniformly chosen key in {0, 1}ùëõ. How does this new
information cause us to update our beliefs on whether Alice sent the
plaintext ùë•0 or the plaintext ùë•1?
P
Before reading the next paragraph, you might want
to try the analysis yourself. You may find it useful to
look at the Wikipedia entry on Bayesian Inference or
these MIT lecture notes.
Let us define ùëù0(ùë¶) to be the probability (taken over ùëò‚àº{0, 1}ùëõ)
that ùë¶= ùê∏ùëò(ùë•0) and similarly ùëù1(ùë¶) to be Prùëò‚àº{0,1}ùëõ[ùë¶= ùê∏ùëò(ùë•1)].
Note that, since Alice chooses the message to send at random, our
a priori probability for observing ùë¶is 1
2ùëù0(ùë¶) + 1
2ùëù1(ùë¶). However,
as per Definition 21.3, the perfect secrecy condition guarantees that
ùëù0(ùë¶) = ùëù1(ùë¶)! Let us denote the number ùëù0(ùë¶) = ùëù1(ùë¶) by ùëù. By the
formula for conditional probability, the probability that Alice sent the
message ùë•0 conditioned on our observation ùë¶is simply
Pr[ùëñ= 0|ùë¶= ùê∏ùëò(ùë•ùëñ)] = Pr[ùëñ= 0 ‚àßùë¶= ùê∏ùëò(ùë•ùëñ)]
Pr[ùë¶= ùê∏ùëò(ùë•)]
.
(21.2)
(The equation (21.2) is a special case of Bayes‚Äô rule which, although
a simple restatement of the formula for conditional probability, is
an extremely important and widely used tool in statistics and data
analysis.)
Since the probability that ùëñ= 0 and ùë¶is the ciphertext ùê∏ùëò(0) is equal
to 1
2 ‚ãÖùëù0(ùë¶), and the a priori probability of observing ùë¶is 1
2ùëù0(ùë¶) +
1
2ùëù1(ùë¶), we can rewrite (21.2) as
Pr[ùëñ= 0|ùë¶= ùê∏ùëò(ùë•ùëñ)] =
1
2ùëù0(ùë¶)
1
2ùëù0(ùë¶) + 1
2ùëù1(ùë¶) =
ùëù
ùëù+ ùëù= 1
2
(21.3)


--- Page 587 ---

cryptography
587
Figure 21.8: A perfectly secret encryption scheme
for two-bit keys and messages. The blue vertices
represent plaintexts and the red vertices represent
ciphertexts, each edge mapping a plaintext ùë•to a ci-
phertext ùë¶= ùê∏ùëò(ùë•) is labeled with the corresponding
key ùëò. Since there are four possible keys, the degree of
the graph is four and it is in fact a complete bipartite
graph. The encryption scheme is valid in the sense
that for every ùëò‚àà{0, 1}2, the map ùë•‚Ü¶ùê∏ùëò(ùë•) is
one-to-one, which in other words means that the set
of edges labeled with ùëòis a matching.
using the fact that ùëù0(ùë¶) = ùëù1(ùë¶) = ùëù. This means that observing the
ciphertext ùë¶did not help us at all! We still would not be able to guess
whether Alice sent ‚Äúattack‚Äù or ‚Äúretreat‚Äù with better than 50/50 odds!
This example can be vastly generalized to show that perfect secrecy
is indeed ‚Äúperfect‚Äù in the sense that observing a ciphertext gives Eve
no additional information about the plaintext beyond her a priori knowl-
edge.
21.4.2 Constructing perfectly secret encryption
Perfect secrecy is an extremely strong condition, and implies that an
eavesdropper does not learn any information from observing the ci-
phertext. You might think that an encryption scheme satisfying such a
strong condition will be impossible, or at least extremely complicated,
to achieve. However it turns out we can in fact obtain perfectly secret
encryption scheme fairly easily. Such a scheme for two-bit messages is
illustrated in Fig. 21.8
In fact, this can be generalized to any number of bits:
Theorem 21.4 ‚Äî One Time Pad (Vernam 1917, Shannon 1949). There is a per-
fectly secret valid encryption scheme (ùê∏, ùê∑) with ùêø(ùëõ) = ùê∂(ùëõ) = ùëõ.
Proof Idea:
Our scheme is the one-time pad also known as the ‚ÄúVernam Ci-
pher‚Äù, see Fig. 21.9. The encryption is exceedingly simple: to encrypt
a message ùë•‚àà{0, 1}ùëõwith a key ùëò‚àà{0, 1}ùëõwe simply output ùë•‚äïùëò
where ‚äïis the bitwise XOR operation that outputs the string corre-
sponding to XORing each coordinate of ùë•and ùëò.
‚ãÜ
Proof of Theorem 21.4. For two binary strings ùëéand ùëèof the same
length ùëõ, we define ùëé‚äïùëèto be the string ùëê‚àà{0, 1}ùëõsuch that
ùëêùëñ= ùëéùëñ+ ùëèùëñmod 2 for every ùëñ‚àà[ùëõ]. The encryption scheme
(ùê∏, ùê∑) is defined as follows: ùê∏ùëò(ùë•) = ùë•‚äïùëòand ùê∑ùëò(ùë¶) = ùë¶‚äïùëò.
By the associative law of addition (which works also modulo two),
ùê∑ùëò(ùê∏ùëò(ùë•)) = (ùë•‚äïùëò) ‚äïùëò= ùë•‚äï(ùëò‚äïùëò) = ùë•‚äï0ùëõ= ùë•, using the fact
that for every bit ùúé‚àà{0, 1}, ùúé+ ùúémod 2 = 0 and ùúé+ 0 = ùúémod 2.
Hence (ùê∏, ùê∑) form a valid encryption.
To analyze the perfect secrecy property, we claim that for every
ùë•‚àà{0, 1}ùëõ, the distribution ùëåùë•= ùê∏ùëò(ùë•) where ùëò‚àº{0, 1}ùëõis simply
the uniform distribution over {0, 1}ùëõ, and hence in particular the
distributions ùëåùë•and ùëåùë•‚Ä≤ are identical for every ùë•, ùë•‚Ä≤ ‚àà{0, 1}ùëõ. Indeed,
for every particular ùë¶‚àà{0, 1}ùëõ, the value ùë¶is output by ùëåùë•if and
only if ùë¶= ùë•‚äïùëòwhich holds if and only if ùëò= ùë•‚äïùë¶. Since ùëòis
chosen uniformly at random in {0, 1}ùëõ, the probability that ùëòhappens


--- Page 588 ---

588
introduction to theoretical computer science
Figure 21.9: In the one time pad encryption scheme we
encrypt a plaintext ùë•‚àà{0, 1}ùëõwith a key ùëò‚àà{0, 1}ùëõ
by the ciphertext ùë•‚äïùëòwhere ‚äïdenotes the bitwise
XOR operation.
to equal ùë•‚äïùë¶is exactly 2‚àíùëõ, which means that every string ùë¶is output
by ùëåùë•with probability 2‚àíùëõ.
‚ñ†
P
The argument above is quite simple but is worth
reading again. To understand why the one-time pad
is perfectly secret, it is useful to envision it as a bi-
partite graph as we‚Äôve done in Fig. 21.8. (In fact the
encryption scheme of Fig. 21.8 is precisely the one-
time pad for ùëõ
=
2.) For every ùëõ, the one-time pad
encryption scheme corresponds to a bipartite graph
with 2ùëõvertices on the ‚Äúleft side‚Äù corresponding to the
plaintexts in {0, 1}ùëõand 2ùëõvertices on the ‚Äúright side‚Äù
corresponding to the ciphertexts {0, 1}ùëõ. For every
ùë•‚àà{0, 1}ùëõand ùëò‚àà{0, 1}ùëõ, we connect ùë•to the vertex
ùë¶= ùê∏ùëò(ùë•) with an edge that we label with ùëò. One can
see that this is the complete bipartite graph, where
every vertex on the left is connected to all vertices on
the right. In particular this means that for every left
vertex ùë•, the distribution on the ciphertexts obtained
by taking a random ùëò
‚àà
{0, 1}ùëõand going to the
neighbor of ùë•on the edge labeled ùëòis the uniform dis-
tribution over {0, 1}ùëõ. This ensures the perfect secrecy
condition.
21.5 NECESSITY OF LONG KEYS
So, does Theorem 21.4 give the final word on cryptography, and
means that we can all communicate with perfect secrecy and live
happily ever after? No it doesn‚Äôt. While the one-time pad is efficient,
and gives perfect secrecy, it has one glaring disadvantage: to commu-
nicate ùëõbits you need to store a key of length ùëõ. In contrast, practically
used cryptosystems such as AES-128 have a short key of 128 bits (i.e.,
16 bytes) that can be used to protect terabytes or more of communica-
tion! Imagine that we all needed to use the one time pad. If that was
the case, then if you had to communicate with ùëöpeople, you would
have to maintain (securely!) ùëöhuge files that are each as long as the
length of the maximum total communication you expect with that per-
son. Imagine that every time you opened an account with Amazon,
Google, or any other service, they would need to send you in the mail
(ideally with a secure courier) a DVD full of random numbers, and
every time you suspected a virus, you‚Äôd need to ask all these services
for a fresh DVD. This doesn‚Äôt sound so appealing.
This is not just a theoretical issue. The Soviets have used the one-
time pad for their confidential communication since before the 1940‚Äôs.
In fact, even before Shannon‚Äôs work, the U.S. intelligence already


--- Page 589 ---

cryptography
589
Figure 21.10: Gene Grabeel, who founded the U.S.
Russian SigInt program on 1 Feb 1943. Photo taken in
1942, see Page 7 in the Venona historical study.
Figure 21.11: An encryption scheme where the num-
ber of keys is smaller than the number of plaintexts
corresponds to a bipartite graph where the degree is
smaller than the number of vertices on the left side.
Together with the validity condition this implies that
there will be two left vertices ùë•, ùë•‚Ä≤ with non-identical
neighborhoods, and hence the scheme does not satisfy
perfect secrecy.
knew in 1941 that the one-time pad is in principle ‚Äúunbreakable‚Äù (see
page 32 in the Venona document). However, it turned out that the
hassle of manufacturing so many keys for all the communication took
its toll on the Soviets and they ended up reusing the same keys for
more than one message. They did try to use them for completely dif-
ferent receivers in the (false) hope that this wouldn‚Äôt be detected. The
Venona Project of the U.S. Army was founded in February 1943 by
Gene Grabeel (see Fig. 21.10), a former home economics teacher from
Madison Heights, Virgnia and Lt. Leonard Zubko. In October 1943,
they had their breakthrough when it was discovered that the Russians
were reusing their keys. In the 37 years of its existence, the project has
resulted in a treasure chest of intelligence, exposing hundreds of KGB
agents and Russian spies in the U.S. and other countries, including
Julius Rosenberg, Harry Gold, Klaus Fuchs, Alger Hiss, Harry Dexter
White and many others.
Unfortunately it turns out that such long keys are necessary for
perfect secrecy:
Theorem 21.5 ‚Äî Perfect secrecy requires long keys. For every perfectly
secret encryption scheme (ùê∏, ùê∑) the length function ùêøsatisfies
ùêø(ùëõ) ‚â§ùëõ.
Proof Idea:
The idea behind the proof is illustrated in Fig. 21.11. We define a
graph between the plaintexts and ciphertexts, where we put an edge
between plaintext ùë•and ciphertext ùë¶if there is some key ùëòsuch that
ùë¶= ùê∏ùëò(ùë•). The degree of this graph is at most the number of potential
keys. The fact that the degree is smaller than the number of plaintexts
(and hence of ciphertexts) implies that there would be two plaintexts
ùë•and ùë•‚Ä≤ with different sets of neighbors, and hence the distribution
of a ciphertext corresponding to ùë•(with a random key) will not be
identical to the distribution of a ciphertext corresponding to ùë•‚Ä≤.
‚ãÜ
Proof of Theorem 21.5. Let ùê∏, ùê∑be a valid encryption scheme with
messages of length ùêøand key of length ùëõ< ùêø. We will show that
(ùê∏, ùê∑) is not perfectly secret by providing two plaintexts ùë•0, ùë•1 ‚àà
{0, 1}ùêøsuch that the distributions ùëåùë•0 and ùëåùë•1 are not identical, where
ùëåùë•is the distribution obtained by picking ùëò‚àº{0, 1}ùëõand outputting
ùê∏ùëò(ùë•).
We choose ùë•0 = 0ùêø. Let ùëÜ0 ‚äÜ{0, 1}‚àóbe the set of all ciphertexts
that have nonzero probability of being output in ùëåùë•0. That is, ùëÜ0 =
{ùë¶| ‚àÉùëò‚àà{0,1}ùëõùë¶= ùê∏ùëò(ùë•0)}. Since there are only 2ùëõkeys, we know that
|ùëÜ0| ‚â§2ùëõ.


--- Page 590 ---

590
introduction to theoretical computer science
We will show the following claim:
Claim I: There exists some ùë•1 ‚àà{0, 1}ùêøand ùëò‚àà{0, 1}ùëõsuch that
ùê∏ùëò(ùë•1) ‚àâùëÜ0.
Claim I implies that the string ùê∏ùëò(ùë•1) has positive probability of
being output by ùëåùë•1 and zero probability of being output by ùëåùë•0 and
hence in particular ùëåùë•0 and ùëåùë•1 are not identical. To prove Claim I, just
choose a fixed ùëò‚àà{0, 1}ùëõ. By the validity condition, the map ùë•‚Ü¶
ùê∏ùëò(ùë•) is a one to one map of {0, 1}ùêøto {0, 1}‚àóand hence in particular
the image of this map which is the set ùêºùëò= {ùë¶| ‚àÉùë•‚àà{0,1}ùêøùë¶= ùê∏ùëò(ùë•)}
has size at least (in fact exactly) 2ùêø. Since |ùëÜ0| ‚â§2ùëõ< 2ùêø, this means
that |ùêºùëò| > |ùëÜ0| and so in particular there exists some string ùë¶in ùêºùëò‚ßµùëÜ0.
But by the definition of ùêºùëòthis means that there is some ùë•‚àà{0, 1}ùêø
such that ùê∏ùëò(ùë•) ‚àâùëÜ0 which concludes the proof of Claim I and hence
of Theorem 21.5.
‚ñ†
21.6 COMPUTATIONAL SECRECY
To sum up the previous episodes, we now know that:
‚Ä¢ It is possible to obtain a perfectly secret encryption scheme with key
length the same as the plaintext.
and
‚Ä¢ It is not possible to obtain such a scheme with key that is even a
single bit shorter than the plaintext.
How does this mesh with the fact that, as we‚Äôve already seen, peo-
ple routinely use cryptosystems with a 16 byte (i.e., 128 bit) key but
many terabytes of plaintext? The proof of Theorem 21.5 does give in
fact a way to break all these cryptosystems, but an examination of this
proof shows that it only yields an algorithm with time exponential in
the length of the key. This motivates the following relaxation of perfect
secrecy to a condition known as ‚Äúcomputational secrecy‚Äù. Intuitively,
an encryption scheme is computationally secret if no polynomial time
algorithm can break it. The formal definition is below:
Definition 21.6 ‚Äî Computational secrecy. Let (ùê∏, ùê∑) be a valid encryp-
tion scheme where for keys of length ùëõ, the plaintexts are of length
ùêø(ùëõ) and the ciphertexts are of length ùëö(ùëõ). We say that (ùê∏, ùê∑) is
computationally secret if for every polynomial ùëù‚à∂‚Ñï‚Üí‚Ñï, and large


--- Page 591 ---

cryptography
591
enough ùëõ, if ùëÉis an ùëö(ùëõ)-input and single output NAND-CIRC
program of at most ùëù(ùëõ) lines, and ùë•0, ùë•1 ‚àà{0, 1}ùêø(ùëõ) then
‚à£
ùîº
ùëò‚àº{0,1}ùëõ[ùëÉ(ùê∏ùëò(ùë•0))] ‚àí
ùîº
ùëò‚àº{0,1}ùëõ[ùëÉ(ùê∏ùëò(ùë•1))]‚à£<
1
ùëù(ùëõ)
(21.4)
P
Definition 21.6 requires a second or third read and
some practice to truly understand. One excellent exer-
cise to make sure you follow it is to see that if we allow
ùëÉto be an arbitrary function mapping {0, 1}ùëö(ùëõ) to
{0, 1}, and we replace the condition in (21.4) that the
lefthand side is smaller than
1
ùëù(ùëõ) with the condition
that it is equal to 0 then we get the perfect secrecy
condition of Definition 21.3. Indeed if the distributions
ùê∏ùëò(ùë•0) and ùê∏ùëò(ùë•1) are identical then applying any
function ùëÉto them we get the same expectation. On
the other hand, if the two distributions above give a
different probability for some element ùë¶‚àó‚àà{0, 1}ùëö(ùëõ),
then the function ùëÉ(ùë¶) that outputs 1 iff ùë¶
=
ùë¶‚àówill
have a different expectation under the former distribu-
tion than under the latter.
Definition 21.6 raises two natural questions:
‚Ä¢ Is it strong enough to ensure that a computationally secret encryp-
tion scheme protects the secrecy of messages that are encrypted
with it?
‚Ä¢ It is weak enough that, unlike perfect secrecy, it is possible to obtain
a computationally secret encryption scheme where the key is much
smaller than the message?
To the best of our knowledge, the answer to both questions is Yes.
This is just one example of a much broader phenomenon. We can
use computational hardness to achieve many cryptographic goals,
including some goals that have been dreamed about for millenia, and
other goals that people have not even dared to imagine.
ÔÉ´Big Idea 27 Computational hardness is necessary and sufficient for
almost all cryptographic applications.
Regarding the first question, it is not hard to show that if, for ex-
ample, Alice uses a computationally secret encryption algorithm to
encrypt either ‚Äúattack‚Äù or ‚Äúretreat‚Äù (each chosen with probability


--- Page 592 ---

592
introduction to theoretical computer science
Figure 21.12: In a stream cipher or ‚Äúderandomized
one-time pad‚Äù we use a pseudorandom generator
ùê∫‚à∂{0, 1}ùëõ‚Üí{0, 1}ùêøto obtain an encryption scheme
with a key length of ùëõand plaintexts of length ùêø.
We encrypt the plaintext ùë•‚àà{0, 1}ùêøwith key
ùëò‚àà{0, 1}ùëõby the ciphertext ùë•‚äïùê∫(ùëò).
1/2), then as long as she‚Äôs restricted to polynomial-time algorithms, an
adversary Eve will not be able to guess the message with probability
better than, say, 0.51, even after observing its encrypted form. (We
omit the proof, but it is an excellent exercise for you to work it out on
your own.)
To answer the second question we will show that under the same
assumption we used for derandomizing BPP, we can obtain a com-
putationally secret cryptosystem where the key is almost exponentially
smaller than the plaintext.
21.6.1 Stream ciphers or the ‚Äúderandomized one-time pad‚Äù
It turns out that if pseudorandom generators exist as in the optimal
PRG conjecture, then there exists a computationally secret encryption
scheme with keys that are much shorter than the plaintext. The con-
struction below is known as a stream cipher, though perhaps a better
name is the ‚Äúderandomized one-time pad‚Äù. It is widely used in prac-
tice with keys on the order of a few tens or hundreds of bits protecting
many terabytes or even petabytes of communication.
We start by recalling the notion of a pseudorandom generator, as de-
fined in Definition 20.9. For this chapter, we will fix a special case of
the definition:
Definition 21.7 ‚Äî Cryptographic pseudorandom generator. Let ùêø‚à∂‚Ñï‚Üí‚Ñïbe
some function. A cryptographic pseudorandom generator with stretch
ùêø(‚ãÖ) is a polynomial-time computable function ùê∫‚à∂{0, 1}‚àó‚Üí{0, 1}‚àó
such that:
‚Ä¢ For every ùëõ‚àà‚Ñïand ùë†‚àà{0, 1}ùëõ, |ùê∫(ùë†)| = ùêø(ùëõ).
‚Ä¢ For every polynomial ùëù‚à∂‚Ñï‚Üí‚Ñïand ùëõlarge enough, if ùê∂is a cir-
cuit of ùêø(ùëõ) inputs, one output, and at most ùëù(ùëõ) gates then
‚à£
Pr
ùë†‚àº{0,1}‚Ñì[ùê∂(ùê∫(ùë†)) = 1] ‚àí
Pr
ùëü‚àº{0,1}ùëö[ùê∂(ùëü) = 1]‚à£<
1
ùëù(ùëõ) .
(21.5)
In this chapter we will call a cryptographic pseudorandom gener-
ator simply a pseudorandom generator or PRG for short. The optimal
PRG conjecture of Section 20.4.2 implies that there is a pseudoran-
dom generator that can ‚Äúfool‚Äù circuits of exponential size and where
the gap in probabilities is at most one over an exponential quantity.
Since exponential grow faster than every polynomial, the optimal PRG
conjecture implies the following:
The crypto PRG conjecture: For every ùëé‚àà‚Ñï, there is a cryptographic
pseudorandom generator with ùêø(ùëõ) = ùëõùëé.


--- Page 593 ---

cryptography
593
The crypto PRG conjecture is a weaker conjecture than the optimal
PRG conjecture, but it too (as we will see) is still stronger than the
conjecture that P ‚â†NP.
Theorem 21.8 ‚Äî Derandomized one-time pad. Suppose that the crypto
PRG conjecture is true. Then for every constant ùëé
‚àà
‚Ñïthere is a
computationally secret encryption scheme (ùê∏, ùê∑) with plaintext
length ùêø(ùëõ) at least ùëõùëé.
Proof Idea:
The proof is illustrated in Fig. 21.12. We simply take the one-time
pad on ùêøbit plaintexts, but replace the key with ùê∫(ùëò) where ùëòis a
string in {0, 1}ùëõand ùê∫‚à∂{0, 1}ùëõ‚Üí{0, 1}ùêøis a pseudorandom gen-
erator. Since the one time pad cannot be broken, an adversary that
breaks the derandomized one-time pad can be used to distinguish
between the output of the pseudorandom generator and the uniform
distribution.
‚ãÜ
Proof of Theorem 21.8. Let ùê∫‚à∂{0, 1}ùëõ‚Üí{0, 1}ùêøfor ùêø= ùëõùëébe the
restriction to input length ùëõof the pseudorandom generator ùê∫whose
existence we are guaranteed from the crypto PRG conjecture. We
now define our encryption scheme as follows: given key ùëò‚àà{0, 1}ùëõ
and plaintext ùë•‚àà{0, 1}ùêø, the encryption ùê∏ùëò(ùë•) is simply ùë•‚äïùê∫(ùëò).
To decrypt a string ùë¶‚àà{0, 1}ùëöwe output ùë¶‚äïùê∫(ùëò). This is a valid
encryption since ùê∫is computable in polynomial time and (ùë•‚äïùê∫(ùëò)) ‚äï
ùê∫(ùëò) = ùë•‚äï(ùê∫(ùëò) ‚äïùê∫(ùëò)) = ùë•for every ùë•‚àà{0, 1}ùêø.
Computational secrecy follows from the condition of a pseudo-
random generator. Suppose, towards a contradiction, that there is
a polynomial ùëù, NAND-CIRC program ùëÑof at most ùëù(ùêø) lines and
ùë•, ùë•‚Ä≤ ‚àà{0, 1}ùêø(ùëõ) such that
‚à£
ùîº
ùëò‚àº{0,1}ùëõ[ùëÑ(ùê∏ùëò(ùë•))] ‚àí
ùîº
ùëò‚àº{0,1}ùëõ[ùëÑ(ùê∏ùëò(ùë•‚Ä≤))]‚à£>
1
ùëù(ùêø) .
(21.6)
(We use here the simple fact that for a {0, 1}-valued random variable
ùëã, Pr[ùëã= 1] = ùîº[ùëã].)
By the definition of our encryption scheme, this means that
‚à£
ùîº
ùëò‚àº{0,1}ùëõ[ùëÑ(ùê∫(ùëò) ‚äïùë•)] ‚àí
ùîº
ùëò‚àº{0,1}ùëõ[ùëÑ(ùê∫(ùëò) ‚äïùë•‚Ä≤)]‚à£>
1
ùëù(ùêø) .
(21.7)
Now since (as we saw in the security analysis of the one-time pad),
for every strings ùë•, ùë•‚Ä≤ ‚àà{0, 1}ùêø, the distribution ùëü‚äïùë•and ùëü‚äïùë•‚Ä≤ are
identical, where ùëü‚àº{0, 1}ùêø. Hence
ùîº
ùëü‚àº{0,1}ùêø[ùëÑ(ùëü‚äïùë•)] =
ùîº
ùëü‚àº{0,1}ùêø[ùëÑ(ùëü‚äïùë•‚Ä≤)] .
(21.8)


--- Page 594 ---

594
introduction to theoretical computer science
By plugging (21.8) into (21.7) we can derive that
‚à£
ùîº
ùëò‚àº{0,1}ùëõ[ùëÑ(ùê∫(ùëò) ‚äïùë•)] ‚àí
ùîº
ùëü‚àº{0,1}ùêø[ùëÑ(ùëü‚äïùë•)] +
ùîº
ùëü‚àº{0,1}ùêø[ùëÑ(ùëü‚äïùë•‚Ä≤)] ‚àí
ùîº
ùëò‚àº{0,1}ùëõ[ùëÑ(ùê∫(ùëò) ‚äïùë•‚Ä≤)]‚à£>
1
ùëù(ùêø) .
(21.9)
(Please make sure that you can see why this is true.)
Now we can use the triangle inequality that |ùê¥+ ùêµ| ‚â§|ùê¥| + |ùêµ| for
every two numbers ùê¥, ùêµ, applying it for ùê¥= ùîºùëò‚àº{0,1}ùëõ[ùëÑ(ùê∫(ùëò) ‚äïùë•)] ‚àí
ùîºùëü‚àº{0,1}ùêø[ùëÑ(ùëü‚äïùë•)] and ùêµ= ùîºùëü‚àº{0,1}ùêø[ùëÑ(ùëü‚äïùë•‚Ä≤)]‚àíùîºùëò‚àº{0,1}ùëõ[ùëÑ(ùê∫(ùëò)‚äïùë•‚Ä≤)]
to derive
‚à£
ùîº
ùëò‚àº{0,1}ùëõ[ùëÑ(ùê∫(ùëò) ‚äïùë•)] ‚àí
ùîº
ùëü‚àº{0,1}ùêø[ùëÑ(ùëü‚äïùë•)]‚à£+‚à£
ùîº
ùëü‚àº{0,1}ùêø[ùëÑ(ùëü‚äïùë•‚Ä≤)] ‚àí
ùîº
ùëò‚àº{0,1}ùëõ[ùëÑ(ùê∫(ùëò) ‚äïùë•‚Ä≤)]‚à£>
1
ùëù(ùêø) .
(21.10)
In particular, either the first term or the second term of the
lefthand-side of (21.10) must be at least
1
2ùëù(ùêø). Let us assume the first
case holds (the second case is analyzed in exactly the same way).
Then we get that
‚à£
ùîº
ùëò‚àº{0,1}ùëõ[ùëÑ(ùê∫(ùëò) ‚äïùë•)] ‚àí
ùîº
ùëü‚àº{0,1}ùêø[ùëÑ(ùëü‚äïùë•)]‚à£>
1
2ùëù(ùêø) .
(21.11)
But if we now define the NAND-CIRC program ùëÉùë•that on input
ùëü‚àà{0, 1}ùêøoutputs ùëÑ(ùëü‚äïùë•) then (since XOR of ùêøbits can be computed
in ùëÇ(ùêø) lines), we get that ùëÉùë•has ùëù(ùêø) + ùëÇ(ùêø) lines and by (21.11) it
can distinguish between an input of the form ùê∫(ùëò) and an input of the
form ùëü‚àº{0, 1}ùëòwith advantage better than
1
2ùëù(ùêø). Since a polynomial
is dominated by an exponential, if we make ùêølarge enough, this will
contradict the (2ùõøùëõ, 2‚àíùõøùëõ) security of the pseudorandom generator ùê∫.
‚ñ†
R
Remark 21.9 ‚Äî Stream ciphers in practice. The two
most widely used forms of (private key) encryption
schemes in practice are stream ciphers and block ciphers.
(To make things more confusing, a block cipher is
always used in some mode of operation and some
of these modes effectively turn a block cipher into
a stream cipher.) A block cipher can be thought as
a sort of a ‚Äúrandom invertible map‚Äù from {0, 1}ùëõto
{0, 1}ùëõ, and can be used to construct a pseudorandom
generator and from it a stream cipher, or to encrypt
data directly using other modes of operations. There
are a great many other security notions and consider-
ations for encryption schemes beyond computational
secrecy. Many of those involve handling scenarios
such as chosen plaintext, man in the middle, and cho-
sen ciphertext attacks, where the adversary is not just
merely a passive eavesdropper but can influence the
communication in some way. While this chapter is


--- Page 595 ---

cryptography
595
meant to give you some taste of the ideas behind cryp-
tography, there is much more to know before applying
it correctly to obtain secure applications, and a great
many people have managed to get it wrong.
21.7 COMPUTATIONAL SECRECY AND NP
We‚Äôve also mentioned before that an efficient algorithm for NP could
be used to break all cryptography. We now give an example of how
this can be done:
Theorem 21.10 ‚Äî Breaking encryption using NP algorithm. If P
=
NP
then there is no computationally secret encryption scheme with
ùêø(ùëõ) > ùëõ.
Furthermore, for every valid encryption scheme (ùê∏, ùê∑) with
ùêø(ùëõ)
>
ùëõ+ 100 there is a polynomial ùëùsuch that for every large
enough ùëõthere exist ùë•0, ùë•1
‚àà
{0, 1}ùêø(ùëõ) and a ùëù(ùëõ)-line NAND-
CIRC program EVE s.t.
Pr
ùëñ‚àº{0,1},ùëò‚àº{0,1}ùëõ[EVE(ùê∏ùëò(ùë•ùëñ)) = ùëñ] ‚â•0.99 .
(21.12)
Note that the ‚Äúfurthermore‚Äù part is extremely strong. It means
that if the plaintext is even a little bit larger than the key, then we can
already break the scheme in a very strong way. That is, there will be
a pair of messages ùë•0, ùë•1 (think of ùë•0 as ‚Äúsell‚Äù and ùë•1 as ‚Äúbuy‚Äù) and
an efficient strategy for Eve such that if Eve gets a ciphertext ùë¶then
she will be able to tell whether ùë¶is an encryption of ùë•0 or ùë•1 with
probability very close to 1. (We model breaking the scheme as Eve
outputting 0 or 1 corresponding to whether the message sent was ùë•0
or ùë•1. Note that we could have just as well modified Eve to output ùë•0
instead of 0 and ùë•1 instead of 1. The key point is that a priori Eve only
had a 50/50 chance of guessing whether Alice sent ùë•0 or ùë•1 but after
seeing the ciphertext this chance increases to better than 99/1.) The
condition P = NP can be relaxed to NP ‚äÜBPP and even the weaker
condition NP ‚äÜP/poly with essentially the same proof.
Proof Idea:
The proof follows along the lines of Theorem 21.5 but this time
paying attention to the computational aspects. If P = NP then for
every plaintext ùë•and ciphertext ùë¶, we can efficiently tell whether there
exists ùëò‚àà{0, 1}ùëõsuch that ùê∏ùëò(ùë•) = ùë¶. So, to prove this result we need
to show that if the plaintexts are long enough, there would exist a pair
ùë•0, ùë•1 such that the probability that a random encryption of ùë•1 also is
a valid encryption of ùë•0 will be very small. The details of how to show
this are below.


--- Page 596 ---

596
introduction to theoretical computer science
‚ãÜ
Proof of Theorem 21.10. We focus on showing only the ‚Äúfurthermore‚Äù
part since it is the more interesting and the other part follows by es-
sentially the same proof.
Suppose that (ùê∏, ùê∑) is such an encryption, let ùëõbe large enough,
and let ùë•0 = 0ùêø(ùëõ). For every ùë•‚àà{0, 1}ùêø(ùëõ) we define ùëÜùë•to be the set
of all valid encryption of ùë•. That is ùëÜùë•= {ùë¶| ‚àÉùëò‚àà{0,1}ùëõùë¶= ùê∏ùëò(ùë•)}. As
in the proof of Theorem 21.5, since there are 2ùëõkeys ùëò, |ùëÜùë•| ‚â§2ùëõfor
every ùë•‚àà{0, 1}ùêø(ùëõ).
We denote by ùëÜ0 the set ùëÜùë•0. We define our algorithm EVE to out-
put 0 on input ùë¶‚àà{0, 1}‚àóif ùë¶‚ààùëÜ0 and to output 1 otherwise. This
can be implemented in polynomial time if P = NP, since the key ùëò
can serve the role of an efficiently verifiable solution. (Can you see
why?) Clearly Pr[EVE(ùê∏ùëò(ùë•0)) = 0] = 1 and so in the case that EVE
gets an encryption of ùë•0 then she guesses correctly with probability
1. The remainder of the proof is devoted to showing that there ex-
ists ùë•1 ‚àà{0, 1}ùêø(ùëõ) such that Pr[EVE(ùê∏ùëò(ùë•1)) = 0] ‚â§0.01, which
will conclude the proof by showing that EVE guesses wrongly with
probability at most 1
20 + 1
20.01 < 0.01.
Consider now the following probabilistic experiment (which we
define solely for the sake of analysis). We consider the sample space
of choosing ùë•uniformly in {0, 1}ùêø(ùëõ) and define the random variable
ùëçùëò(ùë•) to equal 1 if and only if ùê∏ùëò(ùë•) ‚ààùëÜ0. For every ùëò, the map ùë•‚Ü¶
ùê∏ùëò(ùë•) is one-to-one, which means that the probability that ùëçùëò= 1
is equal to the probability that ùë•‚ààùê∏‚àí1
ùëò(ùëÜ0) which is
|ùëÜ0|
2ùêø(ùëõ) . So by the
linearity of expectation ùîº[‚àëùëò‚àà{0,1}ùëõùëçùëò] ‚â§2ùëõ|ùëÜ0|
2ùêø(ùëõ) ‚â§
22ùëõ
2ùêø(ùëõ) .
We will now use the following extremely simple but useful fact
known as the averaging principle (see also Lemma 18.10): for every
random variable ùëç, if ùîº[ùëç] = ùúá, then with positive probability ùëç‚â§ùúá.
(Indeed, if ùëç> ùúáwith probability one, then the expected value of ùëç
will have to be larger than ùúá, just like you can‚Äôt have a class in which
all students got A or A- and yet the overall average is B+.) In our case
it means that with positive probability ‚àëùëò‚àà{0,1}ùëõùëçùëò‚â§
22ùëõ
2ùêø(ùëõ) . In other
words, there exists some ùë•1 ‚àà{0, 1}ùêø(ùëõ) such that ‚àëùëò‚àà{0,1}ùëõùëçùëò(ùë•1) ‚â§
22ùëõ
2ùêø(ùëõ) . Yet this means that if we choose a random ùëò‚àº{0, 1}ùëõ, then
the probability that ùê∏ùëò(ùë•1) ‚ààùëÜ0 is at most
1
2ùëõ‚ãÖ
22ùëõ
2ùêø(ùëõ) = 2ùëõ‚àíùêø(ùëõ).
So, in particular if we have an algorithm EVE that outputs 0 if ùë•‚àà
ùëÜ0 and outputs 1 otherwise, then Pr[EVE(ùê∏ùëò(ùë•0)) = 0] = 1 and
Pr[EVE(ùê∏ùëò(ùë•1)) = 0] ‚â§2ùëõ‚àíùêø(ùëõ) which will be smaller than 2‚àí10 < 0.01
if ùêø(ùëõ) ‚â•ùëõ+ 10.
‚ñ†


--- Page 597 ---

cryptography
597
In retrospect Theorem 21.10 is perhaps not surprising. After all, as
we‚Äôve mentioned before it is known that the Optimal PRG conjecture
(which is the basis for the derandomized one-time pad encryption) is
false if P = NP (and in fact even if NP ‚äÜBPP or even NP ‚äÜP/poly).
21.8 PUBLIC KEY CRYPTOGRAPHY
People have been dreaming about heavier-than-air flight since at least
the days of Leonardo Da Vinci (not to mention Icarus from the greek
mythology). Jules Verne wrote with rather insightful details about
going to the moon in 1865. But, as far as I know, in all the thousands
of years people have been using secret writing, until about 50 years
ago no one has considered the possibility of communicating securely
without first exchanging a shared secret key.
Yet in the late 1960‚Äôs and early 1970‚Äôs, several people started to
question this ‚Äúcommon wisdom‚Äù. Perhaps the most surprising of
these visionaries was an undergraduate student at Berkeley named
Ralph Merkle. In the fall of 1974 Merkle wrote in a project proposal
for his computer security course that while ‚Äúit might seem intuitively
obvious that if two people have never had the opportunity to prear-
range an encryption method, then they will be unable to communicate
securely over an insecure channel‚Ä¶ I believe it is false‚Äù. The project
proposal was rejected by his professor as ‚Äúnot good enough‚Äù. Merkle
later submitted a paper to the communication of the ACM where he
apologized for the lack of references since he was unable to find any
mention of the problem in the scientific literature, and the only source
where he saw the problem even raised was in a science fiction story.
The paper was rejected with the comment that ‚ÄúExperience shows that
it is extremely dangerous to transmit key information in the clear.‚Äù
Merkle showed that one can design a protocol where Alice and Bob
can use ùëáinvocations of a hash function to exchange a key, but an
adversary (in the random oracle model, though he of course didn‚Äôt
use this name) would need roughly ùëá2 invocations to break it. He
conjectured that it may be possible to obtain such protocols where
breaking is exponentially harder than using them, but could not think of
any concrete way to doing so.
We only found out much later that in the late 1960‚Äôs, a few years
before Merkle, James Ellis of the British Intelligence agency GCHQ
was having similar thoughts. His curiosity was spurred by an old
World-War II manuscript from Bell labs that suggested the following
way that two people could communicate securely over a phone line.
Alice would inject noise to the line, Bob would relay his messages,
and then Alice would subtract the noise to get the signal. The idea is
that an adversary over the line sees only the sum of Alice‚Äôs and Bob‚Äôs
signals, and doesn‚Äôt know what came from what. This got James Ellis


--- Page 598 ---

598
introduction to theoretical computer science
thinking whether it would be possible to achieve something like that
digitally. As Ellis later recollected, in 1970 he realized that in princi-
ple this should be possible, since he could think of an hypothetical
black box ùêµthat on input a ‚Äúhandle‚Äù ùõºand plaintext ùë•would give a
‚Äúciphertext‚Äù ùë¶and that there would be a secret key ùõΩcorresponding
to ùõº, such that feeding ùõΩand ùë¶to the box would recover ùë•. However,
Ellis had no idea how to actually instantiate this box. He and others
kept giving this question as a puzzle to bright new recruits until one
of them, Clifford Cocks, came up in 1973 with a candidate solution
loosely based on the factoring problem; in 1974 another GCHQ re-
cruit, Malcolm Williamson, came up with a solution using modular
exponentiation.
But among all those thinking of public key cryptography, probably
the people who saw the furthest were two researchers at Stanford,
Whit Diffie and Martin Hellman. They realized that with the advent
of electronic communication, cryptography would find new applica-
tions beyond the military domain of spies and submarines, and they
understood that in this new world of many users and point to point
communication, cryptography will need to scale up. Diffie and Hell-
man envisioned an object which we now call ‚Äútrapdoor permutation‚Äù
though they called ‚Äúone way trapdoor function‚Äù or sometimes simply
‚Äúpublic key encryption‚Äù. Though they didn‚Äôt have full formal defini-
tions, their idea was that this is an injective function that is easy (e.g.,
polynomial-time) to compute but hard (e.g., exponential-time) to in-
vert. However, there is a certain trapdoor, knowledge of which would
allow polynomial time inversion. Diffie and Hellman argued that us-
ing such a trapdoor function, it would be possible for Alice and Bob
to communicate securely without ever having exchanged a secret key. But
they didn‚Äôt stop there. They realized that protecting the integrity of
communication is no less important than protecting its secrecy. Thus
they imagined that Alice could ‚Äúrun encryption in reverse‚Äù in order to
certify or sign messages.
At the point, Diffie and Hellman were in a position not unlike
physicists who predicted that a certain particle should exist but with-
out any experimental verification. Luckily they met Ralph Merkle,
and his ideas about a probabilistic key exchange protocol, together with
a suggestion from their Stanford colleague John Gill, inspired them
to come up with what today is known as the Diffie Hellman Key Ex-
change (which unbeknownst to them was found two years earlier at
GCHQ by Malcolm Williamson). They published their paper ‚ÄúNew
Directions in Cryptography‚Äù in 1976, and it is considered to have
brought about the birth of modern cryptography.
The Diffie-Hellman Key Exchange is still widely used today for
secure communication. However, it still felt short of providing Diffie


--- Page 599 ---

cryptography
599
Figure 21.13: Top left: Ralph Merkle, Martin Hellman
and Whit Diffie, who together came up in 1976
with the concept of public key encryption and a key
exchange protocol. Bottom left: Adi Shamir, Ron Rivest,
and Leonard Adleman who, following Diffie and
Hellman‚Äôs paper, discovered the RSA function that
can be used for public key encryption and digital
signatures. Interestingly, one can see the equation
P = NP on the blackboard behind them. Right: John
Gill, who was the first person to suggest to Diffie and
Hellman that they use modular exponentiation as an
easy-to-compute but hard-to-invert function.
Figure 21.14: In a public key encryption, Alice generates
a private/public keypair (ùëí, ùëë), publishes ùëíand keeps
ùëësecret. To encrypt a message for Alice, one only
needs to know ùëí. To decrypt it we need to know ùëë.
and Hellman‚Äôs elusive trapdoor function. This was done the next year
by Rivest, Shamir and Adleman who came up with the RSA trapdoor
function, which through the framework of Diffie and Hellman yielded
not just encryption but also signatures. (A close variant of the RSA
function was discovered earlier by Clifford Cocks at GCHQ, though
as far as I can tell Cocks, Ellis and Williamson did not realize the
application to digital signatures.) From this point on began a flurry of
advances in cryptography which hasn‚Äôt died down till this day.
21.8.1 Defining public key encryption
A public key encryption consists of a triple of algorithms:
‚Ä¢ The key generation algorithm, which we denote by ùêæùëíùë¶ùê∫ùëíùëõor KG for
short, is a randomized algorithm that outputs a pair of strings (ùëí, ùëë)
where ùëíis known as the public (or encryption) key, and ùëëis known
as the private (or decryption) key. The key generation algorithm gets
as input 1ùëõ(i.e., a string of ones of length ùëõ). We refer to ùëõas the
security parameter of the scheme. The bigger we make ùëõ, the more
secure the encryption will be, but also the less efficient it will be.
‚Ä¢ The encryption algorithm, which we denote by ùê∏, takes the encryp-
tion key ùëíand a plaintext ùë•, and outputs the ciphertext ùë¶= ùê∏ùëí(ùë•).
‚Ä¢ The decryption algorithm, which we denote by ùê∑, takes the decryp-
tion key ùëëand a ciphertext ùë¶, and outputs the plaintext ùë•= ùê∑ùëë(ùë¶).
We now make this a formal definition:
Definition 21.11 ‚Äî Public Key Encryption. A computationally secret public
key encryption with plaintext length ùêø‚à∂‚Ñï‚Üí‚Ñïis a triple of ran-
domized polynomial-time algorithms (KG, ùê∏, ùê∑) that satisfy the
following conditions:
‚Ä¢ For every ùëõ, if (ùëí, ùëë) is output by KG(1ùëõ) with positive proba-
bility, and ùë•‚àà{0, 1}ùêø(ùëõ), then ùê∑ùëë(ùê∏ùëí(ùë•)) = ùë•with probability
one.
‚Ä¢ For every polynomial ùëù, and sufficiently large ùëõ, if ùëÉis a NAND-
CIRC program of at most ùëù(ùëõ) lines then for every ùë•, ùë•‚Ä≤
‚àà
{0, 1}ùêø(ùëõ), |ùîº[ùëÉ(ùëí, ùê∏ùëí(ùë•))] ‚àíùîº[ùëÉ(ùëí, ùê∏ùëí(ùë•‚Ä≤))]|
<
1/ùëù(ùëõ), where
this probability is taken over the coins of KG and ùê∏.
Definition 21.11 allows ùê∏and ùê∑to be randomized algorithms. In
fact, it turns out that it is necessary for ùê∏to be randomized to obtain
computational secrecy. It also turns out that, unlike the private key
case, we can transform a public-key encryption that works for mes-
sages that are only one bit long into a public-key encryption scheme


--- Page 600 ---

600
introduction to theoretical computer science
that can encrypt arbitrarily long messages, and in particular messages
that are longer than the key. In particular this means that we cannot ob-
tain a perfectly secret public-key encryption scheme even for one-bit
long messages (since it would imply a perfectly secret public-key, and
hence in particular private-key, encryption with messages longer than
the key).
We will not give full constructions for public key encryption
schemes in this chapter, but will mention some of the ideas that
underlie the most widely used schemes today. These generally belong
to one of two families:
‚Ä¢ Group theoretic constructions based on problems such as integer factor-
ing and the discrete logarithm over finite fields or elliptic curves.
‚Ä¢ Lattice/coding based constructions based on problems such as the
closest vector in a lattice or bounded distance decoding.
Group-theory based encryptions such as the RSA cryptosystem, the
Diffie-Hellman protocol, and Elliptic-Curve Cryptography, are cur-
rently more widely implemented. But the lattice/coding schemes are
recently on the rise, particularly because the known group theoretic
encryption schemes can be broken by quantum computers, which we
discuss in Chapter 23.
21.8.2 Diffie-Hellman key exchange
As just one example of how public key encryption schemes are con-
structed, let us now describe the Diffie-Hellman key exchange. We
describe the Diffie-Hellman protocol in a somewhat of an informal
level, without presenting a full security analysis.
The computational problem underlying the Diffie Hellman protocol
is the discrete logarithm problem. Let‚Äôs suppose that ùëîis some integer.
We can compute the map ùë•‚Ü¶ùëîùë•and also its inverse ùë¶‚Ü¶logùëîùë¶. (For
example, we can compute a logarithm is by binary search: start with
some interval [ùë•ùëöùëñùëõ, ùë•ùëöùëéùë•] that is guaranteed to contain logùëîùë¶. We can
then test whether the interval‚Äôs midpoint ùë•ùëöùëñùëësatisfies ùëîùë•ùëöùëñùëë> ùë¶, and
based on that halve the size of the interval.)
However, suppose now that we use modular arithmetic and work
modulo some prime number ùëù. If ùëùhas ùëõbinary digits and ùëîis in [ùëù]
then we can compute the map ùë•‚Ü¶ùëîùë•mod ùëùin time polynomial in ùëõ.
(This is not trivial, and is a great exercise for you to work this out; as a
hint, start by showing that one can compute the map ùëò‚Ü¶ùëî2ùëòmod ùëù
using ùëòmodular multiplications modulo ùëù, if you‚Äôre stumped, you
can look up this Wikipedia entry.) On the other hand, because of the
‚Äúwraparound‚Äù property of modular arithmetic, we cannot run binary
search to find the inverse of this map (known as the discrete logarithm).


--- Page 601 ---

cryptography
601
In fact, there is no known polynomial-time algorithm for computing
this discrete logarithm map (ùëî, ùë•, ùëù) ‚Ü¶logùëîùë•mod ùëù, where we define
logùëîùë•mod ùëùas the number ùëé‚àà[ùëù] such that ùëîùëé= ùë•mod ùëù.
The Diffie-Hellman protocol for Bob to send a message to Alice is as
follows:
‚Ä¢ Alice: Chooses ùëùto be a random ùëõbit long prime (which can be
done by choosing random numbers and running a primality testing
algorithm on them), and ùëîand ùëéat random in [ùëù]. She sends to Bob
the triple (ùëù, ùëî, ùëîùëémod ùëù).
‚Ä¢ Bob: Given the triple (ùëù, ùëî, ‚Ñé), Bob sends a message ùë•‚àà{0, 1}ùêø
to Alice by choosing ùëèat random in [ùëù], and sending to Alice the
pair (ùëîùëèmod ùëù, ùëüùëíùëù(‚Ñéùëèmod ùëù) ‚äïùë•) where ùëüùëíùëù‚à∂[ùëù] ‚Üí{0, 1}‚àó
is some ‚Äúrepresentation function‚Äù that maps [ùëù] to {0, 1}ùêø. (The
function ùëüùëíùëùdoes not need to be one-to-one and you can think of
ùëüùëíùëù(ùëß) as simply outputting ùêøof the bits of ùëßin the natural binary
representation, it does need to satisfy certain technical conditions
which we omit in this description.)
‚Ä¢ Alice: Given ùëî‚Ä≤, ùëß, Alice recovers ùë•by outputting ùëüùëíùëù(ùëî‚Ä≤ùëémod ùëù) ‚äï
ùëß.
The correctness of the protocol follows from the simple fact that
(ùëîùëé)ùëè= (ùëîùëè)ùëéfor every ùëî, ùëé, ùëèand this still holds if we work modulo
ùëù. Its security relies on the computational assumption that computing
this map is hard, even in a certain ‚Äúaverage case‚Äù sense (this computa-
tional assumption is known as the Decisional Diffie Hellman assump-
tion). The Diffie-Hellman key exchange protocol can be thought of as
a public key encryption where the Alice‚Äôs first message is the public
key, and Bob‚Äôs message is the encryption.
One can think of the Diffie-Hellman protocol as being based on
a ‚Äútrapdoor pseudorandom generator‚Äù whereas the triple ùëîùëé, ùëîùëè, ùëîùëéùëè
looks ‚Äúrandom‚Äù to someone that doesn‚Äôt know ùëé, but someone that
does know ùëécan see that raising the second element to the ùëé-th power
yields the third element. The Diffie-Hellman protocol can be described
abstractly in the context of any finite Abelian group for which we can
efficiently compute the group operation. It has been implemented
on other groups than numbers modulo ùëù, and in particular Elliptic
Curve Cryptography (ECC) is obtained by basing the Diffie Hell-
man on elliptic curve groups which gives some practical advantages.
Another common group theoretic basis for key-exchange/public key
encryption protocol is the RSA function. A big disadvantage of Diffie-
Hellman (both the modular arithmetic and elliptic curve variants)
and RSA is that both schemes can be broken in polynomial time by a


--- Page 602 ---

602
introduction to theoretical computer science
quantum computer. We will discuss quantum computing later in this
course.
21.9 OTHER SECURITY NOTIONS
There is a great deal to cryptography beyond just encryption schemes,
and beyond the notion of a passive adversary. A central objective
is integrity or authentication: protecting communications from being
modified by an adversary. Integrity is often more fundamental than
secrecy: whether it is a software update or viewing the news, you
might often not care about the communication being secret as much as
that it indeed came from its claimed source. Digital signature schemes
are the analog of public key encryption for authentication, and are
widely used (in particular as the basis for public key certificates) to
provide a foundation of trust in the digital world.
Similarly, even for encryption, we often need to ensure security
against active attacks, and so notions such as non-malleability and
adaptive chosen ciphertext security have been proposed. An encryp-
tion scheme is only as secure as the secret key, and mechanisms to
make sure the key is generated properly, and is protected against re-
fresh or even compromise (i.e., forward secrecy) have been studied as
well. Hopefully this chapter provides you with some appreciation for
cryptography as an intellectual field, but does not imbue you with a
false self confidence in implementing it.
Cryptographic hash functions is another widely used tool with a va-
riety of uses, including extracting randomness from high entropy
sources, achieving hard-to-forge short ‚Äúdigests‚Äù of files, protecting
passwords, and much more.
21.10 MAGIC
Beyond encryption and signature schemes, cryptographers have man-
aged to obtain objects that truly seem paradoxical and ‚Äúmagical‚Äù. We
briefly discuss some of these objects. We do not give any details, but
hopefully this will spark your curiosity to find out more.
21.10.1 Zero knowledge proofs
On October 31, 1903, the mathematician Frank Nelson Cole,
gave an hourlong lecture to a meeting of the American Mathe-
matical Society where he did not speak a single word. Rather,
he calculated on the board the value 267 ‚àí1 which is equal to
147, 573, 952, 589, 676, 412, 927, and then showed that this number is
equal to 193, 707, 721 √ó 761, 838, 257, 287. Cole‚Äôs proof showed that
267 ‚àí1 is not a prime, but it also revealed additional information,
namely its actual factors. This is often the case with proofs: they teach
us more than just the validity of the statements.


--- Page 603 ---

cryptography
603
In Zero Knowledge Proofs we try to achieve the opposite effect. We
want a proof for a statement ùëãwhere we can rigorously show that the
proofs reveals absolutely no additional information about ùëãbeyond the
fact that it is true. This turns out to be an extremely useful object for
a variety of tasks including authentication, secure protocols, voting,
anonymity in cryptocurrencies, and more. Constructing these ob-
jects relies on the theory of NP completeness. Thus this theory that
originally was designed to give a negative result (show that some prob-
lems are hard) ended up yielding positive applications, enabling us to
achieve tasks that were not possible otherwise.
21.10.2 Fully homomorphic encryption
Suppose that we are given a bit-by-bit encryption of a string
ùê∏ùëò(ùë•0), ‚Ä¶ , ùê∏ùëò(ùë•ùëõ‚àí1). By design, these ciphertexts are supposed to
be ‚Äúcompletely unscrutable‚Äù and we should not be able to extract
any information about ùë•ùëñ‚Äôs from it. However, already in 1978, Rivest,
Adleman and Dertouzos observed that this does not imply that we
could not manipulate these encryptions. For example, it turns out the
security of an encryption scheme does not immediately rule out the
ability to take a pair of encryptions ùê∏ùëò(ùëé) and ùê∏ùëò(ùëè) and compute
from them ùê∏ùëò(ùëéNANDùëè) without knowing the secret key ùëò. But do there
exist encryption schemes that allow such manipulations? And if so, is
this a bug or a feature?
Rivest et al already showed that such encryption schemes could
be immensely useful, and their utility has only grown in the age of
cloud computing. After all, if we can compute NAND then we can
use this to run any algorithm ùëÉon the encrypted data, and map
ùê∏ùëò(ùë•0), ‚Ä¶ , ùê∏ùëò(ùë•ùëõ‚àí1) to ùê∏ùëò(ùëÉ(ùë•0, ‚Ä¶ , ùë•ùëõ‚àí1)). For example, a client
could store their secret data ùë•in encrypted form on the cloud, and
have the cloud provider perform all sorts of computation on these
data without ever revealing to the provider the private key, and so
without the provider ever learning any information about the secret
data.
The question of existence of such a scheme took much longer time
to resolve. Only in 2009 Craig Gentry gave the first construction of
an encryption scheme that allows to compute a universal basis of
gates on the data (known as a Fully Homomorphic Encryption scheme in
crypto parlance). Gentry‚Äôs scheme left much to be desired in terms of
efficiency, and improving upon it has been the focus of an intensive
research program that has already seen significant improvements.
21.10.3 Multiparty secure computation
Cryptography is about enabling mutually distrusting parties to
achieve a common goal. Perhaps the most general primitive achiev-


--- Page 604 ---

604
introduction to theoretical computer science
ing this objective is secure multiparty computation. The idea in secure
multiparty computation is that ùëõparties interact together to compute
some function ùêπ(ùë•0, ‚Ä¶ , ùë•ùëõ‚àí1) where ùë•ùëñis the private input of the ùëñ-th
party. The crucial point is that there is no commonly trusted party or
authority and that nothing is revealed about the secret data beyond the
function‚Äôs output. One example is an electronic voting protocol where
only the total vote count is revealed, with the privacy of the individual
voters protected, but without having to trust any authority to either
count the votes correctly or to keep information confidential. Another
example is implementing a second price (aka Vickrey) auction where
ùëõ‚àí1 parties submit bids to an item owned by the ùëõ-th party, and the
item goes to the highest bidder but at the price of the second highest bid.
Using secure multiparty computation we can implement second price
auction in a way that will ensure the secrecy of the numerical values
of all bids (including even the top one) except the second highest one,
and the secrecy of the identity of all bidders (including even the sec-
ond highest bidder) except the top one. We emphasize that such a
protocol requires no trust even in the auctioneer itself, that will also
not learn any additional information. Secure multiparty computation
can be used even for computing randomized processes, with one exam-
ple being playing Poker over the net without having to trust any server
for correct shuffling of cards or not revealing the information.
‚úì
Chapter Recap
‚Ä¢ We can formally define the notion of security of an
encryption scheme.
‚Ä¢ Perfect secrecy ensures that an adversary does not
learn anything about the plaintext from the cipher-
text, regardless of their computational powers.
‚Ä¢ The one-time pad is a perfectly secret encryption
with the length of the key equaling the length of
the message. No perfectly secret encryption can
have key shorter than the message.
‚Ä¢ Computational secrecy can be as good as perfect
secrecy since it ensures that the advantage that
computationally bounded adversaries gain from
observing the ciphertext is exponentially small. If
the optimal PRG conjecture is true then there exists
a computationally secret encryption scheme with
messages that can be (almost) exponentially bigger
than the key.
‚Ä¢ There are many cryptographic tools that go well be-
yond private key encryption. These include public
key encryption, digital signatures and hash functions,
as well as more ‚Äúmagical‚Äù tools such as multiparty
secure computation, fully homomorphic encryption, zero
knowledge proofs, and many others.


--- Page 605 ---

cryptography
605
21.11 EXERCISES
21.12 BIBLIOGRAPHICAL NOTES
Much of this text is taken from my lecture notes on cryptography.
Shannon‚Äôs manuscript was written in 1945 but was classified, and a
partial version was only published in 1949. Still it has revolutionized
cryptography, and is the forerunner to much of what followed.
The Venona project‚Äôs history is described in this document. Aside
from Grabeel and Zubko, credit to the discovery that the Soviets were
reusing keys is shared by Lt. Richard Hallock, Carrie Berry, Frank
Lewis, and Lt. Karl Elmquist, and there are others that have made
important contribution to this project. See pages 27 and 28 in the
document.
In a 1955 letter to the NSA that only recently came forward, John
Nash proposed an ‚Äúunbreakable‚Äù encryption scheme. He wrote ‚ÄúI
hope my handwriting, etc. do not give the impression I am just a crank or
circle-squarer‚Ä¶. The significance of this conjecture [that certain encryption
schemes are exponentially secure against key recovery attacks] .. is that it is
quite feasible to design ciphers that are effectively unbreakable.‚Äù. John Nash
made seminal contributions in mathematics and game theory, and was
awarded both the Abel Prize in mathematics and the Nobel Memorial
Prize in Economic Sciences. However, he has struggled with mental
illness throughout his life. His biography, A Beautiful Mind was made
into a popular movie. It is natural to compare Nash‚Äôs 1955 letter to the
NSA to G√∂del‚Äôs letter to von Neumann we mentioned before. From
the theoretical computer science point of view, the crucial difference
is that while Nash informally talks about exponential vs polynomial
computation time, he does not mention the word ‚ÄúTuring Machine‚Äù
or other models of computation, and it is not clear if he is aware or not
that his conjecture can be made mathematically precise (assuming a
formalization of ‚Äúsufficiently complex types of enciphering‚Äù).
The definition of computational secrecy we use is the notion of
computational indistinguishability (known to be equivalent to semantic
security) that was given by Goldwasser and Micali in 1982.
Although they used a different terminology, Diffie and Hellman
already made clear in their paper that their protocol can be used as
a public key encryption, with the first message being put in a ‚Äúpub-
lic file‚Äù. In 1985, ElGamal showed how to obtain a signature scheme
based on the Diffie Hellman ideas, and since he described the Diffie-
Hellman encryption scheme in the same paper, the public key encryp-
tion scheme originally proposed by Diffie and Hellman is sometimes
also known as ElGamal encryption.
My survey contains a discussion on the different types of public key
assumptions. While the standard elliptic curve cryptographic schemes


--- Page 606 ---

606
introduction to theoretical computer science
are as susceptible to quantum computers as Diffie-Hellman and RSA,
their main advantage is that the best known classical algorithms for
computing discrete logarithms over elliptic curve groups take time 2ùúñùëõ
for some ùúñ> 0 where ùëõis the number of bits to describe a group ele-
ment. In contrast, for the multiplicative group modulo a prime ùëùthe
best algorithm take time 2ùëÇ(ùëõ1/3ùëùùëúùëôùë¶ùëôùëúùëî(ùëõ)) which means that (assum-
ing the known algorithms are optimal) we need to set the prime to be
bigger (and so have larger key sizes with corresponding overhead in
communication and computation) to get the same level of security.
Zero-knowledge proofs were constructed by Goldwasser, Micali,
and Rackoff in 1982, and their wide applicability was shown (using
the theory of NP completeness) by Goldreich, Micali, and Wigderson
in 1986.
Two party and multiparty secure computation protocols were con-
structed (respectively) by Yao in 1982 and Goldreich, Micali, and
Wigderson in 1987. The latter work gave a general transformation
from security against passive adversaries to security against active
adversaries using zero knowledge proofs.


--- Page 607 ---

22
Proofs and algorithms
‚ÄúLet‚Äôs not try to define knowledge, but try to define zero-knowledge.‚Äù, Shafi
Goldwasser.
Proofs have captured human imagination for thousands of years,
ever since the publication of Euclid‚Äôs Elements, a book second only to
the bible in the number of editions printed.
Plan:
‚Ä¢ Proofs and algorithms
‚Ä¢ Interactive proofs
‚Ä¢ Zero knowledge proofs
‚Ä¢ Propositions as types, Coq and other proof assistants.
22.1 EXERCISES
22.2 BIBLIOGRAPHICAL NOTES
Compiled on 8.26.2020 18:10


--- Page 608 ---



--- Page 609 ---

23
Quantum computing
‚ÄúWe always have had (secret, secret, close the doors!) ‚Ä¶ a great deal of diffi-
culty in understanding the world view that quantum mechanics represents ‚Ä¶
It has not yet become obvious to me that there‚Äôs no real problem. ‚Ä¶ Can I learn
anything from asking this question about computers‚Äìabout this may or may
not be mystery as to what the world view of quantum mechanics is?‚Äù , Richard
Feynman, 1981
‚ÄúThe only difference between a probabilistic classical world and the equations
of the quantum world is that somehow or other it appears as if the probabilities
would have to go negative‚Äù, Richard Feynman, 1981
There were two schools of natural philosophy in ancient Greece.
Aristotle believed that objects have an essence that explains their behav-
ior, and a theory of the natural world has to refer to the reasons (or ‚Äúfi-
nal cause‚Äù to use Aristotle‚Äôs language) as to why they exhibit certain
phenomena. Democritus believed in a purely mechanistic explanation
of the world. In his view, the universe was ultimately composed of
elementary particles (or Atoms) and our observed phenomena arise
from the interactions between these particles according to some local
rules. Modern science (arguably starting with Newton) has embraced
Democritus‚Äô point of view, of a mechanistic or ‚Äúclockwork‚Äù universe
of particles and forces acting upon them.
While the classification of particles and forces evolved with time,
to a large extent the ‚Äúbig picture‚Äù has not changed from Newton till
Einstein. In particular it was held as an axiom that if we knew fully
the current state of the universe (i.e., the particles and their properties
such as location and velocity) then we could predict its future state at
any point in time. In computational language, in all these theories the
state of a system with ùëõparticles could be stored in an array of ùëÇ(ùëõ)
numbers, and predicting the evolution of the system can be done by
running some efficient (e.g., ùëùùëúùëôùë¶(ùëõ) time) deterministic computation
on this array.
Compiled on 8.26.2020 18:10
Learning Objectives:
‚Ä¢ See main aspects in which quantum
mechanics differs from local deterministic
theories.
‚Ä¢ Model of quantum circuits, or equivalently
QNAND-CIRC programs
‚Ä¢ The complexity class BQP and what we know
about its relation to other classes
‚Ä¢ Ideas behind Shor‚Äôs Algorithm and the
Quantum Fourier Transform


--- Page 610 ---

610
introduction to theoretical computer science
Figure 23.1: In the ‚Äúdouble baseball experiment‚Äù we
shoot baseballs from a gun at a soft wall through a
hard barrier that has one or two slits open in it. There
is only ‚Äúconstructive interference‚Äù in the sense that
the dent in each position in the wall when both slits
are open is the sum of the dents when each slit is
open on its own.
Figure 23.2: The setup of the double slit experiment
in the case of photon or electron guns. We see also
destructive interference in the sense that there are
some positions on the wall that get fewer hits when
both slits are open than they get when only one of the
slits is open. See also this video.
23.1 THE DOUBLE SLIT EXPERIMENT
Alas, in the beginning of the 20th century, several experimental re-
sults were calling into question this ‚Äúclockwork‚Äù or ‚Äúbilliard ball‚Äù
theory of the world. One such experiment is the famous double slit ex-
periment. Here is one way to describe it. Suppose that we buy one of
those baseball pitching machines, and aim it at a soft plastic wall, but
put a metal barrier with a single slit between the machine and the plastic
wall (see Fig. 23.1). If we shoot baseballs at the plastic wall, then some
of the baseballs would bounce off the metal barrier, while some would
make it through the slit and dent the wall. If we now carve out an ad-
ditional slit in the metal barrier then more balls would get through,
and so the plastic wall would be even more dented.
So far this is pure common sense, and it is indeed (to my knowl-
edge) an accurate description of what happens when we shoot base-
balls at a plastic wall. However, this is not the same when we shoot
photons. Amazingly, if we shoot with a ‚Äúphoton gun‚Äù (i.e., a laser) at
a wall equipped with photon detectors through some barrier, then
(as shown in Fig. 23.2) in some positions of the wall we will see fewer
hits when the two slits are open than when only one of them is!. In
particular there are positions in the wall that are hit when the first slit
is open, hit when the second gun is open, but are not hit at all when both
slits are open!.
It seems as if each photon coming out of the gun is aware of the
global setup of the experiment, and behaves differently if two slits are
open than if only one is. If we try to ‚Äúcatch the photon in the act‚Äù and
place a detector right next to each slit so we can see exactly the path
each photon takes then something even more bizarre happens. The
mere fact that we measure the path changes the photon‚Äôs behavior, and
now this ‚Äúdestructive interference‚Äù pattern is gone and the number
of times a position is hit when two slits are open is the sum of the
number of times it is hit when each slit is open.
P
You should read the paragraphs above more than
once and make sure you appreciate how truly mind
boggling these results are.
23.2 QUANTUM AMPLITUDES
The double slit and other experiments ultimately forced scientists to
accept a very counterintuitive picture of the world. It is not merely
about nature being randomized, but rather it is about the probabilities
in some sense ‚Äúgoing negative‚Äù and cancelling each other!


--- Page 611 ---

quantum computing
611
To see what we mean by this, let us go back to the baseball exper-
iment. Suppose that the probability a ball passes through the left slit
is ùëùùêøand the probability that it passes through the right slit is ùëùùëÖ.
Then, if we shoot ùëÅballs out of each gun, we expect the wall will be
hit (ùëùùêø+ ùëùùëÖ)ùëÅtimes. In contrast, in the quantum world of photons
instead of baseballs, it can sometimes be the case that in both the first
and second case the wall is hit with positive probabilities ùëùùêøand ùëùùëÖ
respectively but somehow when both slits are open the wall (or a par-
ticular position in it) is not hit at all. It‚Äôs almost as if the probabilities
can ‚Äúcancel each other out‚Äù.
To understand the way we model this in quantum mechanics, it is
helpful to think of a ‚Äúlazy evaluation‚Äù approach to probability. We
can think of a probabilistic experiment such as shooting a baseball
through two slits in two different ways:
‚Ä¢ When a ball is shot, ‚Äúnature‚Äù tosses a coin and decides if it will go
through the left slit (which happens with probability ùëùùêø), right slit
(which happens with probability ùëùùëÖ), or bounce back. If it passes
through one of the slits then it will hit the wall. Later we can look at
the wall and find out whether or not this event happened, but the
fact that the event happened or not is determined independently of
whether or not we look at the wall.
‚Ä¢ The other viewpoint is that when a ball is shot, ‚Äúnature‚Äù computes
the probabilities ùëùùêøand ùëùùëÖas before, but does not yet ‚Äútoss the
coin‚Äù and determines what happened. Only when we actually
look at the wall, nature tosses a coin and with probability ùëùùêø+ ùëùùëÖ
ensures we see a dent. That is, nature uses ‚Äúlazy evaluation‚Äù, and
only determines the result of a probabilistic experiment when we
decide to measure it.
While the first scenario seems much more natural, the end result
in both is the same (the wall is hit with probability ùëùùêø+ ùëùùëÖ) and so
the question of whether we should model nature as following the first
scenario or second one seems like asking about the proverbial tree that
falls in the forest with no one hearing about it.
However, when we want to describe the double slit experiment
with photons rather than baseballs, it is the second scenario that lends
itself better to a quantum generalization. Quantum mechanics as-
sociates a number ùõºknown as an amplitude with each probabilistic
experiment. This number ùõºcan be negative, and in fact even complex.
We never observe the amplitudes directly, since whenever we mea-
sure an event with amplitude ùõº, nature tosses a coin and determines
that the event happens with probability |ùõº|2. However, the sign (or
in the complex case, phase) of the amplitudes can affect whether two
different events have constructive or destructive interference.


--- Page 612 ---

612
introduction to theoretical computer science
Specifically, consider an event that can either occur or not (e.g. ‚Äúde-
tector number 17 was hit by a photon‚Äù). In classical probability, we
model this by a probability distribution over the two outcomes: a pair
of non-negative numbers ùëùand ùëûsuch that ùëù+ ùëû= 1, where ùëùcorre-
sponds to the probability that the event occurs and ùëûcorresponds to
the probability that the event does not occur. In quantum mechanics,
we model this also by pair of numbers, which we call amplitudes. This
is a pair of (potentially negative or even complex) numbers ùõºand ùõΩ
such that |ùõº|2 + |ùõΩ|2 = 1. The probability that the event occurs is |ùõº|2
and the probability that it does not occur is |ùõΩ|2. In isolation, these
negative or complex numbers don‚Äôt matter much, since we anyway
square them to obtain probabilities. But the interaction of positive and
negative amplitudes can result in surprising cancellations where some-
how combining two scenarios where an event happens with positive
probability results in a scenario where it never does.
P
If you don‚Äôt find the above description confusing and
unintuitive, you probably didn‚Äôt get it. Please make
sure to re-read the above paragraphs until you are
thoroughly confused.
Quantum mechanics is a mathematical theory that allows us to
calculate and predict the results of the double-slit and many other ex-
periments. If you think of quantum mechanics as an explanation as to
what ‚Äúreally‚Äù goes on in the world, it can be rather confusing. How-
ever, if you simply ‚Äúshut up and calculate‚Äù then it works amazingly
well at predicting experimental results. In particular, in the double
slit experiment, for any position in the wall, we can compute num-
bers ùõºand ùõΩsuch that photons from the first and second slit hit that
position with probabilities |ùõº|2 and |ùõΩ|2 respectively. When we open
both slits, the probability that the position will be hit is proportional
to |ùõº+ ùõΩ|2, and so in particular, if ùõº= ‚àíùõΩthen it will be the case that,
despite being hit when either slit one or slit two are open, the position
is not hit at all when they both are. If you are confused by quantum
mechanics, you are not alone: for decades people have been trying to
come up with explanations for ‚Äúthe underlying reality‚Äù behind quan-
tum mechanics, including Bohmian Mechanics, Many Worlds and
others. However, none of these interpretations have gained universal
acceptance and all of those (by design) yield the same experimental
predictions. Thus at this point many scientists prefer to just ignore the
question of what is the ‚Äútrue reality‚Äù and go back to simply ‚Äúshutting
up and calculating‚Äù.


--- Page 613 ---

quantum computing
613
R
Remark 23.1 ‚Äî Complex vs real, other simplifications. If
(like the author) you are a bit intimidated by complex
numbers, don‚Äôt worry: you can think of all ampli-
tudes as real (though potentially negative) numbers
without loss of understanding. All the ‚Äúmagic‚Äù of
quantum computing already arises in this case, and
so we will often restrict attention to real amplitudes in
this chapter.
We will also only discuss so-called pure quantum
states, and not the more general notion of mixed states.
Pure states turn out to be sufficient for understanding
the algorithmic aspects of quantum computing.
More generally, this chapter is not meant to be a com-
plete description of quantum mechanics, quantum
information theory, or quantum computing, but rather
illustrate the main points where these differ from
classical computing.
23.2.1 Linear algebra quick review
Linear algebra underlies much of quantum mechanics, and so you
would do well to review some of the basic notions such as vectors,
matrices, and linear subspaces. The operations in quantum mechanics
can be represented as linear functions over the complex numbers, but
we stick to the real numbers in this chapter. This does not cause much
loss in understanding but does allow us to simplify our notation and
eliminate the use of the complex conjugate.
The main notions we use are:
‚Ä¢ A function ùêπ‚à∂‚ÑùùëÅ‚Üí‚ÑùùëÅis linear if ùêπ(ùõºùë¢+ ùõΩùë£) = ùõºùêπ(ùë¢) + ùõΩùêπ(ùë£) for
every ùõº, ùõΩ‚àà‚Ñùand ùë¢, ùë£‚àà‚ÑùùëÅ.
‚Ä¢ The inner product of two vectors ùë¢, ùë£‚àà‚ÑùùëÅcan be defined as ‚ü®ùë¢, ùë£‚ü©=
‚àëùëñ‚àà[ùëÅ] ùë¢ùëñùë£ùëñ. (There can be different inner products but we stick
to this one.) The norm of a vector ùë¢‚àà‚ÑùùëÅis defined as ‚Äñùë¢‚Äñ =
‚àö‚ü®ùë¢, ùë¢‚ü©= ‚àö‚àëùëñ‚àà[ùëÅ] ùë¢2
ùëñ. We say that ùë¢is a unit vector if ‚Äñùë¢‚Äñ = 1.
‚Ä¢ Two vectors ùë¢, ùë£‚àà‚ÑùùëÅare orthogonal if ‚ü®ùë¢, ùë£‚ü©= 0. An orthonormal
basis for ‚ÑùùëÅis a set of ùëÅvectors ùë£0, ùë£1, ‚Ä¶ , ùë£ùëÅ‚àí1 such that ‚Äñùë£ùëñ‚Äñ = 1
for every ùëñ‚àà[ùëÅ] and ‚ü®ùë£ùëñ, ùë£ùëó‚ü©= 0 for every ùëñ‚â†ùëó. A canoncial
example is the standard basis ùëí0, ‚Ä¶ , ùëíùëÅ‚àí1, where ùëíùëñis the vector that
has zeroes in all cooordinates except the ùëñ-th coordinate in which
its value is 1. A quirk of the quantum mechanics literature is that
ùëíùëñis often denoted by |ùëñ‚ü©. We often consider the case ùëÅ= 2ùëõ, in
which case we identify [ùëÅ] with {0, 1}ùëõand for every ùë•‚àà{0, 1}ùëõ,
we denote the standard basis element corresponding to the ùë•-th
coordinate by |ùë•‚ü©.


--- Page 614 ---

614
introduction to theoretical computer science
1 If you are extremely paranoid about Alice and Bob
communicating with one another, you can coordinate
with your assistant to perform the experiment exactly
at the same time, and make sure that the rooms
are sufficiently far apart (e.g., are on two different
continents, or maybe even one is on the moon and
another is on earth) so that Alice and Bob couldn‚Äôt
communicate to each other in time the results of their
respective coins even if they do so at the speed of
light.
‚Ä¢ If ùë¢is a vector in ‚Ñùùëõand ùë£0, ‚Ä¶ , ùë£ùëÅ‚àí1 is an orthonormal basis for
‚ÑùùëÅ, then there are coefficients ùõº0, ‚Ä¶ , ùõºùëÅ‚àí1 such that ùë¢= ùõº0ùë£0 +
‚ãØ+ ùõºùëÅ‚àí1ùë£ùëÅ‚àí1. Consequently, the value ùêπ(ùë¢) is determined by the
values ùêπ(ùë£0), ‚Ä¶, ùêπ(ùë£ùëÅ‚àí1). Moreover, ‚Äñùë¢‚Äñ = ‚àö‚àëùëñ‚àà[ùëÅ] ùõº2
ùëñ.
‚Ä¢ We can represent a linear function ùêπ‚à∂‚ÑùùëÅ‚Üí‚ÑùùëÅas an ùëÅ√ó ùëÅ
matrix ùëÄ(ùêπ) where the coordinate in the ùëñ-th row and ùëó-th column
of ùëÄ(ùêπ) (that is ùëÄ(ùêπ)ùëñ,ùëó) is equal to ‚ü®ùëíùëñ, ùêπ(ùëíùëó)‚ü©or equivalently the
ùëñ-th coordinate of ùêπ(ùëíùëó).
‚Ä¢ A linear function ùêπ‚à∂‚ÑùùëÅ‚Üí‚ÑùùëÅsuch that ‚Äñùêπ(ùë¢)‚Äñ = ‚Äñùë¢‚Äñ for every
ùë¢is called unitary. It can be shown that a function ùêπis unitary if
and only if ùëÄ(ùêπ)ùëÄ(ùêπ)‚ä§= ùêºwhere ‚ä§is the transpose operator
(in the complex case the conjugate transpose) and ùêºis the ùëÅ√ó ùëÅ
identity matrix that has 1‚Äôs on the diagonal and zeroes everywhere
else. (For every two matrices ùê¥, ùêµ, we use ùê¥ùêµto denote the matrix
product of ùê¥and ùêµ.) Another equivalent characterization of this
condition is that ùëÄ(ùêπ)‚ä§= ùëÄ(ùêπ)‚àí1 and yet another is that both the
rows and columns of ùëÄ(ùêπ) form an orthonormal basis.
23.3 BELL‚ÄôS INEQUALITY
There is something weird about quantum mechanics. In 1935 Einstein,
Podolsky and Rosen (EPR) tried to pinpoint this issue by highlighting
a previously unrealized corollary of this theory. They showed that
the idea that nature does not determine the results of an experiment
until it is measured results in so called ‚Äúspooky action at a distance‚Äù.
Namely, making a measurement of one object may instantaneously
effect the state (i.e., the vector of amplitudes) of another object in the
other end of the universe.
Since the vector of amplitudes is just a mathematical abstraction,
the EPR paper was considered to be merely a thought experiment for
philosophers to be concerned about, without bearing on experiments.
This changed when in 1965 John Bell showed an actual experiment
to test the predictions of EPR and hence pit intuitive common sense
against quantum mechanics. Quantum mechanics won: it turns out
that it is in fact possible to use measurements to create correlations
between the states of objects far removed from one another that cannot
be explained by any prior theory. Nonetheless, since the results of
these experiments are so obviously wrong to anyone that has ever sat
in an armchair, that there are still a number of Bell denialists arguing
that this can‚Äôt be true and quantum mechanics is wrong.
So, what is this Bell‚Äôs Inequality? Suppose that Alice and Bob try
to convince you they have telepathic ability, and they aim to prove it
via the following experiment. Alice and Bob will be in separate closed


--- Page 615 ---

quantum computing
615
rooms.1 You will interrogate Alice and your associate will interrogate
Bob. You choose a random bit ùë•‚àà{0, 1} and your associate chooses
a random ùë¶‚àà{0, 1}. We let ùëébe Alice‚Äôs response and ùëèbe Bob‚Äôs
response. We say that Alice and Bob win this experiment if ùëé‚äïùëè=
ùë•‚àßùë¶. In other words, Alice and Bob need to output two bits that
disagree if ùë•= ùë¶= 1 and agree otherwise.
Now if Alice and Bob are not telepathic, then they need to agree in
advance on some strategy. It‚Äôs not hard for Alice and Bob to succeed
with probability 3/4: just always output the same bit. Moreover, by
doing some case analysis, we can show that no matter what strategy
they use, Alice and Bob cannot succeed with higher probability than
that:
Theorem 23.2 ‚Äî Bell‚Äôs Inequality. For every two functions ùëì, ùëî‚à∂{0, 1} ‚Üí
{0, 1}, Prùë•,ùë¶‚àà{0,1}[ùëì(ùë•) ‚äïùëî(ùë¶) = ùë•‚àßùë¶] ‚â§3/4.
Proof. Since the probability is taken over all four choices of ùë•, ùë¶‚àà
{0, 1}, the only way the theorem can be violated if if there exist two
functions ùëì, ùëîthat satisfy
ùëì(ùë•) ‚äïùëî(ùë¶) = ùë•‚àßùë¶
(23.1)
for all the four choices of ùë•, ùë¶‚àà{0, 1}2. Let‚Äôs plug in all these four
choices and see what we get (below we use the equalities ùëß‚äï0 = ùëß,
ùëß‚àß0 = 0 and ùëß‚àß1 = ùëß):
ùëì(0) ‚äïùëî(0)
= 0
(plugging in ùë•= 0, ùë¶= 0)
ùëì(0) ‚äïùëî(1)
= 0
(plugging in ùë•= 0, ùë¶= 1)
ùëì(1) ‚äïùëî(0)
= 0
(plugging in ùë•= 1, ùë¶= 0)
ùëì(1) ‚äïùëî(1)
= 1
(plugging in ùë•= 1, ùë¶= 1)
(23.2)
If we XOR together the first and second equalities we get ùëî(0) ‚äï
ùëî(1) = 0 while if we XOR together the third and fourth equalities we
get ùëî(0) ‚äïùëî(1) = 1, thus obtaining a contradiction.
‚ñ†
R
Remark 23.3 ‚Äî Randomized strategies. Theorem 23.2
above assumes that Alice and Bob use deterministic
strategies ùëìand ùëîrespectively. More generally, Alice
and Bob could use a randomized strategy, or equiva-
lently, each could choose ùëìand ùëîfrom some distri-
butions ‚Ñ±and ùí¢respectively. However the averaging
principle (Lemma 18.10) implies that if all possible
deterministic strategies succeed with probability at
most 3/4, then the same is true for all randomized
strategies.


--- Page 616 ---

616
introduction to theoretical computer science
2 More accurately, one either has to give up on a
‚Äúbilliard ball type‚Äù theory of the universe or believe
in telepathy (believe it or not, some scientists went for
the latter option).
An amazing experimentally verified fact is that quantum mechanics
allows for ‚Äútelepathy‚Äù.2 Specifically, it has been shown that using the
weirdness of quantum mechanics, there is in fact a strategy for Alice
and Bob to succeed in this game with probability larger than 3/4 (in
fact, they can succeed with probability about 0.85, see Lemma 23.5).
23.4 QUANTUM WEIRDNESS
Some of the counterintuitive properties that arise from quantum me-
chanics include:
‚Ä¢ Interference - As we‚Äôve seen, quantum amplitudes can ‚Äúcancel each
other out‚Äù.
‚Ä¢ Measurement - The idea that amplitudes are negative as long as
‚Äúno one is looking‚Äù and ‚Äúcollapse‚Äù (by squaring them) to positive
probabilities when they are measured is deeply disturbing. Indeed,
as shown by EPR and Bell, this leads to various strange outcomes
such as ‚Äúspooky actions at a distance‚Äù, where we can create corre-
lations between the results of measurements in places far removed.
Unfortunately (or fortunately?) these strange outcomes have been
confirmed experimentally.
‚Ä¢ Entanglement - The notion that two parts of the system could be
connected in this weird way where measuring one will affect the
other is known as quantum entanglement.
As counter-intuitive as these concepts are, they have been experi-
mentally confirmed, so we just have to live with them.
The discussion in this chapter of quantum mechanics in general and
quantum computing in particular is quite brief and superficial, the
‚Äúbibliographical notes‚Äù section (Section 23.13) contains references and
links to many other resources that cover this material in more depth.
23.5 QUANTUM COMPUTING AND COMPUTATION - AN EXECUTIVE
SUMMARY
One of the strange aspects of the quantum-mechanical picture of the
world is that unlike in the billiard ball example, there is no obvious
algorithm to simulate the evolution of ùëõparticles over ùë°time periods
in ùëùùëúùëôùë¶(ùëõ, ùë°) steps. In fact, the natural way to simulate ùëõquantum par-
ticles will require a number of steps that is exponential in ùëõ. This is a
huge headache for scientists that actually need to do these calculations
in practice.
In the 1981, physicist Richard Feynman proposed to ‚Äúturn this
lemon to lemonade‚Äù by making the following almost tautological
observation:


--- Page 617 ---

quantum computing
617
3 As its title suggests, Feynman‚Äôs lecture was actually
focused on the other side of simulating physics with
a computer. However, he mentioned that as a ‚Äúside
remark‚Äù one could wonder if it‚Äôs possible to simulate
physics with a new kind of computer - a ‚Äúquantum
computer‚Äù which would ‚Äúnot [be] a Turing machine,
but a machine of a different kind‚Äù. As far as I know,
Feynman did not suggest that such a computer could
be useful for computations completely outside the
domain of quantum simulation. Indeed, he was
more interested in the question of whether quantum
mechanics could be simulated by a classical computer.
4 This ‚Äú95 percent‚Äù is a figure of speech, but not com-
pletely so. At the time of this writing, cryptocurrency
mining electricity consumption is estimated to use
up at least 70Twh or 0.3 percent of the world‚Äôs pro-
duction, which is about 2 to 5 percent of the total
energy usage for the computing industry. All the
current cryptocurrencies will be broken by quantum
computers. Also, for many web servers the TLS pro-
tocol (which is based on the current non-lattice based
systems would be completely broken by quantum
computing) is responsible for about 1 percent of the
CPU usage.
If a physical system cannot be simulated by a computer in ùëásteps, the
system can be considered as performing a computation that would take
more than ùëásteps.
So, he asked whether one could design a quantum system such
that its outcome ùë¶based on the initial condition ùë•would be some
function ùë¶= ùëì(ùë•) such that (a) we don‚Äôt know how to efficiently
compute in any other way, and (b) is actually useful for something.3
In 1985, David Deutsch formally suggested the notion of a quantum
Turing machine, and the model has been since refined in works of
Deutsch, Josza, Bernstein and Vazirani. Such a system is now known
as a quantum computer.
For a while these hypothetical quantum computers seemed useful
for one of two things. First, to provide a general-purpose mecha-
nism to simulate a variety of the real quantum systems that people
care about, such as various interactions inside molecules in quan-
tum chemistry. Second, as a challenge to the Physical Extended Church
Turing Thesis which says that every physically realizable computa-
tion device can be modeled (up to polynomial overhead) by Turing
machines (or equivalently, NAND-TM / NAND-RAM programs).
Quantum chemistry is important (and in particular understand-
ing it can be a bottleneck for designing new materials, drugs, and
more), but it is still a rather niche area within the broader context of
computing (and even scientific computing) applications. Hence for a
while most researchers (to the extent they were aware of it), thought
of quantum computers as a theoretical curiosity that has little bear-
ing to practice, given that this theoretical ‚Äúextra power‚Äù of quantum
computer seemed to offer little advantage in the majority of the prob-
lems people want to solve in areas such as combinatorial optimization,
machine learning, data structures, etc..
To some extent this is still true today. As far as we know, quantum
computers, if built, will not provide exponential speed ups for 95%
of the applications of computing.4 In particular, as far as we know,
quantum computers will not help us solve NP complete problems in
polynomial or even sub-exponential time, though Grover‚Äôs algorithm (
Remark 23.4) does yield a quadratic advantage in many cases.
However, there is one cryptography-sized exception: In 1994 Peter
Shor showed that quantum computers can solve the integer factoring
and discrete logarithm in polynomial time. This result has captured
the imagination of a great many people, and completely energized re-
search into quantum computing. This is both because the hardness of
these particular problems provides the foundations for securing such
a huge part of our communications (and these days, our economy),
as well as it was a powerful demonstration that quantum computers


--- Page 618 ---

618
introduction to theoretical computer science
5 Of course, given that we‚Äôre still hearing of attacks
exploiting ‚Äúexport grade‚Äù cryptography that was
supposed to disappear in 1990‚Äôs, I imagine that
we‚Äôll still have products running 1024 bit RSA when
everyone has a quantum laptop.
could turn out to be useful for problems that a-priori seemed to have
nothing to do with quantum physics.
As we‚Äôll discuss later, at the moment there are several intensive
efforts to construct large scale quantum computers. It seems safe
to say that, as far as we know, in the next five years or so there will
not be a quantum computer large enough to factor, say, a 1024 bit
number. On the other hand, it does seem quite likely that in the very
near future there will be quantum computers which achieve some task
exponentially faster than the best-known way to achieve the same
task with a classical computer. When and if a quantum computer is
built that is strong enough to break reasonable parameters of Diffie
Hellman, RSA and elliptic curve cryptography is anybody‚Äôs guess. It
could also be a ‚Äúself destroying prophecy‚Äù whereby the existence of
a small-scale quantum computer would cause everyone to shift away
to lattice-based crypto which in turn will diminish the motivation
to invest the huge resources needed to build a large scale quantum
computer.5
R
Remark 23.4 ‚Äî Quantum computing and NP. Despite
popular accounts of quantum computers as having
variables that can take ‚Äúzero and one at the same
time‚Äù and therefore can ‚Äúexplore an exponential num-
ber of possibilities simultaneously‚Äù, their true power
is much more subtle and nuanced. In particular, as far
as we know, quantum computers do not enable us to
solve NP complete problems such as 3SAT in polyno-
mial or even sub-exponential time. However, Grover‚Äôs
search algorithm does give a more modest advan-
tage (namely, quadratic) for quantum computers
over classical ones for problems in NP. In particular,
due to Grover‚Äôs search algorithm, we know that the
ùëò-SAT problem for ùëõvariables can be solved in time
ùëÇ(2ùëõ/2ùëùùëúùëôùë¶(ùëõ)) on a quantum computer for every ùëò.
In contrast, the best known algorithms for ùëò-SAT on a
classical computer take roughly 2(1‚àí1
ùëò)ùëõsteps.
ÔÉ´Big Idea 28 Quantum computers are not a panacea and are un-
likely to solve NP complete problems, but they can provide exponen-
tial speedups to certain structured problems.
23.6 QUANTUM SYSTEMS
Before we talk about quantum computing, let us recall how we phys-
ically realize ‚Äúvanilla‚Äù or classical computing. We model a logical bit


--- Page 619 ---

quantum computing
619
that can equal 0 or a 1 by some physical system that can be in one of
two states. For example, it might be a wire with high or low voltage,
charged or uncharged capacitor, or even (as we saw) a pipe with or
without a flow of water, or the presence or absence of a soldier crab. A
classical system of ùëõbits is composed of ùëõsuch ‚Äúbasic systems‚Äù, each
of which can be in either a ‚Äúzero‚Äù or ‚Äúone‚Äù state. We can model the
state of such a system by a string ùë†‚àà{0, 1}ùëõ. If we perform an op-
eration such as writing to the 17-th bit the NAND of the 3rd and 5th
bits, this corresponds to applying a local function to ùë†such as setting
ùë†17 = 1 ‚àíùë†3 ‚ãÖùë†5.
In the probabilistic setting, we would model the state of the system
by a distribution. For an individual bit, we could model it by a pair of
non-negative numbers ùõº, ùõΩsuch that ùõº+ ùõΩ= 1, where ùõºis the prob-
ability that the bit is zero and ùõΩis the probability that the bit is one.
For example, applying the negation (i.e., NOT) operation to this bit
corresponds to mapping the pair (ùõº, ùõΩ) to (ùõΩ, ùõº) since the probability
that NOT(ùúé) is equal to 1 is the same as the probability that ùúéis equal
to 0. This means that we can think of the NOT function as the linear
map ùëÅ‚à∂‚Ñù2 ‚Üí‚Ñù2 such that ùëÅ(ùõº
ùõΩ) = (ùõΩ
ùõº) or equivalently as the
matrix (0
1
1
0).
If we think of the ùëõ-bit system as a whole, then since the ùëõbits can
take one of 2ùëõpossible values, we model the state of the system as a
vector ùëùof 2ùëõprobabilities. For every ùë†‚àà{0, 1}ùëõ, we denote by ùëíùë†
the 2ùëõdimensional vector that has 1 in the coordinate correspond-
ing to ùë†(identifying it with a number in [2ùëõ]), and so can write ùëùas
‚àëùë†‚àà{0,1}ùëõùëùùë†ùëíùë†where ùëùùë†is the probability that the system is in the state
ùë†.
Applying the operation above of setting the 17-th bit to the NAND
of the 3rd and 5th bits, corresponds to transforming the vector ùëùto
the vector ùêπùëùwhere ùêπ‚à∂‚Ñù2ùëõ‚Üí‚Ñù2ùëõis the linear map that maps ùëíùë†to
ùëíùë†0‚ãØùë†16(1‚àíùë†3‚ãÖùë†5)ùë†18‚ãØùë†ùëõ‚àí1. (Since {ùëíùë†}ùë†‚àà{0,1}ùëõis a basis for ùëÖ2ùëõ, it suffices to
define the map ùêπon vectors of this form.)
P
Please make sure you understand why performing the
operation will take a system in state ùëùto a system in
the state ùêπùëù. Understanding the evolution of proba-
bilistic systems is a prerequisite to understanding the
evolution of quantum systems.
If your linear algebra is a bit rusty, now would be a
good time to review it, and in particular make sure
you are comfortable with the notions of matrices, vec-
tors, (orthogonal and orthonormal) bases, and norms.


--- Page 620 ---

620
introduction to theoretical computer science
23.6.1 Quantum amplitudes
In the quantum setting, the state of an individual bit (or ‚Äúqubit‚Äù,
to use quantum parlance) is modeled by a pair of numbers (ùõº, ùõΩ)
such that |ùõº|2 + |ùõΩ|2 = 1. While in general these numbers can be
complex, for the rest of this chapter, we will often assume they are
real (though potentially negative), and hence often drop the absolute
value operator. (This turns out not to make much of a difference in
explanatory power.) As before, we think of ùõº2 as the probability that
the bit equals 0 and ùõΩ2 as the probability that the bit equals 1. As we
did before, we can model the NOT operation by the map ùëÅ‚à∂‚Ñù2 ‚Üí‚Ñù2
where ùëÅ(ùõº, ùõΩ) = (ùõΩ, ùõº).
Following quantum tradition, instead of using ùëí0 and ùëí1 as we did
above, from now on we will denote the vector (1, 0) by |0‚ü©and the
vector (0, 1) by |1‚ü©(and moreover, think of these as column vectors).
This is known as the Dirac ‚Äúket‚Äù notation. This means that NOT is
the unique linear map ùëÅ‚à∂‚Ñù2 ‚Üí‚Ñù2 that satisfies ùëÅ|0‚ü©= |1‚ü©and
ùëÅ|1‚ü©= |0‚ü©. In other words, in the quantum case, as in the probabilistic
case, NOT corresponds to the matrix
ùëÅ= (0
1
1
0) .
(23.3)
In classical computation, we typically think that there are only
two operations that we can do on a single bit: keep it the same or
negate it. In the quantum setting, a single bit operation corresponds
to any linear map OP ‚à∂‚Ñù2 ‚Üí‚Ñù2 that is norm preserving in the sense
that for every ùõº, ùõΩ, if we apply OP to the vector (ùõº
ùõΩ) then we obtain
a vector (ùõº‚Ä≤
ùõΩ‚Ä≤) such that ùõº‚Ä≤2 + ùõΩ‚Ä≤2 = ùõº2 + ùõΩ2. Such a linear map
OP corresponds to a unitary two by two matrix. (As we mentioned,
quantum mechanics actually models states as vectors with complex
coordinates; however, this does not make any qualitative difference to
our discussion.) Keeping the bit the same corresponds to the matrix
ùêº= (1
0
0
1) and (as we‚Äôve seen) the NOT operations corresponds to
the matrix ùëÅ= (0
1
1
0). But there are other operations we can use
as well. One such useful operation is the Hadamard operation, which
corresponds to the matrix
ùêª=
1
‚àö
2 (+1
+1
+1
‚àí1) .
(23.4)


--- Page 621 ---

quantum computing
621
In fact it turns out that Hadamard is all that we need to add to a
classical universal basis to achieve the full power of quantum comput-
ing.
23.6.2 Quantum systems: an executive summary
If you ignore the physics and philosophy, for the purposes of under-
standing the model of quantum computers, all you need to know
about quantum systems is the following. The state of a quantum system
of ùëõqubits is modeled by an 2ùëõdimensional vector ùúìof unit norm
(i.e., squares of all coordinates sums up to 1), which we write as
ùúì= ‚àëùë•‚àà{0,1}ùëõùúìùë•|ùë•‚ü©where |ùë•‚ü©is the column vector that has 0 in all
coordinates except the one corresponding to ùë•(identifying {0, 1}ùëõ
with the numbers {0, ‚Ä¶ , 2ùëõ‚àí1}). We use the convention that if ùëé, ùëè
are strings of lengths ùëòand ‚Ñìrespectively then we can write the 2ùëò+‚Ñì
dimensional vector with 1 in the ùëéùëè-th coordinate and zero elsewhere
not just as |ùëéùëè‚ü©but also as |ùëé‚ü©|ùëè‚ü©. In particular, for every ùë•‚àà{0, 1}ùëõ, we
can write the vector |ùë•‚ü©also as |ùë•0‚ü©|ùë•1‚ü©‚ãØ|ùë•ùëõ‚àí1‚ü©. This notation satisfies
certain nice distributive laws such as |ùëé‚ü©(|ùëè‚ü©+ |ùëè‚Ä≤‚ü©)|ùëê‚ü©= |ùëéùëèùëê‚ü©+ |ùëéùëè‚Ä≤ùëê‚ü©.
A quantum operation on such a system is modeled by a 2ùëõ√ó 2ùëõ
unitary matrix ùëà(one that satisfies UU‚ä§= ùêºwhere ùëà‚ä§is the transpose
operation, or conjugate transpose for complex matrices). If the system
is in state ùúìand we apply to it the operation ùëà, then the new state of
the system is ùëàùúì.
When we measure an ùëõ-qubit system in a state ùúì= ‚àëùë•‚àà{0,1}ùëõùúìùë•|ùë•‚ü©,
then we observe the value ùë•‚àà{0, 1}ùëõwith probability |ùúìùë•|2. In this
case, the system collapses to the state |ùë•‚ü©.
23.7 ANALYSIS OF BELL‚ÄôS INEQUALITY (OPTIONAL)
Now that we have the notation in place, we can show a strategy for
Alice and Bob to display ‚Äúquantum telepathy‚Äù in Bell‚Äôs Game. Re-
call that in the classical case, Alice and Bob can succeed in the ‚ÄúBell
Game‚Äù with probability at most 3/4 = 0.75. We now show that quan-
tum mechanics allows them to succeed with probability at least 0.8.
(The strategy we show is not the best one. Alice and Bob can in fact
succeed with probability cos2(ùúã/8) ‚àº0.854.)
Lemma 23.5 There is a 2-qubit quantum state ùúì‚àà‚ÑÇ4 so that if Alice
has access to the first qubit of ùúì, can manipulate and measure it and
output ùëé‚àà{0, 1} and Bob has access to the second qubit of ùúìand can
manipulate and measure it and output ùëè‚àà{0, 1} then Pr[ùëé‚äïùëè=
ùë•‚àßùë¶] ‚â•0.8.


--- Page 622 ---

622
introduction to theoretical computer science
Proof. Alice and Bob will start by preparing a 2-qubit quantum system
in the state
ùúì=
1
‚àö
2|00‚ü©+
1
‚àö
2|11‚ü©
(23.5)
(this state is known as an EPR pair). Alice takes the first qubit
of the system to her room, and Bob takes the second qubit to his
room. Now, when Alice receives ùë•if ùë•= 0 she does nothing and
if ùë•= 1 she applies the unitary map ùëÖ‚àíùúã/8 to her qubit where
ùëÖùúÉ= (ùëêùëúùë†ùúÉ
‚àísin ùúÉ
sin ùúÉ
cos ùúÉ) is the unitary operation corresponding to
rotation in the plane with angle ùúÉ. When Bob receives ùë¶, if ùë¶= 0 he
does nothing and if ùë¶= 1 he applies the unitary map ùëÖùúã/8 to his qubit.
Then each one of them measures their qubit and sends this as their
response.
Recall that to win the game Bob and Alice want their outputs to
be more likely to differ if ùë•= ùë¶= 1 and to be more likely to agree
otherwise. We will split the analysis in one case for each of the four
possible values of ùë•and ùë¶.
Case 1: ùë•= 0 and ùë¶= 0. If ùë•= ùë¶= 0 then the state does not
change. Because the state ùúìis proportional to |00‚ü©+ |11‚ü©, the measure-
ments of Bob and Alice will always agree (if Alice measures 0 then the
state collapses to |00‚ü©and so Bob measures 0 as well, and similarly for
1). Hence in the case ùë•= ùë¶= 1, Alice and Bob always win.
Case 2: ùë•= 0 and ùë¶= 1. If ùë•= 0 and ùë¶= 1 then after Alice
measures her bit, if she gets 0 then the system collapses to the state
|00‚ü©, in which case after Bob performs his rotation, his qubit is in
the state cos(ùúã/8)|0‚ü©+ sin(ùúã/8)|1‚ü©. Thus, when Bob measures his
qubit, he will get 0 (and hence agree with Alice) with probability
cos2(ùúã/8) ‚â•0.85. Similarly, if Alice gets 1 then the system collapses
to |11‚ü©, in which case after rotation Bob‚Äôs qubit will be in the state
‚àísin(ùúã/8)|0‚ü©+ cos(ùúã/8)|1‚ü©and so once again he will agree with Alice
with probability cos2(ùúã/8).
The analysis for Case 3, where ùë•= 1 and ùë¶= 0, is completely
analogous to Case 2. Hence Alice and Bob will agree with probability
cos2(ùúã/8) in this case as well. (To show this we use the observation
that the result of this experiment is the same regardless of the order
in which Alice and Bob apply their rotations and measurements; this
requires a proof but is not very hard to show.)
Case 4: ùë•= 1 and ùë¶= 1. For the case that ùë•= 1 and ùë¶= 1,
after both Alice and Bob perform their rotations, the state will be
proportional to
ùëÖ‚àíùúã/8|0‚ü©ùëÖùúã/8|0‚ü©+ ùëÖ‚àíùúã/8|1‚ü©ùëÖùúã/8|1‚ü©.
(23.6)


--- Page 623 ---

quantum computing
623
Intuitively, since we rotate one state by 45 degrees and the other
state by -45 degrees, they will become orthogonal to each other, and
the measurements will behave like independent coin tosses that agree
with probability 1/2. However, for the sake of completeness, we now
show the full calculation.
Opening up the coefficients and using cos(‚àíùë•) = cos(ùë•) and
sin(‚àíùë•) = ‚àísin(ùë•), we can see that (23.6) is proportional to
cos2(ùúã/8)|00‚ü©+ cos(ùúã/8) sin(ùúã/8)|01‚ü©
‚àísin(ùúã/8) cos(ùúã/8)|10‚ü©+ sin2(ùúã/8)|11‚ü©
‚àísin2(ùúã/8)|00‚ü©+ sin(ùúã/8) cos(ùúã/8)|01‚ü©
‚àícos(ùúã/8) sin(ùúã/8)|10‚ü©+ cos2(ùúã/8)|11‚ü©.
(23.7)
Using the trigonometric identities 2 sin(ùõº) cos(ùõº) = sin(2ùõº) and
cos2(ùõº) ‚àísin2(ùõº) = cos(2ùõº), we see that the probability of getting any
one of |00‚ü©, |10‚ü©, |01‚ü©, |11‚ü©is proportional to cos(ùúã/4) = sin(ùúã/4) =
1
‚àö
2.
Hence all four options for (ùëé, ùëè) are equally likely, which mean that in
this case ùëé= ùëèwith probability 0.5.
Taking all the four cases together, the overall probability of winning
the game is 1
4 ‚ãÖ1 + 1
2 ‚ãÖ0.85 + 1
4 ‚ãÖ0.5 = 0.8.
‚ñ†
R
Remark 23.6 ‚Äî Quantum vs probabilistic strategies. It
is instructive to understand what is it about quan-
tum mechanics that enabled this gain in Bell‚Äôs
Inequality. For this, consider the following anal-
ogous probabilistic strategy for Alice and Bob.
They agree that each one of them output 0 if he
or she get 0 as input and outputs 1 with prob-
ability ùëùif they get 1 as input. In this case one
can see that their success probability would be
1
4 ‚ãÖ1 + 1
2(1 ‚àíùëù) + 1
4[2ùëù(1 ‚àíùëù)] = 0.75 ‚àí0.5ùëù2 ‚â§0.75.
The quantum strategy we described above can be
thought of as a variant of the probabilistic strategy for
parameter ùëùset to sin2(ùúã/8)
=
0.15. But in the case
ùë•= ùë¶= 1, instead of disagreeing only with probability
2ùëù(1 ‚àíùëù)
=
1/4, the existence of the so called ‚Äúneg-
ative probabilities‚Äô ‚Äô in the quantum world allowed
us to rotate the state in opposing directions to achieve
destructive interference and hence a higher probability
of disagreement, namely sin2(ùúã/4) = 0.5.


--- Page 624 ---

624
introduction to theoretical computer science
6 Readers familiar with quantum computing should
note that ùëàùëÅùê¥ùëÅùê∑is a close variant of the so called
Toffoli gate and so QNAND-CIRC programs corre-
spond to quantum circuits with the Hadamard and
Toffoli gates.
23.8 QUANTUM COMPUTATION
Recall that in the classical setting, we modeled computation as ob-
tained by a sequence of basic operations. We had two types of computa-
tional models:
‚Ä¢ Non uniform models of computation such as Boolean circuits and
NAND-CIRC programs, where a finite function ùëì‚à∂{0, 1}ùëõ‚Üí{0, 1}
is computable in size ùëáif it can be expressed as a combination of
ùëábasic operations (gates in a circuit or lines in a NAND-CIRC
program)
‚Ä¢ Uniform models of computation such as Turing machines and NAND-
TM programs, where an infinite function ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1} is
computable in time ùëá(ùëõ) if there is a single algorithm that on input
ùë•‚àà{0, 1}ùëõevaluates ùêπ(ùë•) using at most ùëá(ùëõ) basic steps.
When considering efficient computation, we defined the class P to
consist of all infinite functions ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1} that can be com-
puted by a Turing machine or NAND-TM program in time ùëù(ùëõ) for
some polynomial ùëù(‚ãÖ). We defined the class P/poly to consists of all
infinite functions ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1} such that for every ùëõ, the re-
striction ùêπ‚Üæùëõof ùêπto {0, 1}ùëõcan be computed by a Boolean circuit or
NAND-CIRC program of size at most ùëù(ùëõ) for some polynomial ùëù(‚ãÖ).
We will do the same for quantum computation, focusing mostly on
the non uniform setting of quantum circuits, since that is simpler, and
already illustrates the important differences with classical computing.
23.8.1 Quantum circuits
A quantum circuit is analogous to a Boolean circuit, and can be de-
scribed as a directed acyclic graph. One crucial difference that the
out degree of every vertex in a quantum circuit is at most one. This is
because we cannot ‚Äúreuse‚Äù quantum states without measuring them
(which collapses their ‚Äúprobabilities‚Äù). Therefore, we cannot use the
same qubit as input for two different gates. (This is known as the No
Cloning Theorem.) Another more technical difference is that to ex-
press our operations as unitary matrices, we will need to make sure all
our gates are reversible. This is not hard to ensure. For example, in the
quantum context, instead of thinking of NAND as a (non reversible)
map from {0, 1}2 to {0, 1}, we will think of it as the reversible map
on three qubits that maps ùëé, ùëè, ùëêto ùëé, ùëè, ùëê‚äïNAND(ùëé, ùëè) (i.e., flip the
last bit if NAND of the first two bits is 1). Equivalently, the NAND
operation corresponds to the 8 √ó 8 unitary matrix ùëàùëÅùê¥ùëÅùê∑such that
(identifying {0, 1}3 with [8]) for every ùëé, ùëè, ùëê‚àà{0, 1}, if |ùëéùëèùëê‚ü©is the
basis element with 1 in the ùëéùëèùëê-th coordinate and zero elsewhere, then
ùëàùëÅùê¥ùëÅùê∑|ùëéùëèùëê‚ü©= |ùëéùëè(ùëê‚äïNAND(ùëé, ùëè))‚ü©.6 If we order the rows and


--- Page 625 ---

quantum computing
625
columns as 000, 001, 010, ‚Ä¶ , 111, then ùëàùëÅùê¥ùëÅùê∑can be written as the
following matrix:
ùëàùëÅùê¥ùëÅùê∑=
‚éõ
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éú
‚éù
0
1
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
‚éû
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚éü
‚é†
(23.8)
If we have an ùëõqubit system, then for ùëñ, ùëó, ùëò‚àà[ùëõ], we will denote
by ùëàùëñ,ùëó,ùëò
ùëÅùê¥ùëÅùê∑as the 2ùëõ√ó 2ùëõunitary matrix that corresponds to applying
ùëàùëÅùê¥ùëÅùê∑to the ùëñ-th, ùëó-th, and ùëò-th bits, leaving the others intact. That is,
for every ùë£= ‚àëùë•‚àà{0,1}ùëõùë£ùë•|ùë•‚ü©, ùëàùëñ,ùëó,ùëò
ùëÅùê¥ùëÅùê∑ùë£= ‚àëùë•‚àà{0,1}ùëõùë£ùë•|ùë•0 ‚ãØùë•ùëò‚àí1(ùë•ùëò‚äï
NAND(ùë•ùëñ, ùë•ùëó))ùë•ùëò+1 ‚ãØùë•ùëõ‚àí1‚ü©.
As mentioned above, we will also use the Hadamard or HAD oper-
ation, A quantum circuit is obtained by applying a sequence of ùëàùëÅùê¥ùëÅùê∑
and HAD gates, where a HAD gates corresponding to applying the
matrix
ùêª=
1
‚àö
2 (+1
+1
+1
‚àí1) .
(23.9)
Another way to write define ùêªis that for ùëè‚àà{0, 1}, ùêª|ùëè‚ü©=
1
‚àö
2|0‚ü©+
1
‚àö
2(‚àí1)ùëè|1‚ü©. We define HADùëñto be the 2ùëõ√ó 2ùëõunitary matrix that
applies HAD to the ùëñ-th qubit and leaves the others intact. Using the
ket notation, we can write this as
HADùëñ
‚àë
ùë•‚àà{0,1}ùëõ
ùë£ùë•|ùë•‚ü©=
1
‚àö
2
‚àë
ùë•‚àà{0,1}ùëõ
|ùë•0 ‚ãØùë•ùëñ‚àí1‚ü©(|0‚ü©+ (‚àí1)ùë•ùëñ|1‚ü©) |ùë•ùëñ+1 ‚ãØùë•ùëõ‚àí1‚ü©.
(23.10)
A quantum circuit is obtained by composing these basic operations
on some ùëöqubits. If ùëö‚â•ùëõ, we use a circuit to compute a function
ùëì‚à∂{0, 1}ùëõ‚Üí{0, 1}:
‚Ä¢ On input ùë•, we initialize the system to hold ùë•0, ‚Ä¶ , ùë•ùëõ‚àí1 in the first ùëõ
qubits, and initialize all remaining ùëö‚àíùëõqubits to zero.
‚Ä¢ We execute each elementary operation one by one: at every step we
apply to the current state either an operation of the form ùëàùëñ,ùëó,ùëò
ùëÅùê¥ùëÅùê∑or
an operation of the form HADùëñfor ùëñ, ùëó, ùëò‚àà[ùëö].
‚Ä¢ At the end of the computation, we measure the system, and output
the result of the last qubit (i.e. the qubit in location ùëö‚àí1). (For
simplicity we restrict attention to functions with a single bit of
output, though the definition of quantum circuits naturally extends
to circuits with multiple outputs.)


--- Page 626 ---

626
introduction to theoretical computer science
‚Ä¢ We say that the circuit computes the function ùëìif the probability that
this output equals ùëì(ùë•) is at least 2/3. Note that this probability
is obtained by summing up the squares of the amplitudes of all
coordinates in the final state of the system corresponding to vectors
|ùë¶‚ü©where ùë¶ùëö‚àí1 = ùëì(ùë•).
Formally we define quantum circuits as follows:
Definition 23.7 ‚Äî Quantum circuit. Let ùë†‚â•ùëö‚â•ùëõ. A quantum circuit of
ùëõinputs, ùëö
‚àí
ùëõauxiliary bits, and ùë†gates over the {ùëàùëÅùê¥ùëÅùê∑, HAD}
basis is a sequence of ùë†unitary 2ùëö√ó 2ùëömatrices ùëà0, ‚Ä¶ , ùëàùë†‚àí1 such
that each matrix ùëà‚Ñìis either of the form NANDùëñ,ùëó,ùëòfor ùëñ, ùëó, ùëò‚àà[ùëö]
or HADùëñfor ùëñ‚àà[ùëö].
A quantum circuit computes a function ùëì‚à∂{0, 1}ùëõ‚Üí{0, 1} if the
following is true for every ùë•‚àà{0, 1}ùëõ:
Let ùë£be the vector
ùë£= ùëàùë†‚àí1ùëàùë†‚àí2 ‚ãØùëà1ùëà0|ùë•0ùëö‚àíùëõ‚ü©
(23.11)
and write ùë£as ‚àëùë¶‚àà{0,1}ùëöùë£ùë¶|ùë¶‚ü©. Then
‚àë
ùë¶‚àà{0,1}ùëös.t. ùë¶ùëö‚àí1=ùëì(ùë•)
|ùë£ùë¶|2 ‚â•2
3 .
(23.12)
P
Please stop here and see that this definition makes
sense to you.
ÔÉ´Big Idea 29 Just as we did with classical computation, we can de-
fine mathematical models for quantum computation, and represent
quantum algorithms as binary strings.
Once we have the notion of quantum circuits, we can define the
quantum analog of P/poly (i.e., the class of functions computable by
polynomial size quantum circuits) as follows:
Definition 23.8 ‚Äî BQP/poly. Let ùêπ
‚à∂
{0, 1}‚àó
‚Üí
{0, 1}. We say that
ùêπ‚ààBQP/poly if there exists some polynomial ùëù‚à∂‚Ñï‚Üí‚Ñïsuch that
for every ùëõ
‚àà
‚Ñï, if ùêπ‚Üæùëõis the restriction of ùêπto inputs of length ùëõ,
then there is a quantum circuit of size at most ùëù(ùëõ) that computes
ùêπ‚Üæùëõ.


--- Page 627 ---

quantum computing
627
R
Remark 23.9 ‚Äî The obviously exponential fallacy. A
priori it might seem ‚Äúobvious‚Äù that quantum com-
puting is exponentially powerful, since to perform a
quantum computation on ùëõbits we need to maintain
the 2ùëõdimensional state vector and apply 2ùëõ√ó 2ùëõma-
trices to it. Indeed popular descriptions of quantum
computing (too) often say something along the lines
that the difference between quantum and classical
computer is that a classical bit can either be zero or
one while a qubit can be in both states at once, and
so in many qubits a quantum computer can perform
exponentially many computations at once.
Depending on how you interpret it, this description
is either false or would apply equally well to proba-
bilistic computation, even though we‚Äôve already seen
that every randomized algorithm can be simulated by
a similar-sized circuit, and in fact we conjecture that
BPP = P.
Moreover, this ‚Äúobvious‚Äù approach for simulating a
quantum computation will take not just exponential
time but exponential space as well, while can be shown
that using a simple recursive formula one can calcu-
late the final quantum state using polynomial space (in
physics this is known as ‚ÄúFeynman path integrals‚Äù).
So, the exponentially long vector description by itself
does not imply that quantum computers are exponen-
tially powerful. Indeed, we cannot prove that they are
(i.e., we have not been able to rule out the possibility
that every QNAND-CIRC program could be simu-
lated by a NAND-CIRC program/ Boolean circuit with
polynomial overhead), but we do have some problems
(integer factoring most prominently) for which they
do provide exponential speedup over the currently
best known classical (deterministic or probabilistic)
algorithms.
23.8.2 QNAND-CIRC programs (optional)
Just like in the classical case, there is an equivalence between circuits
and straight-line programs, and so we can define the programming
language QNAND-CIRC that is the quantum analog of our NAND-
CIRC programming language. To do so, we only add a single op-
eration: HAD(foo) which applies the single-bit operation ùêªto the
variable foo. We also use the following interpretation to make NAND
reversible: foo = NAND(bar,blah) means that we modify foo to be
the XOR of its original value and the NAND of bar and blah. (In
other words, apply the 8 by 8 unitary transformation ùëàùëÅùê¥ùëÅùê∑defined
above to the three qubits corresponding to foo, bar and blah.) If foo
is initialized to zero then this makes no difference.


--- Page 628 ---

628
introduction to theoretical computer science
If ùëÉis a QNAND-CIRC program with ùëõinput variables, ‚Ñì
workspace variables, and ùëöoutput variables, then running it on the
input ùë•‚àà{0, 1}ùëõcorresponds to setting up a system with ùëõ+ ùëö+ ‚Ñì
qubits and performing the following process:
1. We initialize the input variables X[0] ‚Ä¶ X[ùëõ‚àí1] to ùë•0, ‚Ä¶ , ùë•ùëõ‚àí1 and
all other variables to 0.
2. We execute the program line by line, applying the corresponding
physical operation ùêªor ùëàùëÅùê¥ùëÅùê∑to the qubits that are referred to by
the line.
3. We measure the output variables Y[0], ‚Ä¶, Y[ùëö‚àí1] and output
the result (if there is more than one output then we measure more
variables).
23.8.3 Uniform computation
Just as in the classical case, we can define uniform computational
models for quantum computing as well. We will let BQP be the
quantum analog to P and BPP: the class of all Boolean functions
ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1} that can be computed by quantum algorithms
in polynomial time. There are several equivalent ways to define BQP.
For example, there is a computational model of Quantum Turing
Machines that can be used to define BQP just as standard Turing
machines are used to define P. Another alternative is to define the
QNAND-TM programming language to be QNAND-CIRC augmented
with loops and arrays just like NAND-TM is obtained from NAND-
CIRC. Once again, we can define BQP using QNAND-TM programs
analogously to the way P can be defined using NAND-TM programs.
However, we use the following equivalent definition (which is also
the one most popular in the literature):
Definition 23.10 ‚Äî The class BQP. Let ùêπ‚à∂{0, 1}‚àó‚Üí{0, 1}. We say that
ùêπ
‚ààBQP if there exists a polynomial time NAND-TM program ùëÉ
such that for every ùëõ, ùëÉ(1ùëõ) is the description of a quantum circuit
ùê∂ùëõthat computes the restriction of ùêπto {0, 1}ùëõ.
P
Definition 23.10 is the quantum analog of the alter-
native characterization of P that appears in Solved
Exercise 13.4. One way to verify that you‚Äôve under-
stood Definition 23.10 it to see that you can prove
(1) P
‚äÜ
BQP and in fact the stronger statement
BPP
‚äÜ
BQP, (2) BQP
‚äÜ
EXP, and (3) For every
NP-complete function ùêπ, if ùêπ‚ààBQP then NP ‚äÜBQP.
Exercise 23.1 asks you to work these out.


--- Page 629 ---

quantum computing
629
Figure 23.3: Superconducting quantum computer
prototype at Google. Image credit: Google / MIT
Technology Review.
The relation between NP and BQP is not known (see also Re-
mark 23.4). It is widely believed that NP ‚äàBQP, but there is no
consensus whether or not BQP ‚äÜNP. It is quite possible that these
two classes are incomparable, in the sense that NP ‚äàBQP (and in par-
ticular no NP-complete function belongs to BQP) but also BQP ‚äàNP
(and there are some interesting candidates for such problems).
It can be shown that QNANDEVAL (evaluating a quantum circuit
on an input) is computable by a polynomial size QNAND-CIRC pro-
gram, and moreover this program can even be generated uniformly
and hence QNANDEVAL is in BQP. This allows us to ‚Äúport‚Äù many
of the results of classical computational complexity into the quantum
realm, including the notions of a universal quantum Turing machine,
as well as all of the uncomputability results. There is even a quantum
analog of the Cook-Levin Theorem.
R
Remark 23.11 ‚Äî Restricting attention to circuits. Because
the non uniform model is a little cleaner to work with,
in the rest of this chapter we mostly restrict attention
to this model, though all the algorithms we discuss
can be implemented using uniform algorithms as well.
23.9 PHYSICALLY REALIZING QUANTUM COMPUTATION
To realize quantum computation one needs to create a system with ùëõ
independent binary states (i.e., ‚Äúqubits‚Äù), and be able to manipulate
small subsets of two or three of these qubits to change their state.
While by the way we defined operations above it might seem that one
needs to be able to perform arbitrary unitary operations on these two
or three qubits, it turns out that there are several choices for universal
sets - a small constant number of gates that generate all others. The
biggest challenge is how to keep the system from being measured and
collapsing to a single classical combination of states. This is sometimes
known as the coherence time of the system. The threshold theorem says
that there is some absolute constant level of errors ùúèso that if errors
are created at every gate at rate smaller than ùúèthen we can recover
from those and perform arbitrary long computations. (Of course there
are different ways to model the errors and so there are actually several
threshold theorems corresponding to various noise models).
There have been several proposals to build quantum computers:
‚Ä¢ Superconducting quantum computers use superconducting electric
circuits to do quantum computation. This is the direction where
there has been most recent progress towards ‚Äúbeating‚Äù classical
computers.


--- Page 630 ---

630
introduction to theoretical computer science
Figure 23.4: Top: A periodic function. Bottom: An
a-periodic function.
‚Ä¢ Trapped ion quantum computers Use the states of an ion to sim-
ulate a qubit. People have made some recent advances on these
computers too. While it‚Äôs not at all clear that‚Äôs the right measur-
ing yard, the current best implementation of Shor‚Äôs algorithm (for
factoring 15) is done using an ion-trap computer.
‚Ä¢ Topological quantum computers use a different technology. Topo-
logical qubits are more stable by design and hence error correction
is less of an issue, but constructing them is extremely challenging.
These approaches are not mutually exclusive and it could be that
ultimately quantum computers are built by combining all of them
together. In the near future, it seems that we will not be able to achieve
full fledged large scale universal quantum computers, but rather more
restricted machines, sometimes called ‚ÄúNoisy Intermediate-Scale
Quantum Computers‚Äù or ‚ÄúNISQ‚Äù. See this article by John Preskil for
some of the progress and applications of such more restricted devices.
23.10 SHOR‚ÄôS ALGORITHM: HEARING THE SHAPE OF PRIME FAC-
TORS
Bell‚Äôs Inequality is a powerful demonstration that there is some-
thing very strange going on with quantum mechanics. But could
this ‚Äústrangeness‚Äù be of any use to solve computational problems not
directly related to quantum systems? A priori, one could guess the
answer is no. In 1994 Peter Shor showed that one would be wrong:
Theorem 23.12 ‚Äî Shor‚Äôs Algorithm. There is a polynomial-time quan-
tum algorithm that on input an integer ùëÄ(represented in base
two), outputs the prime factorization of ùëÄ.
Another way to state Theorem 23.12 is that if we define
FACTORING ‚à∂{0, 1}‚àó‚Üí{0, 1} to be the function that on input a
pair of numbers (ùëÄ, ùëã) outputs 1 if and only if ùëÄhas a factor ùëÉsuch
that 2 ‚â§ùëÉ‚â§ùëã, then FACTORING is in BQP. This is an exponential
improvement over the best known classical algorithms, which take
roughly 2
ÃÉ
ùëÇ(ùëõ1/3) time, where the
ÃÉùëÇnotation hides factors that are
polylogarithmic in ùëõ. While we will not prove Theorem 23.12 in this
chapter, we will sketch some of the ideas behind the proof.
23.10.1 Period finding
At the heart of Shor‚Äôs Theorem is an efficient quantum algorithm for
finding periods of a given function. For example, a function ùëì‚à∂‚Ñù‚Üí‚Ñù
is periodic if there is some ‚Ñé> 0 such that ùëì(ùë•+ ‚Ñé) = ùëì(ùë•) for every ùë•
(e.g., see Fig. 23.4).


--- Page 631 ---

quantum computing
631
Figure 23.5: Left: The air-pressure when playing a
‚ÄúC Major‚Äù chord as a function of time. Right: The
coefficients of the Fourier transform of the same func-
tion, we can see that it is the sum of three frequencies
corresponding to the C, E and G notes (261.63, 329.63
and 392 Hertz respectively). Credit: Bjarke M√∏nsted‚Äôs
Quora answer.
Figure 23.6: If ùëìis a periodic function then when we
represent it in the Fourier transform, we expect the
coefficients corresponding to wavelengths that do
not evenly divide the period to be very small, as they
would tend to ‚Äúcancel out‚Äù.
Musical notes yield one type of periodic function. When you pull
on a string on a musical instrument, it vibrates in a repeating pattern.
Hence, if we plot the speed of the string (and so also the speed of
the air around it) as a function of time, it will correspond to some
periodic function. The length of the period is known as the wave length
of the note. The frequency is the number of times the function repeats
itself within a unit of time. For example, the ‚ÄúMiddle C‚Äù note has
a frequency of 261.63 Hertz, which means its period is 1/(261.63)
seconds.
If we play a chord by playing several notes at once, we get a more
complex periodic function obtained by combining the functions of
the individual notes (see Fig. 23.5). The human ear contains many
small hairs, each of which is sensitive to a narrow band of frequencies.
Hence when we hear the sound corresponding to a chord, the hairs in
our ears actually separate it out to the components corresponding to
each frequency.
It turns out that (essentially) every periodic function ùëì‚à∂‚Ñù‚Üí‚Ñù
can be decomposed into a sum of simple wave functions (namely
functions of the form ùë•‚Ü¶sin(ùúÉùë•) or ùë•‚Ü¶cos(ùúÉùë•)). This is known as
the Fourier Transform (see Fig. 23.6). The Fourier transform makes it
easy to compute the period of a given function: it will simply be the
least common multiple of the periods of the constituent waves.
23.10.2 Shor‚Äôs Algorithm: A bird‚Äôs eye view
On input an integer ùëÄ, Shor‚Äôs algorithm outputs the prime factoriza-
tion of ùëÄin time that is polynomial in log ùëÄ. The main steps in the
algorithm are the following:
Step 1: Reduce to period finding.
The first step in the algorithm is to
pick a random ùê¥‚àà{0, 1 ‚Ä¶ , ùëÄ‚àí1} and define the function ùêπùê¥‚à∂
{0, 1}ùëö‚Üí{0, 1}ùëöas ùêπùê¥(ùë•) = ùê¥ùë•( mod ùëÄ) where we identify the
string ùë•‚àà{0, 1}ùëöwith an integer using the binary representation,
and similarly represent the integer ùê¥ùë•( mod ùëÄ) as a string. (We will
choose ùëöto be some polynomial in log ùëÄand so in particular {0, 1}ùëö
is a large enough set to represent all the numbers in {0, 1, ‚Ä¶ , ùëÄ‚àí1}).
Some not-too-hard (though somewhat technical) calculations show
that: (1) The function ùêπùê¥is periodic (i.e., there is some integer ùëùùê¥such
that ùêπùê¥(ùë•+ ùëùùê¥) = ùêπùê¥(ùë•) for ‚Äúalmost‚Äù every ùë•) and more importantly
(2) If we can recover the period ùëùùê¥of ùêπùê¥for several randomly cho-
sen ùê¥‚Äôs, then we can recover the factorization of ùëÄ. (We‚Äôll ignore the
‚Äúalmost‚Äù qualifier in the discussion below; it causes some annoying,
yet ultimately manageable, technical issues in the full-fledged algo-
rithm.) Hence, factoring ùëÄreduces to finding out the period of the
function ùêπùê¥. Exercise 23.2 asks you to work out this for the related


--- Page 632 ---

632
introduction to theoretical computer science
task of computing the discrete logarithm (which underlies the security
of the Diffie-Hellman key exchange and elliptic curve cryptography).
Step 2: Period finding via the Quantum Fourier Transform.
Using a simple
trick known as ‚Äúrepeated squaring‚Äù, it is possible to compute the
map ùë•‚Ü¶ùêπùê¥(ùë•) in time polynomial in ùëö, which means we can also
compute this map using a polynomial number of NAND gates,and so
in particular we can generate in polynomial quantum time a quantum
state ùúåthat is (up to normalization) equal to
‚àë
ùë•‚àà{0,1}ùëö
|ùë•‚ü©|ùêπùê¥(ùë•)‚ü©.
(23.13)
In particular, if we were to measure the state ùúå, we would get a ran-
dom pair of the form (ùë•, ùë¶) where ùë¶= ùêπùê¥(ùë•). So far, this is not at all
impressive. After all, we did not need the power of quantum comput-
ing to generate such pairs: we could simply generate a random ùë•and
then compute ùêπùê¥(ùë•).
Another way to describe the state ùúåis that the coefficient of |ùë•‚ü©|ùë¶‚ü©in
ùúåis proportional to ùëìùê¥,ùë¶(ùë•) where ùëìùê¥,ùë¶‚à∂{0, 1}ùëö‚Üí‚Ñùis the function
such that
ùëìùê¥,ùë¶(ùë•) =
‚éß
{
‚é®
{
‚é©
1
ùë¶= ùê¥ùë•(
mod ùëÄ)
0
otherwise
.
(23.14)
The magic of Shor‚Äôs algorithm comes from a procedure known
as the Quantum Fourier Transform. It allows to change the state ùúåinto
the state
ÃÇùúåwhere the coefficient of |ùë•‚ü©|ùë¶‚ü©is now proportional to the
ùë•-th Fourier coefficient of ùëìùê¥,ùë¶. In other words, if we measure the state
ÃÇùúå, we will obtain a pair (ùë•, ùë¶) such that the probability of choosing ùë•
is proportional to the square of the weight of the frequency ùë•in the
representation of the function ùëìùê¥,ùë¶. Since for every ùë¶, the function
ùëìùê¥,ùë¶has the period ùëùùê¥, it can be shown that the frequency ùë•will be
(almost) a multiple of ùëùùê¥. If we make several such samples ùë¶0, ‚Ä¶ , ùë¶ùëò
and obtain the frequencies ùë•1, ‚Ä¶ , ùë•ùëò, then the true period ùëùùê¥divides
all of them, and it can be shown that it is going to be in fact the greatest
common divisor (g.c.d.) of all these frequencies: a value which can be
computed in polynomial time.
As mentioned above, we can recover the factorization of ùëÄfrom
the periods of ùêπùê¥0, ‚Ä¶ , ùêπùê¥ùë°for some randomly chosen ùê¥0, ‚Ä¶ , ùê¥ùë°in
{0, ‚Ä¶ , ùëÄ‚àí1} and ùë°which is polynomial in log ùëÄ.
The resulting algorithm can be described in a high (and somewhat
inaccurate) level as follows:
Shor‚Äôs Algorithm: (sketch)
Input: Number ùëÄ‚àà‚Ñï.
Output: Prime factorization of ùëÄ.
Operations:


--- Page 633 ---

quantum computing
633
1. Repeat the following ùëò= ùëùùëúùëôùë¶(log ùëÄ) number of times:
a. Choose ùê¥‚àà{0, ‚Ä¶ , ùëÄ‚àí1} at random, and let ùëìùê¥‚à∂‚Ñ§ùëÄ‚Üí‚Ñ§ùëÄbe
the map ùë•‚Ü¶ùê¥ùë•mod ùëÄ.
b. For ùë°= ùëùùëúùëôùë¶(log ùëÄ), repeat ùë°times the following step: Quantum
Fourier Transform to create a quantum state |ùúì‚ü©over ùëùùëúùëôùë¶(log(ùëö))
qubits, such that if we measure |ùúì‚ü©we obtain a pair of strings
(ùëó, ùë¶) with probability proportional to the square of the coefficient
corresponding to the wave function ùë•‚Ü¶cos(ùë•ùúãùëó/ùëÄ) or ùë•‚Ü¶
sin(ùë•ùúãùëó/ùëÄ) in the Fourier transform of the function ùëìùê¥,ùë¶‚à∂‚Ñ§ùëö‚Üí
{0, 1} defined as ùëìùê¥,ùë¶(ùë•) = 1 iff ùëìùê¥(ùë•) = ùë¶.
c. If ùëó1, ‚Ä¶ , ùëóùë°are the coefficients we obtained in the previous step,
then the least common multiple of ùëÄ/ùëó1, ‚Ä¶ , ùëÄ/ùëóùë°is likely to be
the period of the function ùëìùê¥.
2. If we let ùê¥0, ‚Ä¶ , ùê¥ùëò‚àí1 and ùëù0, ‚Ä¶ , ùëùùëò‚àí1 be the numbers we chose in
the previous step and the corresponding periods of the functions
ùëìùê¥0, ‚Ä¶ , ùëìùê¥ùëò‚àí1 then we can use classical results in number theory to
obtain from these a non-trivial prime factor ùëÑof ùëÄ(if such exists).
We can now run the algorithm again with the (smaller) input ùëÄ/ùëÑ
to obtain all other factors.
Reducing factoring to order finding is cumbersome, but can be
done in polynomial time using a classical computer. The key quantum
ingredient in Shor‚Äôs algorithm is the quantum fourier transform.
R
Remark 23.13 ‚Äî Quantum Fourier Transform. Despite
its name, the Quantum Fourier Transform does not
actually give a way to compute the Fourier Trans-
form of a function ùëì
‚à∂
{0, 1}ùëö
‚Üí
‚Ñù. This would be
impossible to do in time polynomial in ùëö, as simply
writing down the Fourier Transform would require 2ùëö
coefficients. Rather the Quantum Fourier Transform
gives a quantum state where the amplitude correspond-
ing to an element (think: frequency) ‚Ñéis equal to
the corresponding Fourier coefficient. This allows to
sample from a distribution where ‚Ñéis drawn with
probability proportional to the square of its Fourier
coefficient. This is not the same as computing the
Fourier transform, but is good enough for recovering
the period.
23.11 QUANTUM FOURIER TRANSFORM (ADVANCED, OPTIONAL)
The above description of Shor‚Äôs algorithm skipped over the implemen-
tation of the main quantum ingredient: the Quantum Fourier Transform
algorithm. In this section we discuss the ideas behind this algorithm.
We will be rather brief and imprecise. Section 23.13 contain references
to sources of more information about this topic.


--- Page 634 ---

634
introduction to theoretical computer science
To understand the Quantum Fourier Transform, we need to better
understand the Fourier Transform itself. In particular, we will need
to understand how it applies not just to functions whose input is a
real number but to functions whose domain can be any arbitrary
commutative group. Therefore we now take a short detour to (very
basic) group theory, and define the notion of periodic functions over
groups.
R
Remark 23.14 ‚Äî Group theory. While we define the con-
cepts we use, some background in group or number
theory will be very helpful for fully understanding
this section. In particular we will use the notion of
finite commutative (a.k.a. Abelian) groups. These are
defined as follows.
‚Ä¢ A finite group ùîæis a pair (ùê∫, ‚ãÜ) where ùê∫is a finite
set of elements and ‚ãÜis a binary operation mapping
a pair ùëî, ‚Ñéof elements in ùê∫to the element ùëî‚ãÜ‚Ñéin ùê∫.
We often identify ùîæwith the set ùê∫of its elements,
and so use notation such as ùëî‚ààùîæto indicate that ùëî
is an element of ùîæand |ùîæ| to denote the number of
elements in ùîæ.
‚Ä¢ The operation ‚ãÜsatisfies the sort of properties that
a product operation does, namely, it is associative
(i.e., (ùëî‚ãÜ‚Ñé) ‚ãÜùëì= ùëî‚ãÜ(‚Ñé‚ãÜùëì)) and there is some
element 1 such that ùëî‚ãÜ1
=
ùëîfor all ùëî, and for
every ùëî
‚àà
ùîæthere exists an element ùëî‚àí1 such that
ùëî‚ãÜùëî‚àí1 = 1.
‚Ä¢ A group is called commutative (also known as
Abelian) if ùëî‚ãÜ‚Ñé= ‚Ñé‚ãÜùëîfor all ùëî, ‚Ñé‚ààùîæ.
The Fourier transform is a deep and vast topic, on which we will
barely touch upon here. Over the real numbers, the Fourier trans-
form of a function ùëìis obtained by expressing ùëìin the form ‚àë
ÃÇùëì(ùõº)ùúíùõº
where the ùúíùõº‚Äôs are ‚Äúwave functions‚Äù (e.g. sines and cosines). How-
ever, it turns out that the same notion exists for every Abelian group ùîæ.
Specifically, for every such group ùîæ, if ùëìis a function mapping ùîæto ‚ÑÇ,
then we can write ùëìas
ùëì= ‚àë
ùëî‚ààùîæ
ÃÇùëì(ùëî)ùúíùëî,
(23.15)
where the ùúíùëî‚Äôs are functions mapping ùîæto ‚ÑÇthat are analogs of
the ‚Äúwave functions‚Äù for the group ùîæand for every ùëî‚ààùîæ,
ÃÇùëì(ùëî) is a
complex number known as the Fourier coefficient of ùëìcorresponding to
ùëî. Specifically, the equation (23.15) means that if we think of ùëìas a
|ùîæ| dimensional vector over the complex numbers, then we can write
this vector as a sum (with certain coefficients) of the vectors {ùúíùëî}ùëî‚ààùîæ.


--- Page 635 ---

quantum computing
635
The representation (23.15) is known as the Fourier expansion or Fourier
transform of ùëì, the numbers ( ÃÇùëì(ùëî))ùëî‚ààùîæare known as the Fourier coef-
ficients of ùëìand the functions (ùúíùëî)ùëî‚ààùîæare known as the Fourier char-
acters. The central property of the Fourier characters is that they are
homomorphisms of the group into the complex numbers, in the sense
that for every ùë•, ùë•‚Ä≤ ‚ààùîæ, ùúíùëî(ùë•‚ãÜùë•‚Ä≤) = ùúíùëî(ùë•)ùúíùëî(ùë•‚Ä≤), where ‚ãÜis the group
operation. One corollary of this property is that if ùúíùëî(‚Ñé) = 1 then ùúíùëî
is ‚Ñéperiodic in the sense that ùúíùëî(ùë•‚ãÜ‚Ñé) = ùúíùëî(ùë•) for every ùë•. It turns
out that if ùëìis periodic with minimal period ‚Ñé, then the only Fourier
characters that have non zero coefficients in the expression (23.15)
are those that are ‚Ñéperiodic as well. This can be used to recover the
period of ùëìfrom its Fourier expansion.
23.11.1 Quantum Fourier Transform over the Boolean Cube: Simon‚Äôs
Algorithm
We now describe the simplest setting of the Quantum Fourier Trans-
form: the group {0, 1}ùëõwith the XOR operation, which we‚Äôll denote
by ({0, 1}ùëõ, ‚äï). (Since XOR is equal to addition modulo two, this
group is also often denoted as (‚Ñ§2)ùëõ.) It can be shown that the Fourier
transform over ({0, 1}ùëõ, ‚äï) corresponds to expressing ùëì‚à∂{0, 1}ùëõ‚Üí‚ÑÇ
as
ùëì=
‚àë
ùë¶‚àà{0,1}
ÃÇùëì(ùë¶)ùúíùë¶
(23.16)
where ùúíùë¶‚à∂{0, 1}ùëõ‚Üí‚ÑÇis defined as ùúíùë¶(ùë•) = (‚àí1)‚àëùëñùë¶ùëñùë•ùëñand
ÃÇùëì(ùë¶) =
1
‚àö
2ùëõ‚àëùë•‚àà{0,1}ùëõùëì(ùë•)(‚àí1)‚àëùëñùë¶ùëñùë•ùëñ.
The Quantum Fourier Transform over ({0, 1}ùëõ, ‚äï) is actually quite
simple:
Theorem 23.15 ‚Äî QFT Over the Boolean Cube. Let ùúå= ‚àëùë•‚àà{0,1}ùëõùëì(ùë•)|ùë•‚ü©
be a quantum state where ùëì‚à∂{0, 1}ùëõ‚Üí‚ÑÇis some function satisfy-
ing ‚àëùë•‚àà{0,1}ùëõ|ùëì(ùë•)|2 = 1. Then we can use ùëõgates to transform ùúåto
the state
‚àë
ùë¶‚àà{0,1}ùëõ
ÃÇùëì(ùë¶)|ùë¶‚ü©
(23.17)
where ùëì
= ‚àëùë¶
ÃÇùëì(ùë¶)ùúíùë¶and ùúíùë¶
‚à∂{0, 1}ùëõ
‚Üí‚ÑÇis the function
ùúíùë¶(ùë•) = ‚àí1‚àëùë•ùëñùë¶ùëñ.
Proof Idea:
The idea behind the proof is that the Hadamard operation corre-
sponds to the Fourier transform over the group {0, 1}ùëõ(with the XOR
operations). To show this, we just need to do the calculations.
‚ãÜ


--- Page 636 ---

636
introduction to theoretical computer science
Proof of Theorem 23.15. We can express the Hadamard operation HAD
as follows:
HAD|ùëé‚ü©=
1
‚àö
2(|0‚ü©+ (‚àí1)ùëé|1‚ü©) .
(23.18)
We are given the state
ùúå=
‚àë
ùë•‚àà{0,1}ùëõ
ùëì(ùë•)|ùë•‚ü©.
(23.19)
Now suppose that we apply the HAD operation to each of the ùëõ
qubits. We can see that we get the state
2‚àíùëõ/2
‚àë
ùë•‚àà{0,1}ùëõ
ùëì(ùë•)
ùëõ‚àí1
‚àè
ùëñ=0
(|0‚ü©+ (‚àí1)ùë•ùëñ|1‚ü©) .
(23.20)
We can now use the distributive law and open up a term of the
form
ùëì(ùë•)(|0‚ü©+ (‚àí1)ùë•0|1‚ü©) ‚ãØ(|0‚ü©+ (‚àí1)ùë•ùëõ‚àí1|1‚ü©)
(23.21)
to the following sum over 2ùëõterms:
ùëì(ùë•)
‚àë
ùë¶‚àà{0,1}ùëõ
(‚àí1)‚àëùë¶ùëñùë•ùëñ|ùë¶‚ü©.
(23.22)
(If you find the above confusing, try to work out explicitly this
calculation for ùëõ= 3; namely show that (|0‚ü©+ (‚àí1)ùë•0|1‚ü©)(|0‚ü©+
(‚àí1)ùë•1|1‚ü©)(|0‚ü©+(‚àí1)ùë•2|1‚ü©) is the same as the sum over 23 terms |000‚ü©+
(‚àí1)ùë•2|001‚ü©+ ‚ãØ+ (‚àí1)ùë•0+ùë•1+ùë•2|111‚ü©.)
By changing the order of summations, we see that the final state is
‚àë
ùë¶‚àà{0,1}ùëõ
2‚àíùëõ/2(
‚àë
ùë•‚àà{0,1}ùëõ
ùëì(ùë•)(‚àí1)‚àëùë•ùëñùë¶ùëñ)|ùë¶‚ü©
(23.23)
which exactly corresponds to
ÃÇùúå.
‚ñ†
23.11.2 From Fourier to Period finding: Simon‚Äôs Algorithm (advanced,
optional)
Using Theorem 23.15 it is not hard to get an algorithm that can recover
a string ‚Ñé‚àó‚àà{0, 1}ùëõgiven a circuit that computes a function ùêπ‚à∂
{0, 1}ùëõ‚Üí{0, 1}‚àóthat is ‚Ñé‚àóperiodic in the sense that ùêπ(ùë•) = ùêπ(ùë•‚Ä≤) for
distinct ùë•, ùë•‚Ä≤ if and only if ùë•‚Ä≤ = ùë•‚äï‚Ñé‚àó. The key observation is that if
we compute the state ‚àëùë•‚àà{0,1}ùëõ|ùë•‚ü©|ùêπ(ùë•)‚ü©, and perform the Quantum
Fourier transform on the first ùëõqubits, then we would get a state such
that the only basis elements with nonzero coefficients would be of the
form |ùë¶‚ü©where
‚àëùë¶ùëñ‚Ñé‚àó
ùëñ= 0(
mod 2)
(23.24)


--- Page 637 ---

quantum computing
637
So, by measuring the state, we can obtain a sample of a random
ùë¶satisfying (23.24). But since (23.24) is a linear equation modulo 2
about the unknown ùëõvariables ‚Ñé‚àó
0, ‚Ä¶ , ‚Ñé‚àó
ùëõ‚àí1, if we repeat this proce-
dure to get ùëõsuch equations, we will have at least as many equations
as variables and (it can be shown that) this will suffice to recover ‚Ñé‚àó.
This result is known as Simon‚Äôs Algorithm, and it preceded and
inspired Shor‚Äôs algorithm.
23.11.3 From Simon to Shor (advanced, optional)
Theorem 23.15 seemed to really use the special bit-wise structure of
the group {0, 1}ùëõ, and so one could wonder if it can be extended to
other groups. However, it turns out that we can in fact achieve such a
generalization.
The key step in Shor‚Äôs algorithm is to implement the Fourier trans-
form for the group ‚Ñ§ùêøwhich is the set of numbers {0, ‚Ä¶ , ùêø‚àí1} with
the operation being addition modulo ùêø. In this case it turns out that
the Fourier characters are the functions ùúíùë¶(ùë•) = ùúîùë¶ùë•where ùúî= ùëí2ùúãùëñ/ùêø
(ùëñhere denotes the complex number
‚àö
‚àí1). The ùë¶-th Fourier coeffi-
cient of a function ùëì‚à∂‚Ñ§ùêø‚Üí‚ÑÇis
ÃÇùëì(ùë¶) =
1
‚àö
ùêø‚àë
ùë•‚àà‚Ñ§ùêø
ùëì(ùë•)ùúîùë•ùë¶.
(23.25)
The key to implementing the Quantum Fourier Transform for such
groups is to use the same recursive equations that enable the classical
Fast Fourier Transform (FFT) algorithm. Specifically, consider the case
that ùêø= 2‚Ñì. We can separate the sum over ùë•in (23.25) to the terms
corresponding to even ùë•‚Äôs (of the form ùë•= 2ùëß) and odd ùë•‚Äôs (of the
form ùë•= 2ùëß+ 1) to obtain
ÃÇùëì(ùë¶) =
1
‚àö
ùêø
‚àë
ùëß‚ààùëçùêø/2
ùëì(2ùëß)(ùúî2)ùë¶ùëß+ ùúîùë¶
‚àö
ùêø‚àë
ùëß‚àà‚Ñ§ùêø/2
ùëì(2ùëß+ 1)(ùúî2)ùë¶ùëß
(23.26)
which reduces computing the Fourier transform of ùëìover the group
‚Ñ§2‚Ñìto computing the Fourier transform of the functions ùëìùëíùë£ùëíùëõand
ùëìùëúùëëùëë(corresponding to the applying ùëìto only the even and odd ùë•‚Äôs
respectively) which have 2‚Ñì‚àí1 inputs that we can identify with the
group ‚Ñ§2‚Ñì‚àí1 = ‚Ñ§ùêø/2.
Specifically, the Fourier characters of the group ‚Ñ§ùêø/2 are the func-
tions ùúíùë¶(ùë•) = ùëí2ùúãùëñ/(ùêø/2)ùë¶ùë•= (ùúî2)ùë¶ùë•for every ùë•, ùë¶‚àà‚Ñ§ùêø/2. Moreover,
since ùúîùêø= 1, (ùúî2)ùë¶= (ùúî2)ùë¶mod ùêø/2 for every ùë¶‚àà‚Ñï. Thus (23.26)
translates into
ÃÇùëì(ùë¶) =
ÃÇùëìùëíùë£ùëíùëõ(ùë¶
mod ùêø/2) + ùúîùë¶ÃÇùëìùëúùëëùëë(ùë¶
mod ùêø/2) .
(23.27)
This observation is usually used to obtain a fast (e.g. ùëÇ(ùêølog ùêø))
time to compute the Fourier transform in a classical setting, but it can


--- Page 638 ---

638
introduction to theoretical computer science
be used to obtain a quantum circuit of ùëùùëúùëôùë¶(log ùêø) gates to transform a
state of the form ‚àëùë•‚àà‚Ñ§ùêøùëì(ùë•)|ùë•‚ü©to a state of the form ‚àëùë¶‚àà‚Ñ§ùêø
ÃÇùëì(ùë¶)|ùë¶‚ü©.
The case that ùêøis not an exact power of two causes some complica-
tions in both the classical case of the Fast Fourier Transform and the
quantum setting of Shor‚Äôs algorithm. However, it is possible to handle
these. The idea is that we can embed ùëçùêøin the group ‚Ñ§ùê¥‚ãÖùêøfor any
integer ùê¥, and we can find an integer ùê¥such that ùê¥‚ãÖùêøwill be close
enough to a power of 2 (i.e., a number of the form 2ùëöfor some ùëö), so
that if we do the Fourier transform over the group ‚Ñ§2ùëöthen we will
not introduce too many errors.
Figure 23.7: Conjectured status of BQP with respect to
other complexity classes. We know that P ‚äÜBPP ‚äÜ
BQP and BQP ‚äÜPSPACE ‚äÜEXP. It is not known if
any of these inclusions are strict though it is believed
that they are. The relation between BQP and NP is
unknown but they are believed to be incomparable,
and that NP-complete problems can not be solved in
polynomial time by quantum computers. However, it
is possible that BQP contains NP and even PSPACE,
and it is also possible that quantum computers
offer no super-polynomial speedups and that P =
BQP. The class ‚ÄúNISQ‚Äù above is not a well defined
complexity class, but rather captures the current
status of quantum devices, which seem to be able to
solve a set of computational tasks that is incomparable
with the set of tasks solvable by classical computers.
The diagram is also inaccurate in the sense that at the
moment the ‚Äúquantum supremacy‚Äù tasks on which
such devices seem to offer exponential speedups
do not correspond to Boolean functions/ decision
problems.
‚úì
Chapter Recap
‚Ä¢ The state of an ùëõ-qubit quantum system can be
modeled as a 2ùëõdimensional vector
‚Ä¢ An operation on the state corresponds to applying
a unitary matrix to this vector.
‚Ä¢ Quantum circuits are obtained by composing basic
operations such as HAD and ùëàùëÅùê¥ùëÅùê∑.
‚Ä¢ We can use quantum circuits to define the classes
BQP/poly and BQP which are the quantum analogs
of P/poly and BPP respectively.
‚Ä¢ There are some problems for which the best known
quantum algorithm is exponentially faster than
the best known, but quantum computing is not a
panacea. In particular, as far as we know, quantum
computers could still require exponential time to
solve NP-complete problems such as SAT.


--- Page 639 ---

quantum computing
639
7 You can use ùëàùëÅùê¥ùëÅùê∑to simulate NAND gates.
8 Use the alternative characterization of P as in Solved
Exercise 13.4.
9 You can use the HAD gate to simulate a coin toss.
10 In exponential time simulating quantum computa-
tion boils down to matrix multiplication.
11 If a reduction can be implemented in P it can be
implemented in BQP as well.
12 We are given ‚Ñé= ùëîùë•and need to recover ùë•. To
do so we can compute the order of various elements
of the form ‚Ñéùëéùëîùëè. The order of such an element is
a number ùëêsatisfying ùëê(ùë•ùëé+ ùëè) = 0 (mod |ùîæ|).
With a few random examples we will get a non trivial
equation on ùë•(where ùëêis not zero modulo |ùîæ|) and
then we can use our knowledge of ùëé, ùëè, ùëêto recover ùë•.
23.12 EXERCISES
Exercise 23.1 ‚Äî Quantum and classical complexity class relations. Prove the
following relations between quantum complexity classes and classical
ones:
1. P/poly ‚äÜBQP/poly. See footnote for hint.7
2. P ‚äÜBQP. See footnote for hint.8
3. BPP ‚äÜBQP. See footnote for hint.9
4. BQP ‚äÜEXP. See footnote for hint.10
5. If SAT ‚ààBQP then NP ‚äÜBQP. See footnote for hint.11
‚ñ†
Exercise 23.2 ‚Äî Discrete logarithm from order finding. Show a probabilistic
polynomial time classical algorithm that given an Abelian finite group
ùîæ(in the form of an algorithm that computes the group operation),
a generator ùëîfor the group, and an element ‚Ñé‚ààùîæ, as well as access to
a black box that on input ùëì‚ààùîæoutputs the order of ùëì(the smallest ùëé
such that ùëìùëé= 1), computes the discrete logarithm of ‚Ñéwith respect to
ùëî. That is the algorithm should output a number ùë•such that ùëîùë•= ‚Ñé.
See footnote for hint.12
‚ñ†
23.13 BIBLIOGRAPHICAL NOTES
An excellent gentle introduction to quantum computation is given in
Mermin‚Äôs book [Mer07]. In particular the first 100 pages (Chapter
1 to 4) of [Mer07] cover all the material of this chapter in a much
more comprehensive way. This material is also covered in the first
5 chapters of De-Wolf‚Äôs online lecture notes. For a more condensed
exposition, the chapter on quantum computation in my book with
Arora (see draft here) is one relatively short source that contains
full descriptions of Grover‚Äôs, Simon‚Äôs and Shor‚Äôs algorithms. This
blog post of Aaronson contains a high level explanation of Shor‚Äôs
algorithm which ends with links to several more detailed expositions.
Chapters 9 and 10 in Aaronson‚Äôs book [Aar13] give an informal but
highly informative introduction to the topics of this chapter and much
more. Chapter 10 in Avi Wigderson‚Äôs book also provides a high level
overview of quantum computing. Other recommended resources
include Andrew Childs‚Äô lecture notes on quantum algorithms, as
well as the lecture notes of Umesh Vazirani, John Preskill, and John
Watrous.
There are many excellent videos available online covering some of
these materials. The videos of Umesh Vazirani‚Äôs EdX course are an


--- Page 640 ---

640
introduction to theoretical computer science
accessible and recommended introduction to quantum computing.
Regarding quantum mechanics in general, this video illustrates the
double slit experiment, this Scientific American video is a nice ex-
position of Bell‚Äôs Theorem. This talk and panel moderated by Brian
Greene discusses some of the philosophical and technical issues
around quantum mechanics and its so called ‚Äúmeasurement prob-
lem‚Äù. The Feynman lecture on the Fourier Transform and quantum
mechanics in general are very much worth reading. The Fourier trans-
form is covered in these videos of Dr. Chris Geoscience, Clare Zhang
and Vi Hart. See also Kelsey Houston-Edwards‚Äôs video on Shor‚Äôs Al-
gorithm.
The form of Bell‚Äôs game we discuss in Section 23.3 was given by
Clauser, Horne, Shimony, and Holt.
The Fast Fourier Transform, used as a component in Shor‚Äôs algo-
rithm, is one of the most widely used algorithms invented. The stories
of its discovery by Gauss in trying to calculate asteroid orbits and
rediscovery by Tukey during the cold war are fascinating as well.
The image in Fig. 23.2 is taken from Wikipedia.
Thanks to Scott Aaronson for many helpful comments about this
chapter.


--- Page 641 ---

VI
APPENDICES


--- Page 642 ---



--- Page 643 ---

Bibliography
[Aar05]
Scott Aaronson. ‚ÄúNP-complete problems and physical
reality‚Äù. In: ACM Sigact News 36.1 (2005). Available on
https://arxiv.org/abs/quant-ph/0502072, pp. 30‚Äì52.
[Aar13]
Scott Aaronson. Quantum computing since Democritus.
Cambridge University Press, 2013.
[Aar16]
Scott Aaronson. ‚ÄúP =? NP‚Äù. In: Open problems in mathe-
matics. Available on https://www.scottaaronson.com/
papers/pnp.pdf. Springer, 2016, pp. 1‚Äì122.
[Aar20]
Scott Aaronson. ‚ÄúThe Busy Beaver Frontier‚Äù. In: SIGACT
News (2020). Available on https://www.scottaaronson.
com/papers/bb.pdf.
[AKS04]
Manindra Agrawal, Neeraj Kayal, and Nitin Saxena.
‚ÄúPRIMES is in P‚Äù. In: Annals of mathematics (2004),
pp. 781‚Äì793.
[Asp18]
James Aspens. Notes on Discrete Mathematics. Online
textbook for CS 202. Available on http://www.cs.yale.
edu/homes/aspnes/classes/202/notes.pdf. 2018.
[Bar84]
H. P. Barendregt. The lambda calculus : its syntax and
semantics. Amsterdam New York New York, N.Y: North-
Holland Sole distributors for the U.S.A. and Canada,
Elsevier Science Pub. Co, 1984. isbn: 0444875085.
[Bjo14]
Andreas Bjorklund. ‚ÄúDeterminant sums for undirected
hamiltonicity‚Äù. In: SIAM Journal on Computing 43.1
(2014), pp. 280‚Äì299.
[Bl√§13]
Markus Bl√§ser. Fast Matrix Multiplication. Graduate Sur-
veys 5. Theory of Computing Library, 2013, pp. 1‚Äì60.
doi: 10.4086/toc.gs.2013.005. url: http://www.
theoryofcomputing.org/library.html.
[Boo47]
George Boole. The mathematical analysis of logic. Philo-
sophical Library, 1847.
Compiled on 8.26.2020 18:10


--- Page 644 ---

644
introduction to theoretical computer science
[BU11]
David Buchfuhrer and Christopher Umans. ‚ÄúThe com-
plexity of Boolean formula minimization‚Äù. In: Journal of
Computer and System Sciences 77.1 (2011), pp. 142‚Äì153.
[Bur78]
Arthur W Burks. ‚ÄúBooke review: Charles S. Peirce, The
new elements of mathematics‚Äù. In: Bulletin of the Ameri-
can Mathematical Society 84.5 (1978), pp. 913‚Äì918.
[Cho56]
Noam Chomsky. ‚ÄúThree models for the description of
language‚Äù. In: IRE Transactions on information theory 2.3
(1956), pp. 113‚Äì124.
[Chu41]
Alonzo Church. The calculi of lambda-conversion. Prince-
ton London: Princeton University Press H. Milford,
Oxford University Press, 1941. isbn: 978-0-691-08394-0.
[CM00]
Bruce Collier and James MacLachlan. Charles Babbage:
And the engines of perfection. Oxford University Press,
2000.
[Coh08]
Paul Cohen. Set theory and the continuum hypothesis. Mine-
ola, N.Y: Dover Publications, 2008. isbn: 0486469212.
[Coh81]
Danny Cohen. ‚ÄúOn holy wars and a plea for peace‚Äù. In:
Computer 14.10 (1981), pp. 48‚Äì54.
[Coo66]
SA Cook. ‚ÄúOn the minimum computation time for mul-
tiplication‚Äù. In: Doctoral dissertation, Harvard University
Department of Mathematics, Cambridge, Mass. 1 (1966).
[Coo87]
James W Cooley. ‚ÄúThe re-discovery of the fast Fourier
transform algorithm‚Äù. In: Microchimica Acta 93.1-6
(1987), pp. 33‚Äì45.
[Cor+09]
Thomas H. Cormen et al. Introduction to Algorithms, 3rd
Edition. MIT Press, 2009. isbn: 978-0-262-03384-8. url:
http://mitpress.mit.edu/books/introduction-algorithms.
[CR73]
Stephen A Cook and Robert A Reckhow. ‚ÄúTime bounded
random access machines‚Äù. In: Journal of Computer and
System Sciences 7.4 (1973), pp. 354‚Äì375.
[CRT06]
Emmanuel J Candes, Justin K Romberg, and Terence Tao.
‚ÄúStable signal recovery from incomplete and inaccurate
measurements‚Äù. In: Communications on Pure and Applied
Mathematics: A Journal Issued by the Courant Institute of
Mathematical Sciences 59.8 (2006), pp. 1207‚Äì1223.
[CT06]
Thomas M. Cover and Joy A. Thomas. ‚ÄúElements of
information theory 2nd edition‚Äù. In: Willey-Interscience:
NJ (2006).
[Dau90]
Joseph Warren Dauben. Georg Cantor: His mathematics and
philosophy of the infinite. Princeton University Press, 1990.


--- Page 645 ---

BIBLIOGRAPHY
645
[De 47]
Augustus De Morgan. Formal logic: or, the calculus of
inference, necessary and probable. Taylor and Walton, 1847.
[Don06]
David L Donoho. ‚ÄúCompressed sensing‚Äù. In: IEEE Trans-
actions on information theory 52.4 (2006), pp. 1289‚Äì1306.
[DPV08]
Sanjoy Dasgupta, Christos H. Papadimitriou, and Umesh
V. Vazirani. Algorithms. McGraw-Hill, 2008. isbn: 978-0-
07-352340-8.
[Ell10]
Jordan Ellenberg. ‚ÄúFill in the blanks: Using math to turn
lo-res datasets into hi-res samples‚Äù. In: Wired Magazine
18.3 (2010), pp. 501‚Äì509.
[Ern09]
Elizabeth Ann Ernst. ‚ÄúOptimal combinational multi-
level logic synthesis‚Äù. Available on https://deepblue.
lib.umich.edu/handle/2027.42/62373. PhD thesis.
University of Michigan, 2009.
[Fle18]
Margaret M. Fleck. Building Blocks for Theoretical Com-
puter Science. Online book, available at http://mfleck.cs.
illinois.edu/building-blocks/. 2018.
[F√ºr07]
Martin F√ºrer. ‚ÄúFaster integer multiplication‚Äù. In: Pro-
ceedings of the 39th Annual ACM Symposium on Theory of
Computing, San Diego, California, USA, June 11-13, 2007.
2007, pp. 57‚Äì66.
[FW93]
Michael L Fredman and Dan E Willard. ‚ÄúSurpassing
the information theoretic bound with fusion trees‚Äù.
In: Journal of computer and system sciences 47.3 (1993),
pp. 424‚Äì436.
[GKS17]
Daniel G√ºnther, √Ågnes Kiss, and Thomas Schneider.
‚ÄúMore efficient universal circuit constructions‚Äù. In: Inter-
national Conference on the Theory and Application of Cryp-
tology and Information Security. Springer. 2017, pp. 443‚Äì
470.
[Gom+08]
Carla P Gomes et al. ‚ÄúSatisfiability solvers‚Äù. In: Founda-
tions of Artificial Intelligence 3 (2008), pp. 89‚Äì134.
[Gra05]
Judith Grabiner. The origins of Cauchy‚Äôs rigorous cal-
culus. Mineola, N.Y: Dover Publications, 2005. isbn:
9780486438153.
[Gra83]
Judith V Grabiner. ‚ÄúWho gave you the epsilon? Cauchy
and the origins of rigorous calculus‚Äù. In: The American
Mathematical Monthly 90.3 (1983), pp. 185‚Äì194.
[Hag98]
Torben Hagerup. ‚ÄúSorting and searching on the word
RAM‚Äù. In: Annual Symposium on Theoretical Aspects of
Computer Science. Springer. 1998, pp. 366‚Äì398.


--- Page 646 ---

646
introduction to theoretical computer science
[Hal60]
Paul R Halmos. Naive set theory. Republished in 2017 by
Courier Dover Publications. 1960.
[Har41]
GH Hardy. ‚ÄúA Mathematician‚Äôs Apology‚Äù. In: (1941).
[HJB85]
Michael T Heideman, Don H Johnson, and C Sidney
Burrus. ‚ÄúGauss and the history of the fast Fourier trans-
form‚Äù. In: Archive for history of exact sciences 34.3 (1985),
pp. 265‚Äì277.
[HMU14]
John Hopcroft, Rajeev Motwani, and Jeffrey Ullman.
Introduction to automata theory, languages, and compu-
tation. Harlow, Essex: Pearson Education, 2014. isbn:
1292039051.
[Hof99]
Douglas Hofstadter. G√∂del, Escher, Bach : an eternal golden
braid. New York: Basic Books, 1999. isbn: 0465026567.
[Hol01]
Jim Holt. ‚ÄúThe Ada perplex: how Byron‚Äôs daughter came
to be celebrated as a cybervisionary‚Äù. In: The New Yorker
5 (2001), pp. 88‚Äì93.
[Hol18]
Jim Holt. When Einstein walked with G√∂del : excursions to
the edge of thought. New York: Farrar, Straus and Giroux,
2018. isbn: 0374146705.
[HU69]
John E Hopcroft and Jeffrey D Ullman. ‚ÄúFormal lan-
guages and their relation to automata‚Äù. In: (1969).
[HU79]
John E. Hopcroft and Jeffrey D. Ullman. Introduction to
Automata Theory, Languages and Computation. Addison-
Wesley, 1979. isbn: 0-201-02988-X.
[HV19]
David Harvey and Joris Van Der Hoeven. ‚ÄúInteger multi-
plication in time O(n log n)‚Äù. working paper or preprint.
Mar. 2019. url: https://hal.archives-ouvertes.fr/hal-
02070778.
[Joh12]
David S Johnson. ‚ÄúA brief history of NP-completeness,
1954‚Äì2012‚Äù. In: Documenta Mathematica (2012), pp. 359‚Äì
376.
[Juk12]
Stasys Jukna. Boolean function complexity: advances and
frontiers. Vol. 27. Springer Science & Business Media,
2012.
[Kar+97]
David R. Karger et al. ‚ÄúConsistent Hashing and Random
Trees: Distributed Caching Protocols for Relieving Hot
Spots on the World Wide Web‚Äù. In: Proceedings of the
Twenty-Ninth Annual ACM Symposium on the Theory of
Computing, El Paso, Texas, USA, May 4-6, 1997. 1997,
pp. 654‚Äì663. doi: 10.1145/258533.258660. url: https:
//doi.org/10.1145/258533.258660.


--- Page 647 ---

BIBLIOGRAPHY
647
[Kar95]
ANATOLII ALEXEEVICH Karatsuba. ‚ÄúThe complexity
of computations‚Äù. In: Proceedings of the Steklov Institute of
Mathematics-Interperiodica Translation 211 (1995), pp. 169‚Äì
183.
[Kle91]
Israel Kleiner. ‚ÄúRigor and Proof in Mathematics: A
Historical Perspective‚Äù. In: Mathematics Magazine 64.5
(1991), pp. 291‚Äì314. issn: 0025570X, 19300980. url:
http://www.jstor.org/stable/2690647.
[Kle99]
Jon M Kleinberg. ‚ÄúAuthoritative sources in a hyper-
linked environment‚Äù. In: Journal of the ACM (JACM) 46.5
(1999), pp. 604‚Äì632.
[Koz97]
Dexter Kozen. Automata and computability. New York:
Springer, 1997. isbn: 978-3-642-85706-5.
[KT06]
Jon M. Kleinberg and √âva Tardos. Algorithm design.
Addison-Wesley, 2006. isbn: 978-0-321-37291-8.
[Kun18]
Jeremy Kun. A programmer‚Äôs introduction to mathematics.
Middletown, DE: CreateSpace Independent Publishing
Platform, 2018. isbn: 1727125452.
[Liv05]
Mario Livio. The equation that couldn‚Äôt be solved : how
mathematical genius discovered the language of symmetry.
New York: Simon & Schuster, 2005. isbn: 0743258207.
[LLM18]
Eric Lehman, Thomson F. Leighton, and Albert R. Meyer.
Mathematics for Computer Science. 2018.
[LMS16]
Helger Lipmaa, Payman Mohassel, and Seyed Saeed
Sadeghian. ‚ÄúValiant‚Äôs Universal Circuit: Improvements,
Implementation, and Applications.‚Äù In: IACR Cryptology
ePrint Archive 2016 (2016), p. 17.
[Lup58]
O Lupanov. ‚ÄúA circuit synthesis method‚Äù. In: Izv. Vuzov,
Radiofizika 1.1 (1958), pp. 120‚Äì130.
[Lup84]
Oleg B. Lupanov. Asymptotic complexity bounds for control
circuits. In Russian. MSU, 1984.
[Lus+08]
Michael Lustig et al. ‚ÄúCompressed sensing MRI‚Äù. In:
IEEE signal processing magazine 25.2 (2008), pp. 72‚Äì82.
[L√ºt02]
Jesper L√ºtzen. ‚ÄúBetween Rigor and Applications: De-
velopments in the Concept of Function in Mathematical
Analysis‚Äù. In: The Cambridge History of Science. Ed. by
Mary JoEditor Nye. Vol. 5. The Cambridge History of
Science. Cambridge University Press, 2002, pp. 468‚Äì487.
doi: 10.1017/CHOL9780521571999.026.


--- Page 648 ---

648
introduction to theoretical computer science
[LZ19]
Harry Lewis and Rachel Zax. Essential Discrete Mathemat-
ics for Computer Science. Princeton University Press, 2019.
isbn: 9780691190617.
[Maa85]
Wolfgang Maass. ‚ÄúCombinatorial lower bound argu-
ments for deterministic and nondeterministic Turing
machines‚Äù. In: Transactions of the American Mathematical
Society 292.2 (1985), pp. 675‚Äì693.
[Mer07]
N David Mermin. Quantum computer science: an introduc-
tion. Cambridge University Press, 2007.
[MM11]
Cristopher Moore and Stephan Mertens. The nature of
computation. Oxford University Press, 2011.
[MP43]
Warren S McCulloch and Walter Pitts. ‚ÄúA logical calcu-
lus of the ideas immanent in nervous activity‚Äù. In: The
bulletin of mathematical biophysics 5.4 (1943), pp. 115‚Äì133.
[MR95]
Rajeev Motwani and Prabhakar Raghavan. Randomized
algorithms. Cambridge university press, 1995.
[MU17]
Michael Mitzenmacher and Eli Upfal. Probability and
computing: randomization and probabilistic techniques in
algorithms and data analysis. Cambridge university press,
2017.
[Neu45]
John von Neumann. ‚ÄúFirst Draft of a Report on the ED-
VAC‚Äù. In: (1945). Reprinted in the IEEE Annals of the
History of Computing journal, 1993.
[NS05]
Noam Nisan and Shimon Schocken. The elements of com-
puting systems: building a modern computer from first princi-
ples. MIT press, 2005.
[Pag+99]
Lawrence Page et al. The PageRank citation ranking: Bring-
ing order to the web. Tech. rep. Stanford InfoLab, 1999.
[Pie02]
Benjamin Pierce. Types and programming languages. Cam-
bridge, Mass: MIT Press, 2002. isbn: 0262162091.
[Ric53]
Henry Gordon Rice. ‚ÄúClasses of recursively enumerable
sets and their decision problems‚Äù. In: Transactions of the
American Mathematical Society 74.2 (1953), pp. 358‚Äì366.
[Rog96]
Yurii Rogozhin. ‚ÄúSmall universal Turing machines‚Äù. In:
Theoretical Computer Science 168.2 (1996), pp. 215‚Äì240.
[Ros19]
Kenneth Rosen. Discrete mathematics and its applications.
New York, NY: McGraw-Hill, 2019. isbn: 125967651x.
[RS59]
Michael O Rabin and Dana Scott. ‚ÄúFinite automata and
their decision problems‚Äù. In: IBM journal of research and
development 3.2 (1959), pp. 114‚Äì125.


--- Page 649 ---

BIBLIOGRAPHY
649
[Sav98]
John E Savage. Models of computation. Vol. 136. Available
electronically at http://cs.brown.edu/people/jsavage/
book/. Addison-Wesley Reading, MA, 1998.
[Sch05]
Alexander Schrijver. ‚ÄúOn the history of combinatorial
optimization (till 1960)‚Äù. In: Handbooks in operations
research and management science 12 (2005), pp. 1‚Äì68.
[Sha38]
Claude E Shannon. ‚ÄúA symbolic analysis of relay and
switching circuits‚Äù. In: Electrical Engineering 57.12 (1938),
pp. 713‚Äì723.
[Sha79]
Adi Shamir. ‚ÄúFactoring numbers in O (logn) arith-
metic steps‚Äù. In: Information Processing Letters 8.1 (1979),
pp. 28‚Äì31.
[She03]
Saharon Shelah. ‚ÄúLogical dreams‚Äù. In: Bulletin of the
American Mathematical Society 40.2 (2003), pp. 203‚Äì228.
[She13]
Henry Maurice Sheffer. ‚ÄúA set of five independent pos-
tulates for Boolean algebras, with application to logical
constants‚Äù. In: Transactions of the American mathematical
society 14.4 (1913), pp. 481‚Äì488.
[She16]
Margot Shetterly. Hidden figures : the American dream and
the untold story of the Black women mathematicians who
helped win the space race. New York, NY: William Morrow,
2016. isbn: 9780062363602.
[Sin97]
Simon Singh. Fermat‚Äôs enigma : the quest to solve the world‚Äôs
greatest mathematical problem. New York: Walker, 1997.
isbn: 0385493622.
[Sip97]
Michael Sipser. Introduction to the theory of computation.
PWS Publishing Company, 1997. isbn: 978-0-534-94728-
6.
[Sob17]
Dava Sobel. The Glass Universe : How the Ladies of the
Harvard Observatory Took the Measure of the Stars. New
York: Penguin Books, 2017. isbn: 9780143111344.
[Sol14]
Daniel Solow. How to read and do proofs : an introduction
to mathematical thought processes. Hoboken, New Jersey:
John Wiley & Sons, Inc, 2014. isbn: 9781118164020.
[SS71]
Arnold Sch√∂nhage and Volker Strassen. ‚ÄúSchnelle mul-
tiplikation grosser zahlen‚Äù. In: Computing 7.3-4 (1971),
pp. 281‚Äì292.
[Ste87]
Dorothy Stein. Ada : a life and a legacy. Cambridge, Mass:
MIT Press, 1987. isbn: 0262691167.
[Str69]
Volker Strassen. ‚ÄúGaussian elimination is not optimal‚Äù.
In: Numerische mathematik 13.4 (1969), pp. 354‚Äì356.


--- Page 650 ---

650
introduction to theoretical computer science
[Swa02]
Doron Swade. The difference engine : Charles Babbage and
the quest to build the first computer. New York: Penguin
Books, 2002. isbn: 0142001449.
[Tho84]
Ken Thompson. ‚ÄúReflections on trusting trust‚Äù. In: Com-
munications of the ACM 27.8 (1984), pp. 761‚Äì763.
[Too63]
Andrei L Toom. ‚ÄúThe complexity of a scheme of func-
tional elements realizing the multiplication of integers‚Äù.
In: Soviet Mathematics Doklady. Vol. 3. 4. 1963, pp. 714‚Äì
716.
[Tur37]
Alan M Turing. ‚ÄúOn computable numbers, with an ap-
plication to the Entscheidungsproblem‚Äù. In: Proceedings
of the London mathematical society 2.1 (1937), pp. 230‚Äì265.
[Vad+12]
Salil P Vadhan et al. ‚ÄúPseudorandomness‚Äù. In: Founda-
tions and Trends¬Æ in Theoretical Computer Science 7.1‚Äì3
(2012), pp. 1‚Äì336.
[Val76]
Leslie G Valiant. ‚ÄúUniversal circuits (preliminary re-
port)‚Äù. In: Proceedings of the eighth annual ACM sympo-
sium on Theory of computing. ACM. 1976, pp. 196‚Äì203.
[Wan57]
Hao Wang. ‚ÄúA variant to Turing‚Äôs theory of computing
machines‚Äù. In: Journal of the ACM (JACM) 4.1 (1957),
pp. 63‚Äì92.
[Weg87]
Ingo Wegener. The complexity of Boolean functions. Vol. 1.
BG Teubner Stuttgart, 1987.
[Wer74]
PJ Werbos. ‚ÄúBeyond regression: New tools for prediction
and analysis in the behavioral sciences. Ph. D. thesis,
Harvard University, Cambridge, MA, 1974.‚Äù In: (1974).
[Wig19]
Avi Wigderson. Mathematics and Computation. Draft
available on https://www.math.ias.edu/avi/book.
Princeton University Press, 2019.
[Wil09]
Ryan Williams. ‚ÄúFinding paths of length k in ùëÇ‚àó(2ùëò)
time‚Äù. In: Information Processing Letters 109.6 (2009),
pp. 315‚Äì318.
[WN09]
Damien Woods and Turlough Neary. ‚ÄúThe complex-
ity of small universal Turing machines: A survey‚Äù. In:
Theoretical Computer Science 410.4-5 (2009), pp. 443‚Äì450.
[WR12]
Alfred North Whitehead and Bertrand Russell. Principia
mathematica. Vol. 2. University Press, 1912.
